<html><head/><body>


	
		<title>B12365_08_Final_JC_ePub</title>
		
	
	
		<div><h1>第八章:使用基于注意力的神经网络构建聊天机器人</h1>
			<p>如果你曾经看过任何未来科幻电影，你很可能会看到一个人和一个机器人说话。基于机器的智能一直是小说作品中的一个长期特征；然而，由于NLP和深度学习的最新进展，与计算机对话不再是幻想。虽然我们可能距离真正的智能还有许多年，在真正的智能中，计算机能够像人类一样理解语言的含义，但机器至少能够进行基本的对话，并给出智能的初步印象。</p>
			<p>在前一章中，我们看了如何构建序列到序列的模型来将句子从一种语言翻译成另一种语言。一个能够进行基本互动的对话聊天机器人的工作方式大致相同。当我们与聊天机器人交谈时，我们的句子成为模型的输入。输出是聊天机器人选择回复的任何内容。因此，我们不是训练我们的聊天机器人学习如何解释我们输入的句子，而是教它如何回应。</p>
			<p>我们将扩展上一章的序列对序列模型，在我们的模型中增加一些叫做<strong class="bold">注意力</strong>的东西。对序列到序列模型的这种改进意味着我们的模型学习在输入句子的哪里寻找以获得它需要的信息，而不是使用整个输入句子决策。这一改进使我们能够创建效率更高、性能一流的序列间模型。</p>
			<p>在本章中，我们将探讨以下主题:</p>
			<ul>
				<li>神经网络中的注意力理论</li>
				<li>在神经网络中实现注意力以构建聊天机器人</li>
			</ul>
			<h1><a id="_idTextAnchor140"/>技术要求</h1>
			<p>本章的所有代码都可以在https://github . com/packt publishing/Hands-On-Natural Language-Processing-with-py torch-1 . x找到。</p>
			<h1><a id="_idTextAnchor141"/>神经网络内的注意力理论</h1>
			<p>在前一章中，在我们的句子翻译的序列到序列模型中(没有注意实现)，我们同时使用了编码器和解码器。编码器从输入的句子中获得了一个<a id="_idIndexMarker405"/>隐藏状态，这是我们的句子的一个表示。然后，解码器使用这个隐藏状态来执行翻译步骤。对此的基本图解如下:</p>
			<div><div><img src="img/B12365_08_1.jpg" alt="Figure 8.1 – Graphical representation of sequence-to-sequence models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图8.1–序列间模型的图形表示</p>
			<p>然而，对整个隐藏状态进行解码不一定是使用该任务的最有效的方式。这是因为隐藏状态代表输入句子的整体；然而，在一些任务中(例如预测句子中的下一个单词)，我们不需要考虑输入句子的整体，只需要考虑与我们试图做出的预测相关的部分。我们可以通过序列间神经网络中的注意力来证明这一点。我们可以教我们的模型只查看输入的相关部分，以便进行预测，从而得到一个更高效、更准确的模型。</p>
			<p>考虑下面的例子:</p>
			<p class="author-quote">我将在3月2日去法国的首都巴黎旅行。我的航班从伦敦希思罗机场出发，大约需要一个小时。</p>
			<p>假设我们正在训练一个模型来预测句子中的下一个单词。我们可以先输入句子的开头:</p>
			<p class="author-quote">法国的首都是_____。</p>
			<p>在这种情况下，我们希望我们的模型能够检索单词<strong class="bold"> Paris </strong>。如果我们使用基本的序列对序列模型，我们会将整个输入转换成隐藏状态的<a id="_idIndexMarker406"/>，然后我们的模型会尝试从中提取相关信息。这包括所有与航班无关的信息。您可能会注意到，我们只需要查看输入句子的一小部分，就可以确定完成句子所需的相关信息:</p>
			<p class="author-quote">3月2日，我将前往法国的首都<strong class="bold">巴黎。我的航班从伦敦希思罗机场出发，大约需要一个小时。</strong></p>
			<p>因此，如果我们可以训练我们的模型只使用输入句子内的相关信息，我们就可以做出更准确和相关的预测。为了实现这一点，我们可以在我们的网络中实施<strong class="bold">注意力</strong>。</p>
			<p>我们可以实现两种主要类型的注意机制:局部和全局注意。</p>
			<h2><a id="_idTextAnchor142"/>比较局部和全局注意力</h2>
			<p>我们可以在网络中实施的两种注意力形式非常相似，但有细微的关键区别。我们将从关注本地开始。</p>
			<p>在<strong class="bold">局部注意</strong>中，我们的模型<a id="_idIndexMarker410"/>只从编码器中查看几个隐藏状态。例如，如果我们正在执行句子翻译任务，并且我们正在<a id="_idIndexMarker411"/>计算我们翻译中的第二个单词，则模型可能希望仅查看来自编码器的与输入句子中的第二个单词<a id="_idIndexMarker412"/>相关的隐藏状态。这意味着我们的模型需要<a id="_idIndexMarker413"/>查看来自编码器的第二个隐藏状态(<em class="italic"> h </em> 2)，但也可能是它之前的隐藏状态(<em class="italic"> h </em> 1)。</p>
			<p>在下图中，我们可以看到这一点:</p>
			<div><div><img src="img/B12365_08_2.jpg" alt="Figure 8.2 – Local attention model&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.2-局部注意力模型</p>
			<p>我们首先从我们的最终隐藏状态<em class="italic"> h </em> n开始计算对齐位置<em class="italic"> p </em> t，这告诉<a id="_idIndexMarker414"/>我们需要查看哪些隐藏状态来进行预测。然后，我们计算我们的局部<a id="_idIndexMarker415"/>权重，并将它们应用于我们的隐藏状态，以确定我们的上下文向量。这些权重可能会告诉我们更多地关注<a id="_idIndexMarker416"/>最相关的隐藏状态(<em class="italic"> h </em> 2)，而不是之前的隐藏状态(<em class="italic"> h </em> 1)。</p>
			<p>然后，我们获取上下文向量，并将其传递给解码器，以便进行预测。在我们的基于非注意力的序列到序列模型中，我们将仅向前传递我们的最终隐藏状态，<em class="italic"> h </em> n，但是我们在这里看到，相反，我们仅考虑我们的模型认为进行其预测所必需的相关隐藏状态。</p>
			<p>全球注意力模型以非常相似的方式工作。然而，我们不是只查看几个隐藏状态，而是希望查看模型的所有隐藏状态——因此命名为global。我们可以在这里看到一个全局注意力层的图示:</p>
			<div><div><img src="img/B12365_08_3.jpg" alt="Figure 8.3 – Global attention model&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.3-全球注意力模型</p>
			<p>我们<a id="_idIndexMarker418"/>可以在前面的图表中看到，虽然这看起来非常类似于我们的局部注意力<a id="_idIndexMarker419"/>框架，但我们的模型现在正在查看所有隐藏状态，并计算所有隐藏状态的全局权重。这允许我们的模型查看输入句子中它认为相关的任何给定部分，而不是局限于由局部注意力方法确定的局部区域。我们的模型可能希望只查看一个小的局部区域，但是这在模型的能力范围之内。一种简单的方式来思考全球注意力框架，它本质上是学习一个面具，只允许通过与我们的预测相关的隐藏状态:</p>
			<div><div><img src="img/B12365_08_4.jpg" alt="Figure 8.4 – Combined model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图8.4-组合模型</p>
			<p>我们<a id="_idIndexMarker421"/>可以在前面的图中看到，通过<a id="_idIndexMarker422"/>学习要注意哪些隐藏状态，我们的模型控制在<a id="_idIndexMarker423"/>解码步骤中使用哪些状态来确定我们的预测输出。一旦我们决定了要关注哪些隐藏状态，我们就可以使用许多不同的方法将它们组合起来——或者是串联，或者是取加权点积。</p>
			<h1><a id="_idTextAnchor143"/>使用序列对序列神经网络构建聊天机器人</h1>
			<p><a id="_idIndexMarker424"/>要准确说明如何在我们的神经网络<a id="_idIndexMarker425"/>中实现注意力，最简单的方法是通过一个例子来完成。我们现在将通过一个应用了注意力框架的序列到序列模型从头开始构建一个聊天机器人所需的步骤。</p>
			<p>与所有其他NLP模型一样，我们的第一步是获取并处理数据集<a id="_idIndexMarker427"/>来训练我们的模型。</p>
			<h2><a id="_idTextAnchor144"/>获取我们的数据集</h2>
			<p>为了训练我们的聊天机器人，我们需要一个对话数据集，通过这个数据集我们的模型可以学习如何回应。我们的聊天机器人将接受一行人类输入的内容，并用生成的句子来回应它。因此，一个理想的数据集应该由多行带有适当响应的<a id="_idIndexMarker428"/>对话组成。对于这样的任务来说，完美的数据集应该是来自两个人类用户之间对话的实际聊天日志。不幸的是，这些数据包含私人信息，很难在公共领域获得，因此对于这个任务，我们将使用电影脚本数据集。</p>
			<p>电影剧本由两个或更多角色之间的对话组成。虽然这些数据不是我们想要的格式，但是我们可以很容易地将其转换成我们需要的格式。以两个角色之间的简单对话为例:</p>
			<ul>
				<li><strong class="bold">1号线</strong>:Bethan你好。</li>
				<li>汤姆，你好吗？</li>
				<li>很好，谢谢，你今晚有什么安排？</li>
				<li>我还没有任何计划。</li>
				<li>你愿意和我一起去吃饭吗？</li>
			</ul>
			<p>现在，我们需要将它转换成调用和响应的输入和输出对，其中输入是脚本中的一行(调用)，预期的输出是脚本的下一行(响应)。我们可以将一个<em class="italic"> n </em>行的脚本转换成<em class="italic"> n-1 </em>对输入/输出:</p>
			<div><div><img src="img/B12365_08_05.jpg" alt="Figure 8.5 – Table of input and output&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.5–输入和输出表</p>
			<p>我们可以使用这些输入/输出对来训练我们的网络，其中输入是人类输入的代理，输出是我们期望从模型中得到的响应。</p>
			<p>构建模型的第一步是读入这些数据，并执行所有必要的预处理步骤。</p>
			<h2><a id="_idTextAnchor145"/>处理数据集</h2>
			<p>幸运的是，为这个例子提供的数据集已经被格式化，因此每一行<a id="_idIndexMarker430"/>代表一个输入/输出对。我们可以先读入数据并检查一些行:</p>
			<pre>corpus = "movie_corpus"
corpus_name = "movie_corpus"
datafile = os.path.join(corpus, "formatted_movie_lines.txt")
with open(datafile, 'rb') as file:
    lines = file.readlines()
    
for line in lines[:3]:
    print(str(line) + '\n')</pre>
			<p>这会打印出以下结果:</p>
			<div><div><img src="img/B12365_08_06.jpg" alt="Figure 8.6 – Examining the dataset&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.6–检查数据集</p>
			<p>你首先会注意到，我们的线条和预期的一样，因为第一行的后半部分变成了下一行的<a id="_idIndexMarker431"/>前半部分。我们还可以注意到，每一行的调用和响应部分由制表符分隔符(<code>/t</code>)分隔，并且我们的每一行都由新的行分隔符(<code>/n</code>)分隔。当我们处理数据集时，我们必须考虑这一点。</p>
			<p>第一步是创建一个词汇表或语料库，其中包含我们数据集中所有独特的单词。</p>
			<h2><a id="_idTextAnchor146"/>创造词汇</h2>
			<p>过去，我们的语料库由几个字典组成，这些字典由我们的语料库中的独特单词以及单词和索引之间的查找组成。但是，我们可以通过创建一个包含所有必需元素的词汇表类，以一种更优雅的方式做到这一点:</p>
			<ol>
				<li>我们从创建我们的<code>Vocabulary</code>类开始。我们用空字典— <code>word2index</code>和<code>word2count</code>初始化这个类。我们还用占位符初始化了<code>index2word</code>字典<a id="_idIndexMarker433"/>，用于填充标记，以及我们的<strong class="bold">句首</strong> ( <strong class="bold"> SOS </strong>)和<strong class="bold">句尾</strong> ( <strong class="bold"> EOS </strong>)标记。我们还对<a id="_idIndexMarker434"/>的词汇数量进行了连续计数(一开始是3个，因为我们的语料库已经包含了提到的三个单词)。这些是空词汇的默认值；但是，当我们在:<pre>PAD_token = 0  SOS_token = 1 EOS_token = 2 class Vocabulary:     def __init__(self, name):         self.name = name         self.trimmed = False         self.word2index = {}         self.word2count = {}         self.index2word = {PAD_token: "PAD", SOS_token:                           "SOS", EOS_token: "EOS"}         self.num_words = 3</pre>中读取数据时，它们将被填充</li>
				<li>接下来，我们创建将用于填充词汇表的函数。<code>addWord</code>将一个单词作为输入。如果这是一个不在我们的词汇表中的新单词，我们将这个单词添加到索引中，将这个单词的计数设置为1，并将词汇表中的单词总数增加1。如果这个单词已经在我们的词汇表中，我们只需将这个单词的计数增加1: <pre>def addWord(self, w):     if w not in self.word2index:         self.word2index[w] = self.num_words         self.word2count[w] = 1         self.index2word[self.num_words] = w         self.num_words += 1     else:         self.word2count[w] += 1</pre></li>
				<li>We also use the <code>addSentence</code> function to apply the <code>addWord</code> function to all the words within a given sentence:<pre>def addSentence(self, sent):
    for word in sent.split(' '):
        self.addWord(word)</pre><p>为了加快模型的训练，我们可以做的一件事就是减少我们的词汇量。这意味着任何嵌入层将会小得多，并且我们的模型内的学习参数的总数可以更少。做到这一点的一个简单方法是从我们的词汇中删除任何低频词。任何在我们的数据集中只出现一次或两次的单词都不太可能具有巨大的预测能力，因此，在我们的最终模型中，将它们从我们的语料库中删除并用空白标记替换它们可以减少我们的模型训练所需的时间，并减少过度拟合，而不会对我们的模型的预测产生太大的负面影响。</p></li>
				<li>为了从我们的词汇表中删除低频词，我们可以实现一个<code>trim</code>函数。该函数首先遍历单词计数字典，如果单词的出现次数大于所需的最小计数，则将其追加到一个新列表:<pre>def trim(self, min_cnt):     if self.trimmed:         return     self.trimmed = True     words_to_keep = []     for k, v in self.word2count.items():         if v &gt;= min_cnt:             words_to_keep.append(k)     print('Words to Keep: {} / {} = {:.2%}'.format(         len(words_to_keep), len(self.word2index),             len(words_to_keep) / len(self.word2index)))</pre></li>
				<li>最后，我们的<a id="_idIndexMarker437"/>索引从新的<code>words_to_keep</code>列表中重建。我们将所有的索引设置为它们的初始空值，然后通过使用<code>addWord</code>函数:<pre>    self.word2index = {}     self.word2count = {}     self.index2word = {PAD_token: "PAD",\                        SOS_token: "SOS",\                        EOS_token: "EOS"}     self.num_words = 3     for w in words_to_keep:         self.addWord(w)</pre>循环遍历我们保存的字来重新填充它们</li>
			</ol>
			<p>我们现在已经<a id="_idIndexMarker438"/>定义了一个词汇表类，可以很容易地用我们的输入句子填充它。接下来，我们实际上需要加载数据集来创建训练数据。</p>
			<h2><a id="_idTextAnchor147"/>加载数据</h2>
			<p>我们将使用以下步骤<a id="_idIndexMarker439"/>开始加载数据:</p>
			<ol>
				<li value="1">读入数据的第一步是执行任何必要的步骤来清理数据，使其更易于阅读。我们首先将它从Unicode转换成ASCII格式。我们可以很容易地使用一个函数来做到这一点:<pre>def unicodeToAscii(s):     return ''.join(         c for c in unicodedata.normalize('NFD', s)         if unicodedata.category(c) != 'Mn'     )</pre></li>
				<li>接下来，我们希望处理我们的输入字符串，使它们都是小写的，并且不包含任何尾随空格或标点符号，除了最基本的字符。我们可以通过使用一系列正则表达式来做到这一点:<pre>def cleanString(s):     s = unicodeToAscii(s.lower().strip())     s = re.sub(r"([.!?])", r" \1", s)     s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)     s = re.sub(r"\s+", r" ", s).strip()     return s</pre></li>
				<li>Finally, we apply this function within a wider function—<code>readVocs</code>. This function reads our data file into lines and then applies the <code>cleanString</code> function to every line. It also creates an instance of the <code>Vocabulary</code> class that we created earlier, meaning this function outputs both our data and vocabulary:<pre>def readVocs(datafile, corpus_name):
    lines = open(datafile, encoding='utf-8').\
        read().strip().split('\n')
    pairs = [[cleanString(s) for s in l.split('\t')]               for l in lines]
    voc = Vocabulary(corpus_name)
    return voc, pairs</pre><p>接下来，我们根据输入对的最大长度对其进行过滤。这也是为了减少我们模型的潜在维度。预测几百个单词长的句子需要非常深入的架构。为了节省训练时间，我们希望将这里的训练数据限制在输入和输出长度小于10个单词的情况下。</p></li>
				<li>为此，我们创建了几个过滤函数。第一个函数<code>filterPair</code>，根据当前行的输入和输出长度是否小于最大长度，返回一个布尔值。我们的第二个函数<code>filterPairs</code>，简单地将这个条件应用于我们数据集中的所有对，只保留满足这个条件的对:<pre>def filterPair(p, max_length):     return len(p[0].split(' ')) &lt; max_length and len(p[1].split(' ')) &lt; max_length def filterPairs(pairs, max_length):     return [pair for pair in pairs if filterPair(pair,             max_length)]</pre></li>
				<li>Now, we just need to create one final function that applies all the previous functions <a id="_idIndexMarker440"/>we have put together and run it to create our vocabulary and data pairs:<pre>def loadData(corpus, corpus_name, datafile, save_dir, max_length):
    voc, pairs = readVocs(datafile, corpus_name)
    print(str(len(pairs)) + " Sentence pairs")
    pairs = filterPairs(pairs,max_length)
    print(str(len(pairs))+ " Sentence pairs after           trimming")
    for p in pairs:
        voc.addSentence(p[0])
        voc.addSentence(p[1])
    print(str(voc.num_words) + " Distinct words in           vocabulary")
    return voc, pairs
max_length = 10 
voc, pairs = loadData(corpus, corpus_name, datafile,                       max_length)</pre><p>我们可以<a id="_idIndexMarker441"/>看到我们的输入数据集包含超过200，000对。当我们将其过滤为输入和输出都少于10个单词的句子时，这就减少到只有64，000对，由18，000个不同的单词组成:</p><div><img src="img/B12365_08_07.jpg" alt="Figure 8.7 – Value of sentences in the dataset&#13;"/></div><p class="figure-caption">图8.7-数据集中句子的价值</p></li>
				<li>We can print a selection of our processed input/output pairs in order to verify that our functions have all worked correctly:<pre>print("Example Pairs:")
for pair in pairs[-10:]:
    print(pair)</pre><p>产生以下<a id="_idIndexMarker442"/>输出:</p></li>
			</ol>
			<div><div><img src="img/B12365_08_08.jpg" alt="Figure 8.8 – Processed input/output pairs&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.8–已处理的输入/输出对</p>
			<p>看起来我们已经成功地将数据集分成输入和输出对，在此基础上我们可以训练我们的网络。</p>
			<p>最后，在我们开始构建模型之前，我们必须从我们的语料库和数据对中删除罕见的单词。</p>
			<h2><a id="_idTextAnchor148"/>去除生僻字</h2>
			<p>如前所述，包含在我们的数据集中只出现几次的单词将<a id="_idIndexMarker443"/>增加我们模型的维度，增加我们模型的复杂性和训练模型所需的时间。因此，最好将它们从我们的训练数据中删除，以保持我们的模型尽可能精简和高效。</p>
			<p>您可能还记得，我们在词汇表中构建了一个<code>trim</code>函数，它允许我们从词汇表中删除不经常出现的单词。我们现在可以创建一个函数来删除这些罕见的单词，并从我们的词汇表中调用<code>trim</code>方法作为我们的第一步。你会发现这从我们的词汇中删除了很大一部分单词，表明我们词汇中的大多数单词很少出现。这是意料之中的，因为任何语言模型中单词的分布都将遵循长尾分布。我们将使用以下步骤来删除单词:</p>
			<ol>
				<li value="1">We first calculate the percentage of words that we will keep within our model:<pre>def removeRareWords(voc, all_pairs, minimum):
    voc.trim(minimum)</pre><p>这会产生以下输出:</p><div><img src="img/B12365_08_09.jpg" alt="Figure 8.9 – Percentage of words to be kept&#13;"/></div><p class="figure-caption">图8.9–要保留的单词百分比</p></li>
				<li>Within this same function, we loop through all the words in the input and output sentences. If for a given pair either the input or output sentence has a word that isn't in our <a id="_idIndexMarker444"/>new trimmed corpus, we drop this pair from our dataset. We print the output and see that even though we have dropped over half of our vocabulary, we only drop around 17% of our training pairs. This again reflects how our corpus of words is distributed over our individual training pairs:<pre>pairs_to_keep = []
for p in all_pairs:
    keep = True
    for word in p[0].split(' '):
        if word not in voc.word2index:
            keep = False
            break
    for word in p[1].split(' '):
        if word not in voc.word2index:
            keep = False
            break
    if keep:
        pairs_to_keep.append(p)
print("Trimmed from {} pairs to {}, {:.2%} of total".\
       format(len(all_pairs), len(pairs_to_keep),
              len(pairs_to_keep)/ len(all_pairs)))
return pairs_to_keep
minimum_count = 3
pairs = removeRareWords(voc, pairs, minimum_count)</pre><p>这会产生以下输出:</p></li>
			</ol>
			<p class="figure-caption"><img src="img/B12365_08_10.png" alt="Figure 8.10 – Final value after building our dataset&#13;"/></p>
			<p class="figure-caption">图8.10–构建数据集后的最终值</p>
			<p>既然我们已经有了最终的数据集，我们需要构建一些函数来将数据集转换成可以传递给我们的模型的批量张量。</p>
			<h2><a id="_idTextAnchor149"/>将句子对转换成张量</h2>
			<p>我们知道，我们的模型不会将原始文本作为输入，而是将句子的张量表示作为输入。我们也不会一个接一个地处理我们的句子，而是小批量地处理<a id="_idIndexMarker446"/>。为此，我们需要将输入和输出句子都转换为张量，其中张量的宽度表示我们希望训练的批量的大小:</p>
			<ol>
				<li value="1">我们从创建几个辅助函数开始，我们可以用它们将我们的对转换成张量。我们首先创建一个<code>indexFromSentence</code>函数，它从词汇表中获取句子中每个单词的索引，并在末尾附加一个EOS标记:<pre>def indexFromSentence(voc, sentence):     return [voc.word2index[word] for word in\             sent.split(' ')] + [EOS_token]</pre></li>
				<li>其次，我们创建一个<code>zeroPad</code>函数，它用零填充所有张量，这样张量中的所有句子实际上长度相同:<pre>def zeroPad(l, fillvalue=PAD_token):     return list(itertools.zip_longest(*l,\                 fillvalue=fillvalue))</pre></li>
				<li>然后，为了<a id="_idIndexMarker447"/>生成我们的输入张量，我们应用这两个函数。首先，我们获取输入句子的索引，然后应用填充，然后将输出转换成<code>LongTensor</code>。我们也将获得每个输入句子的长度，作为张量输出:<pre>def inputVar(l, voc):     indexes_batch = [indexFromSentence(voc, sentence)\                      for sentence in l]     padList = zeroPad(indexes_batch)     padTensor = torch.LongTensor(padList)     lengths = torch.tensor([len(indexes) for indexes\                            in indexes_batch])     return padTensor, lengths</pre></li>
				<li>在我们的网络中，我们的填充令牌通常应该被忽略。我们不想在这些填充的标记上训练我们的模型，所以我们创建一个布尔掩码来忽略这些标记。为此，我们使用一个<code>getMask</code>函数，我们将它应用于输出张量。如果输出由一个单词组成，那么简单地返回<code>1</code>，如果输出由一个填充符组成，那么返回【T2:】T3</li>
				<li>然后我们将这个应用到我们的<code>outputVar</code>函数中。这与<code>inputVar</code>函数相同，只是除了索引输出张量和长度张量之外，我们还返回输出张量的布尔掩码。当输出张量中有一个单词时，这个布尔掩码只返回<code>True</code>,当<a id="_idIndexMarker448"/>是一个填充标记时，返回<code>False</code>。我们还返回输出张量中句子的最大长度:<pre>def outputVar(l, voc):     indexes_batch = [indexFromSentence(voc, sentence)                       for sentence in l]     max_target_len = max([len(indexes) for indexes in                           indexes_batch])     padList = zeroPad(indexes_batch)     mask = torch.BoolTensor(getMask(padList))     padTensor = torch.LongTensor(padList)     return padTensor, mask, max_target_len</pre></li>
				<li>最后，为了同时创建输入和输出批处理，我们遍历批处理中的对，并使用前面创建的函数为两对创建输入和输出张量。然后我们返回所有必要的变量:<pre>def batch2Train(voc, batch):     batch.sort(key=lambda x: len(x[0].split(" ")),\                reverse=True)          input_batch = []     output_batch = []          for p in batch:         input_batch.append(p[0])         output_batch.append(p[1])              inp, lengths = inputVar(input_batch, voc)     output, mask, max_target_len = outputVar(output_                                   batch, voc)          return inp, lengths, output, mask, max_target_len</pre></li>
				<li>This function should be all we need to transform our training pairs into tensors for training our model. We can validate that this is working correctly by performing <a id="_idIndexMarker449"/> a single iteration of our <code>batch2Train</code> function on a random selection of our data. We set our batch size to <code>5</code> and run this once:<pre>test_batch_size = 5
batches = batch2Train(voc, [random.choice(pairs) for _\                            in range(test_batch_size)])
input_variable, lengths, target_variable, mask, max_target_len = batches</pre><p>在这里，我们可以验证我们的输入张量已经被正确地创建。注意<a id="_idIndexMarker450"/>句子如何以填充(0个记号)结束，其中句子长度小于张量的最大长度(在本例中为9)。张量的宽度也对应于批量大小(在本例中为5):</p></li>
			</ol>
			<div><div><img src="img/B12365_08_11.jpg" alt="Figure 8.11 – Input tensor&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.11-输入张量</p>
			<p>我们还可以验证相应的输出数据和掩码。注意掩码中的<code>False</code>值如何与输出张量中的填充标记(零)重叠:</p>
			<div><div><img src="img/B12365_08_12.jpg" alt="Figure 8.12 – The target and mask tensors&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.12–目标和遮罩张量</p>
			<p>现在我们已经获得、清理和转换了我们的数据，我们准备开始训练基于注意力的模型，这将成为我们聊天机器人的基础。</p>
			<h2><a id="_idTextAnchor150"/>构建模型</h2>
			<p>我们<a id="_idIndexMarker452"/>从创建我们的编码器开始，就像我们的其他序列到序列模型一样。这将把我们输入句子的初始张量表示转换成隐藏状态。</p>
			<h3>构建编码器</h3>
			<p>我们现在将<a id="_idIndexMarker453"/>通过以下步骤创建编码器:</p>
			<ol>
				<li value="1">As with all of our PyTorch models, we start by creating an <code>Encoder</code> class that inherits from <code>nn.Module</code>. All the elements here should look familiar to the ones used in previous chapters:<pre>class EncoderRNN(nn.Module):
    def __init__(self, hidden_size, embedding,\
                 n_layers=1, dropout=0):
        super(EncoderRNN, self).__init__()
        self.n_layers = n_layers
        self.hidden_size = hidden_size
        self.embedding = embedding</pre><p>接下来，我们<a id="_idIndexMarker454"/>创建我们的<strong class="bold">递归</strong> <strong class="bold">神经</strong> <strong class="bold">网络</strong> ( <strong class="bold"> RNN </strong>)模块。在这个聊天机器人中，我们将使用一个<strong class="bold">门控循环单元</strong> ( <strong class="bold"> GRU </strong>)而不是我们之前看到的<strong class="bold">长</strong> <strong class="bold">短</strong> <strong class="bold">记忆</strong> ( <strong class="bold"> LSTM </strong>)模型。gru比LSTM稍微简单一些，因为虽然它们仍然控制着通过RNN的信息流，但是它们没有像LSTM那样有单独的遗忘和更新门。我们在这种情况下使用GRUs有几个主要原因:</p><p>a)gru已被证明计算效率更高，因为需要学习的参数更少。这意味着我们的模型用GRUs训练要比用LSTMs快得多。</p><p>b)gru已被证明在短数据序列上具有与LSTMs相似的性能水平。LSTMs在学习较长的数据序列时更有用。在这种情况下，我们只使用不超过10个单词的输入句子，所以GRUs应该会产生类似的结果。</p><p>c)gru已被证明在从小数据集学习方面比LSTMs更有效。由于相对于我们试图学习的任务的复杂性，我们的训练数据的规模较小，所以我们应该选择使用GRUs。</p></li>
				<li>We now define our GRU, taking into account the size of our input, the number of layers, and whether we should implement dropout:<pre>self.gru = nn.GRU(hidden_size, hidden_size, n_layers,
                  dropout=(0 if n_layers == 1 else\
                           dropout), bidirectional=True)</pre><p>请注意我们是如何在模型中实现双向的。你会记得在前面的章节中，双向RNN允许我们从一个句子中学习,<a id="_idIndexMarker457"/>在句子中向前移动，也可以向后移动。这使我们能够更好地捕捉句子中每个单词相对于其前后出现的单词的上下文。在我们的GRU中，双向意味着我们的编码器看起来像这样:</p><div><img src="img/B12365_08_13.jpg" alt="Figure 8.13 – Encoder layout&#13;"/></div><p class="figure-caption">图8.13–编码器布局</p><p>我们在输入语句中维护两个隐藏状态，以及每一步的输出。</p></li>
				<li>接下来，我们需要为我们的编码器创建一个前向通道。我们首先嵌入输入句子，然后在嵌入中使用<code>pack_padded_sequence</code>函数。这个函数“打包”我们的填充序列，这样我们所有的输入都具有相同的长度。然后，我们通过我们的GRU传递打包的序列，以执行正向传递:<pre>def forward(self, input_seq, input_lengths, hidden=None):     embedded = self.embedding(input_seq)     packed = nn.utils.rnn.pack_padded_sequence(embedded,                                       input_lengths)     outputs, hidden = self.gru(packed, hidden)</pre></li>
				<li>在此之后，我们打开填充并对GRU输出求和。然后我们可以返回这个<a id="_idIndexMarker459"/>相加的输出，以及我们最终的隐藏状态，以完成我们的正向传递:<pre>outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) outputs = outputs[:, :, :self.hidden_size] + a \           outputs[:, : ,self.hidden_size:] return outputs, hidden</pre></li>
			</ol>
			<p>现在，我们将在下一节创建一个注意力模块。</p>
			<h3>构建注意模块</h3>
			<p>接下来，我们<a id="_idIndexMarker460"/>需要构建我们的注意力模块，我们将把它应用于我们的编码器，这样我们就可以从编码器输出的相关部分中学习。我们将这样做:</p>
			<ol>
				<li value="1">首先为注意力模型创建一个类:<pre>class Attn(nn.Module):     def __init__(self, hidden_size):         super(Attn, self).__init__()         self.hidden_size = hidden_size</pre></li>
				<li>然后，在这个类中创建<code>dot_score</code>函数。这个函数简单地计算我们的编码器输出与我们的编码器隐藏状态输出的点积。虽然有其他方法可以将这两个张量转换成一个表示，但是使用点积是最简单的方法之一:<pre>def dot_score(self, hidden, encoder_output):     return torch.sum(hidden * encoder_output, dim=2)</pre></li>
				<li>然后我们在向前传递中使用这个函数。首先，基于<code>dot_score</code>方法计算注意力权重/能量，然后转置结果，并返回softmax转换后的概率得分:<pre>def forward(self, hidden, encoder_outputs):     attn_energies = self.dot_score(hidden, \                                    encoder_outputs)     attn_energies = attn_energies.t()     return F.softmax(attn_energies, dim=1).unsqueeze(1)</pre></li>
			</ol>
			<p>接下来，我们<a id="_idIndexMarker461"/>可以在我们的解码器中使用这个注意力模块来创建一个注意力集中的解码器。</p>
			<h3>构建解码器</h3>
			<p>我们<a id="_idIndexMarker462"/>现在将构建解码器，如下所示:</p>
			<ol>
				<li value="1">我们首先创建我们的<code>DecoderRNN</code>类，从<code>nn.Module</code>继承并定义初始化参数:<pre>class DecoderRNN(nn.Module):     def __init__(self, embedding, hidden_size, \                  output_size, n_layers=1, dropout=0.1):         super(DecoderRNN, self).__init__()         self.hidden_size = hidden_size         self.output_size = output_size         self.n_layers = n_layers         self.dropout = dropout</pre></li>
				<li>然后我们在这个模块中创建我们的层。我们将创建一个嵌入层和相应的辍学层。我们再次使用GRUs作为我们的解码器；然而，这一次，我们不需要使我们的GRU层双向，因为我们将顺序解码编码器的输出。我们还将创建两个线性图层—一个常规图层用于计算输出，另一个图层可用于连接。该层的宽度是常规隐藏层的两倍，因为它将用于两个连接的向量，每个向量的长度为<code>hidden_size</code>。我们还初始化了上一节中注意力模块的一个实例，以便能够在我们的<code>Decoder</code>类:<pre>self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers,  dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(2 * hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(hidden_size)</pre>中使用它</li>
				<li>在定义了我们所有的层之后，我们需要为解码器创建一个正向通道。注意向前传球是如何一步一步(单词)地使用的。我们从获得当前输入单词的嵌入开始，并通过GRU层向前传递以获得我们的输出和隐藏状态:<pre>def forward(self, input_step, last_hidden, encoder_outputs):     embedded = self.embedding(input_step)     embedded = self.embedding_dropout(embedded)     rnn_output, hidden = self.gru(embedded, last_hidden)</pre></li>
				<li>接下来，我们使用注意力模块从GRU输出中获得注意力权重。然后将这些权重乘以编码器输出，有效地给出我们的注意力权重和编码器输出的加权和:<pre>attn_weights = self.attn(rnn_output, encoder_outputs) context = attn_weights.bmm(encoder_outputs.transpose(0,                                                      1))</pre></li>
				<li>然后，我们将加权上下文向量与GRU的输出连接起来，并应用一个<code>tanh</code>函数来得到最终的连接输出:<pre>rnn_output = rnn_output.squeeze(0) context = context.squeeze(1) concat_input = torch.cat((rnn_output, context), 1) concat_output = torch.tanh(self.concat(concat_input))</pre></li>
				<li>对于解码器中的最后一步<a id="_idIndexMarker464"/>,我们简单地使用这个最终连接的输出来预测下一个字，并应用一个<code>softmax</code>函数。正向传递最终返回这个输出，以及最终的隐藏状态。这个前向传递将被迭代，下一个前向传递使用句子中的下一个单词和这个新的隐藏状态:<pre>output = self.out(concat_output) output = F.softmax(output, dim=1) return output, hidden</pre></li>
			</ol>
			<p>现在我们已经定义了我们的模型，我们准备定义培训过程</p>
			<h2><a id="_idTextAnchor151"/>定义培训流程</h2>
			<p>训练过程的第一步是为我们的模型定义损失的度量。由于我们的输入<a id="_idIndexMarker465"/>张量可能由填充序列组成，由于我们的输入句子都是不同长度的，我们不能简单地计算真实输出和预测输出张量之间的差异。考虑到这一点，我们将定义一个损失函数，该函数对输出应用布尔掩码，并且只计算非填充令牌的损失:</p>
			<ol>
				<li value="1">在下面的函数中，我们可以看到我们计算了整个输出张量的交叉熵损失。然而，为了得到总损失，我们只对布尔掩码选择的张量元素进行平均:<pre>def NLLMaskLoss(inp, target, mask):     TotalN = mask.sum()     CELoss = -torch.log(torch.gather(inp, 1,\                        target.view(-1, 1)).squeeze(1))     loss = CELoss.masked_select(mask).mean()     loss = loss.to(device)     return loss, TotalN.item()</pre></li>
				<li>对于我们的大部分训练，我们需要两个主要函数——一个函数<code>train()</code>，它对我们的一批训练数据执行训练；另一个函数<code>trainIters()</code>，它遍历我们的整个数据集，并对每一批数据调用<code>train()</code>。我们从定义<code>train()</code>开始，以便在单批数据上训练<a id="_idIndexMarker466"/>。创建<code>train()</code>函数，然后将渐变设置为0，定义设备选项，并初始化变量:<pre>def train(input_variable, lengths, target_variable,\           mask, max_target_len, encoder, decoder,\           embedding, encoder_optimizer,\           decoder_optimizer, batch_size, clip,\           max_length=max_length):     encoder_optimizer.zero_grad()     decoder_optimizer.zero_grad()     input_variable = input_variable.to(device)     lengths = lengths.to(device)     target_variable = target_variable.to(device)     mask = mask.to(device)     loss = 0     print_losses = []     n_totals = 0</pre></li>
				<li>然后，通过编码器执行输入和序列长度的正向传递，以获得输出和隐藏状态:<pre>encoder_outputs, encoder_hidden = encoder(input_variable, lengths)</pre></li>
				<li>Next, we create our initial decoder input, starting with SOS tokens for each sentence. We then set the initial hidden state of our decoder to be equal to that of the encoder:<pre>decoder_input = torch.LongTensor([[SOS_token for _ in \
                                   range(batch_size)]])
decoder_input = decoder_input.to(device)
decoder_hidden = encoder_hidden[:decoder.n_layers]</pre><p>接下来，我们实施教师强制。如果你还记得上一章，教师强制，当以给定的概率生成输出序列时，我们使用真实的先前输出标记而不是预测的先前输出标记来生成输出序列中的下一个单词。使用教师强制有助于我们的<a id="_idIndexMarker467"/>模型更快地收敛；然而，我们必须注意不要让教师强制比率太高，否则我们的模型将过于依赖于教师强制，将无法学习独立生成正确的输出。</p></li>
				<li>确定我们是否应该对当前步骤使用教师强制:<pre>use_TF = True if random.random() &lt; teacher_forcing_ratio else False</pre></li>
				<li>然后，如果我们确实需要实现教师强制，运行下面的代码。我们将每个序列批次通过解码器来获得输出。然后，我们将下一个输入设置为真实输出(<code>target</code>)。最后，我们使用损失函数计算并累积损失，并将其打印到控制台:<pre>for t in range(max_target_len): decoder_output, decoder_hidden = decoder(   decoder_input, decoder_hidden, encoder_outputs) decoder_input = target_variable[t].view(1, -1) mask_loss, nTotal = NLLMaskLoss(decoder_output, \      target_variable[t], mask[t]) loss += mask_loss print_losses.append(mask_loss.item() * nTotal) n_totals += nTotal</pre></li>
				<li>如果我们不在给定的批次上实现教师强制，过程几乎是相同的。然而，我们没有使用真实的输出作为序列的下一个输入，而是使用模型生成的输出:<pre>_, topi = decoder_output.topk(1) decoder_input = torch.LongTensor([[topi[i][0] for i in \                                    range(batch_size)]]) decoder_input = decoder_input.to(device)</pre></li>
				<li>最后，与我们的所有模型一样，最后的步骤是执行反向传播，实现梯度裁剪，并通过我们的编码器和解码器优化器使用梯度下降来更新权重。记住，我们裁剪掉<a id="_idIndexMarker468"/>渐变是为了防止消失/爆炸渐变的问题，这在前面的章节中已经讨论过了。最后，我们的训练步骤返回我们的平均损失:<pre>loss.backward() _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip) _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip) encoder_optimizer.step() decoder_optimizer.step() return sum(print_losses) / n_totals</pre></li>
				<li>接下来，如前所述，我们需要创建<code>trainIters()</code>函数，它在不同批次的输入数据上重复<a id="_idIndexMarker469"/>调用我们的训练函数。首先，我们使用之前创建的<code>batch2Train</code>函数:<pre>def trainIters(model_name, voc, pairs, encoder, decoder,\                encoder_optimizer, decoder_optimizer,\                embedding, encoder_n_layers, \                decoder_n_layers, save_dir, n_iteration,\                batch_size, print_every, save_every, \                clip, corpus_name, loadFilename):     training_batches = [batch2Train(voc,\                        [random.choice(pairs) for _ in\                         range(batch_size)]) for _ in\                         range(n_iteration)]</pre>将数据分成几批</li>
				<li>然后我们创建几个变量，允许我们计算迭代次数，并跟踪每个时期的总损失:<pre>print('Starting ...') start_iteration = 1 print_loss = 0 if loadFilename:     start_iteration = checkpoint['iteration'] + 1</pre></li>
				<li>接下来，我们定义我们的训练循环。对于每次迭代，我们从批次列表中获得一个训练批次。然后，我们从批处理中提取相关字段，并使用这些参数运行<a id="_idIndexMarker470"/>单次训练迭代。最后，我们将这一批的损失加到我们的总损失中:<pre>print("Beginning Training...") for iteration in range(start_iteration, n_iteration + 1):     training_batch = training_batches[iteration - 1]     input_variable, lengths, target_variable, mask, \           max_target_len = training_batch     loss = train(input_variable, lengths,\                  target_variable, mask, max_target_len,\                  encoder, decoder, embedding, \                  encoder_optimizer, decoder_optimizer,\                  batch_size, clip)     print_loss += loss</pre></li>
				<li>在每一次迭代中，我们也确保打印出我们迄今为止的进展，记录我们已经完成了多少次迭代，以及我们在每个时期的损失:<pre>if iteration % print_every == 0:     print_loss_avg = print_loss / print_every     print("Iteration: {}; Percent done: {:.1f}%;\     Mean loss: {:.4f}".format(iteration,                           iteration / n_iteration \                           * 100, print_loss_avg))     print_loss = 0</pre></li>
				<li>为了完整起见，我们还需要在每隔几个时期后保存我们的模型状态。这允许我们重新审视我们训练过的任何历史模型；例如，如果我们的模型开始过度拟合，我们可以返回到更早的迭代:<pre>if (iteration % save_every == 0):     directory = os.path.join(save_dir, model_name,\                              corpus_name, '{}-{}_{}'.\                              format(encoder_n_layers,\                              decoder_n_layers, \                              hidden_size))             if not os.path.exists(directory):                 os.makedirs(directory)             torch.save({                 'iteration': iteration,                 'en': encoder.state_dict(),                 'de': decoder.state_dict(),                 'en_opt': encoder_optimizer.state_dict(),                 'de_opt': decoder_optimizer.state_dict(),                 'loss': loss,                 'voc_dict': voc.__dict__,                 'embedding': embedding.state_dict()             }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))</pre></li>
			</ol>
			<p>现在<a id="_idIndexMarker471"/>我们已经完成了开始训练模型的所有必要步骤，我们需要创建函数来评估模型的性能。</p>
			<h2><a id="_idTextAnchor152"/>定义评估流程</h2>
			<p>评估一个<a id="_idIndexMarker472"/>聊天机器人与评估其他序列对序列模型略有不同。在我们的文本翻译任务中，一个英语句子将被直接翻译成德语。虽然可能有多种正确的翻译，但在大多数情况下，从一种语言到另一种语言只有一种正确的翻译。</p>
			<p>对于聊天机器人，有多个不同的有效输出。从与聊天机器人的对话中选取以下三行:</p>
			<p><strong class="bold">输入</strong> : <em class="italic">【你好】</em></p>
			<p><strong class="bold">输出</strong> : <em class="italic">【你好】</em></p>
			<p><strong class="bold">输入</strong> : <em class="italic">“你好”</em></p>
			<p><strong class="bold">输出</strong> : <em class="italic">“你好。你好吗？”</em></p>
			<p><strong class="bold">输入</strong>:<em class="italic">你好</em></p>
			<p><strong class="bold">输出</strong> : <em class="italic">“你想要什么？”</em></p>
			<p>这里，我们有三种不同的回答，每一种都是同样有效的回答。因此，在我们与聊天机器人对话的每个阶段，都不会有单一的“正确”回答。所以，评估要困难得多。测试聊天机器人是否产生有效输出的最直观的方法是与它对话！这意味着我们需要设置我们的聊天机器人，使我们能够与它进行对话，以确定它是否工作正常:</p>
			<ol>
				<li value="1">我们将从定义一个允许我们解码编码输入并产生文本的类开始。我们通过使用我们预先训练的编码器和解码器<pre>class GreedySearchDecoder(nn.Module):     def __init__(self, encoder, decoder):         super(GreedySearchDecoder, self).__init__()         self.encoder = encoder         self.decoder = decoder</pre>中的<code>GreedyEncoder()</code>类来实现<a id="_idIndexMarker474"/></li>
				<li>接下来，为我们的解码器定义一个正向传递。我们通过编码器传递输入，以获得编码器的输出和隐藏状态。我们将编码器的最终隐藏层作为解码器的第一个隐藏输入:<pre>def forward(self, input_seq, input_length, max_length):     encoder_outputs, encoder_hidden = \                     self.encoder(input_seq, input_length)     decoder_hidden = encoder_hidden[:decoder.n_layers]</pre></li>
				<li>然后，使用SOS标记创建解码器输入，并初始化张量以将解码后的字附加到(初始化为单个零值):<pre>decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token all_tokens = torch.zeros([0], device=device, dtype=torch.long) all_scores = torch.zeros([0], device=device)</pre></li>
				<li>After that, iterate through the sequence, decoding one word at a time. We perform <a id="_idIndexMarker475"/>a forward pass through the encoder and add a <code>max</code> function to obtain the highest-scoring predicted word and its score, which we then append to the <code>all_tokens</code> and<code> all_scores</code> variables. Finally, we take this predicted token and use it as the next input to our decoder. After the whole sequence has been iterated over, we return the complete predicted sentence:<pre>for _ in range(max_length):
    decoder_output, decoder_hidden = self.decoder\
        (decoder_input, decoder_hidden, encoder_outputs)
    decoder_scores, decoder_input = \
         torch.max (decoder_output, dim=1)
    all_tokens = torch.cat((all_tokens, decoder_input),\
                            dim=0)
    all_scores = torch.cat((all_scores, decoder_scores),\
                            dim=0)
    decoder_input = torch.unsqueeze(decoder_input, 0)
return all_tokens, all_scores</pre><p>所有的部分都开始拼凑起来了。我们已经定义了训练和评估函数，所以最后一步是编写一个函数，它实际上将我们的输入作为文本，传递给我们的模型，并从模型获得响应。这将是我们聊天机器人的“界面”，我们实际上是在这里进行对话的。</p></li>
				<li>我们首先定义一个<code>evaluate()</code>函数，它接受我们的输入函数并返回预测的输出单词。我们首先使用我们的词汇将输入的句子转换成索引。然后我们得到每个句子长度的张量，并把它转置:<pre>def evaluate(encoder, decoder, searcher, voc, sentence,\              max_length=max_length):     indices = [indexFromSentence(voc, sentence)]     lengths = torch.tensor([len(indexes) for indexes \                             in indices])     input_batch = torch.LongTensor(indices).transpose(0, 1)</pre></li>
				<li>然后，我们<a id="_idIndexMarker476"/>将我们的长度和输入张量分配给相关设备。接下来，通过搜索器(<code>GreedySearchDecoder</code>)运行输入，以获得预测输出的单词索引。最后，我们将这些单词索引转换回单词标记，然后作为函数输出返回:<pre>input_batch = input_batch.to(device) lengths = lengths.to(device) tokens, scores = searcher(input_batch, lengths, \                           max_length) decoded_words = [voc.index2word[token.item()] for \                  token in tokens] return decoded_words</pre></li>
				<li>最后，我们创建一个<code>runchatbot</code>函数，它充当我们的聊天机器人的接口。这个函数接受人工输入并打印聊天机器人的响应。我们将这个函数创建为一个<code>while</code>循环，直到我们终止这个函数或者输入<code>quit</code>作为我们的输入:<pre>def runchatbot(encoder, decoder, searcher, voc):     input_sentence = ''     while(1):         try:             input_sentence = input('&gt; ')             if input_sentence == 'quit': break</pre></li>
				<li>然后，我们获取键入的输入并将其规范化，然后将规范化的输入传递给我们的<code>evaluate()</code>函数，该函数从聊天机器人返回预测的单词:<pre>input_sentence = cleanString(input_sentence) output_words = evaluate(encoder, decoder, searcher,\                         voc, input_sentence)</pre></li>
				<li>最后，我们<a id="_idIndexMarker477"/>在打印聊天机器人的响应之前，获取这些输出单词并格式化它们，忽略EOS和填充标记。因为这是一个<code>while</code>循环，这允许我们无限期地继续与聊天机器人的对话:<pre>output_words[:] = [x for x in output_words if \                    not (x == 'EOS' or x == 'PAD')] print('Response:', ' '.join(output_words))</pre></li>
			</ol>
			<p>既然我们已经构建了训练、评估和使用聊天机器人所需的所有功能，现在是开始最后一步的时候了——训练我们的模型，并与我们训练过的聊天机器人进行对话。</p>
			<h2><a id="_idTextAnchor153"/>训练模型</h2>
			<p>由于我们<a id="_idIndexMarker478"/>已经定义了所有必要的函数，训练模型就变成了初始化我们的超参数和调用我们的训练函数的情况:</p>
			<ol>
				<li value="1">我们首先初始化超参数。虽然这些只是建议的超参数，但我们的模型已经以某种方式建立，允许它们适应传递给它们的任何超参数。用不同的超参数进行试验，看看哪些超参数会产生最佳的模型配置，这是一种很好的做法。在这里，您可以尝试增加编码器和解码器中的层数，增加或减少隐藏层的大小，或者增加批次大小。所有这些超参数都会影响模型的学习效果，以及许多其他因素，例如训练模型所需的时间:<pre>model_name = 'chatbot_model' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.15 batch_size = 64</pre></li>
				<li>之后，我们可以加载我们的检查点。如果我们先前已经训练了一个模型，我们可以从先前的迭代中<a id="_idIndexMarker479"/>加载检查点和模型状态。这使我们不必每次都重新训练我们的模型:<pre>loadFilename = None checkpoint_iter = 4000 if loadFilename:     checkpoint = torch.load(loadFilename)     encoder_sd = checkpoint['en']     decoder_sd = checkpoint['de']     encoder_optimizer_sd = checkpoint['en_opt']     decoder_optimizer_sd = checkpoint['de_opt']     embedding_sd = checkpoint['embedding']     voc.__dict__ = checkpoint['voc_dict']</pre></li>
				<li>之后，我们可以开始建立我们的模型。我们首先从词汇表中加载我们的嵌入。如果我们已经训练了一个模型，我们可以加载训练好的嵌入层:<pre>embedding = nn.Embedding(voc.num_words, hidden_size) if loadFilename:     embedding.load_state_dict(embedding_sd)</pre></li>
				<li>然后，我们对编码器和解码器做同样的事情，使用定义的超参数创建模型实例。同样，如果我们已经训练了一个模型，我们只需将训练好的模型状态加载到我们的模型中:<pre>encoder = EncoderRNN(hidden_size, embedding, \                      encoder_n_layers, dropout) decoder = DecoderRNN(embedding, hidden_size, \                       voc.num_words, decoder_n_layers,                      dropout) if loadFilename:     encoder.load_state_dict(encoder_sd)     decoder.load_state_dict(decoder_sd)</pre></li>
				<li>Last <a id="_idIndexMarker480"/>but not least, we specify a device for each of our models to be trained on. Remember, this is a crucial step if you wish to use GPU training:<pre>encoder = encoder.to(device)
decoder = decoder.to(device)
print('Models built and ready to go!')</pre><p>如果这一切都正常工作，并且您的模型创建无误，您应该会看到以下内容:</p><div><img src="img/B12365_08_14.jpg" alt="Figure 8.14 – Successful output&#13;"/></div><p class="figure-caption">图8.14-成功输出</p><p>现在我们已经创建了编码器和解码器的实例，我们准备开始训练它们。</p><p>我们首先初始化一些训练超参数。与我们的模型超参数一样，可以调整这些超参数来影响训练时间和我们的模型如何学习。剪辑控制渐变剪辑，教师强制控制我们在模型中使用教师强制的频率。请注意我们是如何使用教师强制比率1的，因此我们总是使用教师强制。降低教学强制比将意味着我们的模型需要更长的时间来收敛；然而，从长远来看，它可能会帮助我们的模型更好地自己生成正确的句子。</p></li>
				<li>我们还需要定义模型的学习率和解码器学习率。你会发现，当解码器在梯度下降过程中执行<a id="_idIndexMarker481"/>较大的参数更新时，你的模型表现更好。因此，我们引入了解码器学习率，将乘数应用于学习率，使得解码器的学习率大于编码器的学习率。我们还定义我们的模型打印和保存结果的频率，以及我们希望我们的模型运行多少个时期:<pre>save_dir = './' clip = 50.0 teacher_forcing_ratio = 1.0 learning_rate = 0.0001 decoder_learning_ratio = 5.0 epochs = 4000 print_every = 1 save_every = 500</pre></li>
				<li>接下来，像往常一样，当在PyTorch中训练模型时，我们将模型切换到训练模式，以允许参数被更新:<pre>encoder.train() decoder.train()</pre></li>
				<li>接下来，我们为编码器和解码器创建优化器。我们将这些优化器初始化为Adam优化器，但是其他优化器也同样适用。试验不同的优化器可能会产生不同级别的模型性能。如果您之前已经训练了一个模型，如果需要，您还可以加载优化器状态:<pre>print('Building optimizers ...') encoder_optimizer = optim.Adam(encoder.parameters(), \                                lr=learning_rate) decoder_optimizer = optim.Adam(decoder.parameters(),                 lr=learning_rate * decoder_learning_ratio) if loadFilename:     encoder_optimizer.load_state_dict(\                                    encoder_optimizer_sd)     decoder_optimizer.load_state_dict(\                                    decoder_optimizer_sd)</pre></li>
				<li>运行培训前的最后一步是确保CUDA被配置为如果您希望使用GPU培训时被调用。为此，我们只需遍历编码器和解码器的优化器状态，并在所有状态中启用CUDA:<pre>for state in encoder_optimizer.state.values():     for k, v in state.items():         if isinstance(v, torch.Tensor):             state[k] = v.cuda() for state in decoder_optimizer.state.values():     for k, v in state.items():         if isinstance(v, torch.Tensor):             state[k] = v.cuda()</pre></li>
				<li>Finally, we <a id="_idIndexMarker483"/>are ready to train our model. This can be done by simply calling the <code>trainIters</code> function with all the required parameters:<pre>print("Starting Training!")
trainIters(model_name, voc, pairs, encoder, decoder,\
           encoder_optimizer, decoder_optimizer, \
           embedding, encoder_n_layers, \
           decoder_n_layers, save_dir, epochs, \
            batch_size,print_every, save_every, \
            clip, corpus_name, loadFilename)</pre><p>如果工作正常，您应该看到以下输出开始打印:</p></li>
			</ol>
			<div><div><img src="img/B12365_08_15.jpg" alt="Figure 8.15 – Training the model&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.15–训练模型</p>
			<p>你的<a id="_idIndexMarker484"/>模特现在正在训练！根据多种因素，例如您将模型设定为训练多少个纪元，以及您是否使用GPU，您的模型可能需要一些时间来训练。完成后，您将看到以下输出。如果一切正常，你的模型的平均损失将比你开始训练时显著降低，表明你的模型学到了有用的东西:</p>
			<div><div><img src="img/B12365_08_16.jpg" alt="Figure 8.16 – Average loss after 4,000 iterations&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.16–4000次迭代后的平均损失</p>
			<p>现在我们的模型已经训练好了，我们可以开始评估过程，并开始使用我们的聊天机器人。</p>
			<h3>评估模型</h3>
			<h3>既然我们已经成功地创建并训练了我们的模型，现在是时候评估它的性能了。为此，我们将采取以下步骤:</h3>
			<ol>
				<li value="1">为了开始评估，我们首先将模型切换到评估模式。与所有其他PyTorch模型一样，这样做是为了防止在评估过程中发生任何进一步的参数更新:<pre>encoder.eval() decoder.eval()</pre></li>
				<li>我们还初始化了<code>GreedySearchDecoder</code>的一个实例，以便能够执行评估并以文本形式返回预测的输出:<pre>searcher = GreedySearchDecoder(encoder, decoder)</pre></li>
				<li>Finally, to run the chatbot, we simply call the <code>runchatbot</code> function, passing it <code>encoder</code>, <code>decoder</code>, <code>searcher</code>, and <code>voc</code>:<pre>runchatbot(encoder, decoder, searcher, voc)</pre><p>这样做将打开一个输入提示，让您输入文本:</p></li>
			</ol>
			<div><div><img src="img/B12365_08_17.jpg" alt="Figure 8.17 – UI element for entering text&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.17–用于输入文本的用户界面元素</p>
			<p>在这里输入<a id="_idIndexMarker486"/>你的文本并按下<em class="italic">回车</em>会将你的输入发送到聊天机器人。使用我们训练过的模型，我们的聊天机器人将创建一个响应并将其打印到控制台:</p>
			<div><div><img src="img/B12365_08_18.jpg" alt="Figure 8.18 – Output for the chatbot&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.18-聊天机器人的输出</p>
			<p>你可以多次重复这个过程，以便与聊天机器人进行“对话”。在简单的对话层面，聊天机器人可以产生令人惊讶的好结果:</p>
			<div><div><img src="img/B12365_08_19.jpg" alt="Figure 8.19 – Output for the chatbot&#13;"/>
				</div>
			</div>
			<p class="figure-caption">图8.19-聊天机器人的输出</p>
			<p>然而，一旦<a id="_idIndexMarker487"/>对话变得更加复杂，聊天机器人显然无法和人类进行同样水平的对话:</p>
			<div><div><img src="img/B12365_08_20.jpg" alt="Figure 8.20 – Limitations of the chatbot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图8.20-聊天机器人的局限性</p>
			<p>在许多情况下，您的聊天机器人的回答可能是无意义的:</p>
			<div><div><img src="img/B12365_08_21.jpg" alt="Figure 8.21 – Wrong output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图8.21-错误的输出</p>
			<p>很明显，我们已经创造了一个聊天机器人，能够进行简单的来回对话。然而，在我们的聊天机器人能够通过图灵测试，并能够让我们相信我们实际上在与人类交谈之前，我们还有很长的路要走。然而，考虑到我们用来训练模型的数据相对较少，在我们的序列对序列模型中使用注意力的<a id="_idIndexMarker488"/>已经显示出相当好的结果，证明了这些架构是多么的多才多艺。</p>
			<p>虽然最好的聊天机器人是在数十亿个数据点的庞大语料库上训练的，但我们的模型已经证明在相对较小的数据集上相当有效。然而，基本的注意力网络不再是最先进的，在我们的下一章中，我们将讨论NL <a id="_idTextAnchor154"/> P学习的一些最新发展，这些发展已经产生了更真实的聊天机器人。</p>
			<h1><a id="_idTextAnchor155"/>总结</h1>
			<p>在这一章中，我们应用了我们从递归模型和序列对序列模型中学到的所有知识，并将它们与注意力机制相结合，构建了一个完全有效的聊天机器人。虽然与我们的聊天机器人交谈不太可能与与真人交谈没有区别，但有了相当大的数据集，我们可能希望实现更真实的聊天机器人。</p>
			<p>尽管关注的序列到序列模型在2017年是最先进的，但机器学习是一个快速发展的领域，从那以后，对这些模型进行了多次改进。在最后一章中，我们将更详细地讨论这些最先进的模型，以及涵盖NLP机器学习中使用的其他几种当代技术，其中许多技术仍在开发中。</p>
		</div>
	

</body></html>