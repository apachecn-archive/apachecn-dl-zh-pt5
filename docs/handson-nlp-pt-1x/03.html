<html><head/><body>


	
		<title>B12365_02_FInal_JC_ePub</title>
		
	
	
		<div><h1><em class="italic"> <a id="_idTextAnchor029"/>第二章</em>:自然语言处理PyTorch 1.x入门</h1>
			<p><strong class="bold"> PyTorch </strong>是一个基于Python的机器学习库。它由两个主要功能组成:通过硬件加速(使用GPU)高效执行张量运算的能力，以及构建深度神经网络的能力。PyTorch还使用动态计算图而不是静态图，这使它有别于TensorFlow等类似的库。通过演示如何使用张量来表示语言，以及如何使用神经网络从NLP中学习，我们将展示这两种功能对于自然语言处理都特别有用。</p>
			<p>在这一章中，我们将向您展示如何在您的计算机上安装并运行PyTorch，以及演示它的一些关键功能。然后，我们将PyTorch与其他一些深度学习框架进行比较，然后探索PyTorch的一些NLP功能，例如它执行张量运算的能力，最后演示如何构建一个简单的神经网络。总之，本章将涵盖以下主题:</p>
			<ul>
				<li>安装PyTorch</li>
				<li>PyTorch与其他深度学习框架的比较</li>
				<li>PyTorch的NLP功能</li>
			</ul>
			<h1><a id="_idTextAnchor030"/>技术要求</h1>
			<p>对于本章，需要安装Python。建议使用最新版本的Python (3.6或更高版本)。还建议使用Anaconda包管理器来安装PyTorch。在GPU上运行张量运算需要兼容CUDA的GPU。本章的所有代码可以在<a href="https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x">https://github . com/packt publishing/Hands-On-Natural Language-Processing-with-py torch-1 . x</a>找到。</p>
			<h1><a id="_idTextAnchor031"/>安装和使用PyTorch 1.x</h1>
			<p>像大多数Python包一样，PyTorch安装起来非常简单。这样做有两种主要方式。第一个<a id="_idIndexMarker064"/>是使用<a id="_idIndexMarker065"/>命令行中的<code>pip</code>简单地安装它。只需键入以下命令:</p>
			<pre>pip install torch torchvision</pre>
			<p>虽然这种安装方法很快，但还是建议使用Anaconda进行安装，因为这包含了PyTorch运行所需的所有依赖项和二进制文件。此外，Anaconda稍后将被要求在使用CUDA的GPU上启用训练模型。通过在命令行中输入以下命令，可以通过Anaconda安装PyTorch:</p>
			<pre>conda install torch torchvision -c pytorch</pre>
			<p>为了检查PyTorch是否正常工作，我们可以打开一个Jupyter笔记本并运行几个简单的命令:</p>
			<ol>
				<li>To define a Tensor in PyTorch, we can do the following:<pre>import torch
x = torch.tensor([1.,2.])
print(x)</pre><p>这会产生以下输出:</p><p class="figure-caption"><code><img src="img/B12365_02_1.png" alt="Figure 2.1 – Tensor output&#13;&#10;"/></code></p><p class="figure-caption">图2.1–张量输出</p><p>这表明PyTorch中的张量被保存为它们自己的数据类型(与NumPy中数组的保存方式没有什么不同)。</p></li>
				<li>We can perform basic <a id="_idIndexMarker066"/>operations such as multiplication <a id="_idIndexMarker067"/>using standard Python operators:<pre>x = torch.tensor([1., 2.])
y = torch.tensor([3., 4.])
print(x * y)</pre><p>这会产生以下输出:</p><div><img src="img/B12365_02_2.jpg" alt="Figure 2.2 – Tensor multiplication output&#13;&#10;"/></div><p class="figure-caption">图2.2–张量乘法输出</p></li>
				<li>We can also select individual elements from a tensor, as follows:<pre>x = torch.tensor([[1., 2.],[5., 3.],[0., 4.]])
print(x[0][1])</pre><p>这会产生以下输出:</p></li>
			</ol>
			<div><div><img src="img/B12365_02_3.jpg" alt="Figure 2.3 – Tensor selection output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.3–张量选择输出</p>
			<p>但是，请注意，与NumPy数组不同，从张量对象<a id="_idIndexMarker069"/>中选择一个<a id="_idIndexMarker068"/>单独元素会返回另一个张量。为了从张量中返回单个值，可以使用<code>.item()</code>函数:</p>
			<pre>print(x[0][1].item())</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_4.jpg" alt="Figure 2.4 – Output of the .item() function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.4–的输出。item()函数</p>
			<h2><a id="_idTextAnchor032"/>张量</h2>
			<p>在我们继续之前，你必须完全了解张量的性质。张量有一个被称为<a id="_idIndexMarker071"/>的<a id="_idIndexMarker070"/>属性，即<strong class="bold">阶</strong>，它本质上决定了张量的维度。一阶张量是一个一维的张量，相当于一个向量或一列数。二阶张量是一个二维张量，相当于一个矩阵，而三阶张量由三维组成。PyTorch中张量的最大阶数没有限制:</p>
			<div><div><img src="img/B12365_02_5.jpg" alt="Figure 2.5 – Tensor matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.5–张量矩阵</p>
			<p>您可以通过键入以下命令来检查<a id="_idIndexMarker072"/>任何张量的大小:</p>
			<pre>x.shape</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_6.jpg" alt="Figure 2.6 – Tensor shape output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.6–张量形状输出</p>
			<p>这说明这是一个3x2张量(2阶)。</p>
			<h1><a id="_idTextAnchor033"/>使用CUDA实现PyTorch加速</h1>
			<p>PyTorch的主要优势之一是它能够通过使用<strong class="bold">图形处理单元</strong> ( <strong class="bold"> GPU </strong>)来实现加速。深度学习是一项易于并行化的计算任务，这意味着计算可以被分解为更小的<a id="_idIndexMarker073"/>任务，并在许多更小的处理器上进行计算。这个<a id="_idIndexMarker074"/>意味着不需要在单个CPU上执行任务，在GPU上执行计算更有效率。</p>
			<p>GPU最初是为了有效地渲染图形而创建的，但由于深度学习越来越受欢迎，GPU因其同时执行多种计算的能力而被频繁使用。传统的CPU可能由大约四个或八个核心组成，而GPU由数百个较小的核心组成。因为计算可以在所有这些核心上同时执行，所以GPU可以快速减少执行深度学习任务所需的时间。</p>
			<p>考虑神经网络中的单次传递。我们可以取一小批数据，通过我们的网络来获得我们的损失，然后反向传播，根据梯度调整我们的参数。如果我们在传统CPU上有许多批数据要做，我们必须等到批1完成后，才能计算批2的数据:</p>
			<div><div><img src="img/B12365_02_7.jpg" alt="Figure 2.7 – One pass in a neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.7-神经网络中的一次传递</p>
			<p>然而，在GPU上，我们可以同时执行所有这些步骤，这意味着不需要<a id="_idIndexMarker076"/>在批处理2开始之前完成批处理1。我们可以同时计算所有批次的参数更新<a id="_idIndexMarker077"/>，然后一次性执行所有参数更新(因为结果彼此独立)。并行方法可以大大加快机器学习过程:</p>
			<div><div><img src="img/B12365_02_8.jpg" alt="Figure 2.8 – Parallel approach to perform passes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.8–执行走刀的平行方法</p>
			<p><strong class="bold">计算统一设备架构</strong> ( <strong class="bold"> CUDA </strong>)是专门针对Nvidia GPUs的<a id="_idIndexMarker078"/>技术，能够在PyTorch上实现硬件加速。为了启用CUDA，我们<a id="_idIndexMarker079"/>必须首先确保我们系统上的显卡与CUDA兼容。兼容CUDA的GPU列表<a id="_idIndexMarker081"/>可以在这里找到:<a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a>。如果你有兼容CUDA的GPU，那么可以从<a id="_idIndexMarker082"/>这个链接:<a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>安装CUDA。我们将通过以下步骤激活它:</p>
			<ol>
				<li value="1">首先，为了在PyTorch上启用CUDA支持，您必须从源代码构建PyTorch。关于如何做到这一点的细节可以在这里找到:https://github.com/pytorch/pytorch#from-source.</li>
				<li>Then, to actually CUDA within our PyTorch code, we must type the following into our Python code:<pre>cuda = torch.device('cuda') </pre><p>这将我们默认的CUDA设备的名称设置为<code>'cuda'</code>。</p></li>
				<li>We can then execute operations on this device by manually specifying the device argument in any tensor operations:<pre>x = torch.tensor([5., 3.], device=cuda)</pre><p>或者，我们可以通过调用<code>cuda</code>方法来做到这一点:</p><pre>y = torch.tensor([4., 2.]).cuda()</pre></li>
				<li>We can then run a <a id="_idIndexMarker083"/>simple operation to <a id="_idIndexMarker084"/>ensure this is working correctly:<pre>x*y</pre><p>这会产生以下输出:</p></li>
			</ol>
			<div><div><img src="img/B12365_02_9.jpg" alt="Figure 2.9 – Tensor multiplication output using CUDA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.9–使用CUDA的张量乘法输出</p>
			<p>速度的变化在这个阶段不会很明显，因为我们只是创建一个张量，但当我们稍后开始大规模训练模型时，我们将看到使用CUDA并行计算的速度优势。通过并行训练我们的模型，我们将能够大大减少这一过程所需的时间。</p>
			<h1>【PyTorch与其他深度学习框架的比较</h1>
			<p>PyTorch是当今深度学习中使用的主要框架之一。也有其他广泛使用的框架<a id="_idIndexMarker085"/>可用，比如<a id="_idIndexMarker086"/>tensor flow、Theano和Caffe。虽然它们在许多方面非常相似，但在操作方式上有一些关键的不同。其中包括以下内容:</p>
			<ul>
				<li>模型是如何计算的</li>
				<li>计算图形的编译方式</li>
				<li>创建具有可变图层的动态计算图的能力</li>
				<li>语法上的差异</li>
			</ul>
			<p>可以说，PyTorch和其他框架的主要区别在于模型本身的计算方式。PyTorch使用一种叫做<strong class="bold">自动签名</strong>的自动微分<a id="_idIndexMarker087"/>方法，它允许计算图形被动态定义和执行。这与其他框架(如TensorFlow)形成对比，后者是一个静态框架。在这些静态框架中，计算图在最终被执行之前必须被定义和编译。虽然使用预编译的模型可能会导致生产中的高效实现，但是它们在研究和探索项目中不能提供相同级别的灵活性。</p>
			<p>PyTorch等框架不需要在训练模型之前预编译计算图。PyTorch使用的动态计算图形意味着图形是在执行时编译的，这允许随时定义图形。模型构造的动态方法在NLP领域特别有用。让我们考虑我们希望对其执行情感分析的两个<a id="_idIndexMarker088"/>句子:</p>
			<div><div><img src="img/B12365_02_10.jpg" alt="Figure 2.10 – Model construction in PyTorch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.10-py torch中的模型构造</p>
			<p>我们可以将这些句子中的每一个都表示为一系列单独的单词向量，然后这些单词向量将形成我们对神经网络的输入。然而，正如我们所看到的，我们的每个输入都有不同的大小。在一个<a id="_idIndexMarker090"/>固定的计算图中，这些变化的输入大小可能是一个问题，但是对于像PyTorch这样的框架，模型能够动态调整以考虑输入结构的变化。这也是PyTorch经常被首选用于NLP相关的深度学习的一个原因。</p>
			<p>PyTorch和其他深度学习框架的另一个主要区别是语法。PyTorch通常是有Python经验的开发人员的首选，因为它被认为本质上非常Python化。PyTorch与Python生态系统的其他方面集成得很好，如果你有Python的先验知识，学习起来非常容易。我们现在将通过从头开始编写我们自己的神经网络来演示PyTorch语法。</p>
			<h1><a id="_idTextAnchor035"/>在PyTorch中构建简单的神经网络</h1>
			<p>我们现在将在PyTorch中从头开始构建一个神经网络。这里，我们有一个小的<code>.csv</code>文件，其中包含了来自MNIST数据集的几个图像示例。MNIST数据集<a id="_idIndexMarker091"/>由一组0到9之间的手绘数字组成，我们试图对其进行分类。以下是来自MNIST数据集的示例，由手绘数字1组成:</p>
			<div><div><img src="img/B12365_02_11.jpg" alt="Figure 2.11 – Sample image from the MNIST dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.11-来自MNIST数据集的样本图像</p>
			<p>这些图像的大小为28x28:总共784像素。我们在<code>train.csv</code>中的数据集由1000个这样的图像组成，每个图像由784个像素值组成，以及数字的正确分类(在本例中为1)。</p>
			<h2><a id="_idTextAnchor036"/>加载数据</h2>
			<p>我们将从加载数据开始，如下所示:</p>
			<ol>
				<li value="1">First, we <a id="_idIndexMarker092"/>need to load our training dataset, as follows:<pre>train = pd.read_csv("train.csv")
train_labels = train['label'].values
train = train.drop("label",axis=1).values.reshape(len(train),1,28,28)</pre><p>请注意，我们将输入整形为(<code>1,</code> <code>1,</code> <code>28,</code> <code>28</code>)，这是一个1000幅图像的张量，每幅图像由28×28像素组成。</p></li>
				<li>接下来，我们将训练数据和训练标签转换成PyTorch张量，这样它们就可以输入神经网络:<pre>X = torch.Tensor(train.astype(float)) y = torch.Tensor(train_labels).long()</pre></li>
			</ol>
			<p>注意这两个张量的数据类型。浮点张量由32位浮点数组成，而长张量由64位整数组成。为了让PyTorch能够计算梯度，我们的<code>X</code>特性必须是浮点数，而我们的标签在这个分类模型中必须是整数(因为我们试图预测1、2、3等等的值)，所以预测1.5没有意义。</p>
			<h2><a id="_idTextAnchor037"/>构建分类器</h2>
			<p>接下来，我们可以开始<a id="_idIndexMarker093"/>构建我们实际的神经网络分类器:</p>
			<pre>class MNISTClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 392)
        self.fc2 = nn.Linear(392, 196)
        self.fc3 = nn.Linear(196, 98)
        self.fc4 = nn.Linear(98, 10)</pre>
			<p>我们构建我们的分类器，就像我们在Python中构建一个普通的类一样，继承自PyTorch中的<code>nn.Module</code>。在我们的<code>init</code>方法中，我们定义了神经网络的每一层。这里，我们定义不同大小的完全连接的线性层。</p>
			<p>我们的第一层接受<strong class="bold"> 784 </strong>个输入，因为这是我们要分类的每个图像的大小(28x28)。然后我们看到一层的输出必须与下一层的输入具有相同的值，这意味着我们的第一个全连接层输出<strong class="bold"> 392 </strong>个单元，我们的第二层将<strong class="bold"> 392 </strong>个单元<a id="_idIndexMarker094"/>作为输入。对每一层重复这一过程，每次都有一半数量的单元，直到我们到达最终完全连接的层，该层输出<strong class="bold"> 10 </strong>个单元。这是我们分类层的长度。</p>
			<p>我们的网络现在看起来像这样:</p>
			<div><div><img src="img/B12365_02_12.jpg" alt="Figure 2.12 – Our neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.12–我们的神经网络</p>
			<p>在这里，我们可以看到我们的最后一层输出<strong class="bold"> 10 </strong>个单位。这是因为我们希望预测每个图像是否是0到9之间的数字，总共有10种不同的可能分类。我们的输出是一个长度为<strong class="bold"> 10 </strong>的向量，包含图像的10个可能的<a id="_idIndexMarker095"/>值中的每一个的预测。在进行最终分类时，我们将具有最高值的数字分类作为模型的最终预测。例如，对于给定的预测，我们的模型可能预测图像为类型1的概率为10%，类型2的概率为10%，类型3的概率为80%。因此，我们将类型3作为预测，因为它是以最高概率预测的。</p>
			<h2><a id="_idTextAnchor038"/>实施辍学</h2>
			<p>在我们的<code>MNISTClassifier</code>类的<code>init</code>方法<a id="_idIndexMarker096"/>中，我们还定义了一个dropout方法来帮助调整网络:</p>
			<pre>self.dropout = nn.Dropout(p=0.2)</pre>
			<p>辍学是调整我们的神经网络以防止过度适应的一种方式。在每个训练时期，对于应用了丢失的层中的每个节点，有一个概率(这里定义为<em class="italic"> p </em> = 20%)该层中的每个节点将不被用于训练/反向传播。这意味着在训练时，我们的网络对于<a id="_idIndexMarker097"/>过拟合变得健壮，因为在训练过程的每次迭代中不会使用每个节点。这防止我们的网络变得过于依赖来自我们网络内特定节点的预测。</p>
			<h2><a id="_idTextAnchor039"/>定义正向传递</h2>
			<p>接下来，我们在分类器中定义<a id="_idIndexMarker098"/>正向传递:</p>
			<pre>    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))
        x = F.log_softmax(self.fc4(x), dim=1)</pre>
			<p>在我们的分类器中的<code>forward()</code>方法是我们应用激活函数的地方，并且定义在我们的网络中在哪里应用丢弃。我们的<code>forward</code>方法定义了我们的输入将通过网络的路径。它首先获取我们的输入，<code>x,</code>，并对其进行整形以供网络使用，将其转换为一维向量。然后，我们将它通过第一个完全连接的层，并将其封装在一个<code>ReLU</code>激活函数中，使其成为非线性的。我们还将它包装在我们的dropout中，如我们的<code>init</code>方法中所定义的。我们对网络中的所有其它层重复这一过程。</p>
			<p>对于我们的最终预测层，我们将其包装在一个log <code>softmax</code>层中。我们将使用它来轻松计算我们的<a id="_idIndexMarker099"/>损失函数，正如我们接下来将看到的。</p>
			<h2><a id="_idTextAnchor040"/>设置模型参数</h2>
			<p>接下来，我们定义<a id="_idIndexMarker100"/>模型参数:</p>
			<pre>model = MNISTClassifier()
loss_function = nn.NLLLoss()
opt = optim.Adam(model.parameters(), lr=0.001)</pre>
			<p>我们将<code>MNISTClassifier</code>类的一个实例初始化为一个模型。我们还将<a id="_idIndexMarker101"/>我们的损失定义为<strong class="bold">负对数似然损失</strong>:</p>
			<p class="author-quote">损失(y)=-对数(y)</p>
			<p>假设我们的图像是数字7。如果我们用概率1预测7类，我们的损失将是<em class="italic"> -log(1) = 0 </em>，但如果我们只用概率0.7预测7类，我们的损失将是<em class="italic"> -log(0.7) = 0.3 </em>。这意味着我们离正确的预测越远，我们的损失就越接近无穷大:</p>
			<div><div><img src="img/B12365_02_13.jpg" alt="Figure 2.13 – Representation of loss for our network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.13-我们网络的损失表示</p>
			<p>然后对数据集中所有正确的类求和，计算总损失。请注意，我们在构建分类器时定义了一个log <a id="_idIndexMarker102"/> softmax，因为它已经应用了一个softmax函数(将预测输出限制在0和1之间)并获取了日志。这意味着<em class="italic"> log(y) </em>已经计算过了，因此我们计算网络总损耗所要做的就是计算输出的负和。</p>
			<p>我们还将把我们的优化器定义为Adam优化器。一个优化器在我们的模型中控制<strong class="bold">学习速率</strong>。模型的学习率<a id="_idIndexMarker103"/>定义了在每个训练时期参数更新的幅度。学习率的大小越大，梯度下降期间参数更新的大小就越大。优化器动态地控制这个学习速率，使得当模型被初始化时，参数更新很大。然而，随着模型学习并向损失最小化的点移动，优化器控制学习速率，因此参数更新变得更小，并且可以更精确地定位局部最小值。</p>
			<h2><a id="_idTextAnchor041"/>培训我们的网络</h2>
			<p>最后，我们可以开始训练我们的网络:</p>
			<ol>
				<li value="1">首先，创建一个循环，在我们训练的每个时期运行一次。这里，我们将运行我们的训练<a id="_idIndexMarker104"/>循环50个时期。我们首先获取图像的输入张量和标签的输出张量，并将它们转换为PyTorch变量。一个<code>variable</code>是一个PyTorch对象，它包含一个<code>backward()</code>方法，我们可以用它来通过我们的网络执行反向传播:<pre>for epoch in range(50):      images = Variable(X)     labels = Variable(y)</pre></li>
				<li>接下来，我们调用优化器上的<code>zero_grad()</code>将我们计算的梯度设置为零。在PyTorch中，梯度是在每次反向传播时累积计算的。虽然这在一些模型中是有用的，例如当训练RNNs时，对于我们的例子，我们希望在每个时期后从头开始计算梯度，所以我们确保在每次通过后将梯度重置为零:<pre>opt.zero_grad()</pre></li>
				<li>接下来，我们使用模型的当前状态对数据集进行预测。这实际上是我们的前向传递，因为我们随后使用这些预测来计算我们的损失:<pre>outputs = model(images)</pre></li>
				<li>使用数据集的输出和真实标签，我们使用定义的损失函数计算模型的总损失，在这种情况下是负对数似然。在计算这个损失时，我们可以打一个<code>backward()</code>电话通过网络反向传播我们的损失。然后，我们使用优化器<a id="_idIndexMarker105"/>使用<code>step()</code>来相应地更新我们的模型参数:<pre>loss = loss_function(outputs, labels) loss.backward() opt.step()</pre></li>
				<li>最后，在每个时期完成后，我们打印总损失。我们可以观察这一点，以确保我们的模型正在学习:<pre>print ('Epoch [%d/%d] Loss: %.4f' %(epoch+1, 50,         loss.data.item()))</pre></li>
			</ol>
			<p>一般来说，我们希望在每个时期后损失会减少。我们的输出将如下所示:</p>
			<div><div><img src="img/B12365_02_14.jpg" alt="Figure 2.14 – Training epochs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.14–培训时期</p>
			<h2><a id="_idTextAnchor042"/>做预测</h2>
			<p>现在我们的模型已经被训练好了，我们可以用它来对看不见的数据进行预测。我们从<a id="_idIndexMarker106"/>读入我们的测试数据集开始(它没有用于训练我们的模型):</p>
			<pre>test = pd.read_csv("test.csv")
test_labels = test['label'].values
test = test.drop("label",axis=1).values.reshape(len(test),                  1,28,28)
X_test = torch.Tensor(test.astype(float))
y_test = torch.Tensor(test_labels).long()</pre>
			<p>这里，我们执行与加载训练数据集时相同的步骤:我们重塑测试数据并将其转换为PyTorch张量。接下来，为了使用我们训练好的模型进行预测，我们只需运行以下命令:</p>
			<pre>preds = model(X_test)</pre>
			<p>与我们在模型中向前传递训练数据时计算输出的方式相同，我们现在通过模型传递测试数据并获得预测。我们可以查看其中一幅图像的预测，如下所示:</p>
			<pre>print(preds[0])</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_15.jpg" alt="Figure 2.15 – Prediction outputs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.15–预测输出</p>
			<p>在这里，我们可以看到我们的预测是一个长度为10的向量，每个可能的类都有一个预测(数字在0到9之间)。具有最高预测值的那个是我们的模型选择作为其预测的那个。在这种情况下，它是我们矢量的第10个单位，<a id="_idIndexMarker107"/>相当于数字9。注意，由于我们之前使用了log softmax，我们的预测是对数而不是原始概率。为了将这些转换回概率，我们可以使用<em class="italic"> x </em>来转换它们。</p>
			<p>现在，我们可以构建一个包含真实测试数据标签以及模型预测标签的汇总数据框架:</p>
			<pre>_, predictionlabel = torch.max(preds.data, 1)
predictionlabel = predictionlabel.tolist()
predictionlabel = pd.Series(predictionlabel)
test_labels = pd.Series(test_labels)
pred_table = pd.concat([predictionlabel, test_labels], axis=1)
pred_table.columns =['Predicted Value', 'True Value']
display(pred_table.head())</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_16.jpg" alt="Figure 2.16 – Prediction table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.16–预测表</p>
			<p>注意<code>torch.max()</code>函数如何自动选择具有最高值的预测。我们可以看到，根据我们的一小部分数据，我们的模型似乎做出了一些不错的预测！</p>
			<h2><a id="_idTextAnchor043"/>评估我们的模型</h2>
			<p>现在我们已经从模型中获得了一些预测，我们可以使用这些预测来评估我们的模型有多好。评估模型性能的一个基本方法是准确性，如前一章所述。这里，我们简单地计算我们的正确预测(其中预测的图像标签等于实际图像标签)占我们的模型做出的预测总数的百分比:</p>
			<pre>preds = len(predictionlabel)
correct = len([1 for x,y in zip(predictionlabel, test_labels)               if x==y])
print((correct/preds)*100)</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_17.jpg" alt="Figure 2.17 – Accuracy score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.17-准确度得分</p>
			<p>恭喜你！你的第一个神经网络能够正确识别几乎90%的看不见的数字图像。随着我们的进展，我们将看到有更多复杂的模型<a id="_idIndexMarker110"/>可能会带来性能的提高。然而，就目前而言，我们已经证明了使用PyTorch创建一个简单的深度神经网络是非常简单的。这可以用几行代码来编码，并导致性能超过基本机器学习模型(如回归)的性能。</p>
			<h1><a id="_idTextAnchor044"/>py torch的NLP</h1>
			<p>既然我们已经学习了如何构建神经网络，我们将看到如何使用PyTorch为NLP构建模型。在这个例子中，我们将创建一个基本的单词包分类器，以便对给定句子的语言进行分类。</p>
			<h2><a id="_idTextAnchor045"/>设置分类器</h2>
			<p>在这个例子中，我们将选择西班牙语和英语中的句子:</p>
			<ol>
				<li value="1">First, we split each <a id="_idIndexMarker112"/>sentence into a list of words and take the language of each sentence as a label. We take a section of sentences to train our model on and keep a small section to one side as our test set. We do this so that we can evaluate the performance of our model after it has been trained:<pre>("This is my favourite chapter".lower().split(),\
 "English"),
("Estoy en la biblioteca".lower().split(), "Spanish")</pre><p>请注意，我们还将每个单词转换成小写，这可以防止单词在我们的单词包中被重复计算。如果我们有单词<code>book</code>和单词<code>Book</code>，我们希望这些被算作同一个单词，所以我们将它们转换成小写。</p></li>
				<li>Next, we build our word index, which is simply a dictionary of all the words in our corpus, and <a id="_idIndexMarker113"/>then create a unique index value for each word. This can be easily done with a short <code>for</code> loop:<pre>word_dict = {}
i = 0
for words, language in training_data + test_data:
    for word in words:
        if word not in word_dict:
            word_dict[word] = i
            i += 1
print(word_dict)</pre><p>这会产生以下输出:</p><div><img src="img/B12365_02_18.jpg" alt="Figure 2.18 – Setting up the classifier&#13;&#10;"/></div><p class="figure-caption">图2.18–设置分类器</p><p>注意，在这里，我们循环了所有的训练数据和测试数据。如果我们只是在训练数据上创建我们的单词索引，当评估我们的测试集时，我们会有原始训练中没有看到的新单词，所以我们无法为这些单词创建真正的单词包表示。</p></li>
				<li>Now, we build our classifier in a similar fashion to how we built our neural network in the previous section; that is, by building a new class that inherits from <code>nn.Module</code>.<p>在这里，我们定义了我们的分类器，使它由一个单一的线性层与log softmax激活函数近似逻辑回归。我们可以通过在这里添加额外的线性层来轻松地将此扩展为作为神经网络运行的<a id="_idIndexMarker114"/>,但是单层参数将服务于我们的目的。请密切注意我们的线性层的输入和输出大小:</p><pre>corpus_size = len(word_dict)
languages = 2
label_index = {"Spanish": 0, "English": 1}
class BagofWordsClassifier(nn.Module):  
    def __init__(self, languages, corpus_size):
        super(BagofWordsClassifier, self).__init__()
        self.linear = nn.Linear(corpus_size, languages)
    def forward(self, bow_vec):
        return F.log_softmax(self.linear(bow_vec), dim=1)</pre><p>输入的长度为<code>corpus_size</code>，这是我们的语料库中唯一单词的总数。这是因为我们的模型的每个输入将是一个单词包表示，由每个句子中的单词计数组成，如果给定的单词没有出现在我们的句子中，计数为0。我们的输出大小为2，这是我们要预测的语言数量。我们的最终预测将包括我们的句子是英语的概率与我们的句子是西班牙语的概率，我们的最终预测是具有最高概率的一个。</p></li>
				<li>接下来，我们定义一些效用函数。我们首先定义<code>make_bow_vector</code>，它获取句子并将其转换为单词包表示。我们首先创建一个全零向量。然后我们遍历它们，对于句子中的每个<a id="_idIndexMarker115"/>单词，我们将单词袋向量中的索引计数加1。最后，我们使用<code>with .view()</code>重塑这个向量，将其输入到我们的分类器:<pre>def make_bow_vector(sentence, word_index):     word_vec = torch.zeros(corpus_size)     for word in sentence:         word_vec[word_dict[word]] += 1     return word_vec.view(1, -1)</pre></li>
				<li>类似地，我们定义<code>make_target</code>，它简单地获取句子的标签(西班牙语或英语)并返回其相关索引(<code>0</code>或<code>1</code> ): <pre>def make_target(label, label_index):     return torch.LongTensor([label_index[label]])</pre></li>
				<li>我们现在可以创建模型的一个实例，为训练做准备。我们还将损失函数定义为负对数似然，因为我们正在使用log softmax函数，然后定义我们的优化器，以便使用标准的<strong class="bold">随机</strong> <strong class="bold">梯度</strong> <strong class="bold">下降</strong> ( <strong class="bold"> SGD </strong> ): <pre>model = BagofWordsClassifier(languages, corpus_size) loss_function = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=0.1)</pre></li>
			</ol>
			<p>现在，我们准备训练我们的模型。</p>
			<h2><a id="_idTextAnchor046"/>训练分类器</h2>
			<p>首先，我们建立一个循环<a id="_idIndexMarker116"/>,由我们希望模型运行的时期数组成。在这种情况下，我们将选择100个纪元。</p>
			<p>在这个循环中，我们首先将梯度置零(否则，PyTorch会累积计算梯度)，然后对于每个句子/标签对，我们分别将它们转换为单词袋向量和目标。然后，我们通过模型的当前状态向前传递我们的数据来计算这个特定句子对的预测输出。</p>
			<p>使用这个预测，我们然后使用我们预测的和实际的标签，并在这两个标签上调用我们定义的<code>loss_function</code>,以获得这个句子的损失度量。通过调用<code>backward()</code>，我们然后通过我们的模型反向传播这个损失，并且通过在我们的优化器上调用<code>step()</code>，我们更新我们的模型参数。最后，我们在每10个训练步骤后打印我们的损失:</p>
			<pre>for epoch in range(100):
    for sentence, label in training_data:
        model.zero_grad()
        bow_vec = make_bow_vector(sentence, word_dict)
        target = make_target(label, label_index)
        log_probs = model(bow_vec)
        loss = loss_function(log_probs, target)
        loss.backward()
        optimizer.step()
        
    if epoch % 10 == 0:
        print('Epoch: ',str(epoch+1),', Loss: ' +                         str(loss.item()))</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_19.jpg" alt="Figure 2.19 – Training loss&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.19–培训损失</p>
			<p>在这里，我们可以看到，随着模型的学习，我们的<a id="_idIndexMarker117"/>损失正在减少。尽管本例中的训练集非常小，但我们仍然可以证明我们的模型已经学到了一些有用的东西，如下所示:</p>
			<ol>
				<li value="1">我们用测试数据中的几个句子来评估我们的模型，而我们的模型没有被训练过。在这里，我们首先设置<code>torch.no_grad()</code>，它关闭了<code>autograd</code>引擎，因为不再需要计算梯度，因为我们不再训练我们的模型。接下来，我们将测试句子转换成一个单词袋向量，并将其输入到我们的模型中以获得预测。</li>
				<li>We then simply print the sentence, the true label of the sentence, and then the predicted probabilities. Note that we transform the predicted values from log probabilities back into probabilities. We obtain two probabilities for each prediction, but if <a id="_idIndexMarker118"/>we refer back to the label index, we can see that the first probability (index 0) corresponds to Spanish, whereas the other one corresponds to English:<pre>def make_predictions(data):
    with torch.no_grad():
        sentence = data[0]
        label = data[1]
        bow_vec = make_bow_vector(sentence, word_dict)
        log_probs = model(bow_vec)
        print(sentence)
        print(label + ':')
        print(np.exp(log_probs))
        
make_predictions(test_data[0])
make_predictions(test_data[1])</pre><p>这会产生以下输出:</p><div><img src="img/B12365_02_20.jpg" alt="Figure 2.20 – Predicted output&#13;&#10;"/></div><p class="figure-caption">图2.20-预测输出</p><p>在这里，我们可以看到，对于我们的两个预测，我们的模型都预测到了正确的答案，但这是为什么呢？我们的模型到底学到了什么？我们可以看到我们的第一个测试句子包含单词<code>estoy</code>，这个单词之前在我们的训练集中的一个西班牙句子中出现过。类似地，我们可以看到单词<code>book</code>出现在我们的英语句子训练集中。由于我们的模型<a id="_idIndexMarker119"/>由一个单层组成，每个节点上的参数都很容易解释。</p></li>
				<li> Here, we define a function that takes a word as input and returns the weights on each of the parameters within the layer. For a given word, we get the index of this word from our dictionary and then select these parameters from the same index within the model. Note that our model returns two parameters as we are making two predictions; that is, the model's contribution to the Spanish prediction and the model's contribution to the English prediction:<pre>def return_params(word): 
    index = word_dict[word]
    for p in model.parameters():
        dims = len(p.size())
        if dims == 2:
            print(word + ':')
            print('Spanish Parameter = ' +                    str(p[0][index].item()))
            print('English Parameter = ' +                    str(p[1][index].item()))
            print('\n')
            
return_params('estoy')
return_params('book')</pre><p>这会产生以下输出:</p></li>
			</ol>
			<div><div><img src="img/B12365_02_21.jpg" alt="Figure 2.21 – Predicted output for the updated function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.21-更新函数的预测输出</p>
			<p>在这里，我们可以看到对于<a id="_idIndexMarker120"/>的单词<code>estoy</code>，这个参数对于西班牙语的预测是正的，对于英语的预测是负的。这意味着在我们的句子中，对于单词“<code>estoy</code>”的每一次计数，该句子更有可能是西班牙语句子。类似地，对于单词<code>book</code>，我们可以看到这有助于预测该句子是英语。</p>
			<p>我们可以证明我们的模型只是根据它所接受的训练来学习。如果我们试图预测一个模型没有训练过的单词，我们可以看到它无法做出准确的决定。在<a id="_idIndexMarker121"/>这种情况下，我们的模型认为英语单词"<code>not"</code>是西班牙语:</p>
			<pre>new_sentence = (["not"],"English")
make_predictions(new_sentence)</pre>
			<p>这会产生以下输出:</p>
			<div><div><img src="img/B12365_02_22.jpg" alt="Figure 2.22 – Final output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图2.22-<a id="_idTextAnchor047"/>最终输出</p>
			<h1><a id="_idTextAnchor048"/>总结</h1>
			<p>在这一章中，我们介绍了PyTorch和它的一些关键特性。希望你现在能更好地理解PyTorch与其他深度学习框架的不同之处，以及如何用它来构建基本的神经网络。虽然这些简单的例子只是冰山一角，但我们已经说明PyTorch是一个非常强大的NLP分析和学习工具。</p>
			<p>在未来的章节中，我们将展示PyTorch的独特属性如何用于构建高度复杂的模型来解决非常复杂的机器学习任务。</p>
		</div>
	

</body></html>