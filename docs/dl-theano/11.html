<html><head/><body>
<html>
  <head>
    <title>Chapter 11. Learning from the Environment with Reinforcement</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div/><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>第十一章。从强化环境中学习</h1></div></div></div><p class="calibre8">监督和非监督学习描述了训练期间标签或目标的存在或不存在。对于代理来说，更自然的学习环境是在做出正确决策时获得奖励。这种奖励，比如<em class="calibre12">正确地打了网球</em>之类的，可能归因于一个复杂的环境，是多种行动的结果，延迟的或累积的。</p><p class="calibre8">为了<a id="id401" class="calibre1"/>优化人工智能体从环境中获得的回报，<strong class="calibre2">强化学习</strong> ( <strong class="calibre2"> RL </strong>)领域出现了许多算法，如Q-learning，或蒙特卡罗树搜索，随着深度学习的出现，这些算法被修订为新的方法，如深度Q-网络，策略网络，价值网络和策略梯度。</p><p class="calibre8">我们将首先介绍强化学习框架及其在虚拟环境中的潜在应用。然后，我们将开发它的算法以及它们与深度学习的集成，深度学习已经解决了人工智能中最具挑战性的问题。</p><p class="calibre8">本章涵盖的要点如下:</p><div><ul class="itemizedlist"><li class="listitem">强化学习</li><li class="listitem">模拟环境</li><li class="listitem">q学习</li><li class="listitem">蒙特卡罗树搜索</li><li class="listitem">深度Q-网络</li><li class="listitem">政策梯度</li><li class="listitem">异步梯度下降</li></ul></div><p class="calibre8">为了简化本章中神经网络的开发，我们将使用Keras，这是一个高级深度学习库，位于我在<a class="calibre1" title="Chapter 5. Analyzing Sentiment with a Bidirectional LSTM" href="part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b">第5章</a>、<em class="calibre12">使用双向LSTM分析情感</em>中介绍的基础之上。</p></div></body></html>


<html>
  <head>
    <title>Chapter 11. Learning from the Environment with Reinforcement</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec96" class="calibre1"/>强化学习任务</h1></div></div></div><p class="calibre8">强化<a id="id402" class="calibre1"/>学习包括训练一个<strong class="calibre2">代理</strong>，它只是需要来自<strong class="calibre2">环境</strong>的偶尔反馈，以学习最终获得最佳反馈。代理执行<strong class="calibre2">动作</strong>，修改环境的<strong class="calibre2">状态</strong>。</p><p class="calibre8">在环境中导航的操作可以表示为从一个状态到另一个状态的有向边，如下图所示:</p><div><img src="img/00201.jpeg" alt="Reinforcement learning tasks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">在真实环境(行走机器人、电机控制等)或虚拟环境(视频游戏、在线游戏、聊天室等)中工作的机器人必须决定哪些动作(或击键)可以获得最大奖励:</p><div><img src="img/00202.jpeg" alt="Reinforcement learning tasks" class="calibre9"/></div><p class="calibre10"> </p></div></div></body></html>


<html>
  <head>
    <title>Simulation environments</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec97" class="calibre1"/>模拟环境</h1></div></div></div><p class="calibre8">虚拟环境使得模拟成千上万的游戏成为可能，除了计算之外没有任何其他成本。为了对不同的强化学习算法进行基准测试，研究团体已经开发了模拟环境。</p><p class="calibre8">为了找到通用的解决方案，与商业巨头埃隆·马斯克(Elon Musk)合作的开放式人工智能非营利人工智能研究公司(旨在以造福全人类的方式谨慎地促进和发展友好的人工智能)已经将<a id="id404" class="calibre1"/>聚集在其开源模拟环境<strong class="calibre2">开放式人工智能体育馆</strong>(<a class="calibre1" href="https://gym.openai.com/">https://gym.openai.com/</a>)，一个Python工具包中的强化<a id="id405" class="calibre1"/>任务和环境的集合，以测试我们自己的方法。在这些环境中，您会发现:</p><div><ul class="itemizedlist"><li class="listitem">Video games from Atari 2600, a home video game console released by Atari Inc in 1977, wrapping the simulator from the Arcade Learning Environment, one of the most common RL benchmark environment:<div><img src="img/00203.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">MuJoCo, a physics simulator for evaluating agents on continuous control tasks:<div><img src="img/00204.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">Other <a id="id406" class="calibre1"/>well-known games such as Minecraft, Soccer, Doom, and many others:<div><img src="img/00205.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">让我们安装Gym及其Atari 2600环境:</p><div><pre class="programlisting">
<strong class="calibre2">pip</strong> install gym
<strong class="calibre2">pip</strong> install gym[atari]</pre></div><p class="calibre8">也可以在所有环境中安装:</p><div><pre class="programlisting">
<strong class="calibre2">pip</strong> install gym[all]</pre></div><p class="calibre8">使用<code class="email">env.step()</code>方法与健身房环境交互非常简单，给定我们为代理选择的动作，返回新的状态、奖励以及游戏是否已经终止。</p><p class="calibre8">例如，让我们对一个随机动作进行采样:</p><div><pre class="programlisting">
<strong class="calibre2">import</strong> gym

env = gym.make(<strong class="calibre2">'CartPole-v0'</strong>)
env.reset()

<strong class="calibre2">for</strong> _ <strong class="calibre2">in</strong> range(1000):
    env.render()
    action = env.action_space.sample()
    next_state, reward, done, info = env.step(action)
    <strong class="calibre2">if</strong> done:
        env.reset()</pre></div><p class="calibre8">Gym还<a id="id407" class="calibre1"/>提供复杂的监控方法，以记录视频和算法性能。记录可以上传到Open-AI API，用其他算法打分。</p><p class="calibre8">人们也可以看看:</p><div><ul class="itemizedlist"><li class="listitem">3D car <a id="id408" class="calibre1"/>racing simulator Torcs (<a class="calibre1" href="http://torcs.sourceforge.net/">http://torcs.sourceforge.net/</a>), which is more realistic with smaller discretization of actions, but with less sparse rewards than simple Atari games, and also fewer possible actions than continuous motor control in MuJoCo:<div><img src="img/00206.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">3D environment called Labyrinth for randomly generated mazes:<div><img src="img/00207.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div></div></body></html>


<html>
  <head>
    <title>Q-learning</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec98" class="calibre1"/>Q-学习</h1></div></div></div><p class="calibre8">解决游戏问题的一个主要方法是Q学习法。为了充分理解该方法，一个基本的例子将说明一个简单的情况，其中环境的状态数被限制为6，状态<strong class="calibre2"> 0 </strong>是入口，状态<strong class="calibre2"> 5 </strong>是出口。在每个阶段，一些操作可以跳转到另一个状态，如下图所示:</p><div><img src="img/00208.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">比方说，当代理从状态<strong class="calibre2"> 4 </strong>到状态<strong class="calibre2"> 5 </strong>时，奖励是100。因为这个例子中游戏的目标是找到出口，所以对其他状态没有任何奖励。<a id="id410" class="calibre1"/>奖励是延时的，代理必须从状态0到状态4滚动多个状态才能找到出口。</p><p class="calibre8">在这种情况下，Q学习包括学习矩阵Q，表示状态-动作对的<strong class="calibre2">值:</strong></p><div><ul class="itemizedlist"><li class="listitem">Q矩阵中的每一行对应于代理将处于的状态</li><li class="listitem">每一列从该状态到目标状态</li></ul></div><p class="calibre8">这个值代表在那种状态下选择那个动作会让我们离出口有多近。如果从状态<em class="calibre12"> i </em>到状态<em class="calibre12"> j </em>没有任何动作，我们在Q矩阵中的位置<em class="calibre12"> (i，j) </em>定义一个零值或负值。如果从状态<em class="calibre12"> i </em>到状态<em class="calibre12"> j </em>有一个或多个可能的动作，那么Q矩阵中的值将被选择来表示状态<em class="calibre12"> j </em>将如何帮助我们实现我们的目标。</p><p class="calibre8">例如，将状态<strong class="calibre2"> 3 </strong>设为状态<strong class="calibre2"> 0 </strong>，将使代理远离出口，而将状态<strong class="calibre2"> 3 </strong>设为状态<strong class="calibre2"> 4 </strong>将使我们更接近目标。一种通常使用的算法，称为<em class="calibre12">贪婪</em>算法，用于估计离散空间中的<strong class="calibre2"> Q </strong>，由递归<em class="calibre12">贝尔曼方程</em>给出，其被证明收敛:</p><div><img src="img/00209.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<em class="calibre12"> S' </em>是对状态<em class="calibre12"> S </em>采取动作<em class="calibre12"> a </em>时的新状态；<em class="calibre12"> r </em>定义了从状态<em class="calibre12"> S </em>到<em class="calibre12">S’</em>的路径上的奖励(在这种情况下为空)，并且<img src="img/00210.jpeg" alt="Q-learning" class="calibre23"/>是折扣因子，以阻止对图中太远的状态的动作。多次应用该等式将导致<a id="id411" class="calibre1"/>产生以下Q值:</p><div><img src="img/00211.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">在Q-learning中，<em class="calibre12"> Q </em>代表<em class="calibre12">品质</em>代表行动获得最佳回报的力量。由于后期奖励是贴现的，这些值对应于每对(状态、行动)夫妇的<strong class="calibre2">最大贴现未来奖励</strong>。</p><p class="calibre8">注意<a id="id412" class="calibre1"/>一旦我们知道搜索子树的输出节点的<strong class="calibre2">状态值</strong>，就不需要完整的图形结果:</p><div><img src="img/00212.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">该图中，节点<strong class="calibre2"> 1 </strong>和<strong class="calibre2"> 3 </strong>的值<strong class="calibre2"> 10 </strong>为<strong class="calibre2">最优状态值函数v(s)；</strong>即完美玩法/最优路径下一局游戏的<a id="id413" class="calibre1"/>胜负。实际上，精确的值函数是未知的，而是近似的。</p><p class="calibre8">这种近似与<strong class="calibre2"> DeepMind </strong>算法<strong class="calibre2"> AlphaGo </strong>中的<strong class="calibre2">蒙特卡罗树搜索</strong> ( <strong class="calibre2"> MCTS </strong>)结合使用，击败围棋世界冠军。MCTS由给定策略的采样动作<a id="id414" class="calibre1"/>组成，因此<a id="id415" class="calibre1"/>只有从当前节点到<a id="id416" class="calibre1"/>估计<a id="id417" class="calibre1"/>其Q值的最可能的动作被保留在贝尔曼方程中:</p><div><img src="img/00213.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p></div></body></html>


<html>
  <head>
    <title>Deep Q-network</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec99" class="calibre1"/>深度Q-网络</h1></div></div></div><p class="calibre8">虽然<a id="id418" class="calibre1"/>可能动作的数量通常是有限的(键盘按键或动作的数量)，但可能状态的数量可能是巨大的，搜索空间可能是巨大的，例如，在现实世界环境或真实视频游戏中，机器人配备有摄像头。使用计算机视觉神经网络变得很自然，例如我们在第7章<em class="calibre12">使用残差网络</em>分类图像中用于分类的神经网络，来表示给定输入图像(状态)的动作值，而不是矩阵:</p><div><img src="img/00214.jpeg" alt="Deep Q-network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Q网络<a id="id419" class="calibre1"/>被称为<strong class="calibre2">状态-动作值网络</strong>并预测给定状态的动作值。为了训练Q网络，<a id="id420" class="calibre1"/>的一个自然方法是通过梯度下降使其符合贝尔曼方程:</p><div><img src="img/00215.jpeg" alt="Deep Q-network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">注意，<img src="img/00216.jpeg" alt="Deep Q-network" class="calibre23"/>被评估和固定，而下降是针对<img src="img/00217.jpeg" alt="Deep Q-network" class="calibre23"/>中的导数计算的，并且每个状态的值可以被估计为所有状态-动作值的最大值。</p><p class="calibre8">在用随机权重初始化Q-网络之后，初始预测是随机的，但是随着网络收敛，给定特定状态的动作将变得越来越可预测，因此对新状态的探索下降。利用在线训练的模型需要迫使算法<strong class="calibre2">继续探索</strong>:贪婪方法<img src="img/00218.jpeg" alt="Deep Q-network" class="calibre23"/><strong class="calibre2"/>包括<a id="id421" class="calibre1"/>以概率ε进行随机动作，否则遵循Q网络给出的最大回报动作。这是一种通过试错来学习的方法。经过一定数量的历元后，<img src="img/00218.jpeg" alt="Deep Q-network" class="calibre23"/>被衰变以减少探索。</p></div></body></html>


<html>
  <head>
    <title>Training stability</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec100" class="calibre1"/>训练稳定性</h1></div></div></div><p class="calibre8">不同的<a id="id423" class="calibre1"/>方法可以提高训练期间的稳定性。<strong class="calibre2">在线训练</strong>，即一边玩游戏一边训练模型，忘记之前的<a id="id424" class="calibre1"/>经验，只考虑最后一个，用深度神经网络是根本不稳定的:时间上接近的状态，比如最近的状态，通常是强相似或相关的，训练时取最近的状态收敛不好。</p><p class="calibre8">为了避免<a id="id425" class="calibre1"/>这样的失败，一个可能的解决方案是将体验存储在<strong class="calibre2">重放存储器</strong>中，或者使用人类游戏数据库。从重放存储器或人类游戏数据库中分批和洗牌随机样本导致<a id="id426" class="calibre1"/>更稳定的训练，但是<strong class="calibre2">偏离策略</strong>训练。</p><p class="calibre8">提高稳定性的第二种解决方案是针对数千次的<img src="img/00217.jpeg" alt="Training stability" class="calibre23"/>更新，固定<strong class="calibre2">目标评估</strong> <img src="img/00216.jpeg" alt="Training stability" class="calibre23"/>中参数<img src="img/00219.jpeg" alt="Training stability" class="calibre23"/>的值，减少目标和Q值之间的相关性:</p><div><img src="img/00220.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">通过n步Q-learning可以更有效地进行培训，在<em class="calibre12"> n </em>个之前的行动中传播奖励，而不是一个:</p><p class="calibre8">q学习公式:</p><div><img src="img/00221.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">n步Q学习公式:</p><div><img src="img/00222.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">在这里，每个<a id="id427" class="calibre1"/>步骤都将从<em class="calibre12"> n个</em>下一个奖励中获益:</p><div><img src="img/00223.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">用于训练稳定性和效率的最后一个<a id="id428" class="calibre1"/>解决方案是<strong class="calibre2">异步梯度下降</strong>，其中多个代理在环境的多个实例上并行执行，具有不同的探索策略，使得每个梯度更新更少相关:每个学习代理在同一台机器上的不同线程中运行，与其他代理共享其模型和目标模型参数，但是为环境的不同部分计算梯度。并行actor学习器具有稳定效果，支持策略强化，减少训练时间，以及在GPU或多核CPU上的可比性能，这非常棒！</p><p class="calibre8">稳定化效果导致更好的<strong class="calibre2">数据效率</strong>:数据效率由收敛到期望的训练损失或精度所需的历元数(一个历元是当全部训练数据集已经呈现给算法时)来测量。总训练时间受数据效率、并行性(线程或机器的数量)和并行性开销(在给定内核、机器数量和算法分发效率的情况下，它与线程数量呈次线性关系)的影响。</p><p class="calibre8">让我们看看实践中的情况。为了实现多个代理探索环境的不同部分，我们将使用Python多处理模块运行多个进程，一个进程用于模型更新(GPU)，而<em class="calibre12"> n </em>进程用于代理探索(CPU)。多处理模块的管理器对象控制一个服务器进程，该进程持有在进程之间共享的Q网络的权重。存储代理的经验并为它们提供一次模型更新的通信通道是通过一个进程安全队列实现的:</p><div><pre class="programlisting">
<strong class="calibre2">from</strong> multiprocessing <strong class="calibre2">import</strong> *
manager = Manager()
weight_dict = manager.dict()
mem_queue = manager.Queue(args.queue_size)

pool = Pool(args.processes + 1, init_worker)

<strong class="calibre2">for</strong> i <strong class="calibre2">in</strong> range(args.processes):
    pool.apply_async(generate_experience_proc, (mem_queue, weight_dict, i))

pool.apply_async(learn_proc, (mem_queue, weight_dict))

pool.close()
pool.join()</pre></div><p class="calibre8">现在，让我们生成体验并将它们放入公共队列对象中。</p><p class="calibre8">为此，每个代理创建其游戏环境，编译Q-网络并从管理器加载权重:</p><div><pre class="programlisting">env = gym.make(args.game)

load_net = build_networks(observation_shape, env.action_space.n)

load_net.compile(<strong class="calibre2">optimizer</strong>='rmsprop', <strong class="calibre2">loss</strong>='mse', <strong class="calibre2">loss_weights</strong>=[0.5, 1.])

<strong class="calibre2">while</strong> 'weights' <strong class="calibre2">not in</strong> weight_dict:
    time.sleep(0.1)
load_net.set_weights(weight_dict['weights'])</pre></div><p class="calibre8">为了产生一种体验，代理选择一个动作并在其环境中执行它:</p><div><pre class="programlisting">observation, reward, done, _ = env.step(action)</pre></div><p class="calibre8">代理的每个<a id="id430" class="calibre1"/>经验都存储在一个列表中，直到游戏终止或者列表长于<em class="calibre12"> n_step </em>，用<em class="calibre12"> n-steps </em> Q-learning评估状态-动作值:</p><div><pre class="programlisting">
<strong class="calibre2">if</strong> done or counter <strong class="calibre2">&gt;=</strong> args.n_step:
    r = 0.
    <strong class="calibre2">if not</strong> done:
        r = value_net.predict(observations[None, ...])[0]
    <strong class="calibre2">for</strong> i <strong class="calibre2">in</strong> range(counter):
        r = n_step_rewards[i] + discount * r
        mem_queue.put((n_step_observations[i], n_step_actions[i], r))</pre></div><p class="calibre8">偶尔，代理会根据学习过程更新其权重:</p><div><pre class="programlisting">load_net.set_weights(weight_dict['weights'])</pre></div><p class="calibre8">现在让我们看看如何在学习代理中更新权重。</p></div></body></html>


<html>
  <head>
    <title>Policy gradients with REINFORCE algorithms</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec101" class="calibre1"/>带有增强算法的策略梯度</h1></div></div></div><p class="calibre8"><strong class="calibre2">策略梯度</strong> ( <strong class="calibre2"> PG </strong> ) /强化算法的思想<a id="id431" class="calibre1"/>非常简单:它在于在强化学习任务的情况<a id="id432" class="calibre1"/>中重新使用分类损失函数。</p><p class="calibre8">请记住，分类损失是由负对数似然性给出的，使用梯度下降将其最小化遵循关于网络权重的负对数似然性导数:</p><div><img src="img/00224.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<em class="calibre12"> y </em>是选择动作，<img src="img/00225.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/>是给定输入X和权重<img src="img/00219.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/>时该动作的预测概率。</p><p class="calibre8">强化定理引入了强化学习的等价形式，其中<em class="calibre12"> r </em>是奖励。以下导数:</p><div><img src="img/00226.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">代表相对于<a id="id433" class="calibre1"/>网络权重的预期回报导数的无偏估计:</p><div><img src="img/00227.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">所以，跟随衍生品会鼓励代理人最大化回报。</p><p class="calibre8">这样的梯度下降使我们能够为我们的代理优化一个<strong class="calibre2">策略网络</strong>:一个策略<img src="img/00228.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/>是合法行为的概率分布，在<a id="id434" class="calibre1"/>在线学习期间对要执行的行为进行采样，也可以用一个参数化的神经网络来近似。</p><p class="calibre8">这在连续的情况下特别有用，例如对于电机控制，其中动作空间的离散化可能导致一些次优的假象，并且在无限动作空间下在动作值网络Q上的最大化是不可能的。</p><p class="calibre8">此外，还可以增强策略网络的重现性(LSTM，GRU)，这样代理就可以针对多个先前的状态选择它的动作。</p><p class="calibre8">强化定理给出了优化参数化策略网络的梯度下降法。为了鼓励在这种基于策略的情况下进行探索，还可以向损失函数添加正则化项，即策略的熵。</p><p class="calibre8">在该策略下，可以计算每个状态<img src="img/00229.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/>的值:</p><div><ul class="itemizedlist"><li class="listitem">要么从该州的政策出发进行博弈</li><li class="listitem">Or, if parameterized into a <strong class="calibre2">state value network</strong>, by gradient descent, the current <a id="id435" class="calibre1"/>parameter serving as target, as for the state-action value network seen in the previous section with discounted rewards:<div><img src="img/00230.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">通常选择这个<a id="id436" class="calibre1"/>值作为强化基线<em class="calibre12"> b </em>以减少政策梯度估计的方差，Q值可以作为预期报酬:</p><div><img src="img/00231.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">增强导数中的第一个因素:</p><div><img src="img/00232.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">在状态 <em class="calibre12"> s </em>中被称为行动a的<strong class="calibre2">优势。</strong></p><p class="calibre8">策略网络和价值网络的梯度下降都可以通过我们的并行参与者学习器异步执行。</p><p class="calibre8">让我们在Keras中创建我们的政策网络和州价值网络，共享它们的第一层:</p><div><pre class="programlisting">
<strong class="calibre2">from</strong> keras.models <strong class="calibre2">import</strong> Model
<strong class="calibre2">from</strong> keras.layers <strong class="calibre2">import</strong> Input, Conv2D, Flatten, Dense

<strong class="calibre2">def </strong>build_networks(input_shape, output_shape):
    state = Input(<strong class="calibre2">shape</strong>=input_shape)
    h = Conv2D(16, (8, 8) , <strong class="calibre2">strides</strong>=(4, 4), <strong class="calibre2">activation</strong>='relu', <strong class="calibre2">data_format</strong>="channels_first")(state)
    h = Conv2D(32, (4, 4) , <strong class="calibre2">strides</strong>=(2, 2), <strong class="calibre2">activation</strong>='relu', <strong class="calibre2">data_format</strong>="channels_first")(h)
    h = Flatten()(h)
    h = Dense(256, <strong class="calibre2">activation</strong>='relu')(h)

    value = Dense(1, <strong class="calibre2">activation</strong>='linear', <strong class="calibre2">name</strong>='value')(h)
    policy = Dense(output_shape, <strong class="calibre2">activation</strong>='softmax', <strong class="calibre2">name</strong>='policy')(h)

    value_network = Model(<strong class="calibre2">inputs</strong>=state, <strong class="calibre2">outputs</strong>=value)
    policy_network = Model(<strong class="calibre2">inputs</strong>=state, <strong class="calibre2">outputs</strong>=policy)
    train_network = Model(<strong class="calibre2">inputs</strong>=state, <strong class="calibre2">outputs</strong>=[value, policy])

    <strong class="calibre2">return</strong> value_network, policy_network, train_network</pre></div><p class="calibre8">我们的<a id="id437" class="calibre1"/>学习过程也构建模型，与其他过程共享权重，并利用各自的损失对它们进行编译以进行训练:</p><div><pre class="programlisting">_, _, train_network = build_networks(observation_shape, env.action_space.n)
weight_dict['weights'] = train_net.get_weights()
	
<strong class="calibre2">from</strong> keras <strong class="calibre2">import</strong> backend <strong class="calibre2">as</strong> K

<strong class="calibre2">def</strong> policy_loss(<strong class="calibre2">advantage</strong>=0., <strong class="calibre2">beta</strong>=0.01):
    <strong class="calibre2">def</strong> loss(y_true, y_pred):
        <strong class="calibre2">return </strong>-K.sum(K.log(K.sum(y_true * y_pred, axis=-1) + \<strong class="calibre2">
</strong>K.epsilon()) * K.flatten(advantage)) + \
           	beta * K.sum(y_pred * K.log(y_pred + K.epsilon()))
    <strong class="calibre2">return</strong> loss

<strong class="calibre2">def </strong>value_loss():
    <strong class="calibre2">def</strong> loss(y_true, y_pred):
        <strong class="calibre2">return</strong> 0.5 * K.sum(K.square(y_true - y_pred))
    <strong class="calibre2">return</strong> loss

train_net.compile(<strong class="calibre2">optimizer</strong>=RMSprop(<strong class="calibre2">epsilon</strong>=0.1, <strong class="calibre2">rho</strong>=0.99),
            <strong class="calibre2">loss</strong>=[value_loss(), policy_loss(advantage, args.beta)])</pre></div><p class="calibre8">政策损失是鼓励勘探的强化损失加上熵损失。价值损失是简单的均方误差损失。</p><p class="calibre8">我们的学习过程将体验排入一个批次，在批次上训练模型，并更新权重字典:</p><div><pre class="programlisting">loss = train_net.train_on_batch([last_obs, advantage], [rewards, targets])</pre></div><p class="calibre8">要运行完整的代码:</p><div><pre class="programlisting">
<strong class="calibre2">pip</strong> install -r requirements.txt

<strong class="calibre2">python</strong> 1-train.py --game=Breakout-v0 --processes=16
<strong class="calibre2">python</strong> 2-play.py --game=Breakout-v0 --model=model-Breakout-v0-35750000.h5</pre></div><p class="calibre8">学习<a id="id438" class="calibre1"/>大概花了24个小时。</p><p class="calibre8">基于政策的优势行动者批评家通常胜过基于价值的方法。</p></div></body></html>


<html>
  <head>
    <title>Related articles</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec102" class="calibre1"/>相关文章</h1></div></div></div><p class="calibre8">可以参考以下文章:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre12">联结主义强化学习的简单统计梯度跟踪算法</em>，罗纳德·j·威廉姆斯，1992</li><li class="listitem"><em class="calibre12">函数逼近强化学习的策略梯度方法</em>，Richard S. Sutton，David McAllester，Satinder Singh，Yishay Mansour，1999</li><li class="listitem"><em class="calibre12">用深度强化学习玩雅达利</em>，沃洛季米尔·姆尼赫，科雷·卡武克库奥卢，大卫·西尔弗，亚历克斯·格雷夫斯，约安尼斯·安东诺格鲁，金奎大·威斯特拉，马丁·里德米勒，2013</li><li class="listitem"><em class="calibre12">通过深度神经网络和树搜索掌握围棋游戏</em>，大卫·西尔弗、阿贾·黄、克里斯·j·马迪森、阿瑟·古兹、劳伦特·西弗、乔治·范·登·德里斯切、朱利安·施利特维泽、约安尼斯·安东诺格鲁、韦达·潘妮尔·谢尔瓦姆、马克·兰托特、桑德·迪耶曼、张秀坤·格雷韦、约翰·纳姆、纳尔·卡尔施布伦纳、伊利亚·苏茨基弗、蒂莫西·莉莉拉普、玛德琳·利奇、可雷·卡武克库奥卢、托雷·格雷佩尔&amp;戴密斯·哈萨比斯，2016</li><li class="listitem"><em class="calibre12">深度强化学习的异步方法</em>，Volodymyr Mnih，Adrià Puigdomènech Badia，Mehdi Mirza，Alex Graves，Tim Harley，Timothy P. LilliCrap，David Silver，Koray Kavukcuoglu，2016年2月</li><li class="listitem"><em class="calibre12">深度强化学习无线电控制和信号检测与KeRLym </em>，健身房RL代理Timothy J. O'Shea和T. Charles Clancy，2016</li></ul></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec103" class="calibre1"/>总结</h1></div></div></div><p class="calibre8">强化学习描述了优化偶然遇到奖励的代理的任务。在深度神经网络的帮助下，针对各种游戏和模拟环境，开发了在线、离线、基于值或基于策略的算法。</p><p class="calibre8">策略梯度是一种强力解决方案，需要在训练期间对动作进行采样，并且更适合于小的动作空间，尽管它们为连续搜索空间提供了第一解决方案。</p><p class="calibre8">策略梯度还用于训练神经网络中的不可微随机层，并通过它们反向传播梯度。例如，当通过模型的传播需要遵循参数化子模型进行采样时，来自顶层的梯度可以被认为是对底层网络的奖励。</p><p class="calibre8">在更复杂的环境中，当没有明显的回报时(例如从环境中存在的物体理解和推断可能的行动)，推理可以帮助人类优化他们的行动，目前研究没有提供任何解决方案。当前的RL算法特别适合精确的游戏、快速的反应，但是没有长期的计划和推理。此外，RL算法需要大量数据集，而仿真环境很容易提供这些数据集。但这也带来了现实世界中的缩放问题。</p><p class="calibre8">在下一章中，我们将探索最新的解决方案来生成与现实世界数据没有区别的新数据。</p></div></body></html>
</body></html>