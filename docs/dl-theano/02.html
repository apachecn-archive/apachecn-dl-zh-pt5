<html><head/><body>
<html>
  <head>
    <title>Chapter 2. Classifying Handwritten Digits with a Feedforward Network</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div/><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>第二章。用前馈网络对手写数字进行分类</h1></div></div></div><p class="calibre8">第一章介绍了作为计算引擎的ano，及其不同的功能和特性。有了这些知识，我们将通过一个例子，介绍深度学习的一些主要概念，建立三个神经网络，并在手写数字分类的问题上训练它们。</p><p class="calibre8">深度<a id="id73" class="calibre1"/>学习是机器学习的一个领域，其中多层模块堆叠在彼此之上:本章介绍了一个简单的<a id="id74" class="calibre1"/>单线层模型，然后在其上添加第二层，以创建一个<strong class="calibre2">多层感知器</strong> ( <strong class="calibre2"> MLP </strong>)，最后使用多个卷积层创建一个<strong class="calibre2">卷积神经网络</strong> ( <strong class="calibre2"> CNN </strong>)。</p><p class="calibre8">与此同时，本章为那些不熟悉数据科学的人概述了基本的机器学习概念，如过度拟合、验证和损失分析:</p><div><ul class="itemizedlist"><li class="listitem">小图像分类</li><li class="listitem">手写数字识别挑战</li><li class="listitem">构建神经网络的层设计</li><li class="listitem">经典目标/损失函数的设计</li><li class="listitem">随机梯度下降的反向传播</li><li class="listitem">通过验证对数据集进行训练</li><li class="listitem">卷积神经网络</li><li class="listitem">数字分类的最新成果</li></ul></div></div></body></html>


<html>
  <head>
    <title>Chapter 2. Classifying Handwritten Digits with a Feedforward Network</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec18" class="calibre1"/>MNIST数据集</h1></div></div></div><p class="calibre8"><strong class="calibre2">改进的国家标准与技术研究所</strong> ( <strong class="calibre2"> MNIST </strong> ) <strong class="calibre2">数据集</strong>是一个非常<a id="id75" class="calibre1"/>知名的<a id="id76" class="calibre1"/>手写数字数据集{0，1，2，3，4，5，6，7，8，9}用于训练和测试分类模型。</p><p class="calibre8">分类模型是在给定输入的情况下预测观察一个类别的概率的模型。</p><p class="calibre8">训练的任务是<em class="calibre12">学习</em>参数以使模型尽可能地适合数据，以便对于任何输入图像，预测正确的标签。对于此训练任务，MNIST数据集包含60，000幅图像，每个示例都有一个目标标签(0到9之间的数字)。</p><p class="calibre8">为了验证训练是有效的并决定何时停止训练，我们通常将训练数据集分成两个数据集:80%到90%的图像用于训练，而剩余的10-20%的图像不会呈现给算法用于训练，而是验证模型在未观察到的数据上概括得很好。</p><p class="calibre8">有一个单独的数据集，该算法在训练期间永远不会看到，称为测试集，它由MNIST数据集中的10，000幅图像组成。</p><p class="calibre8">在MNIST数据集中，每个示例的输入数据都是一幅28x28的归一化单色图像和一个标注，对于每个示例，标注都表示为一个介于0和9之间的简单整数。让我们展示其中的一些:</p><div><ol class="orderedlist"><li class="listitem" value="1">首先，下载一个预先打包好的数据集版本，以便于从Python加载:<div> <pre class="programlisting"> <strong class="calibre2">wget</strong> http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz -P /sharedfiles</pre> </div></li><li class="listitem" value="2">Then load the data into a Python session:<div><pre class="programlisting">
<strong class="calibre2">import</strong> pickle, gzip
<strong class="calibre2">with</strong> gzip.open("/sharedfiles/mnist.pkl.gz", 'rb') <strong class="calibre2">as</strong> f:
   train_set, valid_set, test_set = pickle.load(f)</pre></div><p class="calibre24">对于<code class="email">Python3</code>，由于它被序列化的方式，我们需要<code class="email">pickle.load(f, encoding='latin1')</code>。</p><div><pre class="programlisting">train_set[0].shape
<em class="calibre12">(50000, 784)</em>

train_set[1].shape
<em class="calibre12">(50000,)</em>

<strong class="calibre2">import</strong> matplotlib

<strong class="calibre2">import</strong> numpy 

<strong class="calibre2">import</strong> matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (10, 10)

plt.rcParams['image.cmap'] = 'gray'

<strong class="calibre2">for</strong> i <strong class="calibre2">in</strong> range(9):
    plt.subplot(1,10,i+1)
    plt.imshow(train_set[0][i].reshape(28,28))
    plt.axis('off')
    plt.title(str(train_set[1][i]))

plt.show()</pre></div></li></ol><div/></div><p class="calibre8">数据集的前九个<a id="id78" class="calibre1"/>样本<a id="id77" class="calibre1"/>显示在它们的顶部，并带有相应的标签(<em class="calibre12">基本事实</em>，即分类算法所期望的正确答案):</p><div><img src="img/00010.jpeg" alt="The MNIST dataset" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">为了避免向GPU传输太多数据，并且因为完整的数据集足够小，可以放在GPU的内存中，我们通常将完整的训练集放在共享变量中:</p><div><pre class="programlisting">
<strong class="calibre2">import</strong> theano
train_set_x = theano.shared(numpy.asarray(train_set[0], <strong class="calibre2">dtype</strong>=theano.config.floatX))
train_set_y = theano.shared(numpy.asarray(train_set[1], <strong class="calibre2">dtype</strong>='int32'))</pre></div><p class="calibre8">避免这些数据传输使我们能够在GPU上更快地训练，尽管最近的GPU和快速PCIe连接。</p><p class="calibre8">关于数据集的更多信息可在<a class="calibre1" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>获得。</p></div></div></body></html>


<html>
  <head>
    <title>Structure of a training program</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec19" class="calibre1"/>培训计划的结构</h1></div></div></div><p class="calibre8">培训计划的结构<a id="id79" class="calibre1"/>通常由以下步骤组成:</p><div><ol class="orderedlist"><li class="listitem" value="1"><strong class="calibre2">设置脚本环境</strong>:包导入、GPU使用等<a id="id80" class="calibre1"/>。</li><li class="listitem" value="2"><strong class="calibre2"> Load data </strong>:数据<a id="id81" class="calibre1"/> loader类，用于在训练过程中访问数据，通常以随机的顺序进行，以避免同一个类有太多相似的例子，但有时也会以精确的顺序进行，例如，先简单后复杂的课程学习。</li><li class="listitem" value="3"><strong class="calibre2">数据预处理</strong>:一组<a id="id82" class="calibre1"/>变换，如交换图像尺寸、添加模糊或噪声。添加一些数据增强转换是非常常见的，例如随机裁剪、缩放、亮度或对比度抖动，以获得比原始示例更多的示例，并降低数据过度拟合的风险。如果模型中自由参数的数量对于训练数据集的大小来说太重要，则模型可以从可用的示例中学习。此外，如果数据集太小，并且对相同的数据执行了太多迭代，则模型可能会变得过于特定于训练示例，而不能很好地概括新的未见过的示例。</li><li class="listitem" value="4"><strong class="calibre2">建立模型</strong>:用持久变量(共享变量)中的参数定义<a id="id83" class="calibre1"/>模型结构，以便在训练期间更新它们的值，从而适应训练数据</li><li class="listitem" value="5"><strong class="calibre2">Train</strong>: There are <a id="id84" class="calibre1"/>different algorithms either training on the full dataset as a whole or training on each example step by step. The best convergence is usually achieved by training on a batch, a small subset of examples grouped together, from a few tens to a few hundreds.<p class="calibre24">使用批处理的另一个原因是为了提高GPU的训练速度，因为单个数据传输的成本很高，而且GPU内存也不足以容纳完整的数据集。GPU是一种并行架构，因此在某种程度上，处理一批示例通常比逐个处理示例更快。在一定程度上，同时看到更多的例子会加速收敛(在墙时间内)。即使GPU内存足够容纳整个数据集，也是如此:批处理大小的收益递减通常会使较小的批处理比整个数据集更快。注意，这对于现代的CPU也是正确的，但是最佳的批量大小通常更小。</p><div><h3 class="title2"><a id="note05" class="calibre1"/>注意</h3><p class="calibre8">一次迭代定义一个批次的训练。一个历元是算法查看完整数据集所需的迭代次数。</p></div></li><li class="listitem" value="6">During training, after a certain number of iterations, there is usually a <strong class="calibre2">validation</strong> using a split of the training data or a validation dataset that has not been used for learning. The loss is computed on this validation set. Though the algorithm has the objective to reduce the loss given the training data, it does not ensure generalization with unseen data. Validation <a id="id85" class="calibre1"/>data is unseen data used to estimate the generalization performance. A lack of generalization might occur when the training data is not representative, or is an exception and has not been sampled correctly, or if the model overfits the training data.<p class="calibre24">验证数据验证一切正常，并在验证损失不再减少时停止训练，即使训练损失可能继续减少:进一步训练不再值得，并导致过度拟合。</p></li><li class="listitem" value="7"><strong class="calibre2">Saving model parameters</strong> and displaying results, such as best training/validation loss values, train loss curves for convergence analysis.<p class="calibre24">在分类的情况下，我们计算训练期间的准确度(正确分类的百分比)或误差(错误分类的百分比)，以及损失。在训练结束时，混淆矩阵有助于评估分类器的质量。</p><p class="calibre24">让我们看看实践中的这些步骤，并在Python shell会话中启动一个Theano会话:</p><div><pre class="programlisting">
<strong class="calibre2">from</strong> theano <strong class="calibre2">import</strong> theano
<strong class="calibre2">import</strong> theano.tensor <strong class="calibre2">as</strong> T</pre></div></li></ol><div/></div></div></body></html>


<html>
  <head>
    <title>Classification loss function</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec20" class="calibre1"/>分类损失函数</h1></div></div></div><p class="calibre8">损失<a id="id86" class="calibre1"/>函数是在训练期间最小化以获得最佳模型的目标函数。存在许多不同的损失函数。</p><p class="calibre8">在分类问题中，目标是预测k个类别中的正确类别，通常使用交叉熵，因为它测量每个类别的真实概率分布<em class="calibre12"> q </em>和预测的概率分布<em class="calibre12"> p </em>之间的差异:</p><div><img src="img/00011.jpeg" alt="Classification loss function" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<em class="calibre12"> i </em>是样本在数据集中的索引，<em class="calibre12"> n </em>是样本在数据集中的个数，<em class="calibre12"> k </em>是类的个数。</p><p class="calibre8">虽然每个类别的真实概率<img src="img/00012.jpeg" alt="Classification loss function" class="calibre23"/>是未知的，但在实践中可以通过经验分布简单地近似，即按照数据集顺序从数据集随机抽取一个样本<a id="id87" class="calibre1"/>。同样，任何预测概率<code class="email">p</code>的交叉熵可以通过经验交叉熵来近似:</p><div><img src="img/00013.jpeg" alt="Classification loss function" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<img src="img/00014.jpeg" alt="Classification loss function" class="calibre23"/>是由模型为示例<img src="img/00015.jpeg" alt="Classification loss function" class="calibre23"/>的正确类别估计的概率。</p><p class="calibre8">准确性和交叉熵都朝着相同的方向发展，但是测量不同的东西。准确性衡量预测类别的正确程度，而交叉熵衡量概率之间的距离。交叉熵的减少解释了预测正确类别的概率变得更好，但是准确性可能保持不变或者下降。</p><p class="calibre8">虽然精度是离散的并且不可微，但是交叉熵损失是可微的函数，可以容易地用于训练模型。</p></div></body></html>


<html>
  <head>
    <title>Single-layer linear model</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec21" class="calibre1"/>单层线性模型</h1></div></div></div><p class="calibre8">最简单的<a id="id88" class="calibre1"/>模型是线性模型，其中对于每个类别<code class="email">c</code>，输出是输入值的线性组合:</p><div><img src="img/00016.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这种输出是无限的。</p><p class="calibre8">为了获得总计为1的概率分布<code class="email">p<sub class="calibre25">i</sub></code>，线性模型的输出被传递到softmax函数中:</p><div><img src="img/00017.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">因此，输入<code class="email">x</code>的类别<code class="email">c</code>的估计概率用向量重写:</p><div><img src="img/00018.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">用Python翻译的<a id="id89" class="calibre1"/>:</p><div><pre class="programlisting">batch_size <strong class="calibre2">=</strong> 600
n_in <strong class="calibre2">= </strong>28 * 28
n_out <strong class="calibre2">=</strong> 10

x <strong class="calibre2">=</strong> T.matrix(<strong class="calibre2">'x'</strong>)
y <strong class="calibre2">=</strong> T.ivector(<strong class="calibre2">'y'</strong>)
W <strong class="calibre2">=</strong> theano.shared(
            <strong class="calibre2">value</strong>=numpy.zeros(
                (n_in, n_out),
                <strong class="calibre2">dtype</strong>=theano.config.floatX
            ),
            <strong class="calibre2">name</strong>='W',
            <strong class="calibre2">borrow</strong>=True
        )
b <strong class="calibre2">=</strong> theano.shared(
    <strong class="calibre2">value</strong>=numpy.zeros(
        (n_out,),
        <strong class="calibre2">dtype</strong>=theano.config.floatX
    ),
    <strong class="calibre2">name</strong>='b',
    <strong class="calibre2">borrow</strong>=True
)
model <strong class="calibre2">=</strong> T.nnet.softmax(T.dot(x, W) + b)</pre></div><p class="calibre8">给定输入的预测由最可能类别(最大概率)给出:</p><div><pre class="programlisting">y_pred <strong class="calibre2">=</strong> T.argmax(model, <strong class="calibre2">axis</strong>=1)</pre></div><p class="calibre8">在这个具有单一线性层的模型中，信息从输入移动到输出:它是一个<strong class="calibre2">前馈网络</strong>。给定输入计算输出的过程称为<strong class="calibre2">正向传播</strong>。</p><p class="calibre8">这一层被称为完全连接，因为所有输出<img src="img/00019.jpeg" alt="Single-layer linear model" class="calibre23"/>是所有<a id="id90" class="calibre1"/>输入值的总和(通过乘法系数链接到):</p><div><img src="img/00020.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p></div></body></html>


<html>
  <head>
    <title>Cost function and errors</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec22" class="calibre1"/>成本函数和误差</h1></div></div></div><p class="calibre8">给定模型预测概率的成本函数<a id="id91" class="calibre1"/>如下:</p><div><pre class="programlisting">cost <strong class="calibre2">=</strong> -T.mean(T.log(model)[T.arange(y.shape[0]), y])</pre></div><p class="calibre8">误差<a id="id92" class="calibre1"/>是与真实类别不同的预测的数量，由值的总数平均，可以写成平均值:</p><div><pre class="programlisting">error <strong class="calibre2">=</strong> T.mean(T.neq(y_pred, y))</pre></div><p class="calibre8">相反，准确性对应于正确预测的数量除以预测的总数。误差和准确度之和是1。</p><p class="calibre8">对于其他类型的问题，这里有一些其他的损失函数和实现:</p><div><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/></colgroup><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">范畴交叉熵</strong></p>
<p class="calibre20">我们的等效实现</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
</p><div><pre class="programlisting1">T.nnet.categorical_crossentropy(model, y_true).mean()</pre></div><p class="calibre20">
</p>
 </td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">二元交叉熵</strong></p>
<p class="calibre20">对于输出只能取两个值{0，1}的情况</p>
<p class="calibre20">通常在sigmoid激活后使用，预测概率p</p>
</td><td valign="top" class="calibre22"><div> <pre class="programlisting1">T.nnet.binary_crossentropy(model, y_true).mean()</pre> </div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">均方误差</strong></p>
<p class="calibre20">回归问题的L2范数</p>
</td><td valign="top" class="calibre22"><div> <pre class="programlisting1">T.sqr(model – y_true).mean()</pre> </div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">平均绝对误差</strong></p>
<p class="calibre20">回归问题的L1范数</p>
</td><td valign="top" class="calibre22"><div><pre class="programlisting1">T.abs_(model - y_true).mean()</pre>T5】</div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">平滑L1 </strong></p>
<p class="calibre20">大数值的L1和小数值的L2的混合</p>
<p class="calibre20">称为回归的异常值抵抗损失</p>
</td><td valign="top" class="calibre22"><div> <pre class="programlisting1">T.switch(    T.lt(T.abs_(model - y_true) , 1. / sigma),     0.5 * sigma * T.sqr(model - y_true),    T.abs_(model - y_true) – 0.5 / sigma ) .sum(axis=1).mean()</pre> </div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">平方铰链损耗</strong></p>
<p class="calibre20">尤其用于无人监督的问题</p>
</td><td valign="top" class="calibre22"><div> <pre class="programlisting1">T.sqr(T.maximum(1. - y_true * model, 0.)).mean()</pre> </div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20"><strong class="calibre26">铰链损耗</strong></p>
</td><td valign="top" class="calibre22"><div> <pre class="programlisting1">T.maximum(1. - y_true * model, 0.).mean()</pre> </div></td></tr></tbody></table></div></div></body></html>


<html>
  <head>
    <title>Backpropagation and stochastic gradient descent</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec23" class="calibre1"/>反向传播和随机梯度下降</h1></div></div></div><p class="calibre8">反向传播，或误差的反向传播，是用于调整连接权重的最常用的监督<a id="id93" class="calibre1"/>学习算法。</p><p class="calibre8">考虑作为权重<em class="calibre12"> W </em>和<em class="calibre12"> b </em>的函数的误差或成本，成本函数的局部最小值<a id="id94" class="calibre1"/>可以用梯度下降来逼近，其包括沿着负误差梯度改变权重:</p><div><img src="img/00021.jpeg" alt="Backpropagation and stochastic gradient descent" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<img src="img/00022.jpeg" alt="Backpropagation and stochastic gradient descent" class="calibre23"/>是学习率，一个定义下降速度的正常数。</p><p class="calibre8">以下编译的函数在每次前馈运行后更新变量:</p><div><pre class="programlisting">g_W = T.grad(<strong class="calibre2">cost</strong>=cost, <strong class="calibre2">wrt</strong>=W)
g_b = T.grad(<strong class="calibre2">cost</strong>=cost, <strong class="calibre2">wrt</strong>=b)

learning_rate=0.13
index = T.lscalar()

train_model = theano.function(
    <strong class="calibre2">inputs</strong>=[index],
    <strong class="calibre2">outputs</strong>=[cost,error],
    <strong class="calibre2">updates</strong>=[(W, W - learning_rate * g_W),(b, b - learning_rate * g_b)],
    <strong class="calibre2">givens</strong>={
        x: train_set_x[index * batch_size: (index + 1) * batch_size],
        y: train_set_y[index * batch_size: (index + 1) * batch_size]
    }
)</pre></div><p class="calibre8"><a id="id95" class="calibre1"/>输入变量是批处理的索引，因为所有的数据集已经在共享变量中一次性传输到GPU。</p><p class="calibre8">训练<a id="id96" class="calibre1"/>包括将每个样本反复呈现给模型(迭代)并多次重复操作(历元):</p><div><pre class="programlisting">n_epochs <strong class="calibre2">=</strong> 1000
print_every <strong class="calibre2">=</strong> 1000

n_train_batches <strong class="calibre2">=</strong> train_set[0].shape[0] // batch_size
n_iters <strong class="calibre2">=</strong> n_epochs * n_train_batches
train_loss <strong class="calibre2">=</strong> np.zeros(n_iters)
train_error <strong class="calibre2">=</strong> npzeros(n_iters)

<strong class="calibre2">for</strong> epoch <strong class="calibre2">in</strong> range(n_epochs):
    <strong class="calibre2">for</strong> minibatch_index <strong class="calibre2">in</strong> range(n_train_batches):
        iteration = minibatch_index + n_train_batches * epoch
        train_loss[iteration], train_error[iteration] <strong class="calibre2">=</strong> train_model(minibatch_index)
        <strong class="calibre2">if</strong> (epoch * train_set[0].shape[0] + minibatch_index) <strong class="calibre2">%</strong> print_every == 0 :
            print('epoch {}, minibatch {}/{}, training error {:02.2f} %, training loss {}'.format(
                epoch,
                minibatch_index + 1,
                n_train_batches,
                train_error[iteration] * 100,
                train_loss[iteration]
            ))</pre></div><p class="calibre8">不过，这只报告了一个小批量的损失和错误。最好还能报告整个数据集的平均值。</p><p class="calibre8">在第一次迭代中，错误率下降得非常快，然后就慢下来了。</p><p class="calibre8">GPU GeForce GTX 980M笔记本电脑的执行时间为67.3秒，而英特尔i7 CPU的执行时间为3分7秒。</p><p class="calibre8">在<a id="id97" class="calibre1"/>长时间之后，模型收敛到5.3 - 5.5%的错误率，并且随着更多的迭代可以进一步下降，但是也可能导致过拟合，当模型很好地拟合训练数据但是在看不见的数据上没有得到<a id="id98" class="calibre1"/>相同的错误率时，发生过拟合。</p><p class="calibre8">在这种情况下，模型过于简单，无法过度拟合这些数据。</p><p class="calibre8">太简单的模型是学不太好的。深度学习的原理是增加更多的层，即增加深度，建立更深的网络，以获得更好的精度。</p><p class="calibre8">我们将在下一节中看到如何计算模型精度和训练停止的更好估计。</p></div></body></html>


<html>
  <head>
    <title>Multiple layer model</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec24" class="calibre1"/>多层模型</h1></div></div></div><p class="calibre8">一个<strong class="calibre2">多层感知器</strong> ( <strong class="calibre2"> MLP </strong>)就是<a id="id99" class="calibre1"/>一个多层前馈<a id="id100" class="calibre1"/>网。名为“隐藏层”的第二个线性层被添加到前面的示例中:</p><div><img src="img/00023.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">使<a id="id101" class="calibre1"/>两个线性层相互跟随相当于一个线性层。</p><p class="calibre8">通过线性之间的<em class="calibre12">非线性函数或非线性或传递函数</em>，模型不再简化为线性模型，而是表示更多可能的函数，以便捕捉数据中更复杂的模式:</p><div><img src="img/00024.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">激活<a id="id102" class="calibre1"/>功能有助于饱和(开-关)并再现生物神经元激活。</p><p class="calibre8"><strong class="calibre2">整流线性单元</strong> ( <strong class="calibre2"> ReLU </strong>)图如下:</p><p class="calibre8"><em class="calibre12"> (x + T.abs_(x)) / 2.0 </em></p><div><img src="img/00025.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><strong class="calibre2">漏整流线性单元</strong> ( <strong class="calibre2">漏整流</strong>)图如下:</p><p class="calibre8"><em class="calibre12"> ( (1 +泄漏)* x+(1-泄漏)* T.abs_(x) ) / 2.0 </em></p><div><img src="img/00026.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<code class="email">leak</code>是定义负值斜率的参数。在泄漏整流器中，该参数是固定的。</p><p class="calibre8">名为PReLU的<a id="id103" class="calibre1"/>激活考虑要学习的<code class="email">leak</code>参数。</p><p class="calibre8">更一般地说，分段线性激活可以通过添加一个线性层，然后是<code class="email">n_pool</code>单元的最大激活来学习:</p><div><pre class="programlisting">T.max([x[:, n::n_pool] <strong class="calibre2">for</strong> n <strong class="calibre2">in</strong> <strong class="calibre2">range</strong>(n_pool)], <strong class="calibre2">axis=0</strong>)</pre></div><p class="calibre8">这将输出基础学习线性的<code class="email">n_pool</code>值或单位:</p><p class="calibre8"><strong class="calibre2">乙状结肠</strong> (T.nnet .乙状结肠)</p><div><img src="img/00027.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><strong class="calibre2"> HardSigmoid </strong>函数给出为:</p><p class="calibre8"><em class="calibre12"> T.clip(X + 0.5，0。, 1.)</em></p><div><img src="img/00028.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><strong class="calibre2"> HardTanh </strong>功能给出如下:</p><p class="calibre8"><em class="calibre12"> T.clip(X，-1。, 1.)</em></p><div><img src="img/00029.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><strong class="calibre2">Tanh</strong>T40】函数给出为:</p><p class="calibre8"><em class="calibre12">唐(x) </em></p><div><img src="img/00030.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这个用Python写的两层网络模型如下:</p><div><pre class="programlisting">batch_size <strong class="calibre2">=</strong> 600
n_in <strong class="calibre2">=</strong> 28 * 28
n_hidden <strong class="calibre2">=</strong> 500
n_out <strong class="calibre2">=</strong> 10

<strong class="calibre2">def</strong> shared_zeros(shape, <strong class="calibre2">dtype</strong>=theano.config.floatX, <strong class="calibre2">name</strong>='', <strong class="calibre2">n</strong>=None):
    shape <strong class="calibre2">=</strong> shape <strong class="calibre2">if</strong> n <strong class="calibre2">is</strong> None <strong class="calibre2">else</strong> (n,) + shape
    <strong class="calibre2">return</strong> theano.shared(np.zeros(shape, dtype=dtype), <strong class="calibre2">name</strong>=name)

<strong class="calibre2">def</strong> shared_glorot_uniform(shape, <strong class="calibre2">dtype</strong>=theano.config.floatX, <strong class="calibre2">name</strong>='', <strong class="calibre2">n</strong>=None):
    <strong class="calibre2">if</strong> isinstance(shape, int):
        high <strong class="calibre2">=</strong> np.sqrt(6. / shape)
    <strong class="calibre2">else</strong>:
        high <strong class="calibre2">=</strong> np.sqrt(6. / (np.sum(shape[:2]) * np.prod(shape[2:])))
    shape <strong class="calibre2">=</strong> shape <strong class="calibre2">if</strong> n <strong class="calibre2">is</strong> None <strong class="calibre2">else</strong> (n,) + shape
    <strong class="calibre2">return</strong> theano.shared(np.asarray(
        np.random.uniform(
            <strong class="calibre2">low</strong>=-high,
            <strong class="calibre2">high</strong>=high,
            <strong class="calibre2">size</strong>=shape),
        <strong class="calibre2">dtype</strong>=dtype), <strong class="calibre2">name</strong>=name)

W1 <strong class="calibre2">=</strong> shared_glorot_uniform( (n_in, n_hidden), <strong class="calibre2">name</strong>='W1' )
b1 <strong class="calibre2">=</strong> shared_zeros( (n_hidden,), <strong class="calibre2">name</strong>='b1' )

hidden_output <strong class="calibre2">=</strong> T.tanh(T.dot(x, W1) + b1)

W2 <strong class="calibre2">=</strong> shared_zeros( (n_hidden, n_out), <strong class="calibre2">name</strong>='W2' )
b2 <strong class="calibre2">=</strong> shared_zeros( (n_out,), <strong class="calibre2">name</strong>='b2' )

model <strong class="calibre2">=</strong> T.nnet.softmax(T.dot(hidden_output, W2) + b2)
params <strong class="calibre2">=</strong> [W1,b1,W2,b2]</pre></div><p class="calibre8">在深网中，如果用<code class="email">shared_zeros</code>方法将权重初始化为零，信号将不会<a id="id105" class="calibre1"/>正确地从一端到另一端流过网络。如果用太大的值初始化权重，在几个步骤之后，大多数激活函数饱和。因此，我们需要确保值可以在传播过程中传递到下一层，以及在反向传播过程中传递到上一层的梯度。</p><p class="calibre8">我们还需要打破神经元之间的对称性。如果所有神经元的权重都为零(或者都相等)，那么它们都将以完全相同的方式进化，模型也不会学到很多东西。</p><p class="calibre8">研究人员Xavier Glorot研究了一种以最佳方式初始化权重的算法。它包括从零均值的高斯或均匀分布和以下方差中提取权重:</p><div><img src="img/00031.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">以下是前面公式中的变量:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">n<sub class="calibre25">in</sub></code>是前馈传播期间该层接收的输入数</li><li class="listitem"><code class="email">n<sub class="calibre25">out</sub></code>是层在反向传播过程中接受的梯度数</li></ul></div><p class="calibre8">在线性模型的<a id="id106" class="calibre1"/>情况下，形状参数是一个元组，<code class="email">v</code>就是简单的<code class="email">numpy.sum( shape[:2] )</code>(在这种情况下，<code class="email">numpy.prod(shape[2:])</code>就是<code class="email">1</code>)。</p><p class="calibre8">在<em class="calibre12"> [-a，a] </em>上均匀分布的方差由<em class="calibre12"> a**2 / 3 </em>给出，那么界限<code class="email">a</code>可以计算如下:</p><div><img src="img/00032.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">成本可以像以前一样定义，但是梯度下降需要适应处理参数列表，<code class="email">[W1,b1,W2,b2]</code>:</p><div><pre class="programlisting">g_params = T.grad(<strong class="calibre2">cost</strong>=cost, <strong class="calibre2">wrt</strong>=params)</pre></div><p class="calibre8">训练循环需要更新的训练功能:</p><div><pre class="programlisting">learning_rate <strong class="calibre2">=</strong> 0.01
updates <strong class="calibre2">=</strong> [
        (param, param - learning_rate * gparam)
        <strong class="calibre2">for</strong> param, gparam <strong class="calibre2">in</strong> zip(params, g_params)
    ]

train_model = theano.function(
    <strong class="calibre2">inputs</strong>=[index],
    <strong class="calibre2">outputs</strong>=cost,
    <strong class="calibre2">updates</strong>=updates,
    <strong class="calibre2">givens</strong>={
        x: train_set_x[index * batch_size: (index + 1) * batch_size],
        y: train_set_y[index * batch_size: (index + 1) * batch_size]
    }
)</pre></div><p class="calibre8">在这种情况下，学习率对于网络是全局的，所有权重以相同的速率更新。学习率设置为0.01，而不是0.13。我们将在培训部分讨论超参数调整。</p><p class="calibre8"><a id="id107" class="calibre1"/>训练循环保持不变。完整代码在<code class="email">2-multi.py</code>文件中给出。</p><p class="calibre8">在GPU上的执行时间是5分55秒，而在CPU上是51分36秒。</p><p class="calibre8">经过1000次迭代后，误差下降到了2%，这比之前5%的误差率好了很多，但部分原因可能是由于过度拟合。我们稍后将比较不同的模型。</p></div></body></html>


<html>
  <head>
    <title>Convolutions and max layers</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec25" class="calibre1"/>卷积和最大层数</h1></div></div></div><p class="calibre8">随着<a id="id109" class="calibre1"/>MNIST数据库上卷积层的发明，图像分类取得了巨大的进步:</p><div><img src="img/00033.jpeg" alt="Convolutions and max layers" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">虽然<a id="id110" class="calibre1"/>先前的全连接层利用输入的所有<a id="id111" class="calibre1"/>输入值(在图像的情况下是像素)执行计算，但是2D卷积层将只考虑2D输入图像的NxN像素的小块或窗口或感受域，用于每个输出单元。面片的维数称为核维数，N是核大小，系数/参数是核。</p><p class="calibre8">在输入图像的每个位置，内核产生一个标量，所有位置值将导致一个矩阵(2D张量)，称为<em class="calibre12">特征图</em>。将输入图像上的核作为滑动窗口进行卷积会创建一个新的输出图像。内核的步幅定义了在图像上移动补丁/窗口的像素数:步幅为2时，每隔2个像素计算一次内核的卷积<a id="id112" class="calibre1"/>。</p><p class="calibre8">例如，在224 x 224的输入图像上，我们得到以下结果:</p><div><ul class="itemizedlist"><li class="listitem">步幅为1的2×2内核输出223×223的特征图</li><li class="listitem">步幅为1的3×3内核输出222×222的特征图</li></ul></div><p class="calibre8">为了<a id="id113" class="calibre1"/>保持输出特征图与输入图像的尺寸相同，有一种称为<em class="calibre12">相同</em>或<em class="calibre12">一半</em>的零填充类型，可实现以下功能:</p><div><ul class="itemizedlist"><li class="listitem">在步长为1的2x2内核的情况下，在输入图像的末尾添加一行和一列零</li><li class="listitem">在步长为1的3×3内核的情况下，垂直和水平地添加两行和两列零，一个在输入图像的前面，一个在输入图像的末尾</li></ul></div><p class="calibre8">因此，输出的尺寸与原始尺寸相同，即224 x 224的特征地图。</p><p class="calibre8">带零填充:</p><div><ul class="itemizedlist"><li class="listitem">具有步幅2和零填充的2×2内核将输出112×112的特征图</li><li class="listitem">步幅为2的3×3内核将输出112×112的特征图</li></ul></div><p class="calibre8">如果没有补零，事情会变得更加复杂:</p><div><ul class="itemizedlist"><li class="listitem">步幅为2的2×2内核将输出112×112的特征图</li><li class="listitem">步幅为2的3×3内核将输出111×111的特征图</li></ul></div><p class="calibre8">请注意，每个维度的内核维度和跨度可以不同。在这种情况下，我们说内核宽度、内核高度、步幅宽度或步幅高度。</p><p class="calibre8">在一个卷积层中，可以输出多个特征图，每个特征图用不同的核(和核权重)计算，并表示一个特征。我们无差别地说输出、神经元、核、特征、特征图、单元或输出通道，以给出这些具有不同核的不同卷积的数量。准确地说，神经元通常是指特征图中的特定位置。核是核本身，其他的是指卷积运算的结果。它们的数量是相同的，这就是为什么这些词经常被用来描述同一个事物。我将使用通道、输出和功能这几个词。</p><p class="calibre8">常用的卷积运算符可应用于多声道输入。这使得可以将它们应用于三通道图像(例如RGB图像)或另一个卷积的输出，以便进行链接。</p><p class="calibre8">让我们<a id="id114" class="calibre1"/>在前一个MLP模式之前包括两个内核大小为5的卷积:</p><div><img src="img/00034.jpeg" alt="Convolutions and max layers" class="calibre9"/></div><p class="calibre10">2D <a id="id115" class="calibre1"/>卷积运算符需要一个4D张量输入。第一维是批量大小，第二维是输入或输入通道的数量(在“通道优先格式”中)，第三维和第四维是特征映射的两个维度(在“通道最后格式”中，通道是最后的维度)。存储在一维向量中的MNIST灰度图像(一个通道)需要转换成28×28的矩阵，其中28是图像的高度和宽度:</p><p class="calibre8">然后，在变换后的输入之上添加20个通道的第一卷积层，我们得到:</p><div><pre class="programlisting">layer0_input = x.reshape((batch_size, 1, 28, 28))</pre></div><p class="calibre8">在这种情况下，Xavier初始化(以其发明者Xavier Glorot的名字命名)将输入/输出通道的数量乘以内核中的参数数量<code class="email">numpy.prod(shape[2:]) = 5 x 5 = 25</code>，以获得初始化公式中输入/输出渐变的总数。</p><div><pre class="programlisting">
<strong class="calibre2">from</strong> theano.tensor.nnet <strong class="calibre2">import</strong> conv2d

n_conv1 <strong class="calibre2">=</strong> 20

W1 <strong class="calibre2">=</strong> shared_glorot_uniform( (n_conv1, 1, 5, 5) )

conv1_out <strong class="calibre2">=</strong> conv2d(
    <strong class="calibre2">input</strong>=layer0_input,
    <strong class="calibre2">filters</strong>=W1,
    <strong class="calibre2">filter_shape</strong>=(n_conv1, 1, 5, 5),
    <strong class="calibre2">input_shape</strong>=(batch_size, 1, 28, 28)
)</pre></div><p class="calibre8">28×28个输入上的20个大小为5×5和步长为1的核将产生20个大小为24×24的特征图。所以第一个卷积输出是(<code class="email">batch_size,20,24,24</code>)。</p><p class="calibre8">最佳<a id="id116" class="calibre1"/>性能网络使用最大池层，以促进对噪声的平移不变性和稳定性。max-pooling层在滑动窗口/面片上执行最大值操作，以仅将一个值保留在面片之外。除了提高速度性能之外，它还减小了特征图的大小，并且总计算复杂度和训练时间也减少了:</p><p class="calibre8">2x2最大池层的输出将是(<code class="email">batch_size,20,12,12</code>)。批次大小和通道数量保持不变。只有要素地图的大小发生了变化。</p><div><pre class="programlisting">
<strong class="calibre2">from</strong> theano.tensor.signal <strong class="calibre2">import</strong> pool
pooled_out<strong class="calibre2"> =</strong> pool.pool_2d(<strong class="calibre2">input</strong>=conv1_out, <strong class="calibre2">ws</strong>=(2, 2), <strong class="calibre2">ignore_border</strong>=True)</pre></div><p class="calibre8">在前一个卷积层之上添加第二个50信道的卷积层和最大池层导致大小为(<code class="email">batch_size,50,4,4</code>)的输出:</p><p class="calibre8">为了创建一个分类器，我们在MLP的顶部连接两个完全连接的线性层和一个softmax，如前所述:</p><div><pre class="programlisting">n_conv2 <strong class="calibre2">=</strong> 50

W2 <strong class="calibre2">=</strong> shared_glorot_uniform( (n_conv2, n_conv1, 5, 5) )

conv2_out = conv2d(
    <strong class="calibre2">input</strong>=pooled_out,
    <strong class="calibre2">filters</strong>=W2,
    <strong class="calibre2">filter_shape</strong>=(n_conv2, n_conv1, 5, 5),
    <strong class="calibre2">input_shape</strong>=(batch_size, n_conv1, 12, 12)
)

pooled2_out <strong class="calibre2">=</strong> pool.pool_2d(<strong class="calibre2">input</strong>=conv2_out, <strong class="calibre2">ds</strong>=(2, 2),<strong class="calibre2">ignore_border</strong>=True)</pre></div><p class="calibre8">这样一个模型被命名为<strong class="calibre2">卷积神经网络</strong> ( <strong class="calibre2"> CNN </strong>)。</p><div><pre class="programlisting">hidden_input <strong class="calibre2">=</strong> pooled2_out.flatten(2)

n_hidden <strong class="calibre2">=</strong> 500

W3 <strong class="calibre2">=</strong> shared_zeros( (n_conv2 * 4 * 4, n_hidden), <strong class="calibre2">name</strong>='W3' )
b3 <strong class="calibre2">=</strong> shared_zeros( (n_hidden,), <strong class="calibre2">name</strong>='b3' )

hidden_output <strong class="calibre2">=</strong> T.tanh(T.dot(hidden_input, W3) + b3)

n_out <strong class="calibre2">=</strong> 10

W4 <strong class="calibre2">=</strong> shared_zeros( (n_hidden, n_out), <strong class="calibre2">name</strong>='W4' )
b4 <strong class="calibre2">=</strong> shared_zeros( (n_out,), <strong class="calibre2">name</strong>='b4' )

model <strong class="calibre2">=</strong> T.nnet.softmax(T.dot(hidden_output, W4) + b4)
params <strong class="calibre2">=</strong> [W1,W2,W3,b3,W4,b4]</pre></div><p class="calibre8">完整的代码在<code class="email">3-cnn.py</code>文件中给出。</p><p class="calibre8">训练<a id="id118" class="calibre1"/>要慢得多，因为参数的数量再次成倍增加，使用GPU更有意义:在<a id="id119" class="calibre1"/>GPU上的总训练时间增加到了1小时48分27秒。对CPU的训练需要几天时间。</p><p class="calibre8">经过几次迭代后，训练误差为零，部分原因是过度拟合。让我们在下一节看看如何计算一个测试损失和准确性，更好地解释模型的效率。</p><p class="calibre8"><a id="ch02lvl1sec26" class="calibre1"/>培训</p></div></body></html>


<html>
  <head>
    <title>Training</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">为了<a id="id120" class="calibre1"/>很好地衡量模型如何处理训练期间不可见的数据，验证数据集用于计算训练期间的验证损失和准确性。</h1></div></div></div><p class="calibre8">验证数据集使我们能够选择最佳模型，而测试数据集仅在最后用于获得模型的最终测试精度/误差。训练、测试和验证数据集是离散的数据集，没有共同的示例。验证数据集通常比测试数据集小10倍，以尽可能降低训练过程的速度。测试数据集通常占训练数据集的10-20%左右。训练数据集和验证数据集都是训练计划的一部分，因为第一个数据集用于学习，第二个数据集用于在训练时根据看不见的数据选择最佳模型。</p><p class="calibre8">测试数据集完全在训练过程之外，用于获得由训练和模型选择产生的模型的准确性。</p><p class="calibre8">例如，如果模型因为在相同的图像上被训练了太多次而使训练集过拟合，那么验证集和测试集将不会受到这种行为的影响，并且将提供对模型准确性的真实估计。</p><p class="calibre8">通常，在没有模型梯度更新的情况下编译验证函数，以简单地仅计算输入批次的成本和误差。</p><p class="calibre8">批量数据<em class="calibre12"> (x，y) </em>通常会在每次迭代时传输到GPU，因为数据集通常太大，不适合GPU的内存。在这种情况下，我们仍然可以使用共享变量的技巧将整个验证数据集放在GPU的内存中，但是让我们看看如果我们必须在每个步骤将批处理传输到GPU，而不使用前面的技巧，我们会怎么做。我们会使用更常见的形式:</p><p class="calibre8">它需要批量输入的转移。验证不是在每次迭代时计算，而是在训练<code class="email">for</code>循环中的<code class="email">validation_interval</code>次迭代时计算:</p><div><pre class="programlisting">validate_model <strong class="calibre2">=</strong> theano.function(
    <strong class="calibre2">inputs</strong>=[x,y],
    <strong class="calibre2">outputs</strong>=[cost,error]
)</pre></div><p class="calibre8">让我们看看<a id="id121" class="calibre1"/>这个简单的第一个模型:</p><div><pre class="programlisting">
<strong class="calibre2">if</strong> iteration <strong class="calibre2">%</strong> validation_interval <strong class="calibre2">==</strong> 0 :
    val_index <strong class="calibre2">=</strong> iteration <strong class="calibre2">//</strong> validation_interval
    valid_loss[val_index], valid_error[val_index] <strong class="calibre2">=</strong> np.mean([
            validate_model(
                valid_set[0][i * batch_size: (i + 1) * batch_size],
                numpy.asarray(valid_set[1][i * batch_size: (i + 1) * batch_size], <strong class="calibre2">dtype</strong>="int32")
                )
                <strong class="calibre2">for</strong> i <strong class="calibre2">in</strong> range(n_valid_batches)
             ], <strong class="calibre2">axis</strong>=0)</pre></div><p class="calibre8">在完整的训练程序中，对应于时期总数的验证间隔，以及时期的平均验证分数，将更有意义。</p><div><pre class="programlisting">epoch 0, minibatch 1/83, validation error 40.05 %, validation loss 2.16520105302

epoch 24, minibatch 9/83, validation error 8.16 %, validation loss 0.288349323906
epoch 36, minibatch 13/83, validation error 7.96 %, validation loss 0.278418215923
epoch 48, minibatch 17/83, validation error 7.73 %, validation loss 0.272948684171
epoch 60, minibatch 21/83, validation error 7.65 %, validation loss 0.269203903154
epoch 72, minibatch 25/83, validation error 7.59 %, validation loss 0.26624627877
epoch 84, minibatch 29/83, validation error 7.56 %, validation loss 0.264540277421
...
epoch 975, minibatch 76/83, validation error 7.10 %, validation loss 0.258190142922
epoch 987, minibatch 80/83, validation error 7.09 %, validation loss 0.258411859162</pre></div><p class="calibre8">为了更好地估计训练的表现，让我们画出训练和有效损失。为了显示早期迭代中的下降，我将在100次迭代时停止绘制。如果我在图中使用1000次迭代，我将看不到早期的迭代:</p><p class="calibre8">To better estimate how the training performs, let's plot the training and valid loss. In order to display the descent in early iterations, I'll stop the drawing at 100 iterations. If I use 1,000 iterations in the plot, I won't see the early iterations:</p><div><img src="img/00035.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10"><a id="id122" class="calibre1"/>训练损失看起来像一个宽带，因为它在不同的值之间振荡。每个值对应一个批次。该批次可能太小，无法提供稳定的损失值。历元上训练损失的平均值将提供更稳定的值来与有效损失进行比较，并显示过度拟合。</p><p class="calibre8">还要注意，损耗图提供了网络如何收敛的信息，但没有给出任何有价值的误差信息。因此，划分训练误差和有效误差也是非常重要的。</p><p class="calibre8">对于第二个模型:</p><p class="calibre8">同样，<a id="id123" class="calibre1"/>训练曲线给出了更好的见解:</p><div><pre class="programlisting">epoch 0, minibatch 1/83, validation error 41.25 %, validation loss 2.35665753484
epoch 24, minibatch 9/83, validation error 10.20 %, validation loss 0.438846310601
epoch 36, minibatch 13/83, validation error 9.40 %, validation loss 0.399769391865
epoch 48, minibatch 17/83, validation error 8.85 %, validation loss 0.379035864025
epoch 60, minibatch 21/83, validation error 8.57 %, validation loss 0.365624915808
epoch 72, minibatch 25/83, validation error 8.31 %, validation loss 0.355733696371
epoch 84, minibatch 29/83, validation error 8.25 %, validation loss 0.348027150147
epoch 96, minibatch 33/83, validation error 8.01 %, validation loss 0.34150374867
epoch 108, minibatch 37/83, validation error 7.91 %, validation loss 0.335878048092
...
epoch 975, minibatch 76/83, validation error 2.97 %, validation loss 0.167824191041
epoch 987, minibatch 80/83, validation error 2.96 %, validation loss 0.167092795949</pre></div><p class="calibre8">Again, the <a id="id123" class="calibre1"/>training curves give better insights:</p><div><img src="img/00036.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10">对于第三款<a id="id124" class="calibre1"/>:</p><p class="calibre8">参考<a id="id125" class="calibre1"/>下图:</p><div><pre class="programlisting">epoch 0, minibatch 1/83, validation error 53.81 %, validation loss 2.29528842866
epoch 24, minibatch 9/83, validation error 1.55 %, validation loss 0.048202780541
epoch 36, minibatch 13/83, validation error 1.31 %, validation loss 0.0445762014715
epoch 48, minibatch 17/83, validation error 1.29 %, validation loss 0.0432346871821
epoch 60, minibatch 21/83, validation error 1.25 %, validation loss 0.0425786205451
epoch 72, minibatch 25/83, validation error 1.20 %, validation loss 0.0413943211024
epoch 84, minibatch 29/83, validation error 1.20 %, validation loss 0.0416557886347
epoch 96, minibatch 33/83, validation error 1.19 %, validation loss 0.0414686980075
...
epoch 975, minibatch 76/83, validation error 1.08 %, validation loss 0.0477593478863
epoch 987, minibatch 80/83, validation error 1.08 %, validation loss 0.0478142946085</pre></div><p class="calibre8">Refer to <a id="id125" class="calibre1"/>the following graph:</p><div><img src="img/00037.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10">在这里，我们看到了train和valid之间的差异，损失要么是由于对训练数据的轻微过度拟合，要么是训练数据集和测试数据集之间的差异。</p><p class="calibre8">过度拟合的主要原因如下:</p><p class="calibre8"><strong class="calibre2">数据集太小</strong>:收集更多数据</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">学习率太高</strong>:网络对早期的例子学习得太快</li><li class="listitem"><strong class="calibre2">缺乏正则化</strong>:增加更多的丢失(见下一节)，或者损失函数中权重范数的惩罚</li><li class="listitem"><strong class="calibre2">过小型号</strong>:增加不同层的过滤器/单元数量</li><li class="listitem">验证<a id="id126" class="calibre1"/>损失和误差给出了比训练损失和误差更好的估计，训练损失和误差噪声更大，并且在训练期间，它们也用于决定哪个模型参数是最好的:</li></ul></div><p class="calibre8"><strong class="calibre2">简单模型</strong>:518年期6.96 %</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2"> MLP模型</strong>:987年时2.96 %</li><li class="listitem"><strong class="calibre2"> CNN模型</strong>:722年时1.06 %</li><li class="listitem">这些结果还表明，模型可能不会随着进一步的训练而有很大的改善。</li></ul></div><p class="calibre8">以下是三种模型验证损失的比较:</p><p class="calibre8">Here's a comparison of the three models' validation losses:</p><div><img src="img/00038.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10">请注意，MLP仍在改进，训练尚未结束，而CNN和simple <a id="id127" class="calibre1"/>网络已经收敛。</p><p class="calibre8">使用所选择的模型，您可以轻松地计算测试数据集上的测试损失和错误，以最终确定它。</p><p class="calibre8">机器学习的最后一个重要概念是超参数调整。超参数定义了在训练期间没有学习到的模型参数。以下是一些例子:</p><p class="calibre8">对于学习速率，下降太慢可能会阻止找到更全局的最小值，而下降太快会破坏最终的收敛。找到最佳的初始学习率至关重要。然后，通常在多次迭代之后降低学习率，以便对模型进行更精确的微调。</p><div><pre class="programlisting">learning rate
number of hidden neurons
batch size</pre></div><p class="calibre8">超参数选择要求我们针对超参数的不同值多次运行之前的运行；例如，测试超参数的所有组合可以在简单的网格搜索中完成。</p><p class="calibre8">下面是给读者的一个练习:</p><p class="calibre8">用不同的超参数训练模型，绘制训练损失曲线，看超参数如何影响最终损失。</p><div><ul class="itemizedlist"><li class="listitem">一旦模型被训练，可视化第一层神经元的内容，以查看特征从输入图像中捕捉到了什么。针对这个任务，编写一个具体的可视化函数:<div> <pre class="programlisting">visualize_layer1 <strong class="calibre2">=</strong> theano.function(     <strong class="calibre2">inputs</strong>=[x,y],     <strong class="calibre2">outputs</strong>=conv1_out )</pre> </div></li><li class="listitem"><a id="ch02lvl1sec27" class="calibre1"/>辍学</li></ul></div></div></body></html>


<html>
  <head>
    <title>Dropout</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">Dropout <a id="id129" class="calibre1"/>是一种广泛使用的技术，用于改善神经网络的收敛性和鲁棒性，并防止神经网络过度拟合。它包括为我们想要应用的层设置一些随机值为零。它会在每个时期的数据中引入一些随机性。</h1></div></div></div><p class="calibre8">通常，在全连接层之前使用dropout，在卷积层中不常使用。让我们在两个完全连接的层之前添加以下行:</p><p class="calibre8">完整的脚本在<code class="email">5-cnn-with-dropout.py</code>里。经过1000次迭代后，有丢包的CNN验证误差继续下降到1.08%，而无丢包的CNN验证误差不会下降1.22%。</p><div><pre class="programlisting">dropout <strong class="calibre2">=</strong> 0.5

<strong class="calibre2">if</strong> dropout <strong class="calibre2">&gt;</strong> 0 :
    mask <strong class="calibre2">=</strong> srng.binomial(<strong class="calibre2">n</strong>=1, <strong class="calibre2">p</strong>=1-dropout, <strong class="calibre2">size</strong>=hidden_input.shape)
    # The cast is important because
    # int * float32 = float64 which make execution slower
    hidden_input <strong class="calibre2">=</strong> hidden_input * T.cast(mask, theano.config.floatX)</pre></div><p class="calibre8">想进一步了解辍学现象的读者应该看看maxout单位。它们可以很好地处理辍学问题，并取代tanh非线性，从而获得更好的结果。由于<a id="id130" class="calibre1"/> dropout执行一种模型平均，最大输出单元试图找到问题的最佳非线性。</p><p class="calibre8"><a id="ch02lvl1sec28" class="calibre1"/>推论</p></div></body></html>


<html>
  <head>
    <title>Inference</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">推理是使用模型产生预测的过程。</h1></div></div></div><p class="calibre8">对于推理，权重参数不需要更新，因此推理函数比训练函数简单:</p><p class="calibre8"><a id="ch02lvl1sec29" class="calibre1"/>优化和其他更新规则</p><div><pre class="programlisting">infer_model = theano.function(
    <strong class="calibre2">inputs</strong>=[x],
    <strong class="calibre2">outputs</strong>=[y_pred]
)</pre></div></div></body></html>


<html>
  <head>
    <title>Optimization and other update rules</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">学习<a id="id132" class="calibre1"/>速率是正确设置的一个非常重要的参数。太低的<a id="id133" class="calibre1"/>学习率会使学习变得困难，训练速度会变慢，而太高的学习率会增加对异常值的敏感性，增加数据中的噪声量，训练太快而无法学习泛化，并陷入局部最小值:</h1></div></div></div><p class="calibre8">Learning <a id="id132" class="calibre1"/>rate is a very important parameter to set correctly. Too low a <a id="id133" class="calibre1"/>learning rate will make it difficult to learn and will train slower, while too high a learning rate will increase sensitivity to outlier values, increase the amount of noise in the data, train too fast to learn generalization, and get stuck in local minima:</p><div><img src="img/00039.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10">当<a id="id134" class="calibre1"/>训练损失在一次或几次<a id="id135" class="calibre1"/>迭代中不再改善时，学习率可以降低一个系数:</p><p class="calibre8">When <a id="id134" class="calibre1"/>training loss does not improve anymore for one or a few more <a id="id135" class="calibre1"/>iterations, the learning rate can be reduced by a factor:</p><div><img src="img/00040.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10">它有助于网络学习数据中的细微差异，如训练残差网络时所示(<a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">第7章</a>、<em class="calibre12">用残差网络对图像进行分类</em>):</p><p class="calibre8">为了检查<a id="id136" class="calibre1"/>训练过程，通常打印参数的标准、梯度、更新以及NaN值。</p><div><img src="img/00041.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10">本章看到的更新<a id="id137" class="calibre1"/>规则是最简单的更新形式，称为<a id="id138" class="calibre1"/>的<strong class="calibre2">随机梯度下降</strong> ( <strong class="calibre2"> SGD </strong>)。修剪规范以避免饱和和NaN值是一个很好的做法。给<code class="email">theano</code>函数的更新列表如下:</p><p class="calibre8">为了改善下降，已经试验了一些非常简单的变体，并且在许多深度学习库中提出了这些变体。让我们在电影院看他们。</p><p class="calibre8"><strong class="calibre2">气势</strong></p><div><pre class="programlisting">
<strong class="calibre2">def</strong> clip_norms(gs, c):
    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))
    return [ T.switch(T.ge(norm, c), g*c/norm, g) <strong class="calibre2">for</strong> g <strong class="calibre2">in</strong> gs]

updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p,g <strong class="calibre2">in</strong> zip(params,grads):
    updated_p = p - learning_rate * g
    updates.append((p, updated_p))</pre></div><p class="calibre8">对于每一个<a id="id139" class="calibre1"/>参数，动量(<em class="calibre12"> v </em>，作为速度)是从随时间衰减的迭代累积的梯度中计算出来的。先前的动量值乘以0.5和0.9之间的衰减参数(待交叉验证)并加到当前梯度上以提供新的动量值。</p><p class="calibre8">为了更快地学习，梯度的动量在更新中起到惯性力矩的作用。该想法还在于，连续梯度中的振荡将在动量中被抵消，从而将参数以更直接的路径移向解:</p><p class="calibre8">For each <a id="id139" class="calibre1"/>parameter, a momentum (<em class="calibre12">v</em>, as velocity) is computed from the gradients accumulated over the iterations with a time decay. The previous momentum value is multiplied by a decay parameter between 0.5 and 0.9 (to be cross-validated) and added to the current gradient to provide the new momentum value.</p><p class="calibre8">在0.5和0.9之间的衰减<a id="id140" class="calibre1"/>参数是一个超参数，通常被称为动量，在语言的滥用中:</p><div><img src="img/00042.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10"><strong class="calibre2">内斯特罗夫加速梯度</strong></p><p class="calibre8">代替将<em class="calibre12"> v </em>加到参数上的<a id="id141" class="calibre1"/>，想法是将动量<code class="email">v - learning_rate g</code>的未来值加到目录上，以便让它直接在下一个位置计算下一次迭代中的梯度:</p><div><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p,g <strong class="calibre2">in</strong> zip(params,grads):
    m = theano.shared(p.get_value() * 0.)
    v = (momentum * m) - (learning_rate * g)
    updates.append((m, v))
    updates.append((p, p + v))</pre></div><p class="calibre8">阿达格勒</p><p class="calibre8">该更新<a id="id142" class="calibre1"/>规则以及以下规则包括调整学习率<strong class="calibre2">参数方式</strong>(每个参数不同)。梯度的逐元素平方和被累积到每个参数的共享变量中，以便以逐元素方式衰减学习速率:</p><div><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p, g <strong class="calibre2">in</strong> zip(params, grads):
    m = theano.shared(p.get_value() * 0.)
    v = (momentum * m) - (learning_rate * g)
    updates.append((m,v))
    updates.append((p, p + momentum * v - learning_rate * g))</pre></div><p class="calibre8"><code class="email">Adagrad</code>是一个<a id="id143" class="calibre1"/>攻击性的方法，接下来的两个规则，<code class="email">AdaDelta</code>和<code class="email">RMSProp</code>，尽量减少其攻击性。</p><p class="calibre8"><strong class="calibre2">阿达德尔塔</strong></p><div><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p,g <strong class="calibre2">in</strong> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_t = acc + g ** 2
    updates.append((acc, acc_t))
    p_t = p - (learning_rate / T.sqrt(acc_t + 1e-6)) * g
    updates.append((p, p_t))</pre></div><p class="calibre8">每个参数创建两个<a id="id144" class="calibre1"/>累加器，以累加平方梯度和移动平均值的更新，由衰减<code class="email">rho</code>参数化:</p><p class="calibre8"><strong class="calibre2"> RMSProp </strong></p><p class="calibre8">这个更新的<a id="id145" class="calibre1"/>规则在很多情况下非常有效。它是对<code class="email">Adagrad</code>更新规则的改进，使用移动平均值(由<code class="email">rho</code>参数化)来获得更小的衰减:</p><div><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p,g <strong class="calibre2">in</strong> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_delta = theano.shared(p.get_value() * 0.)
    acc_new = rho * acc + (1 - rho) * g ** 2
    updates.append((acc,acc_new))
    update = g * T.sqrt(acc_delta + 1e-6) / T.sqrt(acc_new + 1e-6)
    updates.append((p, p - learning_rate * update))
    updates.append((acc_delta, rho * acc_delta + (1 - rho) * update ** 2))</pre></div><p class="calibre8"><strong class="calibre2">亚当</strong></p><p class="calibre8">这是<code class="email">RMSProp</code>与momemtum，学习规则的最佳选择之一。时间步长<a id="id146" class="calibre1"/>在共享变量<code class="email">t</code>中保持跟踪。计算两个移动平均值，一个用于过去的平方梯度，另一个用于过去的梯度:</p><div><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<strong class="calibre2">for</strong> p,g <strong class="calibre2">in</strong> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_new = rho * acc + (1 - rho) * g ** 2
    updates.append((acc, acc_new))
    updated_p = p - learning_rate * (g / T.sqrt(acc_new + 1e-6))
    updates.append((p, updated_p))</pre></div><p class="calibre8">总结更新规则，许多最近的研究论文仍然倾向于简单的SGD规则，并以正确的学习速率进行架构和层的初始化。对于更复杂的网络，或者如果数据是稀疏的，自适应学习速率方法是更好的，使你不必为寻找正确的学习速率而痛苦。</p><p class="calibre8"><a id="ch02lvl1sec30" class="calibre1"/>相关文章</p><div><pre class="programlisting">b1=0.9, b2=0.999, l=1-1e-8
updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)  
t = theano.shared(floatX(1.))
b1_t = b1 * l **(t-1)

<strong class="calibre2">for</strong> p, g <strong class="calibre2">in</strong> zip(params, grads):
    m = theano.shared(p.get_value() * 0.)
    v = theano.shared(p.get_value() * 0.)
    m_t = b1_t * m + (1 - b1_t) * g
    v_t = b2 * v + (1 - b2) * g**2 
    updates.append((m, m_t))
    updates.append((v, v_t))
    updates.append((p, p - (learning_rate * m_t / (1 - b1**t)) / (T.sqrt(v_t / (1 - b2**t)) + 1e-6)) )
updates.append((t, t + 1.))</pre></div><p class="calibre8">您可以<a id="id0" class="calibre1"/>参考以下文档，深入了解本章涵盖的主题:</p></div></body></html>


<html>
  <head>
    <title>Related articles</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><em class="calibre12">Deeplearning.net·泰诺教程:单层</em>(<a class="calibre1" href="http://deeplearning.net/tutorial/logreg.html">http://deeplearning.net/tutorial/logreg.html</a>)、MLP(<a class="calibre1" href="http://deeplearning.net/tutorial/mlp.html">http://deeplearning.net/tutorial/mlp.html</a>)、卷积(<a class="calibre1" href="http://deeplearning.net/tutorial/lenet.html">http://deeplearning.net/tutorial/lenet.html</a>)</h1></div></div></div><p class="calibre8">所有损失函数:用于分类、回归和联合嵌入(<a class="calibre1" href="http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html">http://Christopher 5106 . github . io/deep/learning/2016/09/16/about-loss-functions-多项式-logistic-对数-交叉熵-平方误差-euclidian-absolute-Frobenius-hinge . html</a>)</p><div><ul class="itemizedlist"><li class="listitem">最后一个例子对应于Yann Lecun的5-5层网络，如应用于文档识别的基于梯度的学习(<a class="calibre1" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></li><li class="listitem">理解训练深度前馈神经网络的困难，Xavier Glorot，Yoshua Bengio，2010</li><li class="listitem">最大输出网络:伊恩·古德菲勒、大卫·沃德-法利、迈赫迪·米尔扎、亚伦·库维尔、约舒阿·本吉奥</li><li class="listitem">梯度下降算法概述:<a class="calibre1" href="http://sebastianruder.com/optimizing-gradient-descent/">http://sebastianruder.com/optimizing-gradient-descent/</a></li><li class="listitem">CS231n卷积神经网络用于视觉识别，<a class="calibre1" href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></li><li class="listitem">是的，你应该了解backprop，Andrej Karpathy，2016年，<a class="calibre1" href="https://medium.com/@karpathy/">https://medium.com/@karpathy/</a></li><li class="listitem">追求简单:全卷积网络，Jost Tobias Springenberg，Alexey Dosovitskiy，Thomas Brox，Martin Riedmiller，2014年</li><li class="listitem">部分最高统筹，本杰明·格拉哈姆，2014年</li><li class="listitem">批量<a id="id1" class="calibre1"/>标准化:通过减少内部协变量转移加速深度网络训练，Sergey Ioffe，Christian Szegedy，2015</li><li class="listitem">可视化和理解卷积网络，马修·D·泽勒，罗布·弗格斯，2013</li><li class="listitem">深入了解卷积，Christian Szegedy，，，Jia，Pierre Sermanet，Scott Reed，Dragomir Anguelov，Dumitru Erhan，Vincent Vanhoucke，Andrew Rabinovich，2014</li><li class="listitem"><a id="ch02lvl1sec31" class="calibre1"/>总结</li><li class="listitem">分类是机器学习中一个非常广泛的话题。它包括预测一个类或一个类别，正如我们在手写数字例子中所展示的。在<a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">第7章</a>、<em class="calibre12">用残差网络对图像进行分类</em>中，我们将看到如何对更广泛的自然图像和物体进行分类。</li></ul></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">分类可以应用于不同的问题，交叉熵/负对数似然是通过梯度下降解决这些问题的常用损失函数。对于回归(均方误差损失)或无监督联合学习(铰链损失)等问题，还有许多其他损失函数。</h1></div></div></div><p class="calibre8">在这一章中，我们一直使用一个非常简单的梯度下降规则，称为随机梯度下降，并提出了一些其他梯度下降变体(<code class="email">Momentum</code>、<code class="email">Nesterov</code>、<code class="email">RMSprop</code>、<code class="email">ADAM</code>、<code class="email">ADAGRAD</code>、<code class="email">ADADELTA</code>)。已经有一些关于二阶优化的研究，例如Hessian Free或K-FAC，其在深度或循环网络中提供了更好的结果，但是仍然复杂和昂贵，并且直到现在还没有被广泛采用。研究人员一直在寻找性能更好的新架构，而不需要这种优化技术。</p><p class="calibre8">在训练网络时，我强烈建议您使用以下两个Linux命令:</p><p class="calibre8"><strong class="calibre2">屏幕</strong>:要分离您的shell，请在服务器上运行脚本并稍后重新连接，因为培训通常需要几天时间。</p><p class="calibre8"><strong class="calibre2"> Tee </strong>:您将正在运行的程序的输出通过管道传输到这个接口，以便将显示的结果保存到一个文件中，同时继续在您的shell中可视化输出。这将使您的代码摆脱日志函数和框架的负担。</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Screen</strong>: To detach your shell, run scripts on the server and reconnect later, since training usually takes a few days.</li><li class="listitem"><strong class="calibre2">Tee</strong>: To which you pipe the output of your running program, in order to save the displayed results to a file, while continuing to visualize the output in your shell. This will spare your code the burden of log functions and frameworks.</li></ul></div></div></body></html>
</body></html>