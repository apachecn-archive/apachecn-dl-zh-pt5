<html><head/><body>
<html>
  <head>
    <title>Chapter 10. Predicting Times Sequences with Advanced RNN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div/><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>第十章。用高级RNN预测时间序列</h1></div></div></div><p class="calibre8">本章涵盖了递归神经网络的先进技术。</p><p class="calibre8">在<a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">第2章</a>、<em class="calibre12">中看到的使用前馈网络</em>对手写数字进行分类的技术，对于前馈网络来说，如使用更多层更深入，或添加一个漏失层，对于递归网络来说更具挑战性，需要一些新的设计原则。</p><p class="calibre8">由于添加新层会增加消失/爆炸梯度问题，一种基于身份连接的新技术对于<a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">第7章</a>、<em class="calibre12">使用残差网络对图像进行分类</em>已被证明可提供最先进的结果。</p><p class="calibre8">涵盖的主题有:</p><div><ul class="itemizedlist"><li class="listitem">变分RNN</li><li class="listitem">堆叠RNN</li><li class="listitem">深度过渡RNN</li><li class="listitem">公路连接及其在RNN的应用</li></ul></div></div></body></html>


<html>
  <head>
    <title>Chapter 10. Predicting Times Sequences with Advanced RNN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec88" class="calibre1"/>RNN辍学</h1></div></div></div><p class="calibre8">在神经网络内部应用dropout<a id="id382" class="calibre1"/>长期以来一直是一个研究课题，因为将dropout简单应用于递归连接会给训练RNN带来更多的不稳定性和困难。</p><p class="calibre8">一个解决方案<a id="id383" class="calibre1"/>已经被发现，它来自变分的<strong class="calibre2">贝叶斯网络</strong>理论。由此产生的想法非常简单，包括为RNN正在训练的整个序列保留相同的缺失掩码，如下图所示，并在每个新序列生成一个新的缺失掩码:</p><div><img src="img/00179.jpeg" alt="Dropout for RNN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这种技术被称为<strong class="calibre2">变分RNN。</strong>对于上图中具有相同箭头<a id="id384" class="calibre1"/>的连接，我们将保持所有序列的噪声屏蔽不变。</p><p class="calibre8">为此目的，我们将引入符号变量<code class="email">_is_training</code>和<code class="email">_noise_x</code>以在训练期间向输入、输出和循环连接添加随机(变化)噪声(丢失):</p><div><pre class="programlisting">_is_training <strong class="calibre2">=</strong> T.iscalar('is_training')
_noise_x <strong class="calibre2">=</strong> T.matrix('noise_x')
inputs <strong class="calibre2">=</strong> apply_dropout(_is_training, inputs, T.shape_padright(_noise_x.T))</pre></div></div></div></body></html>


<html>
  <head>
    <title>Deep approaches for RNN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">RNN的深层途径</h1></div></div></div><p class="calibre8">深度学习提高网络代表力的核心<a id="id386" class="calibre1"/>原理是增加更多层。对于RNN，增加层数的两种方法是可能的:</p><div><ul class="itemizedlist"><li class="listitem">The first one is known as <strong class="calibre2">stacking</strong> or <strong class="calibre2">stacked recurrent network</strong>, where the output of the hidden layer of a first recurrent net is used as input to a second recurrent net, and so on, with as many recurrent networks on top of each other:<div><img src="img/00180.jpeg" alt="Deep approaches for RNN" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">对于<a id="id387" class="calibre1"/>深度<em class="calibre12"> d </em>和<em class="calibre12"> T </em>时间步长，输入和输出之间的最大连接数<a id="id388" class="calibre1"/>为<em class="calibre12">d+T–1</em>:</p><div><ul class="itemizedlist"><li class="listitem">The second approach is the <strong class="calibre2">deep transition network</strong>, consisting of adding more layers to the recurrent connection:<div><img src="img/00181.jpeg" alt="Deep approaches for RNN" class="calibre9"/><div><p class="calibre29">图2</p></div></div><p class="calibre27">在这种情况下，输入和输出之间的最大连接数是<em class="calibre12"> d x T </em>，这已经被证明是强大了很多。</p></li></ul></div><p class="calibre8">这两种方法都提供了更好的结果。</p><p class="calibre8">然而，在第二种方法中，随着层数增加一个因子，训练变得更加复杂和不稳定，因为信号衰减或爆发得更快。我们稍后将通过解决循环高速公路<a id="id389" class="calibre1"/>连接的原则来解决这个问题。</p><p class="calibre8">首先，通常，单词序列被表示为词汇表中的索引值的数组，并且维数为(<code class="email">batch_size, num_steps</code>)，被嵌入到维数为(<code class="email">num_steps, batch_size, hidden_size</code>)的输入张量中:</p><p class="calibre8">符号输入变量<code class="email">_lr</code>能够在训练期间降低学习率:</p><div><pre class="programlisting">embedding <strong class="calibre2">=</strong> shared_uniform(( config.vocab_size,config.hidden_size), config.init_scale)
params <strong class="calibre2">=</strong> [embedding]
inputs <strong class="calibre2">=</strong> embedding[_input_data.T]</pre></div><p class="calibre8">让我们从第一种方法开始，堆叠循环网络。</p><div><pre class="programlisting">_lr <strong class="calibre2">=</strong> theano.shared(cast_floatX(config.learning_rate), 'lr')</pre></div><p class="calibre8"><a id="ch10lvl1sec90" class="calibre1"/>堆叠递归网络</p></div></body></html>


<html>
  <head>
    <title>Stacked recurrent networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">为了堆叠<a id="id390" class="calibre1"/>递归网络，我们将下面递归网络的隐藏层连接到前面递归网络的输入端:</h1></div></div></div><p class="calibre8">To stack <a id="id390" class="calibre1"/>recurrent networks, we connect the hidden layer of the following recurrent network, to the input of the preceding recurrent network:</p><div><img src="img/00182.jpeg" alt="Stacked recurrent networks" class="calibre9"/></div><p class="calibre10">当层数为1时，我们的实现是一个循环网络，如前一章所述。</p><p class="calibre8">首先，我们在简单的RNN模型中实现辍学:</p><p class="calibre8">我们在LSTM模型中做了同样的事情<a id="id391" class="calibre1"/>:</p><div><pre class="programlisting">
<strong class="calibre2">def</strong> model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias):
    noise_i_for_H = get_dropout_noise((batch_size, hidden_size), drop_i)
    i_for_H = apply_dropout(_is_training, inputs, noise_i_for_H)
    i_for_H = linear.model(i_for_H, params, hidden_size, 
                   hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias)

    # Dropout noise for recurrent hidden state.
    noise_s <strong class="calibre2">=</strong> get_dropout_noise((batch_size, hidden_size), drop_s)

    <strong class="calibre2">def </strong>step(i_for_H_t, y_tm1, noise_s):
        s_lm1_for_H = apply_dropout(_is_training,y_tm1, noise_s)
        <strong class="calibre2">return </strong>T.tanh(i_for_H_t + linear.model(s_lm1_for_H, 
                  params, hidden_size, hidden_size, init_scale))

    y_0 <strong class="calibre2">=</strong> shared_zeros((batch_size, hidden_size), <strong class="calibre2">name</strong>='h0')
    y, _ <strong class="calibre2">=</strong> theano.scan(step, <strong class="calibre2">sequences</strong>=i_for_H, <strong class="calibre2">outputs_info</strong>=[y_0], <strong class="calibre2">non_sequences</strong> = [noise_s])

    y_last <strong class="calibre2">=</strong> y[-1]
    sticky_state_updates <strong class="calibre2">=</strong> [(y_0, y_last)]

   <strong class="calibre2"> return</strong> y, y_0, sticky_state_updates</pre></div><p class="calibre8">运行我们的堆叠网络:</p><div><pre class="programlisting">
<strong class="calibre2">def </strong>model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias, tied_noise):
    noise_i_for_i = get_dropout_noise((batch_size, hidden_size), drop_i)
    noise_i_for_f = get_dropout_noise((batch_size, hidden_size), drop_i)<strong class="calibre2"> if not</strong> tied_noise <strong class="calibre2">else</strong> noise_i_for_i
    noise_i_for_c = get_dropout_noise((batch_size, hidden_size), drop_i) <strong class="calibre2">if not </strong>tied_noise <strong class="calibre2">else </strong>noise_i_for_i
    noise_i_for_o = get_dropout_noise((batch_size, hidden_size), drop_i) <strong class="calibre2">if not</strong> tied_noise <strong class="calibre2">else</strong> noise_i_for_i

    i_for_i = apply_dropout(_is_training, inputs, noise_i_for_i)
    i_for_f = apply_dropout(_is_training, inputs, noise_i_for_f)
    i_for_c = apply_dropout(_is_training, inputs, noise_i_for_c)
    i_for_o = apply_dropout(_is_training, inputs, noise_i_for_o)

    i_for_i = linear.model(i_for_i, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias)
    i_for_f = linear.model(i_for_f, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias)
    i_for_c = linear.model(i_for_c, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias)
    i_for_o = linear.model(i_for_o, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias)

    # Dropout noise for recurrent hidden state.
    noise_s = get_dropout_noise((batch_size, hidden_size), drop_s)
    <strong class="calibre2">if not</strong> tied_noise:
      noise_s = T.stack(noise_s, get_dropout_noise((batch_size, hidden_size), drop_s),
 get_dropout_noise((batch_size, hidden_size), drop_s), get_dropout_noise((batch_size, hidden_size), drop_s))


    <strong class="calibre2">def </strong>step(i_for_i_t,i_for_f_t,i_for_c_t,i_for_o_t, y_tm1, c_tm1, noise_s):
        noise_s_for_i = noise_s if tied_noise else noise_s[0]
        noise_s_for_f = noise_s if tied_noise else noise_s[1]
        noise_s_for_c = noise_s if tied_noise else noise_s[2]
        noise_s_for_o = noise_s if tied_noise else noise_s[3]

        s_lm1_for_i = apply_dropout(_is_training,y_tm1, noise_s_for_i)
        s_lm1_for_f = apply_dropout(_is_training,y_tm1, noise_s_for_f)
        s_lm1_for_c = apply_dropout(_is_training,y_tm1, noise_s_for_c)
        s_lm1_for_o = apply_dropout(_is_training,y_tm1, noise_s_for_o)

        i_t = T.nnet.sigmoid(i_for_i_t + linear.model(s_lm1_for_i, params, hidden_size, hidden_size, init_scale))
        f_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_f, params, hidden_size, hidden_size, init_scale))
        c_t = f_t * c_tm1 + i_t * T.tanh(i_for_c_t + linear.model(s_lm1_for_c, params, hidden_size, hidden_size, init_scale))
        o_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_o, params, hidden_size, hidden_size, init_scale))
        <strong class="calibre2">return </strong>o_t * T.tanh(c_t), c_t

    y_0 = shared_zeros((batch_size,hidden_size),<strong class="calibre2"> name</strong>='h0')
    c_0 = shared_zeros((batch_size,hidden_size), <strong class="calibre2">name</strong>='c0')
    [y, c], _ = theano.scan(step, <strong class="calibre2">sequences</strong>=[i_for_i,i_for_f,i_for_c,i_for_o], <strong class="calibre2">outputs_info</strong>=[y_0,c_0], <strong class="calibre2">non_sequences</strong> = [noise_s])

<strong class="calibre2">  </strong>  y_last = y[-1]
    sticky_state_updates = [(y_0, y_last)]

    <strong class="calibre2">return</strong> y, y_0, sticky_state_updates</pre></div><p class="calibre8">我们得到15，203，150个RNN参数，在CPU上每秒326 <strong class="calibre2">字</strong> (WPS)，在GPU上每秒4，806 WPS。</p><div><pre class="programlisting">
<strong class="calibre2">python</strong> train_stacked.py --model=<strong class="calibre2">rnn</strong>
<strong class="calibre2">python</strong> train_stacked.py --model=<strong class="calibre2">lstm</strong>
</pre></div><p class="calibre8">对于LSTM，在GPU上速度为1，445 WPS的情况下，参数数量为35，882，600。</p><p class="calibre8">正如我们可能想象的那样，叠加RNN并不收敛:消失/爆炸梯度问题随着深度而增加。</p><p class="calibre8">旨在减少此类问题的LSTM在堆叠时比单层时收敛得更好。</p><p class="calibre8"><a id="ch10lvl1sec91" class="calibre1"/>深度跃迁轮回网络</p></div></body></html>


<html>
  <head>
    <title>Deep transition recurrent network</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">与<a id="id393" class="calibre1"/>堆叠递归网络相反，深度过渡递归网络包括通过在递归连接内部添加更多层或<em class="calibre12">微时间步长</em>来增加网络沿时间方向的深度。</h1></div></div></div><p class="calibre8">为了说明这一点，让我们回到循环网络中的转换/循环连接的定义:它将先前的状态<img src="img/00183.jpeg" alt="Deep transition recurrent network" class="calibre23"/>和在时间步长<em class="calibre12"> t </em>的输入数据<img src="img/00184.jpeg" alt="Deep transition recurrent network" class="calibre23"/>作为输入，以预测它的新状态<img src="img/00185.jpeg" alt="Deep transition recurrent network" class="calibre23"/>。</p><p class="calibre8">在深度转换递归网络(图2)中，递归转换由多个层构成，直到递归深度<em class="calibre12"> L </em>:初始状态设置为最后一个转换的输出:</p><p class="calibre8">In a deep transition recurrent network (figure 2), the recurrent transition is developed with more than one layer, up to a recurrency depth <em class="calibre12">L</em>: the initial state is set to the output of the last transition:</p><div><img src="img/00186.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10">此外，在转换内部，计算多个状态或步骤:</p><p class="calibre8">Furthermore, inside the transition, multiple states or steps are computed:</p><div><img src="img/00187.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10"><a id="id394" class="calibre1"/>最终状态是跃迁的输出:</p><p class="calibre8">The <a id="id394" class="calibre1"/>final state is the output of the transition:</p><div><img src="img/00188.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10"><a id="ch10lvl1sec92" class="calibre1"/>公路网设计原则</p></div></body></html>


<html>
  <head>
    <title>Highway networks design principle</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">在过渡连接中增加更多的层增加了在长期依赖性中反向传播期间的消失或爆炸梯度问题。</h1></div></div></div><p class="calibre8">在<a id="id396" class="calibre1"/> <a class="calibre1" title="Chapter 4. Generating Text with a Recurrent Neural Net" href="part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b">第四章</a>、<em class="calibre12">用递归神经网络生成文本</em>中，LSTM和GRU网络已经被引入作为解决这个问题的解决方案。二阶优化技术也有助于克服这个问题。</p><p class="calibre8">基于<strong class="calibre2">身份连接</strong>的更一般的原理，以改进深度网络中的训练<a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">第7章</a>、<em class="calibre12">用剩余网络对图像进行分类</em>，也可以应用于深度过渡网络。</p><p class="calibre8">理论上的原则是:</p><p class="calibre8">给定一个输入<em class="calibre12"> x </em>到一个隐藏层<em class="calibre12"> H </em>带权重<img src="img/00189.jpeg" alt="Highway networks design principle" class="calibre23"/>:</p><p class="calibre8">Given an input <em class="calibre12">x</em> to a hidden layer <em class="calibre12">H</em> with weigh <img src="img/00189.jpeg" alt="Highway networks design principle" class="calibre23"/>:</p><div><img src="img/00190.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10">作为一种快捷方式，公路网设计包括将原始输入信息(带有标识图层)添加到图层或图层组的输出中:</p><p class="calibre8"><em class="calibre12"> y = x </em></p><p class="calibre8">两个混合门，<em class="calibre12">变换门</em> <img src="img/00191.jpeg" alt="Highway networks design principle" class="calibre23"/>和<em class="calibre12">进位门</em>，<img src="img/00192.jpeg" alt="Highway networks design principle" class="calibre23"/>学习调制隐藏层中变换的影响，以及允许通过的原始信息量:</p><p class="calibre8">Two mixing gates, the <em class="calibre12">transform gate</em> <img src="img/00191.jpeg" alt="Highway networks design principle" class="calibre23"/> and the <em class="calibre12">carry gate</em>, <img src="img/00192.jpeg" alt="Highway networks design principle" class="calibre23"/> learn to modulate the influence of the transformation in the hidden layer, and the amount of original information to allow to pass through:</p><div><img src="img/00193.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10">通常，为了<a id="id397" class="calibre1"/>减少参数总数以获得更快的训练网络，进位门被认为是变换门的1的补充:</p><p class="calibre8">Usually, to <a id="id397" class="calibre1"/>reduce the total number of parameters in order to get faster-to-train networks, the carry gate is taken as the complementary to 1 for the transform gate:</p><div><img src="img/00194.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10"><a id="ch10lvl1sec93" class="calibre1"/>循环公路网</p></div></body></html>


<html>
  <head>
    <title>Recurrent Highway Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">因此，让我们<a id="id398" class="calibre1"/>将公路网设计应用于深度转换循环网络，这导致了<strong class="calibre2">循环公路网</strong> ( <strong class="calibre2"> RHN </strong>)的定义，并在给定<img src="img/00183.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>转换输入的情况下预测输出<img src="img/00185.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>:</h1></div></div></div><p class="calibre8">So, let's <a id="id398" class="calibre1"/>apply the highway network design to deep transition recurrent networks, which leads to the definition of <strong class="calibre2">Recurrent Highway Networks</strong> (<strong class="calibre2">RHN</strong>), and predict the output <img src="img/00185.jpeg" alt="Recurrent Highway Networks" class="calibre23"/> given <img src="img/00183.jpeg" alt="Recurrent Highway Networks" class="calibre23"/> the input of the transition:</p><div><img src="img/00186.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10">过渡由多级高速公路连接构成:</p><p class="calibre8">The transition is built with multiple steps of highway connections:</p><div><img src="img/00195.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><div><img src="img/00188.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10">这里的转换门如下:</p><p class="calibre8">Here the transform gate is as follows:</p><div><img src="img/00196.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10">为了减少权重的数量，进位门与转换门互补:</p><p class="calibre8">And, to reduce the number of weights, the carry gate is taken as the complementary to the transform gate:</p><div><img src="img/00194.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10">为了在GPU上进行更快的计算，最好在单个大矩阵乘法中计算不同时间步长<img src="img/00197.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>和<img src="img/00198.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>上的输入的线性变换，所有步长的输入矩阵<img src="img/00199.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>和<img src="img/00200.jpeg" alt="Recurrent Highway Networks" class="calibre23"/>同时进行，因为GPU将使用更好的<a id="id399" class="calibre1"/>并行化，并将这些输入提供给递归:</p><p class="calibre8">每一步之间有一个深度过渡:</p><div><pre class="programlisting">y_0 = shared_zeros((batch_size, hidden_size))
y, _ = theano.scan(deep_step_fn, <strong class="calibre2">sequences</strong> = [<strong class="calibre2">i_for_H</strong>, i_for_T],
            <strong class="calibre2">outputs_info</strong> = [y_0], <strong class="calibre2">non_sequences</strong> = <strong class="calibre2">[noise_s]</strong>)</pre></div><p class="calibre8">RHN的递归<a id="id400" class="calibre1"/>隐藏状态是粘性的(一批的最后一个隐藏状态被结转到下一批，作为初始隐藏状态)。这些状态保存在一个共享变量中。</p><div><pre class="programlisting">
<strong class="calibre2">def </strong>deep_step_fn(i_for_H_t, i_for_T_t, y_tm1, noise_s):
  s_lm1 = y_tm1
  <strong class="calibre2">for</strong> l <strong class="calibre2">in</strong> range(transition_depth):
    <strong class="calibre2">if</strong> l <strong class="calibre2">==</strong> 0:
      H = T.tanh(i_for_H_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))
      Tr = T.nnet.sigmoid(i_for_T_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))
    <strong class="calibre2">else</strong>:
      H = T.tanh(linear(s_lm1, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_H_bias))
      Tr = T.nnet.sigmoid(linear(s_lm1, params, hidden_size, hidden_size, init_scale, <strong class="calibre2">bias_init</strong>=init_T_bias))
    s_l = H <strong class="calibre2">*</strong> Tr + s_lm1 <strong class="calibre2">*</strong> ( 1 - Tr )
    s_lm1 = s_l
  y_t = s_l
  <strong class="calibre2">return</strong> y_t</pre></div><p class="calibre8">让我们运行模式:</p><p class="calibre8">堆栈RHN的参数数量为<em class="calibre12">84172000</em>，其速度为<em class="calibre12"> 420 </em> wps。</p><div><pre class="programlisting">
<strong class="calibre2">python</strong> train_stacked.py</pre></div><p class="calibre8">该模型是文本上递归神经网络准确性的最新模型。</p><p class="calibre8"><a id="ch10lvl1sec94" class="calibre1"/>延伸阅读</p></div></body></html>


<html>
  <head>
    <title>Further reading</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">您可以参考以下主题了解更多信息:</h1></div></div></div><p class="calibre8"><em class="calibre12">嗨</em>嗨<em class="calibre12">高速公路网</em>在:<a class="calibre1" href="https://arxiv.org/abs/1505.00387">https://arxiv.org/abs/1505.00387</a></p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre12">https://arxiv.org/abs/1508.03790的深度门控LSTM </em></li><li class="listitem"><em class="calibre12">在递归神经网络</em> <em class="calibre12">中学习更长的记忆</em>在:<a class="calibre1" href="https://arxiv.org/abs/1412.7753">https://arxiv.org/abs/1412.7753</a></li><li class="listitem"><em class="calibre12">格长短期记忆</em>，纳尔·卡尔·布伦纳，伊沃·达尼埃尔卡，亚历克斯·格雷夫斯</li><li class="listitem">Zilly，J，Srivastava，R，Koutnik，J，Schmidhuber，J .，<em class="calibre12">循环公路网</em>，2016</li><li class="listitem">Gal，Y，<em class="calibre12">递归神经网络中辍学的一个理论上的落地应用</em>，2015。</li><li class="listitem">Zaremba，W，Sutskever，I，Vinyals，O，<em class="calibre12">递归神经网络正则化</em>，2014。</li><li class="listitem">Press，O，Wolf，L，<em class="calibre12">利用输出嵌入改进语言模型</em>，2016。</li><li class="listitem">门控反馈递归神经网络:钟俊英，卡格拉尔·古尔切雷，赵京贤，Yoshua Bengio 2015</li><li class="listitem">发条RNN:2014年扬·库特尼克、克劳斯·格雷夫、福斯蒂诺·戈麦斯、于尔根·施密德胡伯</li><li class="listitem"><a id="ch10lvl1sec95" class="calibre1"/>总结</li></ul></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">用于提高网络鲁棒性的经典丢弃方法可以按顺序或分批应用于递归网络，以避免递归转换的不稳定性和破坏。例如，当应用于单词输入/输出时，它相当于从句子中删除相同的单词，用空白值替换它们。</h1></div></div></div><p class="calibre8">深度学习中通过堆叠层来提高精度的原理适用于可以在深度方向无负担堆叠的递归网络。</p><p class="calibre8">在循环网络的过渡中应用相同的原理增加了消失/爆炸问题，但是通过具有身份连接的高速公路网络的发明来抵消。</p><p class="calibre8">递归神经网络的先进技术在序列预测中给出了最先进的结果。</p><p class="calibre8">Advanced techniques for recurrent neural nets give state-of-the-art results in sequence prediction.</p></div></body></html>
</body></html>