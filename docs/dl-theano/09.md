

# 第九章。用注意机制选择相关的输入或记忆

本章介绍了关注神经网络性能的机制，并使网络能够通过关注其输入或记忆的相关部分来提高其性能。

有了这样一种机制，翻译、注释、解释和分段，如前一章所见，就有了更高的准确性。

神经网络的输入和输出也可以连接到外部存储器的*读*和*写*。这些网络，即**存储网络**，通过外部存储器得到增强，能够决定从哪里存储或检索什么信息。

在本章中，我们将讨论:

*   注意的机制
*   对齐翻译
*   图像中的焦点
*   神经图灵机
*   内存网络
*   动态存储网络



# 可区分的注意机制

当翻译句子、描述图像内容、注释句子或转录音频时，在移动到下一部分之前，为了全局理解，在某个顺序下，一次专注于输入句子或图像的一个部分，以获得块的意义并对其进行转换，这听起来是很自然的。

例如，在德语中，在某些条件下，动词出现在句子的末尾，因此，当翻译成英语时，一旦主题已经被阅读和翻译，一个好的机器翻译神经网络可以将其焦点移动到句子的末尾，以找到动词并将其翻译成英语。通过*注意机制*，将输入位置与当前输出预测相匹配的过程是可能的。

首先，让我们回到使用 softmax 层设计的分类网络(参见[第 2 章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 2. Classifying Handwritten Digits with a Feedforward Network")、*使用前馈网络*对手写数字进行分类)，它输出一个非负权重向量![Differentiable mechanism of attention](img/00130.jpeg)，在给定输入 X:

![Differentiable mechanism of attention](img/00131.jpeg)

然后:

![Differentiable mechanism of attention](img/00132.jpeg)

分类的目的是对于正确的类别 *k* 使![Differentiable mechanism of attention](img/00133.jpeg)尽可能接近 *1* ，而对于其他类别则接近零。

但是![Differentiable mechanism of attention](img/00133.jpeg)是一个概率分布，也可以作为一个权重向量来关注位置 *k* 处内存向量![Differentiable mechanism of attention](img/00134.jpeg)的一些值:

![Differentiable mechanism of attention](img/00135.jpeg)

如果重量集中在位置 *k* ，则返回![Differentiable mechanism of attention](img/00136.jpeg)。根据权重的清晰度，输出或多或少会有些模糊。

这种在特定位置寻址矢量 *m* 值的机制是一种**注意机制**:也就是说，它是线性的、可微分的，并且具有用于特定任务训练的反向传播梯度下降。



## 具有注意力机制的更好翻译

注意力机制的应用非常大。为了更好的理解，我们先用机器翻译的例子来说明一下。注意机制将源句子和目标句子对齐(预测翻译)，并避免长句的翻译质量下降:

![Better translations with attention mechanism](img/00137.jpeg)

在前一章中，我们通过编码器-解码器框架和编码器向解码器提供的固定长度编码向量 *c* 解决了机器翻译问题。利用注意机制，如果编码循环网络的每一步都产生隐藏状态*h*I，则在每个解码时间步 *t* 提供给解码器的向量将是可变的，并且由下式给出:

![Better translations with attention mechanism](img/00138.jpeg)

使用![Better translations with attention mechanism](img/00139.jpeg)由 softmax 函数产生的校准系数:

![Better translations with attention mechanism](img/00140.jpeg)

根据解码器的先前隐藏状态![Better translations with attention mechanism](img/00141.jpeg)和编码隐藏状态![Better translations with attention mechanism](img/00142.jpeg)的，先前解码器隐藏状态和每个编码器隐藏状态之间的嵌入点积产生一个描述它们应该如何匹配的权重:

![Better translations with attention mechanism](img/00143.jpeg)

经过几个时期的训练后，该模型通过关注输入的一部分来预测每个下一个单词:

![Better translations with attention mechanism](img/00144.jpeg)

为了学习更好地对齐，可以使用数据集中存在的对齐注释，并为注意力机制产生的权重添加交叉熵损失，以在训练的第一个时期使用。



## 使用注意机制更好地注释图像

同样的注意力机制也可以应用到注释图像或转录音频的任务中。

对于图像，注意机制在每个预测时间步集中于特征的相关部分:

![Better annotate images with attention mechanism](img/00145.jpeg)

展示、参与和讲述:视觉注意下的神经图像字幕生成

让我们来看看训练有素的模特在图像上的注意点:

![Better annotate images with attention mechanism](img/00146.jpeg)

(*展示、参与和讲述:视觉注意下的神经图像字幕生成*，Kelvin Xu 等人，2015)



# 在神经图灵机中存储和检索信息

注意机制可以用来访问记忆增强网络中的一部分记忆。

神经图灵机中的记忆概念受到了神经科学和计算机硬件的启发。

存储信息的 RNN 隐藏状态不能存储足够大量的数据并检索它，即使当 RNN 增加了存储单元时，例如在 LSTM 的情况下。

为了解决这个问题，**神经图灵机** ( **NTM** )首先被设计成带有**外部存储库**和读/写头，同时保留了通过梯度下降进行训练的魔力。

读取内存条是通过关注变量内存条给出的，就像之前例子中关注输入一样:

![Store and retrieve information in Neural Turing Machines](img/00147.jpeg)

这可以用下面的方式来说明:

![Store and retrieve information in Neural Turing Machines](img/00148.jpeg)

而向存储体写入值包括将新值分配给部分存储器，这要归功于另一种注意机制:

![Store and retrieve information in Neural Turing Machines](img/00149.jpeg)![Store and retrieve information in Neural Turing Machines](img/00150.jpeg)

描述要存储的信息，和![Store and retrieve information in Neural Turing Machines](img/00151.jpeg)要擦除的信息，和分别是存储体的大小:

![Store and retrieve information in Neural Turing Machines](img/00152.jpeg)

读写头设计为硬盘，其移动性由注意力权重![Store and retrieve information in Neural Turing Machines](img/00153.jpeg)和![Store and retrieve information in Neural Turing Machines](img/00154.jpeg)决定。

记忆![Store and retrieve information in Neural Turing Machines](img/00155.jpeg)会像 LSTM 的细胞记忆一样在每个时间步进化；但是，由于内存库被设计得很大，网络倾向于在每个时间步长存储和组织输入的数据，与任何经典的 RNN 相比，干扰更少。

使用记忆的过程自然是由循环神经网络驱动的，该神经网络在每个时间步充当**控制器**:

![Store and retrieve information in Neural Turing Machines](img/00156.jpeg)

控制器网络在每个时间步的输出:

*   每个写/读磁头的定位或注意系数
*   为写磁头存储或擦除的值

原 NTM 提出两种方法定义*头定位*，也称为*寻址*，由权重![Store and retrieve information in Neural Turing Machines](img/00157.jpeg)定义:

*   A content-based positioning, to place similar content in the same area of the memory, which is useful for retrieval, sorting or counting tasks:![Store and retrieve information in Neural Turing Machines](img/00158.jpeg)
*   一种基于位置的定位，它基于磁头的先前位置，可用于复印任务。门![Store and retrieve information in Neural Turing Machines](img/00159.jpeg)定义了先前的权重对新生成的权重的影响，以计算头部的位置。移位权重![Store and retrieve information in Neural Turing Machines](img/00160.jpeg)定义了相对于该位置从该位置平移的量。

最后，锐化权重![Store and retrieve information in Neural Turing Machines](img/00161.jpeg)减少了头部位置的模糊:

![Store and retrieve information in Neural Turing Machines](img/00162.jpeg)![Store and retrieve information in Neural Turing Machines](img/00163.jpeg)

所有的操作都是可微分的。

许多两个以上的读取头是可能的，特别是对于诸如两个存储值相加的任务，单个读取头可能会受到限制。

在检索输入序列中的下一项、多次重复输入序列或从分布中抽样等任务中，这些 NTM 已经显示出比 LSTM 更好的能力。



# 记忆网络

给定一些事实或故事来回答问题或解决问题导致了一种新型网络的设计，即记忆网络。在这种情况下，事实或故事被嵌入到记忆库中，就好像它们是输入一样。为了解决需要对事实进行排序或在事实之间建立转换的任务，记忆网络在记忆库的多个步骤或跳跃中使用循环推理过程。

首先，查询或问题 *q* 被转换成常量输入嵌入:

![Memory networks](img/00164.jpeg)

而在推理的每一步，回答问题的事实 *X* 被嵌入到两个存储体中，其中嵌入系数是时间步长的函数:

![Memory networks](img/00165.jpeg)

要计算注意力权重:

![Memory networks](img/00166.jpeg)

并且:

![Memory networks](img/00167.jpeg)

注意选择:

![Memory networks](img/00168.jpeg)

如前所述，每个推理时间步的输出随后与同一性连接相结合，以提高递归的效率:

![Memory networks](img/00169.jpeg)

一个线性层和分类 softmax 层被添加到最后的![Memory networks](img/00170.jpeg):

A linear layer and classification softmax layer are added to the last ![Memory networks](img/00170.jpeg):

![Memory networks](img/00171.jpeg)

情景记忆与动态记忆网络



## 另一个设计已经引入了动态存储网络。首先，N 个事实用分隔符标记连接起来，然后用 RNN 编码:RNN 在每个分隔符![Episodic memory with dynamic memory networks](img/00172.jpeg) 处的输出被用作输入嵌入。这种对事实进行编码的方式更加自然，也保留了时间依赖性。这个问题也用 RNN 编码以产生一个向量 *q* 。

第二，依靠与 RNN 混合的注意机制，用情节记忆代替记忆库，以便也保持事实之间的时间依赖性:

Secondly, the memory bank is replaced with an episodic memory, relying on an attention mechanism mixed with an RNN, in order to preserve time dependency between the facts as well:

![Episodic memory with dynamic memory networks](img/00173.jpeg)

门![Episodic memory with dynamic memory networks](img/00174.jpeg)由多层感知器根据推理![Episodic memory with dynamic memory networks](img/00175.jpeg)的先前状态、问题和作为输入嵌入的输入![Episodic memory with dynamic memory networks](img/00176.jpeg)给出。

RNN 的推理方式相同:

The reasoning occurs the same way with a RNN:

![Episodic memory with dynamic memory networks](img/00177.jpeg)

下图说明了计算情景记忆时输入和输出之间的相互作用:

问我任何问题:自然语言处理的动态记忆网络

![Episodic memory with dynamic memory networks](img/00178.jpeg)

Ask Me Anything: dynamic memory networks for natural language processing

为了对这些网络进行基准测试，脸书研究所综合了 bAbI 数据集，使用 NLP 工具为一些随机建模的故事创建事实、问题和答案。数据集由不同的任务组成，以测试不同的推理技能，例如对一个、两个或三个事实、时间、大小或位置进行推理，计数、列出或理解论据、否定、动机和寻找路径之间的关系。

至于机器翻译中的引导对齐，当数据集还包含导致答案的事实的注释时，也可以使用监督训练来:

注意机制

*   当使用的事实数量足以回答问题时，何时停止推理循环，产生停止标记
*   延伸阅读



# 您可以参考以下主题，了解更多信息:

*问我任何问题:自然语言处理的动态记忆网络*，安基特·库马尔，奥赞·伊尔索伊，彼得·翁德鲁斯卡，莫希特·伊耶，詹姆斯·布拉德伯里，伊桑·古尔拉贾尼，维克多·钟，罗曼·保卢斯，理查德·索彻，2015

*   *注意力和增强循环神经网络*，克里斯·奥拉赫，山·卡特，2016 年 9 月【http://distill.pub/2016/augmented-rnns/ 
*   *主题感知神经机器翻译的引导对齐训练*，陈，Evgeny Matusov，Shahram Khadivi，Jan-Thorsten Peter，2016 年 7 月
*   *展示、参与和讲述:视觉注意力的神经图像字幕生成*，Kelvin Xu，Jimmy Ba，Ryan Kiros，Kyunghyun Cho，Aaron，Ruslan Salakhutdinov，Richard Zemel，Yoshua Bengio，Fev 2015
*   *走向人工智能-完整的问题回答:一组先决玩具任务*，杰森·韦斯顿、安托万·博德斯、苏米特·乔普拉、亚历山大·m·拉什、巴特·范·梅林博尔、阿曼德·朱林、托马斯·米科洛夫，2015
*   *记忆网络*，杰森·韦斯顿，苏米特·乔普拉，安托万·博德斯，2014
*   *端到端存储网络*，Sainbayar Sukhbaatar，Arthur Szlam，Jason Weston，Rob Fergus，2015
*   *神经图灵机*，亚历克斯·格雷夫斯，格雷格·韦恩，伊沃·达尼埃尔卡，2014
*   *用于生成图像描述的深度视觉语义对齐*，安德烈·卡帕西，李菲菲，2014
*   摘要



# 注意机制是一个聪明的选择，可以帮助神经网络选择正确的信息，并集中精力产生正确的输出。它可以直接放置在输入或要素上(由几个图层处理的输入)。在翻译、图像注释和语音识别的情况下，精确度增加了，特别是当输入的维度很重要时。

注意力机制导致了新型的网络，通过外部存储器增强，作为输入/输出，从外部存储器读取数据或向外部存储器写入数据。这些网络已被证明在问答挑战中非常强大，自然语言处理中的大多数任务都可以被转换为:标记、分类、序列到序列或问答任务。

在下一章，我们将看到更先进的技术以及它们在更一般的循环神经网络中的应用，以提高精确度。

In the next chapter, we'll see more advanced techniques and their application to the more general case of recurrent neural networks, to improve accuracy.