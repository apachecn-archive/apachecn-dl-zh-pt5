<html><head/><body>
<html>
  <head>
    <title>Chapter 12. Learning Features with Unsupervised Generative Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div/><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch12" class="calibre1"/>第十二章。无监督生成网络学习特征</h1></div></div></div><p class="calibre8">本章重点介绍一种新的模型，生成模型，包括<strong class="calibre2">受限玻尔兹曼机</strong>、<strong class="calibre2">深度信念网络</strong>、<strong class="calibre2">变分自动编码器</strong>、<strong class="calibre2">自回归模型，以及生成对抗</strong>网络。对于第一个网络，我们将介绍限制在理论上，而最后一个用实际代码和建议详细解释。</p><p class="calibre8">这些<a id="id439" class="calibre1"/>网不需要训练任何标签，称为<em class="calibre12">无监督学习</em>。无监督学习帮助从数据中计算特征<a id="id440" class="calibre1"/>，没有标签的偏见。这些<a id="id441" class="calibre1"/>模型是可生成的，因为它们被训练<a id="id442" class="calibre1"/>以生成听起来真实的新数据。</p><p class="calibre8">将涵盖以下<a id="id443" class="calibre1"/>点:</p><div><ul class="itemizedlist"><li class="listitem">生成模型</li><li class="listitem">无监督学习</li><li class="listitem">受限玻尔兹曼机器</li><li class="listitem">深度信念网络</li><li class="listitem">生成对抗模型</li><li class="listitem">半监督学习</li></ul></div></div></body></html>


<html>
  <head>
    <title>Chapter 12. Learning Features with Unsupervised Generative Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch12lvl1sec104" class="calibre1"/>生成模型</h1></div></div></div><p class="calibre8">神经处理中的生成式<a id="id444" class="calibre1"/>模型是在给定噪声向量<em class="calibre12"> z </em>作为输入的情况下生成数据的模型:</p><div><img src="img/00233.jpeg" alt="Generative models" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">训练的目的是找到生成尽可能接近真实数据的数据的参数。</p><p class="calibre8">生成网络的应用<a id="id445" class="calibre1"/>包括数据降维、合成数据生成、无监督特征学习、预训练/训练效率。预训练有助于泛化，因为预训练侧重于数据中的模式，而不是数据-标签关系。</p></div></div></body></html>


<html>
  <head>
    <title>Chapter 12. Learning Features with Unsupervised Generative Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch12lvl2sec28" class="calibre1"/>受限玻尔兹曼机器</h2></div></div></div><p class="calibre8">受限<a id="id446" class="calibre1"/>玻尔兹曼机<a id="id447" class="calibre1"/>是最简单的生成网络，由一个全连通的隐含层组成，如图所示:</p><div><img src="img/00234.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">完整的玻尔兹曼机器也有隐藏到隐藏和可见到可见的回路连接，而<em class="calibre12">受限</em>版本没有任何连接。</p><p class="calibre8">在一般情况下，RBM被定义为<em class="calibre12">基于能量的模型</em>，这意味着它们通过能量函数来定义概率分布:</p><div><img src="img/00235.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><div><img src="img/00236.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><em class="calibre12"> Z </em>是<strong class="calibre2">配分函数</strong>，<em class="calibre12"> E(v) </em>是<strong class="calibre2">自由能</strong>函数(不依赖于隐藏状态)。</p><div><h3 class="title2"><a id="note06" class="calibre1"/>注意</h3><p class="calibre8">最小化负对数似然相当于最小化能量函数。</p></div><p class="calibre8"><a id="id448" class="calibre1"/> RBM将能量函数<a id="id449" class="calibre1"/>定义为模型参数的线性:</p><div><img src="img/00237.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><div><img src="img/00238.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">能量和自由能之间的关系由下式给出:</p><div><img src="img/00239.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">就RBM而言:</p><div><img src="img/00240.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里<img src="img/00241.jpeg" alt="Restricted Boltzmann Machines" class="calibre23"/>表示第I个隐藏神经元的可能值之和。</p><p class="calibre8">RBM通常是在<code class="email">v</code>和<code class="email">h</code>是<em class="calibre12"> {0，1} </em>中的二项式值的特殊情况下考虑的，所以:</p><div><img src="img/00242.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"><a id="id450" class="calibre1"/>模型是对称的，遵循<a id="id451" class="calibre1"/>模型中的对称性:隐藏和可见在能量函数中具有相同的位置:</p><div><img src="img/00243.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">RBM在两个方向上(从输入到隐藏，以及从隐藏到输入)都是一个简单的随机全连接层。</p><p class="calibre8">RBM负对数似然的<a id="id452" class="calibre1"/>梯度或导数有两项，定义为<strong class="calibre2">正相位和负相位</strong>，其中第一项增加数据的概率，第二项减少生成样本的概率:</p><div><img src="img/00244.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，总和是对所有可能输入<img src="img/00245.jpeg" alt="Restricted Boltzmann Machines" class="calibre23"/>的概率(期望值)加权。至少，我们数据样本自由能的任何增加都会降低总数据的期望值。</p><p class="calibre8">根据经验，这种负相位的和可以转换成观察到的<em class="calibre12"> N </em>的和(<em class="calibre12"> v，h </em>):</p><div><img src="img/00246.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">为了在实践中计算这样的和，观察样本的概率(<em class="calibre12"> v，h </em>)必须满足由上述公式给出的<em class="calibre12"> p(v | h) </em>以及<em class="calibre12"> p(h | v) </em>。</p><p class="calibre8">采样<a id="id454" class="calibre1"/>是通过对比散度算法进行的，实际上:<em class="calibre12"> v </em>是从数据集中采样的，而<em class="calibre12"> h </em>是根据上面给出的<em class="calibre12"> v </em>的分布绘制的。重复此操作，产生一个新的<em class="calibre12"> v </em>给定<em class="calibre12"> h </em>，然后产生一个新的<em class="calibre12"> h </em>给定<em class="calibre12"> v </em>。在实践中，这足以实现样本紧密分布<a id="id455" class="calibre1"/>到真实分布。这些针对<em class="calibre12"> v </em>和<em class="calibre12"> h </em>的观测样本被称为<strong class="calibre2">负粒子</strong>，代价函数中的第二项降低了这些生成样本的概率，而第一项增加了数据的概率。</p><p class="calibre8">下面是带负粒子的配分函数的计算结果:</p><div><pre class="programlisting">W = shared_glorot_uniform((n_visible, n_hidden), <strong class="calibre2">name</strong>='<strong class="calibre2">W</strong>')
hbias = shared_zeros(n_hidden, <strong class="calibre2">name</strong>='<strong class="calibre2">hbias'</strong>)
vbias = shared_zeros(n_visible, <strong class="calibre2">name</strong>=<strong class="calibre2">'vbias'</strong>)
params = [W, hbias, vbias]

<strong class="calibre2">def</strong> sample_h_given_v(v0_sample):
    pre_sigmoid_h1 = T.dot(v0_sample, W) + hbias
    h1_mean = T.nnet.sigmoid(pre_sigmoid_h1)
    h1_sample = theano_rng.binomial(<strong class="calibre2">size</strong>=h1_mean.shape, <strong class="calibre2">n</strong>=1, <strong class="calibre2">p</strong>=h1_mean, <strong class="calibre2">dtype</strong>=theano.config.floatX)
    <strong class="calibre2">return</strong> [pre_sigmoid_h1, h1_mean, h1_sample]

<strong class="calibre2">def</strong> sample_v_given_h(h0_sample):
    pre_sigmoid_v1 = T.dot(h0_sample, W.T) + vbias
    v1_mean = T.nnet.sigmoid(pre_sigmoid_v1)
    v1_sample = theano_rng.binomial(<strong class="calibre2">size</strong>=<strong class="calibre2">v1_mean.shape</strong>, <strong class="calibre2">n</strong>=<strong class="calibre2">1</strong>, <strong class="calibre2">p=</strong>v1_mean, <strong class="calibre2">dtype</strong>=<strong class="calibre2">theano.config.floatX</strong>)
    <strong class="calibre2">return</strong> [pre_sigmoid_v1, v1_mean, v1_sample]

<strong class="calibre2">def</strong> gibbs_hvh(h0_sample):
    pre_sigmoid_v1, v1_mean, <strong class="calibre2">v1_sample</strong> = sample_v_given_h(h0_sample)
    pre_sigmoid_h1, h1_mean, <strong class="calibre2">h1_sample</strong> = sample_h_given_v(v1_sample)
    <strong class="calibre2">return</strong> [pre_sigmoid_v1, v1_mean, v1_sample,
            pre_sigmoid_h1, h1_mean, h1_sample]

chain_start = persistent_chain
(
    [
        pre_sigmoid_nvs,
        nv_means,
        nv_samples,
        pre_sigmoid_nhs,
        nh_means,
        nh_samples
    ],
    updates
) = theano.scan(
    gibbs_hvh,
    <strong class="calibre2">outputs_info</strong>=[None, None, None, None, None, chain_start],
    <strong class="calibre2">n_steps</strong>=k,
    <strong class="calibre2">name</strong>="<strong class="calibre2">gibbs_hvh</strong>"
)

chain_end = nv_samples[-1]

<strong class="calibre2">def</strong> free_energy(v_sample):
    wx_b = T.dot(v_sample, W) + hbias
    vbias_term = T.dot(v_sample, vbias)
    hidden_term = T.sum(T.log(1 + T.exp(wx_b)), <strong class="calibre2">axis</strong>=1)
    <strong class="calibre2">return</strong> -hidden_term - vbias_term

cost = T.mean(free_energy(x)) - T.mean(free_energy(chain_end))</pre></div><p class="calibre8">在15个时期之后，在MNIST数据集上训练<a id="id457" class="calibre1"/>的滤波器的<a id="id456" class="calibre1"/>图片:</p><div><img src="img/00247.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">和一个<a id="id458" class="calibre1"/>小型批次<a id="id459" class="calibre1"/>的负粒子(每行之间1000步采样):</p><div><img src="img/00248.jpeg" alt="Restricted Boltzmann Machines" class="calibre9"/></div><p class="calibre10"> </p></div></div></div></body></html>


<html>
  <head>
    <title>Chapter 12. Learning Features with Unsupervised Generative Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch12lvl2sec29" class="calibre1"/>深深的信仰赌注</h2></div></div></div><p class="calibre8">一个<strong class="calibre2">深度信念网络</strong> ( <strong class="calibre2"> DBN </strong>)是<a id="id460" class="calibre1"/>多个RBM的堆叠，以增加它们的代表性和更好地捕捉数据中的模式。</p><p class="calibre8"><a id="id461" class="calibre1"/>训练逐层发生，首先考虑只有一个RBM具有隐藏状态<img src="img/00249.jpeg" alt="Deep belief bets" class="calibre23"/>。一旦RBM的权重被训练，这些<a id="id462" class="calibre1"/>权重保持固定，第一个RBM <img src="img/00249.jpeg" alt="Deep belief bets" class="calibre23"/>的隐藏层被认为是第二个RBM的可见层，具有一个隐藏状态<img src="img/00250.jpeg" alt="Deep belief bets" class="calibre23"/>。如下图所示，每个新的RBM将捕获以前的RBM没有捕获到的模式:</p><div><img src="img/00251.jpeg" alt="Deep belief bets" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">可以看出，堆栈顶部新RBM的每次添加都会降低负对数似然性。</p><p class="calibre8">作为最后一步，可以在分类网络中使用这些权重，只需在最终隐藏状态的顶部添加一个线性层和一个softmax层，并像往常一样通过梯度下降训练微调所有权重。</p><p class="calibre8">对数据维度的应用保持不变，展开所有层以产生解码器网络，其权重等于编码器网络中权重的转置(初始多层RBM):</p><div><img src="img/00252.jpeg" alt="Deep belief bets" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这种<a id="id463" class="calibre1"/>展开的网络被称为<strong class="calibre2">自动编码器</strong>。</p><p class="calibre8">在<a id="id464" class="calibre1"/>实践中，不通过贪婪的<a id="id465" class="calibre1"/>逐层训练直接通过梯度下降进行训练将需要找到正确的初始化，这可能会更加棘手，因为权重初始化必须足够接近解。这就是为什么自动编码器通常使用的方法是单独训练每个RBM。</p></div></div></div></body></html>


<html>
  <head>
    <title>Chapter 12. Learning Features with Unsupervised Generative Networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch12lvl2sec30" class="calibre1"/>生成敌对网络</h2></div></div></div><p class="calibre8">由于以前模型中的<a id="id466" class="calibre1"/>配分函数<a id="id467" class="calibre1"/>是不可检测的，并且需要使用Gibbs抽样的对比散度算法，博弈论最近提供了一类新的学习生成模型的方法，即<strong class="calibre2">生成对抗网络</strong> ( <strong class="calibre2"> GANs </strong>)，它在今天获得了巨大的成功。</p><p class="calibre8">生成对抗网络由两个模型组成，这两个模型被交替训练以相互竞争。发生器网络<strong class="calibre2"> G </strong>通过产生鉴别器<strong class="calibre2"> D </strong>难以从真实数据中区分的数据，被优化以再现真实数据分布。同时，第二网络D被优化以区分由g产生的真实数据和合成数据。总的来说，训练过程类似于具有以下目标函数的两人最小-最大游戏:</p><div><img src="img/00253.jpeg" alt="Generative adversarial networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<em class="calibre12"> x </em>是从真实数据分布中采样的真实数据，<em class="calibre12"> z </em>是生成式<a id="id468" class="calibre1"/>模型的噪声向量。在某些方面，鉴别器和发生器可以被视为警察和小偷:为了确保训练正确，警察接受的训练是小偷的两倍。</p><p class="calibre8">让我们用图像作为数据的情况来说明GANs。特别地，让我们再次以第2章、<em class="calibre12">中的示例为例，使用关于MNIST数字的前馈网络</em>对手写数字进行分类，并考虑训练生成性对抗网络，以根据我们想要的数字有条件地生成图像。</p><p class="calibre8">GAN方法包括使用第二个模型(鉴别网络)训练生成模型，以鉴别输入数据的真假。在这种情况下，我们可以简单地重新使用我们的MNIST图像分类模型作为鉴别器，具有两个类别<code class="email">real</code>或<code class="email">fake</code>，用于预测输出，并且还以应该生成的数字的标签为条件。为了调节标签上的网络，数字标签与输入连接在一起:</p><div><pre class="programlisting">
<strong class="calibre2">def</strong> conv_cond_concat(x, y):
    <strong class="calibre2">return</strong> T.concatenate([x, y*T.ones((x.shape[0], y.shape[1], x.shape[2], x.shape[3]))], <strong class="calibre2">axis</strong>=1)

<strong class="calibre2">def</strong> discrim(X, Y, w, w2, w3, wy):
    yb = Y.dimshuffle(0, 1, <strong class="calibre2">'x'</strong>, <strong class="calibre2">'x'</strong>)
    X = conv_cond_concat(X, yb)
    h = T.nnet.relu(dnn_conv(X, w, <strong class="calibre2">subsample</strong>=(2, 2), <strong class="calibre2">border_mode</strong>=(2, 2)), <strong class="calibre2">alpha</strong>=0.2 )
    h = conv_cond_concat(h, yb)
    h2 =  T.nnet.relu(batchnorm(dnn_conv(h, w2, <strong class="calibre2">subsample</strong>=(2, 2), <strong class="calibre2">border_mode</strong>=(2, 2))), <strong class="calibre2">alpha</strong>=0.2)
    h2 = T.flatten(h2, 2)
    h2 = T.concatenate([h2, Y], <strong class="calibre2">axis</strong>=1)
    h3 = T.nnet.relu(batchnorm(T.dot(h2, w3)))
    h3 = T.concatenate([h3, Y], <strong class="calibre2">axis</strong>=1)
    y = T.nnet.sigmoid(T.dot(h3, wy))
    <strong class="calibre2">return</strong> y</pre></div><div><h3 class="title2"><a id="tip02" class="calibre1"/>提示</h3><p class="calibre8">注意使用两个泄漏整流线性单元，泄漏为0.2，作为前两个卷积的激活。</p></div><p class="calibre8">为了<a id="id470" class="calibre1"/>生成给定<a id="id471" class="calibre1"/>噪声和标签的图像，生成器网络由一组去卷积组成，使用由100个范围从0到1的实数组成的输入噪声向量z:</p><div><img src="img/00254.jpeg" alt="Generative adversarial networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">为了在ano中创建反卷积，需要创建一个伪卷积正向通道，该梯度用作反卷积:</p><div><pre class="programlisting">
<strong class="calibre2">def</strong> deconv(X, w, <strong class="calibre2">subsample</strong>=(1, 1), <strong class="calibre2">border_mode</strong>=(0, 0), <strong class="calibre2">conv_mode</strong>='conv'):
    img = gpu_contiguous(T.cast(X, <strong class="calibre2">'float32'</strong>))
    kerns = gpu_contiguous(T.cast(w, <strong class="calibre2">'float32'</strong>))
    desc = GpuDnnConvDesc(<strong class="calibre2">border_mode</strong>=border_mode, <strong class="calibre2">subsample</strong>=subsample,
<strong class="calibre2">conv_mode</strong>=conv_mode)(gpu_alloc_empty(img.shape[0], kerns.shape[1], img.shape[2]*subsample[0], img.shape[3]*subsample[1]).shape, kerns.shape)
    out = gpu_alloc_empty(img.shape[0], kerns.shape[1], img.shape[2]*subsample[0], img.shape[3]*subsample[1])
    d_img = GpuDnnConvGradI()(kerns, img, out, desc)
    <strong class="calibre2">return</strong> d_img

<strong class="calibre2">def</strong> gen(Z, Y, w, w2, w3, wx):
    yb = Y.dimshuffle(0, 1, 'x', 'x')
    Z = T.concatenate([Z, Y], <strong class="calibre2">axis</strong>=1)
    h = T.nnet.relu(batchnorm(T.dot(Z, w)))
    h = T.concatenate([h, Y], <strong class="calibre2">axis</strong>=1)
    h2 = T.nnet.relu(batchnorm(T.dot(h, w2)))
    h2 = h2.reshape((h2.shape[0], ngf*2, 7, 7))
    h2 = conv_cond_concat(h2, yb)
    h3 = T.nnet.relu(batchnorm(deconv(h2, w3, <strong class="calibre2">subsample</strong>=(<strong class="calibre2">2</strong>, <strong class="calibre2">2</strong>), <strong class="calibre2">border_mode</strong>=(<strong class="calibre2">2</strong>, <strong class="calibre2">2</strong>))))
    h3 = conv_cond_concat(h3, yb)
    x = T.nnet.sigmoid(deconv(h3, wx, <strong class="calibre2">subsample</strong>=(<strong class="calibre2">2</strong>, <strong class="calibre2">2</strong>), <strong class="calibre2">border_mode</strong>=(<strong class="calibre2">2</strong>, <strong class="calibre2">2</strong>)))
    <strong class="calibre2">return</strong> x</pre></div><p class="calibre8">真实的<a id="id472" class="calibre1"/>数据由元组(X，Y)给出，而生成的数据由噪声和标签(Z，Y)构建:</p><div><pre class="programlisting">X = T.tensor4()
Z = T.matrix()
Y = T.matrix()

gX = gen(Z, Y, *gen_params)
p_real = discrim(X, Y, *discrim_params)
p_gen = discrim(gX, Y, *discrim_params)</pre></div><p class="calibre8">生成器<a id="id473" class="calibre1"/>和鉴别器模型在对抗学习期间竞争:</p><div><ul class="itemizedlist"><li class="listitem">鉴别器被训练为将真实数据标记为真实(<code class="email">1</code>)并将生成的数据标记为生成的(<code class="email">0</code>)，从而最小化以下成本函数:<div> <pre class="programlisting">d_cost = T.nnet.binary_crossentropy(p_real, 								T.ones(p_real.shape)).mean() \ 	+ T.nnet.binary_crossentropy(p_gen, T.zeros(p_gen.shape)).mean()</pre> </div></li><li class="listitem">生成器被训练成尽可能地欺骗鉴别器。发生器的训练信号由鉴别器网络(p_gen)提供给发生器:<div> <pre class="programlisting">g_cost = T.nnet.binary_crossentropy(p_gen,T.ones(p_gen.shape)).mean()</pre> </div></li></ul></div><p class="calibre8">和往常一样的如下。计算关于每个模型的参数的成本，并且训练交替地优化每个模型的权重，使用两倍多的鉴别器。在GANs的情况下，鉴别器和发生器之间的竞争不会导致每个损失的减少。</p><p class="calibre8">从第一个纪元开始:</p><div><img src="img/00255.jpeg" alt="Generative adversarial networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">到<a id="id474" class="calibre1"/>第45纪元:</p><div><img src="img/00256.jpeg" alt="Generative adversarial networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">生成的<a id="id475" class="calibre1"/>示例看起来更接近真实示例:</p><div><img src="img/00257.jpeg" alt="Generative adversarial networks" class="calibre9"/></div><p class="calibre10"> </p><div><div><div><div><h3 class="title2"><a id="ch12lvl3sec01" class="calibre1"/>改进甘斯</h3></div></div></div><p class="calibre8">gan是最近才出现的，非常有前途，但今天仍在进行大量的研究。然而，有一些方法可以改善之前的结果。</p><p class="calibre8">首先，就RBM和其他网络而言，gan可以堆叠起来，以增加其生产能力。例如，StackGan模型提出使用两个堆叠的Gan来生成高质量的图像:第一个GAN生成粗糙和低分辨率的图像，而第二个GAN使用该生成的样本作为输入来生成更高清晰度和真实感的图像，其中对象的细节被精确化。</p><p class="calibre8">甘的主要问题之一是<strong class="calibre2">模型崩溃</strong>，这使得他们很难训练。当生成器开始忽略输入噪声并学习只生成一个样本(总是相同)时，模型崩溃发生。这一代人的多样性已经崩溃。一个非常好的处理方法来自S-GAN模型，包括添加第三个网络来与发电机一起训练。该网络的目的是预测给定输入的噪声:</p><div><img src="img/00258.jpeg" alt="Improve GANs" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">为了<a id="id478" class="calibre1"/>利用发生器优化该第三网络，熵损失被添加到发生器损失，以促使生成的图像<em class="calibre12"> x </em>充分依赖于噪声<em class="calibre12"> z </em>。换句话说，条件熵<em class="calibre12"> H(x | z) </em>必须尽可能低:</p><div><img src="img/00259.jpeg" alt="Improve GANs" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">该第三网络预测辅助分布Q以逼近真实的后验概率P(z | x) ，并且可以被证明是对于<em class="calibre12"> H(x | z) </em>的变分上限。这种损失函数有助于发生器不忽略输入噪声。</p></div></div></div></div></body></html>


<html>
  <head>
    <title>Semi-supervised learning</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec105" class="calibre1"/>半监督学习</h1></div></div></div><p class="calibre8">最后，但同样重要的是，这种生成式对抗网络可以用来增强监督学习本身。</p><p class="calibre8">假设目标是对<em class="calibre12"> K </em>类进行分类，对于这些类，有大量标记数据可用。可以向数据集添加一些来自生成模型的生成样本，并将其视为属于第<em class="calibre12"> (K+1)个</em>类，即伪数据类。</p><p class="calibre8">在两个集合(真实数据和虚假数据)之间分解新分类器的训练交叉熵损失导致以下公式:</p><div><img src="img/00260.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这里，<img src="img/00261.jpeg" alt="Semi-supervised learning" class="calibre23"/>是模型预测的概率:</p><div><img src="img/00262.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">注意<a id="id480" class="calibre1"/>如果我们知道数据是真实的:</p><div><img src="img/00263.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">对真实数据(K类)的训练会导致损失:</p><div><img src="img/00264.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">因此，全局分类器的丢失可以被重写:</p><div><img src="img/00265.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">损耗的第二项对应于GAN的标准无监督损耗:</p><div><img src="img/00266.jpeg" alt="Semi-supervised learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">在监督和非监督损失之间引入的<a id="id481" class="calibre1"/>相互作用仍然没有被很好地理解，但是，当分类不是微不足道时，非监督损失是有帮助的。</p></div></body></html>


<html>
  <head>
    <title>Further reading</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec106" class="calibre1"/>延伸阅读</h1></div></div></div><p class="calibre8">您可以参考以下主题了解更多信息:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre12">Deeplearning.net关于RBM的教程</em>:<a class="calibre1" href="http://deeplearning.net/tutorial/rbm.html">http://deeplearning.net/tutorial/rbm.html</a></li><li class="listitem"><em class="calibre12">Deeplearning.net深度信念网教程</em>:<a class="calibre1" href="http://deeplearning.net/tutorial/DBN.html">http://deeplearning.net/tutorial/DBN.html</a></li><li class="listitem"><em class="calibre12">Deeplearning.net与RBM-RNN生成教程</em>:<a class="calibre1" href="http://deeplearning.net/tutorial/rnnrbm.html">http://deeplearning.net/tutorial/rnnrbm.html</a></li><li class="listitem"><em class="calibre12">高维序列中的时间依赖性建模:应用于复调音乐生成和转录</em>，尼古拉斯·布朗热-莱万多夫斯基、约舒阿·本吉奥、帕斯卡尔·文森特，2012</li><li class="listitem">生成性对抗性网络，伊恩·古德菲勒、让·普盖-阿巴迪、迈赫迪·米尔扎、徐炳、戴维·沃德-法利、谢尔吉尔·奥泽尔、亚伦·库维尔、约舒阿·本吉奥，2014年</li><li class="listitem"><em class="calibre12">甘斯会</em> <em class="calibre12">改变世界</em>，https://medium.com/@Moscow25/ 2016<a class="calibre1" href="https://medium.com/@Moscow25/">尼古拉·亚科文科</a></li><li class="listitem"><em class="calibre12">像素递归神经网络</em>，Aaron van den Oord，Nal Kalchbrenner，Koray Kavukcuoglu，2016</li><li class="listitem"><em class="calibre12"> InfoGAN:通过信息最大化生成对抗网的可解释表示学习，</em>陈曦，闫端，雷因·胡特，约翰·舒尔曼，伊利亚·苏茨基弗，彼得·阿贝耳，2016</li><li class="listitem"><em class="calibre12"> StackGAN:利用堆叠生成对抗网络进行文本到照片级真实感图像合成</em>，张寒、徐涛、李洪生、张少婷、黄小蕾、王晓刚、迪米特里斯·梅塔克萨斯，2016</li><li class="listitem"><em class="calibre12">堆叠式生成高级网络</em>，黄浚，黎一萱，奥米德·波尔赛德，约翰·霍普克罗夫特，塞尔日·贝隆吉，2016</li><li class="listitem"><em class="calibre12">用于神经对话生成的对抗性学习</em>，李继伟，威尔·门罗，史天林，塞巴斯蒂安·让，艾伦·里特，丹·茹拉夫斯基，2017</li><li class="listitem"><em class="calibre12">训练甘斯的改进技术</em>，蒂姆·萨利曼斯，伊恩·古德菲勒，沃伊切赫·扎伦巴，张维基，亚历克·拉德福德，陈曦，2016</li><li class="listitem"><em class="calibre12">深度卷积生成对抗网络的无监督表示学习</em>，亚历克·拉德福德，卢克·梅茨，苏密·钦塔拉，2015</li></ul></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec107" class="calibre1"/>总结</h1></div></div></div><p class="calibre8">生成对抗网络是当今非常活跃的研究领域。它们属于生成模型家族，包括RBM和深层信念网络。</p><p class="calibre8">生成模型旨在生成更多的数据，或者以无监督的方式为监督和其他任务学习更好的特征。</p><p class="calibre8">生成模型可以以一些环境输入为条件，并试图找到真实数据背后的隐藏变量。</p><p class="calibre8">这些模型是最先进的，与Theano一起完成了深度学习网络的概述。下一章将探讨一些先进的概念，以扩展深度学习的概念和未来。</p></div></body></html>
</body></html>