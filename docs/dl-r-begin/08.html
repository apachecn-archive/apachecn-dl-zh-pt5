<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deep Learning Models Using TensorFlow in R</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">R中使用张量流的深度学习模型</h1>

                

            

            

                

<p class="mce-root">这一章是关于在r中使用TensorFlow的。我们已经使用了TensorFlow相当多，因为Keras是一个高级神经网络API，它使用TensorFlow、CNTK或Theano。在R中，Keras在后台使用TensorFlow。TensorFlow在开发深度学习模型方面难度更大。然而，TensorFlow中有两个有趣的包可能会被忽略:TensorFlow估计器和TensorFlow运行。我们将在本章中讨论这两个包。</p>

<p class="mce-root">在本章中，我们将讨论以下主题:</p>

<ul>

<li class="mce-root">张量流简介</li>

<li>使用TensorFlow构建模型</li>

<li>张量流估计量</li>

<li>TensorFlow运行包</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Introduction to the TensorFlow library</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">TensorFlow库简介</h1>

                

            

            

                

<p><strong> TensorFlow </strong>不仅仅是一个深度学习库，而是一种富有表现力的编程语言，可以对数据实施各种优化和数学变换。虽然它主要用于实现深度学习算法，但它可以执行更多。在TensorFlow中，程序表现为计算图，TensorFlow中的数据存储在<kbd>tensors</kbd>中。一个<strong>张量</strong>是具有相同数据类型的数据数组，一个张量的秩是维数。因为张量中的所有数据必须具有相同的类型，所以它们更类似于R矩阵，而不是数据帧。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>这里有一个各种等级张量的例子:</p>

<pre>library(tensorflow)<br/> <br/>&gt; # tensor of rank-0<br/>&gt; var1 &lt;- tf$constant(0.1)<br/>&gt; print(var1)<br/>Tensor("Const:0", shape=(), dtype=float32)<br/> <br/>&gt; sess &lt;- tf$InteractiveSession()<br/>T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3019 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)<br/><br/>&gt; sess$run(tf$global_variables_initializer())<br/>&gt; var2 &lt;- tf$constant(2.3)<br/>&gt; var3 = var1 + var2<br/>&gt; print(var1)<br/>Tensor("Const:0", shape=(), dtype=float32)<br/> num 0.1<br/> <br/>&gt; print(var2)<br/>Tensor("Const_1:0", shape=(), dtype=float32)<br/> num 2.3<br/> <br/>&gt; print(var3)<br/>Tensor("Add:0", shape=(), dtype=float32)<br/> num 2.4<br/> <br/>&gt; # tensor of rank-1<br/>&gt; var4 &lt;- tf$constant(4.5,shape=shape(5L))<br/>&gt; print(var4)<br/>Tensor("Const_2:0", shape=(5,), dtype=float32)<br/> num [1:5(1d)] 4.5 4.5 4.5 4.5 4.5<br/> <br/>&gt; # tensor of rank-2<br/>&gt; var5 &lt;- tf$constant(6.7,shape=shape(3L,3L))<br/>&gt; print(var5)<br/>Tensor("Const_3:0", shape=(3, 3), dtype=float32)<br/> num [1:3, 1:3] 6.7 6.7 6.7 6.7 6.7 ...</pre>

<p class="mce-root">张量流程序有两个部分。首先，你必须建立计算图，它包含张量和对这些张量的运算。当他们定义了图形后，第二部分是创建一个TensorFlow会话来运行图形。在前面的例子中，我们第一次打印出张量<kbd>a</kbd>的值，我们只得到张量定义而没有得到值。我们所做的就是定义计算图的一部分。只有当我们调用<kbd>tf$InteractiveSession</kbd>时，我们才告诉TensorFlow在张量上运行操作。一个会话负责运行计算图。</p>

<p class="mce-root">TensorFlow程序之所以被称为图形，是因为代码可以被构造为图形。这对我们来说可能并不明显，因为我们在本书中构建的大多数深度学习模型都是由层上的顺序操作组成的。在TensorFlow(以及Keras和MXNet)中，可以多次使用一个操作的输出，并在一个操作中组合输入。</p>

<p class="mce-root">随着深度学习模型变得越来越大，可视化和调试它们变得越来越困难。在一些代码块中，我们打印了显示各层的模型摘要，或者绘制了网络。然而，这两个工具对于调试一个有超过1000万个参数的模型中的问题都没有帮助！幸运的是，TensorFlow附带了一个可视化工具，可以帮助总结、调试和修复TensorFlow程序。这被称为TensorBoard，我们将在接下来讨论它。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using TensorBoard to visualize deep learning networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用TensorBoard可视化深度学习网络</h1>

                

            

            

                

<p>TensorFlow中的计算图形可能非常复杂，因此有一个名为<strong> TensorBoard </strong>的可视化工具来可视化这些图形并协助调试。TensorBoard可以绘制计算图，显示来自训练的指标，等等。由于Keras在后端使用TensorFlow，它也可以使用TensorBoard。这是来自Keras的启用了TensorBoard日志记录的MNIST示例。这个代码可以在<kbd>Chapter8/mnist_keras.R</kbd>文件夹中找到。代码的第一部分加载数据，预处理数据，并定义模型架构。希望您在这个阶段对此应该很熟悉:</p>

<pre>library(keras)<br/><br/>mnist_data &lt;- dataset_mnist()<br/>xtrain &lt;- array_reshape(mnist_data$train$x,c(nrow(mnist_data$train$x),28,28,1))<br/>ytrain &lt;- to_categorical(mnist_data$train$y,10)<br/>xtrain &lt;- xtrain / 255.0<br/><br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>  layer_conv_2d(filters=32,kernel_size=c(5,5),activation='relu',<br/>                input_shape=c(28,28,1)) %&gt;% <br/>  layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% <br/>  layer_dropout(rate=0.25) %&gt;% <br/>  layer_conv_2d(filters=32,kernel_size=c(5,5),activation='relu') %&gt;% <br/>  layer_max_pooling_2d(pool_size=c(2,2)) %&gt;% <br/>  layer_dropout(rate=0.25) %&gt;% <br/>  layer_flatten() %&gt;% <br/>  layer_dense(units=256,activation='relu') %&gt;% <br/>  layer_dropout(rate=0.4) %&gt;% <br/>  layer_dense(units=10,activation='softmax')<br/><br/>model %&gt;% compile(<br/>  loss=loss_categorical_crossentropy,<br/>  optimizer="rmsprop",metrics="accuracy"<br/>)</pre>

<p>要启用日志记录，请在<kbd>model.fit</kbd>函数中添加一个<kbd>callbacks</kbd>参数，告诉Keras/TensorFlow将事件记录到一个目录中。以下代码将日志数据输出到<kbd>/tensorflow_logs</kbd>目录:</p>

<pre>model %&gt;% fit(<br/>  xtrain,ytrain,<br/>  batch_size=128,epochs=10,<br/><strong>  callbacks=callback_tensorboard("/tensorflow_logs",</strong><br/><strong>                                 histogram_freq=1,write_images=0),</strong><br/>  validation_split=0.2<br/>)<br/># from cmd line,run 'tensorboard --logdir /tensorflow_logs'</pre>

<div><strong>Warning</strong>: The event logs can take up a lot of space. For 5 epochs on the <kbd>MNIST</kbd> dataset, 1.75 GB of information was created. Most of this was because of the image data that was included, so you may consider setting <kbd>write_images=0</kbd> to reduce the size of the logs.</div>

<div><p>TensorBoard是一个web应用程序，您必须启动TensorBoard程序才能运行。当模型完成训练后，按照以下步骤启动TensorBoard web应用程序:</p>

</div>

<ol>

<li>打开命令提示符并输入以下内容:</li>

</ol>

<pre style="padding-left: 60px"><strong>$ tensorboard --logdir /tensorflow_logs</strong></pre>

<ol start="2"/>

<ol start="2">

<li class="mce-root">如果TensorBoard成功启动，您应该会在命令提示符下看到类似以下内容的消息:</li>

</ol>

<pre style="padding-left: 60px">TensorBoard 0.4.0rc2 at http://xxxxxx:6006 (Press CTRL+C to quit)</pre>

<ol start="3">

<li>打开web浏览器，找到提供的链接。该网页应类似于以下内容:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-705 image-border" src="img/540cf00a-6e23-450c-8612-81d1338fd618.png" style="width:104.50em;height:64.00em;"/></p>

<p>图8.1:tensor board–模型指标</p>

<ol start="4">

<li>前面的屏幕截图向我们展示了培训和验证测试集上的模型指标，这些指标类似于RStudio中在培训期间显示的指标:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/c94ddc77-056d-4868-98b2-377244781156.png" style="width:57.75em;height:38.75em;"/></p>

<p>图8.2:r studio–模型指标</p>

<ol start="5">

<li>如果您点击<strong>图像</strong>选项，您将能够可视化模型中的层，并查看它们如何随时代变化:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-706 image-border" src="img/52c9cc85-97ee-4a2c-a487-7b007b5f1a37.png" style="width:111.92em;height:60.17em;"/></p>

<p>图8.3:张量板——可视化模型层</p>

<ol start="6">

<li>如果点击<strong>图形</strong>选项，将显示模型的计算图形。您也可以将其作为图像文件下载。这是该模型的计算图:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-707 image-border" src="img/27dfd981-e51e-4e7a-bc86-ed62a0bd8244.png" style="width:142.42em;height:98.17em;"/></p>

<p>图8.4:张量板-计算图</p>

<p style="padding-left: 60px">其中一些似乎很熟悉。我们可以看到卷积层、最大池层、展平层、密集层和下降层。其余的就不那么明显了。作为一个更高层次的抽象，Keras在创建计算图的过程中处理了大量的复杂性。</p>

<ol start="7">

<li>通过点击<strong>直方图</strong>选项，您可以看到张量的分布是如何随时间变化的:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/02446e99-2d80-473c-994d-60a44b181cf7.png"/></p>

<p>图8.5:张量板——直方图</p>

<p>可以使用TensorBoard来调试模型。例如，可以研究消失梯度或爆炸梯度问题，以查看模型的权重是消失为零还是爆炸至无穷大。TensorBoard还有更多的内容，所以如果你有兴趣的话，可以参考它的在线文档。</p>

<p>在下一节中，我们将使用TensorFlow建立一个回归模型和一个卷积神经网络。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow models</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">张量流模型</h1>

                

            

            

                

<p>在本节中，我们将使用TensorFlow来构建一些机器学习模型。首先，我们将建立一个简单的线性回归模型，然后是一个卷积神经网络模型，类似于我们在<a href="">第五章</a>、<em>使用卷积神经网络进行图像分类</em>中看到的。</p>

<p>以下代码加载TensorFlow库。我们可以通过设置和访问一个常量字符串值来确认它加载成功:</p>

<pre>&gt; library(tensorflow)<br/><br/># confirm that TensorFlow library has loaded<br/>&gt; sess=tf$Session()<br/>&gt; hello_world &lt;- tf$constant('Hello world from TensorFlow')<br/>&gt; sess$run(hello_world)<br/>b'Hello world from TensorFlow'</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Linear regression using TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用张量流的线性回归</h1>

                

            

            

                

<p>在第一个张量流示例中，我们将研究回归。该部分的代码在<kbd>Chapter8/regression_tf.R</kbd>文件夹中:</p>

<ol>

<li>首先，我们为一个输入值<em> x </em>和一个输出值<em> y </em>创建一些假数据。我们设定<em> y </em>大约等于<kbd>0.8 + x * 1.3</kbd>。我们希望应用程序发现<kbd>beta0</kbd>和<kbd>beta1</kbd>值，它们分别是<kbd>0.8</kbd>和<kbd>1.3</kbd>:</li>

</ol>

<pre style="padding-left: 60px">library(tensorflow)<br/><br/>set.seed(42)<br/># create 50000 x variable between 0 and 100<br/>x_var &lt;- runif(50000,min=0,max=1)<br/>#y = approx(1.3x + 0.8)<br/>y_var &lt;- rnorm(50000,0.8,0.04) + x_var * rnorm(50000,1.3,0.05)<br/><br/># y_pred = beta0 + beta1 * x<br/>beta0 &lt;- tf$Variable(tf$zeros(shape(1L)))<br/>beta1 &lt;- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))<br/>y_pred &lt;- beta0 + beta1*x_var</pre>

<ol start="2">

<li>现在，我们设置我们的<kbd>loss</kbd>函数，以便梯度下降算法可以工作:</li>

</ol>

<pre style="padding-left: 60px"># create our loss value which we want to minimize<br/>loss &lt;- tf$reduce_mean((y_pred-y_var)^2)<br/># create optimizer<br/>optimizer &lt;- tf$train$GradientDescentOptimizer(0.6)<br/>train &lt;- optimizer$minimize(loss)</pre>

<ol start="3">

<li>然后，我们设置一个TensorFlow会话并初始化变量。最后，我们可以运行图表:</li>

</ol>

<pre style="padding-left: 60px"># create TensorFlow session and initialize variables<br/>sess = tf$Session()<br/>sess$run(tf$global_variables_initializer())<br/><br/># solve the regression<br/>for (step in 0:80) {<br/>  if (step %% 10 == 0)<br/>    print(sprintf("Step %1.0f:beta0=%1.4f, beta1=%1.4f",step,sess$run(beta0), sess$run(beta1)))<br/>  sess$run(train)<br/>}<br/>[1] "Step 0:beta0=0.0000, beta1=-0.3244"<br/>[1] "Step 10:beta0=1.0146, beta1=0.8944"<br/>[1] "Step 20:beta0=0.8942, beta1=1.1236"<br/>[1] "Step 30:beta0=0.8410, beta1=1.2229"<br/>[1] "Step 40:beta0=0.8178, beta1=1.2662"<br/>[1] "Step 50:beta0=0.8077, beta1=1.2850"<br/>[1] "Step 60:beta0=0.8033, beta1=1.2932"<br/>[1] "Step 70:beta0=0.8014, beta1=1.2967"<br/>[1] "Step 80:beta0=0.8006, beta1=1.2983"</pre>

<p>我们可以看到，模型设法找到了求解函数<kbd>y=beta0 + beta1*x</kbd>的<kbd>beta0</kbd>和<kbd>beta1</kbd>的值。下一节是一个更复杂的例子，我们将为图像分类建立一个张量流模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Convolutional neural networks using TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用张量流的卷积神经网络</h1>

                

            

            

                

<p>在本节中，我们将在MNIST数据集上构建一个张量流模型。该代码与我们在第5章、<em xmlns:epub="http://www.idpf.org/2007/ops">中看到的Lenet模型具有相似的层和参数，使用卷积神经网络进行图像分类</em>。然而，在TensorFlow中构建模型的代码比在Keras或MXNet中构建模型的代码更复杂。其中一个原因是程序员的工作是确保层的大小正确对齐。在Keras/MXNet模型中，我们可以在一条语句中改变一个层中的节点数。在TensorFlow中，如果我们改变一层中的节点数量，我们必须确保我们也改变下一层中的输入。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>在某些方面，TensorFlow中的编程更接近于我们在<a href="">第三章</a>、<em>深度学习基础</em>中写的手写神经网络代码。在训练循环中与Keras/MXNet的另一个不同之处是，我们需要管理批处理而不仅仅是调用，要求迭代所有数据<em> x </em>次(其中<em> x </em>是一个epoch)。这个例子的代码在<kbd>Chapter8/mnist_tf.R</kbd>文件夹中。首先，我们加载Keras包来获取MNIST数据，但是我们使用TensorFlow来训练模型。下面是代码的第一部分:</p>

<pre>library(RSNNS) # for decodeClassLabels<br/>library(tensorflow)<br/>library(keras)<br/><br/>mnist &lt;- dataset_mnist()<br/>set.seed(42)<br/><br/>xtrain &lt;- array_reshape(mnist$train$x,c(nrow(mnist$train$x),28*28))<br/>ytrain &lt;- decodeClassLabels(mnist$train$y)<br/>xtest &lt;- array_reshape(mnist$test$x,c(nrow(mnist$test$x),28*28))<br/>ytest &lt;- decodeClassLabels(mnist$test$y)<br/>xtrain &lt;- xtrain / 255.0<br/>xtest &lt;- xtest / 255.0<br/>head(ytrain)<br/>     0 1 2 3 4 5 6 7 8 9<br/>[1,] 0 0 0 0 0 1 0 0 0 0<br/>[2,] 1 0 0 0 0 0 0 0 0 0<br/>[3,] 0 0 0 0 1 0 0 0 0 0<br/>[4,] 0 1 0 0 0 0 0 0 0 0<br/>[5,] 0 0 0 0 0 0 0 0 0 1<br/>[6,] 0 0 1 0 0 0 0 0 0 0</pre>

<p>我们使用RSNNS库中的<kbd>decodeClassLabels</kbd>函数，因为TensorFlow需要一个虚拟编码矩阵，所以每个可能的类都表示为一个编码为0/1的列，如前面的代码输出所示。</p>

<p>在下一个代码块中，我们为模型中的输入和输出值创建一些占位符。我们还将输入数据整形为秩为4的张量，即4维数据结构。第一个维度(-1L)用于将被批量处理的记录。接下来的两个维度是图像文件的维度，最后一个维度是通道，这是颜色的数量。因为我们的图像是灰度的，所以只有一个通道。如果图像是彩色图像，将有3个通道。以下代码块创建占位符并重塑数据:</p>

<pre># placeholders<br/>x &lt;- tf$placeholder(tf$float32, shape(NULL,28L*28L))<br/>y &lt;- tf$placeholder(tf$float32, shape(NULL,10L))<br/>x_image &lt;- tf$reshape(x, shape(-1L,28L,28L,1L))</pre>

<p class="mce-root"/>

<p>接下来，我们将定义模型架构。我们将创建卷积模块，就像我们之前所做的那样。然而，还有很多值需要设置。例如，在第一个卷积层中，我们必须定义形状，初始化权重，并考虑偏差变量。下面是张量流模型的代码:</p>

<pre># first convolution layer<br/>conv_weights1 &lt;- tf$Variable(tf$random_uniform(shape(5L,5L,1L,16L), -0.4, 0.4))<br/>conv_bias1 &lt;- tf$constant(0.0, shape=shape(16L))<br/>conv_activ1 &lt;- tf$nn$tanh(tf$nn$conv2d(x_image, conv_weights1, strides=c(1L,1L,1L,1L), padding='SAME') + conv_bias1)<br/>pool1 &lt;- tf$nn$max_pool(conv_activ1, ksize=c(1L,2L,2L,1L),strides=c(1L,2L,2L,1L), padding='SAME')<br/><br/># second convolution layer<br/>conv_weights2 &lt;- tf$Variable(tf$random_uniform(shape(5L,5L,16L,32L), -0.4, 0.4))<br/>conv_bias2 &lt;- tf$constant(0.0, shape=shape(32L))<br/>conv_activ2 &lt;- tf$nn$relu(tf$nn$conv2d(pool1, conv_weights2, strides=c(1L,1L,1L,1L), padding='SAME') + conv_bias2)<br/>pool2 &lt;- tf$nn$max_pool(conv_activ2, ksize=c(1L,2L,2L,1L),strides=c(1L,2L,2L,1L), padding='SAME')<br/><br/># densely connected layer<br/>dense_weights1 &lt;- tf$Variable(tf$truncated_normal(shape(7L*7L*32L,512L), stddev=0.1))<br/>dense_bias1 &lt;- tf$constant(0.0, shape=shape(512L))<br/>pool2_flat &lt;- tf$reshape(pool2, shape(-1L,7L*7L*32L))<br/>dense1 &lt;- tf$nn$relu(tf$matmul(pool2_flat, dense_weights1) + dense_bias1)<br/><br/># dropout<br/>keep_prob &lt;- tf$placeholder(tf$float32)<br/>dense1_drop &lt;- tf$nn$dropout(dense1, keep_prob)<br/><br/># softmax layer<br/>dense_weights2 &lt;- tf$Variable(tf$truncated_normal(shape(512L,10L), stddev=0.1))<br/>dense_bias2 &lt;- tf$constant(0.0, shape=shape(10L))<br/><br/>yconv &lt;- tf$nn$softmax(tf$matmul(dense1_drop, dense_weights2) + dense_bias2)</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mceNonEditable"/>

<p class="mceNonEditable"/>

<p>现在，我们必须定义损失方程，定义要使用的优化器(Adam)，并定义准确性指标:</p>

<pre>cross_entropy &lt;- tf$reduce_mean(-tf$reduce_sum(y * tf$log(yconv), reduction_indices=1L))<br/>train_step &lt;- tf$train$AdamOptimizer(0.0001)$minimize(cross_entropy)<br/>correct_prediction &lt;- tf$equal(tf$argmax(yconv, 1L), tf$argmax(y, 1L))<br/>accuracy &lt;- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))</pre>

<p>最后，我们可以在10个时期内训练模型。然而，一个复杂的问题仍然存在，因此我们必须手动管理批次。我们得到要训练的批次数量，并依次加载它们。如果我们的训练数据集中有60，000个图像，则每个时期有469个批次(60，000/128 = 468.75，向上取整为469)。我们输入每一批，每100批输出一次指标:</p>

<pre>sess &lt;- tf$InteractiveSession()<br/>sess$run(tf$global_variables_initializer())<br/><br/># if you get out of memory errors when running on gpu<br/># then lower the batch_size<br/>batch_size &lt;- 128<br/>batches_per_epoch &lt;- 1+nrow(xtrain) %/% batch_size<br/>for (epoch in 1:10)<br/>{<br/>  for (batch_no in 0:(-1+batches_per_epoch))<br/>  {<br/>    nStartIndex &lt;- 1 + batch_no*batch_size<br/>    nEndIndex &lt;- nStartIndex + batch_size-1<br/>    if (nEndIndex &gt; nrow(xtrain))<br/>      nEndIndex &lt;- nrow(xtrain)<br/>    xvalues &lt;- xtrain[nStartIndex:nEndIndex,]<br/>    yvalues &lt;- ytrain[nStartIndex:nEndIndex,]<br/>    if (batch_no %% 100 == 0) {<br/>      batch_acc &lt;- accuracy$eval(feed_dict=dict(x=xvalues,y=yvalues,keep_prob=1.0))<br/>      print(sprintf("Epoch %1.0f, step %1.0f: training accuracy=%1.4f",epoch, batch_no, batch_acc))<br/>    }<br/>    sess$run(train_step,feed_dict=dict(x=xvalues,y=yvalues,keep_prob=0.5))<br/>  }<br/>  cat("\n")<br/>}</pre>

<p>以下是第一个时期的输出:</p>

<pre>[1] "Epoch 1, step 0: training accuracy=0.0625"<br/>[1] "Epoch 1, step 100: training accuracy=0.8438"<br/>[1] "Epoch 1, step 200: training accuracy=0.8984"<br/>[1] "Epoch 1, step 300: training accuracy=0.9531"<br/>[1] "Epoch 1, step 400: training accuracy=0.8750"</pre>

<p>训练完成后，我们可以通过计算测试集上的准确度来评估模型。同样，我们必须分批执行此操作，以防止出现内存不足的错误:</p>

<pre># calculate test accuracy<br/># have to run in batches to prevent out of memory errors<br/>batches_per_epoch &lt;- 1+nrow(xtest) %/% batch_size<br/>test_acc &lt;- vector(mode="numeric", length=batches_per_epoch)<br/>for (batch_no in 0:(-1+batches_per_epoch))<br/>{<br/>  nStartIndex &lt;- 1 + batch_no*batch_size<br/>  nEndIndex &lt;- nStartIndex + batch_size-1<br/>  if (nEndIndex &gt; nrow(xtest))<br/>    nEndIndex &lt;- nrow(xtest)<br/>  xvalues &lt;- xtest[nStartIndex:nEndIndex,]<br/>  yvalues &lt;- ytest[nStartIndex:nEndIndex,]<br/>  batch_acc &lt;- accuracy$eval(feed_dict=dict(x=xvalues,y=yvalues,keep_prob=1.0))<br/>  test_acc[batch_no+1] &lt;- batch_acc<br/>}<br/># using the mean is not totally accurate as last batch is not a complete batch<br/>print(sprintf("Test accuracy=%1.4f",mean(test_acc)))<br/>[1] "Test accuracy=0.9802"</pre>

<p>我们得到最终精度<kbd xmlns:epub="http://www.idpf.org/2007/ops">0.9802</kbd>。如果我们将此代码与第五章、中的<a xmlns:epub="http://www.idpf.org/2007/ops" href="">、<em xmlns:epub="http://www.idpf.org/2007/ops">使用卷积神经网络的图像分类中的MNIST示例进行比较，TensorFlow代码更冗长，也更容易出错。我们确实可以看到使用更高级别的抽象的好处，比如MXNet或Keras(可以使用TensorFlow作为后端)。对于大多数深度学习用例，尤其是使用现有层作为构建块来构建深度学习模型，在TensorFlow中开发代码几乎没有什么收获。对于这些用例，使用Keras或MXNet更简单、更高效。</em></a></p>

<p>看到这段代码后，您可能想回到Keras和MXNet中更熟悉的内容。然而，下一节将着眼于张量流估计器和张量流运行，这是您应该知道的两个有用的包。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow estimators and TensorFlow runs packages</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">张量流估计器和张量流运行包</h1>

                

            

            

                

<p class="mce-root">TensorFlow估计器和TensorFlow runs包是用于深度学习的很好的包。在本节中，我们将使用这两个工具，根据第四章<a xmlns:epub="http://www.idpf.org/2007/ops" href="">、<em xmlns:epub="http://www.idpf.org/2007/ops">训练深度预测模型</em>中的流失预测数据来训练模型。</a></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow estimators</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">张量流估计量</h1>

                

            

            

                

<p class="mce-root"><strong>张量流估算器</strong>允许您使用更简单的API接口构建张量流模型。在R中，<kbd>tfestimators</kbd>包允许你调用这个API。有不同的模型类型，包括线性模型和神经网络。以下是可用的估计值:</p>

<ul>

<li class="mce-root"><kbd>linear_regressor()</kbd>用于线性回归</li>

<li class="mce-root"><kbd>linear_classifier()</kbd>进行线性分类</li>

<li class="mce-root"><kbd>dnn_regressor()</kbd>用于深度神经网络回归</li>

<li class="mce-root"><kbd>dnn_classifier()</kbd>用于深度神经网络分类</li>

<li class="mce-root"><kbd>dnn_linear_combined_regressor()</kbd>用于深度神经网络的线性组合回归</li>

<li class="mce-root"><kbd>dnn_linear_combined_classifier()</kbd>用于深度神经网络的线性组合分类</li>

</ul>

<p>估计器在创建深度学习模型时隐藏了很多细节，包括构建图表、初始化变量和层，它们也可以与TensorBoard集成。更多细节可在<a href="https://tensorflow.rstudio.com/tfestimators/">https://tensorflow.rstudio.com/tfestimators/</a>获得。我们将使用<kbd>dnn_classifier</kbd>和来自<a href="">第四章</a>、<em>的二元分类任务的数据训练深度预测模型</em>。<kbd>Chapter8/tf_estimators.R</kbd>文件夹中的以下代码演示了张量流估计器。</p>

<ol>

<li>我们仅包含特定于张量流估算器的代码，并省略加载数据并将其分为训练和测试数据的文件开头的代码:</li>

</ol>

<pre style="padding-left: 60px">response &lt;- function() "Y_categ"<br/>features &lt;- function() predictorCols<br/> <br/>FLAGS &lt;- flags(<br/>  flag_numeric("layer1", 256),<br/>  flag_numeric("layer2", 128),<br/>  flag_numeric("layer3", 64),<br/>  flag_numeric("layer4", 32),<br/>  flag_numeric("dropout", 0.2)<br/>)<br/>num_hidden &lt;- c(FLAGS$layer1,FLAGS$layer2,FLAGS$layer3,FLAGS$layer4)<br/><br/>classifier &lt;- dnn_classifier(<br/>  feature_columns = feature_columns(column_numeric(predictorCols)),<br/>  hidden_units = num_hidden,<br/>  activation_fn = "relu",<br/>  dropout = FLAGS$dropout,<br/>  n_classes = 2<br/>)<br/> <br/>bin_input_fn &lt;- function(data)<br/>{<br/> input_fn(data, features = features(), response = response())<br/>}<br/>tr &lt;- train(classifier, input_fn = bin_input_fn(trainData))<br/>[\] Training -- loss: 22.96, step: 2742 <br/><br/>tr<br/>Trained for 2,740 steps. <br/>Final step (plot to see history):<br/> mean_losses: 61.91<br/>total_losses: 61.91</pre>

<ol start="2">

<li>模型定型后，以下代码将绘制定型和验证指标:</li>

</ol>

<pre style="padding-left: 60px">plot(tr)</pre>

<ol start="3">

<li>这会产生以下情节:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/a1f325a3-bc8f-4f5e-a9c6-bf6099c923ff.png" style="width:56.58em;height:38.00em;"/></p>

<p>图8.6:为张量流估算模型训练损失图</p>

<ol start="4">

<li>代码的下一部分调用<kbd>evaluate</kbd>函数来为模型产生度量:</li>

</ol>

<pre style="padding-left: 60px"># predictions &lt;- predict(classifier, input_fn = bin_input_fn(testData))<br/>evaluation &lt;- evaluate(classifier, input_fn = bin_input_fn(testData))<br/>[-] Evaluating -- loss: 37.77, step: 305<br/><br/>for (c in 1:ncol(evaluation))<br/> print(paste(colnames(evaluation)[c]," = ",evaluation[c],sep=""))<br/>[1] "accuracy = 0.77573162317276"<br/>[1] "accuracy_baseline = 0.603221416473389"<br/>[1] "auc = 0.842994153499603"<br/>[1] "auc_precision_recall = 0.887594640254974"<br/>[1] "average_loss = 0.501933991909027"<br/>[1] "label/mean = 0.603221416473389"<br/>[1] "loss = 64.1636199951172"<br/>[1] "precision = 0.803375601768494"<br/>[1] "prediction/mean = 0.562777876853943"<br/>[1] "recall = 0.831795573234558"<br/>[1] "global_step = 2742"</pre>

<p>我们可以看到，我们获得了<kbd xmlns:epub="http://www.idpf.org/2007/ops">77.57%</kbd>的精度，这实际上与我们在第4章、<em xmlns:epub="http://www.idpf.org/2007/ops">训练深度预测模型</em>的<a xmlns:epub="http://www.idpf.org/2007/ops" href="">MXNet模型上获得的精度几乎相同，这两个模型具有相似的架构。<kbd xmlns:epub="http://www.idpf.org/2007/ops">dnn_classifier()</kbd>函数隐藏了很多细节，因此张量流估计器是使用张量流处理结构化数据任务的好方法。</a></p>

<p>使用张量流估算器创建的模型可以保存到磁盘上，以后再加载。<kbd>model_dir()</kbd>函数显示了模型工件保存的位置(通常在<kbd>temp</kbd>目录中，但是也可以复制到其他地方):</p>

<pre>model_dir(classifier)<br/>"C:\\Users\\xxxxxx\\AppData\\Local\\Temp\\tmpv1e_ri23"<br/># dnn_classifier has a model_dir parameter to load an existing model<br/>?dnn_classifier</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p>模型工件中包括TensorBoard可以使用的事件日志。例如，当我加载TensorBoard并将其指向<kbd>temp</kbd>目录中的logs目录时，我可以看到创建的TensorFlow图:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-708 image-border" src="img/8f962c90-1bb5-4ed1-ad1f-1c9698ace6ad.png" style="width:162.50em;height:135.67em;"/></p>

<p>图8.7:张量流估算模型使用张量板的图形</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow runs package</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">TensorFlow跑步包</h1>

                

            

            

                

<p class="mce-root"><kbd>tfruns</kbd>包是一组用于管理深度学习模型的不同训练运行的实用程序。它可以用作使用不同超参数建立多个深度学习模型的框架。它可以跟踪每次训练运行的超参数、指标、输出和源代码，并允许您比较最佳模型，以便您可以看到训练运行之间的差异。这使得超参数调整更加容易，并且可以用于任何<kbd>tfestimator</kbd>型号或<kbd>Keras</kbd>型号。欲了解更多详情，请访问<a href="https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html">https://tensor flow . r studio . com/tools/TF runs/articles/overview . html</a>。</p>

<p class="mce-root">以下代码在<kbd>Chapter8/hyperparams.R</kbd>文件夹中，也使用了我们在<em>张量流估算器</em>部分(<kbd>Chapter8/tf_estimators.R</kbd>)中使用的脚本:</p>

<pre>library(tfruns)<br/># FLAGS &lt;- flags(<br/># flag_numeric("layer1", 256),<br/># flag_numeric("layer2", 128),<br/># flag_numeric("layer3", 64),<br/># flag_numeric("layer4", 32),<br/># flag_numeric("dropout", 0.2),<br/># flag_string("activ","relu")<br/># )<br/><br/>training_run('tf_estimators.R')<br/>training_run('tf_estimators.R', flags = list(layer1=128,layer2=64,layer3=32,layer4=16))<br/>training_run('tf_estimators.R', flags = list(dropout=0.1,activ="tanh"))</pre>

<p>这将使用不同的超级参数运行<kbd xmlns:epub="http://www.idpf.org/2007/ops">Chapter8/tf_estimators.R</kbd>脚本。第一次，我们不改变任何超参数，所以它使用包含在<kbd xmlns:epub="http://www.idpf.org/2007/ops">Chapter8/tf_estimators.R</kbd>中的默认值。每次使用分类脚本训练一个新模型时，它被称为一个<strong xmlns:epub="http://www.idpf.org/2007/ops">训练r</strong>T8】un，训练运行的细节被存储在当前工作目录的<kbd xmlns:epub="http://www.idpf.org/2007/ops">runs</kbd>文件夹中。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>对于每次跑步训练，都会弹出一个新网站，提供跑步的详细信息，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-709 image-border" src="img/de5eff63-ba2e-4e08-a031-0eaf17d39e24.png" style="width:46.17em;height:27.58em;"/></p>

<p>图8.8: TensorFlow训练跑步–总结屏幕</p>

<p>我们可以在图中看到训练的进度，以及训练运行发生的时间和评估指标的细节。我们还可以在右下角看到，在训练运行中使用的<strong>标志</strong>(即超参数)也被显示出来。R代码输出有另一个选项卡，它包括内部文件(<kbd>Chapter8/tf_estimators.R</kbd>)中R代码的所有输出，包括绘图。</p>

<p>所有训练运行完成后，以下代码显示了所有训练运行的摘要:</p>

<pre>ls_runs(order=eval_accuracy)<br/>ls_runs(order=eval_accuracy)[,1:5]<br/>Data frame: 3 x 5 <br/>                    run_dir eval_accuracy eval_accuracy_baseline eval_auc eval_auc_precision_recall<br/>3 runs/2018-08-02T19-50-17Z        0.7746                 0.6032   0.8431                    0.8874<br/>2 runs/2018-08-02T19-52-04Z        0.7724                 0.6032   0.8425                    0.8873<br/>1 runs/2018-08-02T19-53-39Z        0.7711                 0.6032   0.8360                    0.8878</pre>

<p>在这里，我们按<kbd>eval_accuracy</kbd>列对结果进行了排序。如果您关闭显示训练运行摘要的窗口，您可以通过调用<kbd>view_run</kbd>函数并传入文件夹名称来再次显示它。例如，要显示最佳训练运行的摘要，请使用以下代码:</p>

<pre>dir1 &lt;- ls_runs(order=eval_accuracy)[1,1]<br/>view_run(dir1)</pre>

<p>最后，还可以对比两次跑步。这里，我们比较两个最佳模型:</p>

<pre>dir1 &lt;- ls_runs(order=eval_accuracy)[1,1]<br/>dir2 &lt;- ls_runs(order=eval_accuracy)[2,1]<br/>compare_runs(runs=c(dir1,dir2))</pre>

<p>这将打开一个类似如下的页面:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-710 image-border" src="img/6fb8f75a-3e4e-4d89-9ba5-25be05430dd2.png" style="width:44.00em;height:29.75em;"/></p>

<p>图8.9:比较两次TensorFlow运行</p>

<p>该页面显示了两次训练运行的评估指标，还显示了所使用的超参数。正如我们所见，这使得管理调整深度学习模型的过程变得更加容易。这种超参数调整方法具有自动记录、可追溯性，并且很容易比较不同组的超参数。您可以看到用于训练跑步的指标和不同的超参数。没有更多的比较配置文件来尝试和匹配超参数设置输出日志！相比之下，我为第七章、<em xmlns:epub="http://www.idpf.org/2007/ops">中的NLP示例的超参数选择编写的代码，使用深度学习的自然语言处理</em>，相比之下显得粗糙</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在本章中，我们开发了一些张量流模型。我们看了TensorBoard，这是一个可视化和调试深度学习模型的伟大工具。我们使用TensorFlow建立了几个模型，包括一个基本的回归模型和一个用于计算机视觉模型的Lenet模型。从这些例子中，我们看到TensorFlow中的编程比我们在本书其他地方使用的高级API(MXNet和Keras)更复杂，也更容易出错。</p>

<p>然后我们开始使用张量流估算器，这是一个比使用张量流更容易的接口。然后我们在另一个名为<strong> tfruns </strong>的包中使用了这个脚本，它代表TensorFlow runs。这个包允许我们每次用不同的标志调用TensorFlow估计器或Keras脚本。我们将此用于超参数选择、运行和评估多个模型。TensorFlow运行与RStudio完美集成，我们能够查看每次运行的摘要，并比较运行，以了解所用指标和超参数的差异。</p>

<p>在下一章，我们将看看嵌入和自动编码器。我们已经在<a href="">第7章</a>、<em>使用深度学习的自然语言处理</em>中看到了嵌入，因此在下一章我们将看到嵌入如何创建数据的低级编码。我们还将使用训练自动编码器，它创建这些嵌入。我们将使用自动编码器进行异常检测和协同过滤(推荐系统)。</p>





            



            

        

    </body>



</html></body></html>