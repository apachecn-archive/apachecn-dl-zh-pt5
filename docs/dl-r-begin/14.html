<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fraud Detection with Autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">利用自动编码器进行欺诈检测</h1>

                

            

            

                

<p class="mce-root">在这一章中，我们继续我们的深度学习之旅，使用R和<strong>自动编码器</strong>。</p>

<p class="mce-root">传统的自动编码器由三部分组成:</p>

<ul>

<li class="mce-root"><strong>一个编码功能</strong>，压缩你的数据</li>

<li class="mce-root"><strong>解码功能</strong>，其从压缩版本中重构数据</li>

<li class="mce-root"><strong>一个度量或距离</strong>，它计算数据压缩后丢失的信息之间的差异</li>

</ul>

<p>我们通常假设所有这些涉及的函数足够平滑，能够使用反向传播或其他基于梯度的方法，尽管它们不需要，我们可以使用无导数的方法来训练它们。</p>

<p>自动编码是将信息从一个潜在的大型功能集中汇总到一个较小的功能集中的过程。</p>

<p>虽然压缩位可能会让您想起算法，如MP3压缩算法，但一个重要的区别是自动编码器是特定于数据的。在猫和狗的图片中训练的自动编码器可能在建筑物的图片中表现不佳。相比之下，MP3压缩算法通常使用声音的假设，并且不管声音数据如何都可以工作。数据专用位对于广泛应用来说是一个严重的警告，这使得自动编码器很少用于压缩任务。</p>

<p>自动编码器近年来吸引如此多关注的一个原因是因为许多人认为它们可能是<strong>无监督学习的关键，</strong>尽管严格来说，它们是一种<strong>自我监督</strong>学习算法。</p>

<p>有时，从自动编码器中提取的特征可以被输入到监督学习算法中，使它们在某种程度上可以与作为维度缩减技术的<strong>主成分分析</strong> ( <strong> PCA </strong>)相媲美。</p>

<p>自动编码器通常用于图像去噪等计算机视觉问题，或者用于提取颜色、光线和边缘等特征。它们也用于大规模数据集的数据可视化，因为它们可以找到比PCA更有趣的特征。其他最近的应用包括欺诈和入侵检测。</p>

<p class="mce-root">出于我们的目的，自动编码器神经网络只是一种用于无监督学习的算法，它通过将目标值设置为等于输入来应用反向传播，如果<em xmlns:epub="http://www.idpf.org/2007/ops"> x <sub> 1，</sub>x</em><em xmlns:epub="http://www.idpf.org/2007/ops">T5】2，...x <sub> m </sub> </em>为训练示例，y<em xmlns:epub="http://www.idpf.org/2007/ops"/><em xmlns:epub="http://www.idpf.org/2007/ops"><sub>1、</sub> y <sub> 2 </sub> </em> <em xmlns:epub="http://www.idpf.org/2007/ops">、...y<sub>m</sub><strong><sub/></strong></em>是标签，那么我们将通过为<em xmlns:epub="http://www.idpf.org/2007/ops"> i <strong>的所有值设置<em xmlns:epub="http://www.idpf.org/2007/ops">x<sub>I</sub>= y<sub>I</sub><strong><sub/></strong></em>来进行反向传播。</strong> </em></p>

<p class="mce-root">从你之前的机器学习经验来看，你可能对PCA比较熟悉。如果您不熟悉它，请不要担心，这不是我们的目的所严格要求的。PCA是一种<strong>降维</strong>技术，也就是说，给定一组训练样本，应用合适的变换(对于数学极客来说，这只是投影到协方差矩阵的特征向量生成的向量空间中)。这种投影的目标是找到输入数据的最相关的特征，以便最终得到它的简化表示。</p>

<p class="mce-root">自动编码器以类似的方式工作，除了涉及的变换不是投影，而是非线性函数<em xmlns:epub="http://www.idpf.org/2007/ops"> f </em>。给定一个训练示例<em xmlns:epub="http://www.idpf.org/2007/ops"> x，</em>自动编码器使用神经网络将<em xmlns:epub="http://www.idpf.org/2007/ops"> x </em>编码成隐藏状态<em xmlns:epub="http://www.idpf.org/2007/ops"> h:=f(x) </em>，使用函数<em xmlns:epub="http://www.idpf.org/2007/ops"> g </em>对<em xmlns:epub="http://www.idpf.org/2007/ops"> h </em>进行解码，从而带来<em xmlns:epub="http://www.idpf.org/2007/ops"> x = &gt; g(f(x))的整体变换。如果这个过程的结果仅仅是g(f(x))= x(T59)，我们就不会有一个非常有用的转换。下图说明了这一想法:</em></p>

<div><img src="img/7c742d79-713b-4b5e-9ec3-6b43f054f648.png" style="width:21.08em;height:19.67em;"/></div>

<p>一个简单的autoencoder在起作用:将一个三维向量编码成一个二维隐藏状态，<br/>然后再回到一个三维空间。</p>

<p class="mce-root">在左边部分，一个三维输入向量被转换成一个二维编码状态(这是<em> f </em>的动作)，然后被转换回一个三维向量(通过<em> g </em>的动作)。</p>

<p>我们为什么要费编码和解码的劲？这有两个目的。一方面，作为PCA，自动编码器提供了一种在低维空间中自动生成特征的方法。这作为机器学习管道的一部分对于<strong>特征提取</strong>是有用的，同样PCA也是有用的。合成数据并自动生成特征(而不是依赖领域专业知识和手工制作的特征)，以提高监督学习算法的准确性，无论是用于分类还是回归任务。就我们的目的而言，它对于异常值检测也是有用的。随着计算机被迫理解数据的本质特征，任何突出的异常都将在重建过程(即完整的编码-解码周期)中被丢弃，异常值将很容易被识别。</p>

<p>在本章开始讨论欺诈的例子之前，让我们先看看一个更简单的例子，同时准备好我们的工具。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Getting ready</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">做好准备</h1>

                

            

            

                

<p>在这一章中，我们将介绍r的<kbd>keras</kbd>和<kbd>tensorflow</kbd>。<kbd>keras</kbd>是一个模型级的建筑，因为它提供了一个高级接口来快速开发深度学习模型。它没有实现卷积和张量积等低级操作，而是依赖后端的Theano、TensorFlow或CNTK，据开发团队称，未来将支持更多后端。</p>

<p>为什么需要后端？好吧，如果计算变得更复杂，这是深度学习中经常出现的情况，你需要使用不同的计算方法(称为计算图)和硬件(GPU)。出于教学目的，我们所有的示例代码都不使用GPU运行。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Installing Keras and TensorFlow for R</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">为R安装Keras和TensorFlow</h1>

                

            

            

                

<p>根据官方文档，您可以简单地安装Keras:</p>

<div><pre class="sourceCode r">devtools::install_github("rstudio/keras")</pre></div>

<p>Keras R接口默认使用<kbd>tensorflow</kbd>作为后端引擎。要安装核心的<kbd>keras</kbd>库和<kbd>tensorflow</kbd>，那么做:</p>

<pre class="sourceCode r">library(keras)

install_keras()</pre>

<p>不管这看起来有多顺利，我们已经多次看到这种设置失败。我们推荐的B计划是:</p>

<ul>

<li>下载并安装Python的Anaconda发行版。这是标准的数据科学堆栈，包含数据科学家最常用的Python库。</li>

<li>安装Keras后端，例如Tensorflow。</li>

<li>安装Keras(在Python中)。</li>

</ul>

<p>在这之后，您应该准备好在r中安装Keras了。</p>

<p>为了确保您使用的是最新版本的R，您可以使用<kbd>installr</kbd>包从R GUI(而不是RStudio)更新它:</p>

<pre>### Run these from RGUi<br/> install.packages("installr")<br/> installr::updateR()</pre>

<p>一旦完成，你就可以用<kbd>devtools</kbd>安装<kbd>keras</kbd>:</p>

<pre>install.packages("devtools")<br/>devtools::install_github("rstudio/keras")</pre>

<p class="mce-root">最后，通过导入库来检查一切是否正确:</p>

<pre>library(keras)</pre>

<p class="mce-root">前面的说明适用于运行3.4版的Windows 10笔记本电脑。如果您有一个较旧的版本，并且您不想更新，您需要安装如下指定的依赖项(我在3.3版中尝试过):</p>

<pre>install.packages("Rcpp")<br/>install.packages("devtools")<br/>devtools::install_github("rstudio/reticulate", force=TRUE)<br/>devtools::install_github("r-lib/debugme")<br/>devtools::install_github("r-lib/processx")<br/>devtools::install_github("tidyverse/rlang")<br/>devtools::install_github("tidyverse/glue")<br/>devtools::install_github("tidyverse/tidyselect")<br/>devtools::install_github("rstudio/tfruns")<br/>devtools::install_github("rstudio/tensorflow")<br/>devtools::install_github("rstudio/keras")<br/>devtools::install_github("jeroen/jsonlite")</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Installing H2O</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">安装H2O</h1>

                

            

            

                

<p>我们还将向您展示如何在一些练习中使用H2O。安装H2O最简单的方法是从克兰获得它。</p>

<p>它有一些依赖项，特别是包:</p>

<ul class="simple">

<li><kbd>RCurl</kbd></li>

<li><kbd>bitops</kbd></li>

<li><kbd>rjson</kbd></li>

<li><kbd>statmod</kbd></li>

<li><kbd>tools</kbd></li>

</ul>

<p>如果你遇到麻烦，很可能是缺少了某种依赖。重新阅读错误消息并安装任何丢失的软件包。</p>

<p>如果一切顺利，导入库并初始化服务以检查一切正常:</p>

<pre>install.packages("h2o")<br/>library(h2o)<br/>h2o.init()</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Our first examples</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们的第一个例子</h1>

                

            

            

                

<p>让我们从几个简单的例子开始，来理解这是怎么回事。</p>

<p>对于我们中的一些人来说，很容易被诱惑去尝试最闪亮的算法并进行超参数优化，而不是不那么迷人的逐步理解。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>A simple 2D example</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">一个简单的2D例子</h1>

                

            

            

                

<p>让我们用一个简单的二维例子来发展我们对autoencoder如何工作的直觉。</p>

<p>我们首先从均值为0、方差为1的正态分布中生成10，000个点:</p>

<pre>library(MASS)<br/>library(keras)<br/>Sigma &lt;- matrix(c(1,0,0,1),2,2)<br/>n_points &lt;- 10000<br/>df &lt;- mvrnorm(n=n_points, rep(0,2), Sigma)<br/>df &lt;- as.data.frame(df)</pre>

<p>值的分布应该如下所示:</p>

<div><img src="img/a14eb1dc-9201-4586-b536-0d005cee8c94.png" style="width:25.17em;height:17.67em;"/></div>

<p>我们刚刚生成的变量V1的分布；变量V2看起来相当相似。</p>

<div><img src="img/f4b831c4-dd08-4f4d-9763-9c7b52b398c3.png" style="width:24.58em;height:14.33em;"/></div>

<p>我们生成的变量V1和V2的分布。</p>

<p>让我们增加一点趣味，在混合物中加入一些异常值。在许多欺诈应用中，欺诈率约为1–5%，因此我们从正态分布中抽取1%的样本，均值为5，标准差为1:</p>

<pre># Set the outliers<br/> n_outliers &lt;- as.integer(0.01*n_points)<br/> idxs &lt;- sample(n_points,size = n_outliers)<br/> outliers &lt;- mvrnorm(n=n_outliers, rep(5,2), Sigma)<br/> df[idxs,] &lt;- outliers</pre>

<p class="mce-root">新的点数分布现在看起来像这样:</p>

<div><img src="img/8e91d36a-e246-45f1-8895-c2238c0dcefb.png" style="width:24.08em;height:14.75em;"/></div>

<p>添加异常值后点的新分布。</p>

<p>我们将在隐藏层使用一个带有单个神经网络的自动编码器。为什么不多加一些？问题是，如果隐藏状态的维数等于或高于输入状态的维数，那么我们的模型可能会学习恒等函数，也就是说，模型在任何地方都会学习<em> g(f(x))=x </em>。这显然不是一个非常有用的异常值识别方法。我们需要捕捉数据的基本特征，这样那些不寻常的特征将在以后被强调，从而允许检测异常值。</p>

<p>用<kbd>keras,</kbd>建立模型真的很容易，我们需要一个形状为2的输入层，作为我们的二维例子。使用ReLU激活函数将其传递给我们的一维编码器，然后解码回二维空间:</p>

<pre>input_layer &lt;- layer_input(shape=c(2))<br/> encoder &lt;- layer_dense(units=1, activation='relu')(input_layer)<br/> decoder &lt;- layer_dense(units=2)(encoder)<br/> autoencoder &lt;- keras_model(inputs=input_layer, outputs = decoder)</pre>

<p>在使用模型之前，我们需要编译它。我们需要指定一个损失函数、一个要优化的度量以及一个执行梯度下降更新的算法。我们将使用Adam解算器，优化经典的均方误差(在此问题中有效，但我们可能需要针对我们的特定应用进行更改),并选择精度作为要优化的指标:</p>

<pre>autoencoder %&gt;% compile(optimizer='adam',<br/>loss='mean_squared_error',<br/>metrics=c('accuracy'))</pre>

<p>设置完成后，我们就可以开始训练了:</p>

<pre># Coerce the dataframe to matrix to perform the training<br/> df &lt;- as.matrix(df)<br/> history &lt;- autoencoder %&gt;% fit(<br/> df,df,<br/> epochs = 30, batch_size = 128,<br/> validation_split = 0.2<br/> )</pre>

<p>使用命令<kbd>plot(history),</kbd>,我们可以看到本例的训练情况:</p>

<div><img src="img/4900279d-8de1-4506-906c-c2c5080f61b2.png" style="width:32.50em;height:23.25em;"/></div>

<p>培训我们的自动编码器。</p>

<p>因此，我们看到，虽然准确率仍然相当高，但在训练过程中有一个神秘的下降。我们现在不应该太担心它，我们以后会谈到这个问题。至于损失，随着我们添加更多的数据，它会不断减少，这是意料之中的。</p>

<p>最后，我们来看重建。我们首先从经过训练的自动编码器生成预测:</p>

<pre>preds &lt;- autoencoder %&gt;% predict(df)<br/> colnames(preds) &lt;- c("V1", "V2")<br/> preds &lt;- as.data.frame(preds)</pre>

<p>这是根据我们的自动编码器重建的点。我们将把那些重建距离原始图像(欧几里德)距离大于3的点涂成红色，而把其他的点涂成蓝色。为什么是这些点？我们的自动编码器了解到我们的数据集具有一些内在属性(它了解平均点的分布)，因此在那些重建误差异常大的点上，可能有一些值得关注的东西:</p>

<pre># Coerce back the matrix to data frame to use ggplot later<br/>df &lt;- as.data.frame(df)<br/># Euclidean distance larger than 3 = sum of squares larger than 9<br/>df$color &lt;- ifelse((df$V1-preds$V1)**2+(df$V2-preds$V2)**2&gt;9,"red","blue")</pre>

<p>最后，我们可以用<kbd>ggplot</kbd>来看看结果:</p>

<pre>library(ggplot2)<br/> df %&gt;% ggplot(aes(V1,V2),col=df$color)+geom_point(color = df$color, position="jitter")</pre>

<p>下面的截图显示了我们在识别异常点方面做得有多好:</p>

<div><img src="img/1b422fb9-4b5c-4379-a11f-32c6c59fdb3c.png" style="width:34.17em;height:24.50em;"/></div>

<p>自动编码器的输出。蓝色的是我们的自动编码器重建的点。红色表示数据集中的原始点。我们看到有一团红点，由于我们的自动编码器，我们可以识别出不同寻常。</p>

<p>在前面的屏幕截图中，蓝点是来自自动编码器的重建图像。我们看到，正如所料，它正确地了解到大多数点来自以(<em> 0，0 </em>)为中心的正态分布。然而，仍有一些点在原始数据集上是正常的，但被指出是不正常的。没有必要这么快就放弃学习自动编码器，原因是我们使用的自动编码器相当简单。我们将研究更复杂的方法来解决自动编码器的异常检测问题。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Autoencoders and MNIST</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">自动编码器和MNIST</h1>

                

            

            

                

<div><p>研究论文、博客帖子或书籍中的许多深度学习算法的例子都涉及MNIST数据集。我们不应该例外，介绍一个使用MNIST的自动编码器的小用例。</p>

</div>

<p>动机是这样的，假设你想自动检测假钞。然后，你需要教会计算机<em>平均钞票</em>的表示是什么，以便能够检测出那些有显著差异的钞票。由于全球范围内每天都有大量的现金交易发生，并且欺诈者越来越狡猾，因此手动完成这一过程是不可想象的。一种方法是使用复杂的成像软件，这就是验钞机(如D40或D50)的工作原理。</p>

<p>使用MNIST的另一个原因显然是实际的。在撰写本文时，我无法找到一个很好的伪钞训练数据集，MNIST已经预装在<kbd>keras</kbd>中。</p>

<p>我们从加载数据集开始:</p>

<pre>library(keras)<br/>mnist &lt;- dataset_mnist()<br/>X_train &lt;- mnist$train$x<br/>y_train &lt;- mnist$train$y<br/>X_test &lt;- mnist$test$x<br/>y_test &lt;- mnist$test$y</pre>

<p>让我们仔细看看数据集:</p>

<pre>image(X_train[1,,], col=gray.colors(3))<br/>y_train[1]</pre>

<p>如果一切正常，您应该会看到数字5的图像。</p>

<p class="mce-root">在训练我们的自动编码器之前，我们需要做一些预处理。<kbd>X_train</kbd>数据是灰度值的三维数组(图像、宽度、高度)。我们需要首先将这些数组转换成矩阵，将高度和宽度转换成一个向量，这样我们就有了一个<em> 28*28=784 </em>向量，而不是处理28 x 28的正方形。然后，我们将灰度值从范围在0到255之间的整数转换成范围在0到1之间的浮点值:</p>

<pre># reshape<br/>dim(X_train) &lt;- c(nrow(X_train), 784)<br/>dim(X_test) &lt;- c(nrow(X_test), 784)<br/># rescale<br/>X_train &lt;- X_train / 255<br/>X_test &lt;- X_test / 255</pre>

<p>一旦初始预处理完成，我们就定义了自动编码器的拓扑结构。让我们使用具有32个神经元的编码层，以实现<em> 784/32 = 24.5: </em>的压缩比</p>

<pre>input_dim &lt;- 28*28 #784<br/>inner_layer_dim &lt;- 32<br/>input_layer &lt;- layer_input(shape=c(input_dim))<br/>encoder &lt;- layer_dense(units=inner_layer_dim, activation='relu')(input_layer)<br/>decoder &lt;- layer_dense(units=784)(encoder)<br/>autoencoder &lt;- keras_model(inputs=input_layer, outputs = decoder)</pre>

<p>我们准备编译和训练模型:</p>

<pre>autoencoder %&gt;% compile(optimizer='adam', <br/> loss='mean_squared_error', <br/> metrics=c('accuracy'))<br/>history &lt;- autoencoder %&gt;% fit(<br/> X_train,X_train, <br/> epochs = 50, batch_size = 256, <br/> validation_split=0.2<br/>)</pre>

<p>使用<kbd>plot</kbd>命令，我们可以看到我们的模型在训练和验证过程中的表现如下:</p>

<pre>plot(history)</pre>

<p>更好的是，如果您使用RStudio作为您的IDE，在查看器面板中有一个实时视图。使用此命令，您应该会看到两个曲线图，显示精度和损耗与历元数的函数关系:</p>

<div><img src="img/9bb1324b-7548-47e1-8ff0-5caf1c5412b3.png"/></div>

<p>使用查看器面板实时查看培训。点击在新窗口中打开按钮:</p>

<div><img src="img/174eb455-328c-4bc0-9e48-9e35190cfd78.png"/></div>

<p>实时训练。横轴表示纪元编号。损耗和精度显示在纵轴上。</p>

<p>使用<kbd>predict</kbd>方法，我们重建数字并计算重建误差:</p>

<pre># Reconstruct on the test set<br/>preds &lt;- autoencoder %&gt;% predict(X_test)<br/>error &lt;- rowSums((preds-X_test)**2)<br/>error</pre>

<p>直觉上，有些类可能更难预测，因为有些人写数字的方式略有不同。哪些类的重建误差较高？</p>

<pre># Which were more problematic to reconstruct?<br/>eval &lt;- data.frame(error=error, class=as.factor(y_test))<br/>library(dplyr)<br/>eval %&gt;% group_by(class) %&gt;% summarise(avg_error=mean(error))<br/><br/>## OUTPUT<br/># A tibble: 10 x 2<br/> class avg_error<br/> &lt;fctr&gt; &lt;dbl&gt;<br/> 1 0 14.091963<br/> 2 1 6.126858<br/> 3 2 17.245944<br/> 4 3 14.138960<br/> 5 4 13.189842<br/> 6 5 15.170581<br/> 7 6 14.570642<br/> 8 7 11.778826<br/> 9 8 16.120203<br/>10 9 11.645038</pre>

<p>注意，一些小的变化是预期的，因为例如在数据的混洗中涉及到随机成分。然而，总的趋势应该是相似的。</p>

<p>一幅图像比一千个单词更有意义，因此，甚至比用<kbd>dplyr</kbd>总结我们的数据更好的是，我们可以使用<kbd>ggplot2</kbd>来可视化这些信息:</p>

<pre>library(ggplot2)<br/>eval %&gt;% <br/> group_by(class) %&gt;% <br/> summarise(avg_error=mean(error)) %&gt;% <br/> ggplot(aes(x=class,fill=class,y=avg_error))+geom_col()</pre>

<p>我们可以看到，如下，我们的重建误差如何执行每类。这很重要，因为它将让我们知道我们的分类器是否在某些方面有偏差，或者它是否发现某些类比其他类更难训练:</p>

<div><img src="img/0b773ead-6138-4356-98e5-81f2b285ddb8.png" style="width:28.83em;height:20.42em;"/></div>

<p>MNIST数据集中的重建误差。2和8似乎是最有问题的类别，<br/>和1似乎是最容易识别的。</p>

<p>这当然很有用，也很有趣，我们看到2有点难以重建，这可能是因为它有时看起来像7。直觉上，8很容易与9或0混淆，所以结果多少有点道理。</p>

<p>一个更好的方式来看看我们的重建自动编码器是如何执行的，是直接看重建的例子。为此，我们首先需要重塑原始数据并进行重建:</p>

<pre># Reshape original and reconstructed<br/>dim(X_test) &lt;- c(nrow(X_test),28,28)<br/>dim(preds) &lt;- c(nrow(preds),28,28)</pre>

<p>现在让我们看看重建的图像:</p>

<pre class="mce-root">image(255*preds[1,,], col=gray.colors(3))</pre>

<p>让我们看一下测试集的一个典型元素的重建图像:</p>

<div><img src="img/70a6cf34-6e45-40ef-9fce-8d45a93c166e.png" style="width:15.75em;height:11.17em;"/></div>

<p>由我们的自动编码器重建的图像。</p>

<p>与重建前的原始图像相比如何？让我们来看看:</p>

<pre class="mce-root">y_test[1]<br/> image(255*X_test[1,,], col=gray.colors(3))</pre>

<div><img src="img/6dbd821f-cdb4-4666-ba8f-edfb51a0b83c.png" style="width:12.83em;height:7.08em;"/></div>

<p>原图。</p>

<p>总的来说，对于24.5的压缩来说还不错！显然还有很多需要改进的地方，但我们已经可以看到自动编码器学习数据内在特征的潜力。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Outlier detection in MNIST</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">MNIST的异常检测</h1>

                

            

            

                

<p>好吧，无可否认，我们之前的应用程序到目前为止与欺诈或异常值检测无关。我们可以对前面的设置做一个小小的修改，来展示一个类似的框架是如何工作的。为此，让我们假设数字7是一个异常类，我们将尝试从我们的<em>正常</em>数字:0，1，2，3，4，5，6，8，9的结果中识别它。</p>

<p>我们将在<em>普通</em>数据集上训练自动编码器，然后将其应用于测试集。我们的目标是尽可能多地抽象出<em>正常</em>情况的特征。这需要了解<em>正常</em>情况，这转化为标记数据的可用性，因此，这是一个理想的场景，对于许多实际应用，例如信用卡欺诈或入侵检测，我们有时(或者更经常)缺少这样的标记数据。</p>

<p>我们像以前一样开始:</p>

<pre>library(keras)<br/>mnist &lt;- dataset_mnist()<br/>X_train &lt;- mnist$train$x<br/>y_train &lt;- mnist$train$y<br/>X_test &lt;- mnist$test$x<br/>y_test &lt;- mnist$test$y</pre>

<p>但是现在我们将从训练集中排除7，因为在我们的例子中它将是异常值。</p>

<pre>## Exclude "7" from the training set. "7" will be the outlier<br/>outlier_idxs &lt;- which(y_train!=7, arr.ind = T)<br/>X_train &lt;- X_train[outlier_idxs,,]<br/>y_test &lt;- sapply(y_test, function(x){ ifelse(x==7,"outlier","normal")})</pre>

<p>我们继续像以前一样，在定义我们的自动编码器之前进行重新缩放和整形:</p>

<pre># reshape<br/>dim(X_train) &lt;- c(nrow(X_train), 784)<br/>dim(X_test) &lt;- c(nrow(X_test), 784)<br/># rescale<br/>X_train &lt;- X_train / 255<br/>X_test &lt;- X_test / 255<br/>input_dim &lt;- 28*28 #784<br/>inner_layer_dim &lt;- 32<br/># Create the autoencoder<br/>input_layer &lt;- layer_input(shape=c(input_dim))<br/>encoder &lt;- layer_dense(units=inner_layer_dim, activation='relu')(input_layer)<br/>decoder &lt;- layer_dense(units=784)(encoder)<br/>autoencoder &lt;- keras_model(inputs=input_layer, outputs = decoder)<br/>autoencoder %&gt;% compile(optimizer='adam', <br/>                        loss='mean_squared_error', <br/>                        metrics=c('accuracy'))<br/>history &lt;- autoencoder %&gt;% fit(<br/> X_train,X_train, <br/> epochs = 50, batch_size = 256, <br/> validation_split=0.2<br/>)<br/>plot(history)</pre>

<p>一旦对自动编码器进行了训练，我们就可以使用测试集的重建来开始查看性能:</p>

<pre># Reconstruct on the test set<br/>preds &lt;- autoencoder %&gt;% predict(X_test)<br/>error &lt;- rowSums((preds-X_test)**2)<br/>eval &lt;- data.frame(error=error, class=as.factor(y_test))<br/>library(ggplot2)<br/>library(dplyr)<br/>eval %&gt;% <br/> group_by(class) %&gt;% <br/> summarise(avg_error=mean(error)) %&gt;% <br/> ggplot(aes(x=class,fill=class,y=avg_error))+geom_boxplot()</pre>

<p>让我们看看不同类中的重构误差:</p>

<div><img src="img/0a65431c-8f8f-4f4d-80b5-a935c9dc2a55.png" style="width:27.58em;height:19.58em;"/></div>

<p>测试集中重构误差的分布</p>

<p>从图中我们看到，我们可以在<kbd>15,</kbd>设置阈值，也就是说，重建误差大于<kbd>15</kbd>的观察值将被标记为异常值:</p>

<pre class="mce-root">threshold &lt;- 15<br/>y_preds &lt;- sapply(error, function(x) ifelse(x&gt;threshold,"outlier","normal")})</pre>

<p>一旦这样做了，我们就可以计算混淆矩阵。这是一种可视化模型正在做什么的有用方法:</p>

<pre class="mce-root"># Confusion matrix<br/>table(y_preds,y_test)</pre>

<p>这为我们提供了以下信息:</p>

<pre class="mce-root">         y_test<br/>y_preds   normal  outlier<br/>  normal    5707      496<br/>  outlier   3265      532</pre>

<p>所以很明显我们可以做得更好。也许数字1和7共有的竖线造成了巨大的错误率。然而，我们用这个简单的架构捕获了超过50%的异常情况。改善这种情况的一个方法是添加更多的隐藏层。我们将在本章的后面使用这个技巧。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Credit card fraud detection with autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用自动编码器检测信用卡欺诈</h1>

                

            

            

                

<p>欺诈是一个价值数十亿美元的行业，其中信用卡欺诈可能是与我们日常生活最密切相关的。欺诈始于盗窃实体信用卡或可能危及账户安全的数据，如信用卡号、有效期和安全码。如果受害者知道他们的卡被盗，则可以直接报告被盗卡，然而，当数据被盗时，受损的账户可能需要几周甚至几个月才能被使用，并且受害者仅从他们的银行对账单中知道该卡被使用过。</p>

<p>传统上，欺诈检测系统依赖于主题专家手动设计的功能，直接与金融机构或专业软件供应商合作。</p>

<p>欺诈检测中最大的挑战之一是标记数据集的可用性，这些数据集通常很难甚至不可能获得。</p>

<p>我们的第一个欺诈案例来自比利时布鲁塞尔自由大学的研究人员在ka ggle(【https://www.kaggle.com/dalpozz/creditcardfraud】)公开的数据集(完整内容，请阅读他们的论文:Andrea Dal Pozzolo、Olivier Caelen、Reid A. Johnson和Gianluca Bontempi，<em xmlns:epub="http://www.idpf.org/2007/ops">)校准不平衡分类的欠采样概率。</em> <em xmlns:epub="http://www.idpf.org/2007/ops">《计算智能与数据挖掘研讨会(CIDM)，IEEE，2015) </em>。</p>

<p>这些数据集包含欧洲持卡人在2013年9月的两天内通过信用卡进行的交易。在284，807笔交易中，我们有492笔欺诈。不像玩具数据集(我正看着你，Iris)，现实生活中的数据集是高度不平衡的。在本例中，正面类别(欺诈)占所有交易的0.172%。</p>

<p>它只包含数字信息因子，这些因子是PCA变化的后效。因为分类问题，创作者不能给出第一个亮点和更多关于信息的基础数据。特色V1，V2，...V28是PCA得到的主要片段，PCA没有改变的主要特征是<kbd>Time</kbd>和<kbd>Amount.</kbd></p>

<p>特性<kbd>Time</kbd>包含数据集中每个事务和第一个事务之间经过的秒数。特征<kbd>Amount</kbd>是交易的金额，该特征可以用于依赖于示例的、成本敏感的学习。特征<kbd>Class</kbd>是响应变量，在欺诈的情况下取值<kbd>1</kbd>，否则取值<kbd>0</kbd>。鉴于类别不平衡比率，作者建议测量<strong>精确召回曲线</strong> ( <strong> AUC </strong>)下的面积，而不是混淆矩阵。精确度-召回曲线也被称为<strong> ROC </strong>(接收者-操作者特性)。</p>

<p>此时你可能会想:既然这显然是一个二进制分类问题，而且我们已经有了标记数据，我为什么还要为自动编码器费心呢？当然，您可以走传统的方式，尝试使用标准的监督学习算法，如随机森林或支持向量机，只是要小心，要么对欺诈类进行过采样，要么对正常类进行欠采样，以便这些方法能够很好地执行。然而，在许多现实生活中，我们事先没有标记的数据，在复杂的欺诈场景中，可能很难获得准确的标签。假设你是一个愿意实施诈骗的罪犯。</p>

<p>在欺诈之前(甚至之后)，您的帐户可能有完全正常的活动。我们要不要把你所有的交易都标记为欺诈？还是只有某个子集？一些业内人士可能会争辩说，毕竟，交易是由罪犯进行的，所以它们在某种程度上是有污点的，我们应该标记你的所有活动，将偏见引入模型。我们不再依赖标签，而是像以前一样，将问题视为<em>异常检测</em>或<em>异常检测</em>问题，并使用自动编码器。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exploratory data analysis</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">探索性数据分析</h1>

                

            

            

                

<p>一个经常被忽视的步骤是探索性数据分析。在直接进入数据并试图做花哨的深度学习架构之前，让我们后退一步，看看我们周围有什么。</p>

<p>让我们从从ka ggle:(【https://www.kaggle.com/dalpozz/creditcardfraud】)下载数据集并导入到R:</p>

<pre>df &lt;- read.csv("./data/creditcard.csv", stringsAsFactors = F)<br/>head(df)</pre>

<p>在继续之前，我们应该做一个基本的理智检查。我们应该寻找的一些东西是:</p>

<ul>

<li>验证确实只有两个类别(<kbd>0</kbd>表示正常交易，<kbd>1</kbd>表示欺诈交易)</li>

<li>验证时间戳对应于两天</li>

<li>检查没有丢失的值</li>

</ul>

<p>一旦完成，我们可以进行两个快速检查，一个想法是看看在一天的时间和数量之间是否有一个明显的模式。也许欺诈交易发生在某个特定的时间，当我们的系统易受攻击的时候？我们应该首先检查这个:</p>

<pre>library(ggplot2)<br/>library(dplyr)<br/> df %&gt;% ggplot(aes(Time,Amount))+geom_point()+facet_grid(Class~.)</pre>

<p>首先，让我们看看是否有一些季节性模式。我们只是针对每类的数量绘制时间变量:</p>

<div><img src="img/16d73079-ec1f-4f73-9bfe-9a89c0e0dcfb.png" style="width:33.17em;height:23.75em;"/></div>

<p>欺诈快速检查:0级对应正常交易，1级对应欺诈交易。</p>

<p>所以没什么特别的。有趣的是，欺诈交易涉及的金额远低于正常交易。这表明我们应该过滤掉交易，并以正确的尺度来看待它们。为此，让我们使用<kbd>dplyr</kbd>，过滤掉<kbd>300</kbd>以上的交易，并查看较小的交易:</p>

<pre> df$Class &lt;- as.factor(df$Class)<br/> df %&gt;%filter(Amount&lt;300) %&gt;%ggplot(aes(Class,Amount))+geom_violin()</pre>

<p>按阶级分布看起来怎么样？下面的情节告诉我们一些事情:</p>

<div><img src="img/90ac604c-97c7-4900-840c-eb2a00145861.png" style="width:30.83em;height:22.08em;"/></div>

<p>对数据的初步了解:欺诈交易中涉及的金额似乎比非欺诈交易中涉及的金额更有可能在100 <br/>左右。</p>

<p>啊哈！所以我们得到了对数据的第一次洞察！欺诈交易，虽然小得多，但通常集中在100左右。这可能是欺诈者策略的一部分，他们不是定期持有大量现金，而是在一定时间内或多或少地隐藏少量现金。</p>

<p>当然，发现这一点很有趣，但是这绝对不是一个可扩展的方法，并且需要领域知识和直觉。是时候尝试更复杂的东西了。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The autoencoder approach – Keras</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">自动编码器方法——Keras</h1>

                

            

            

                

<p>好了，是时候进入Keras了。我们应该留出一小部分数据作为验证或测试集，并在剩余的数据上开发模型。对于如何做到这一点，没有金科玉律。对于本例，我们将使用10%的测试集和90%的训练集:</p>

<pre># Remove the time and class column<br/>idxs &lt;- sample(nrow(df), size=0.1*nrow(df))<br/>train &lt;- df[-idxs,]<br/>test &lt;- df[idxs,]<br/>y_train &lt;- train$Class<br/>y_test &lt;- test$Class<br/>X_train &lt;- train %&gt;% select(-one_of(c("Time","Class")))<br/>X_test &lt;- test %&gt;% select(-one_of(c("Time","Class")))<br/># Coerce the data frame to matrix to perform the training<br/>X_train &lt;- as.matrix(X_train)<br/>X_test &lt;- as.matrix(X_test)</pre>

<p>注意，我们还排除了<kbd>Class</kbd>和<kbd>Time</kbd>列。我们忽略了标签，并将我们的欺诈检测问题视为无监督学习问题，因此我们需要从训练数据中移除标签列。至于时间信息，正如我们之前看到的，似乎没有明显的时间趋势。此外，在现实生活中的欺诈检测场景中，我们更关注欺诈者的内在属性，例如，所使用的设备、来自CRM系统的地理位置信息或数据，以及帐户属性(余额、平均交易量等)。</p>

<p>对于自动编码器的架构，我们现在将使用一个<strong>堆栈自动编码器</strong>，而不是像以前一样使用一个中间层。堆叠式自动编码器只不过是几层编码器，后面跟着几层解码器。在这种情况下，我们将使用具有14个完全连接的神经元的外部编码器-解码器层、7个神经元的两个内部层和7个神经元的另一个内部层的网络。您可以试验不同的架构，并将结果与我们的进行比较，对于自动编码器来说，没有放之四海而皆准的架构，它仅仅依赖于经验以及通过验证图和其他指标来诊断您的模型。</p>

<p>在每种情况下，我们的输入(和输出)维度都是<kbd>29</kbd>。构建自动编码器的代码是:</p>

<pre>library(keras)<br/>input_dim &lt;- 29<br/>outer_layer_dim &lt;- 14<br/>inner_layer_dim &lt;- 7<br/>input_layer &lt;- layer_input(shape=c(input_dim))<br/>encoder &lt;- layer_dense(units=outer_layer_dim, activation='relu')(input_layer)<br/>encoder &lt;- layer_dense(units=inner_layer_dim, activation='relu')(encoder)<br/>decoder &lt;- layer_dense(units=inner_layer_dim)(encoder)<br/>decoder &lt;- layer_dense(units=outer_layer_dim)(decoder)<br/>decoder &lt;- layer_dense(units=input_dim)(decoder)<br/>autoencoder &lt;- keras_model(inputs=input_layer, outputs = decoder)</pre>

<p>我们可以看看我们的工作来检查一切是否正确:</p>

<pre>autoencoder<br/> Model<br/> _________________________________________________________________________________<br/> Layer (type) Output Shape Param #<br/> ============================================================================<br/> input_5 (InputLayer) (None, 29) 0<br/> _________________________________________________________________________________<br/> dense_17 (Dense) (None, 14) 420<br/> _________________________________________________________________________________<br/> dense_18 (Dense) (None, 7) 105<br/> _________________________________________________________________________________<br/> dense_22 (Dense) (None, 7) 56<br/> _________________________________________________________________________________<br/> dense_23 (Dense) (None, 7) 56<br/> _________________________________________________________________________________<br/> dense_24 (Dense) (None, 14) 112<br/> _________________________________________________________________________________<br/> dense_25 (Dense) (None, 29) 435<br/> ==========================================================================<br/> Total params: 1,184<br/> Trainable params: 1,184<br/> Non-trainable params: 0</pre>

<p>我们现在准备开始训练。我们应该首先编译模型，然后进行拟合:</p>

<pre>autoencoder %&gt;% compile(optimizer='adam',<br/>                        loss='mean_squared_error',<br/>                        metrics=c('accuracy'))</pre>

<pre class="mce-root">history &lt;- autoencoder %&gt;% fit(<br/> X_train,X_train,<br/> epochs = 10, batch_size = 32,<br/> validation_split=0.2<br/> )</pre>

<pre class="mce-root">plot(history)</pre>

<p>我们的结果如下所示。您可以看到，随着历元数量的增加，精确度也在提高:</p>

<div><img src="img/7b74c309-5159-45d2-803c-9af4c72a2ff1.png" style="width:40.50em;height:22.33em;"/></div>

<p>我们14-7-7-7-14架构的诊断图。</p>

<p>一旦我们准备好了自动编码器，我们就用它来重建测试集:</p>

<pre># Reconstruct on the test set<br/> preds &lt;- autoencoder %&gt;% predict(X_test)<br/> preds &lt;- as.data.frame(preds)</pre>

<p>我们将寻找异常大的重建误差，像以前一样，被标记为异常。例如，我们可以查看那些重建误差大于<kbd>30</kbd>的点，并将其声明为异常:</p>

<pre>y_preds &lt;- ifelse(rowSums((preds-X_test)**2)/30&lt;1,rowSums((preds-X_test)**2)/30,1)</pre>

<p>同样，这个阈值并不是一成不变的，在您的特定应用中使用您的测试集，您可以对其进行微调，并找到最适合您的问题的阈值。</p>

<p>最后，让我们生成ROC曲线，看看我们的模型是否正确运行，使用:</p>

<pre>library(ROCR)<br/>pred &lt;- prediction(y_preds, y_test)<br/>perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr") <br/>plot(perf, col=rainbow(10))</pre>

<p>我们看到结果是令人满意的。我们的曲线看起来很直，原因是我们的模型的输出只是二进制的，以及我们的原始标签。当您的模型输入类别概率或其代理时，曲线会更平滑:</p>

<div><img src="img/0e7b7004-1329-427d-99ba-1bc63d2db77d.png" style="width:28.92em;height:18.25em;"/></div>

<p>ROC曲线:它看起来很直，因为模型的输出不是类别概率，而是二元的。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fraud detection with H2O</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">利用H2O进行欺诈检测</h1>

                

            

            

                

<p>让我们尝试一个稍微不同的工具，它可能在实际部署中对我们有所帮助。在不断发展的数据科学领域中尝试不同的工具通常是有用的，即使只是为了检查是否健康。</p>

<p>H2O是一款用于大数据分析的开源软件。这家年轻的初创公司(成立于2016年)的顾问委员会中有数学优化和统计学习理论方面的顶级研究人员。它运行在标准环境(Linux/Mac/Windows)以及大数据系统和云计算环境中。</p>

<p>您可以在R中运行H2O，但是您需要首先安装软件包:</p>

<pre>install.packages("h2o")</pre>

<p>完成后，您就可以加载库了:</p>

<pre>library(h2o)</pre>

<p>然后，您将看到一条欢迎消息，以及一些警告(对其他包屏蔽的对象):</p>

<pre>Your next step is to start H2O:<br/> &gt; h2o.init()<br/>For H2O package documentation, ask for help:<br/> &gt; ??h2o<br/>After starting H2O, you can use the Web UI at http://localhost:54321<br/> For more information visit http://docs.h2o.ai</pre>

<p>让我们开始吧，然后我们就可以开始工作了:</p>

<pre>h2o.init()</pre>

<p>现在我们需要将数据读入H2O。由于计算工作方式有些不同，我们不能使用R中的普通数据帧结构，所以我们要么像往常一样读取文件，然后强制它:</p>

<pre class="mce-root">df &lt;- read.csv("./data/creditcard.csv", stringsAsFactors = F)<br/> df &lt;- as.h2o(df)</pre>

<p>或者我们用<kbd>h2o.uploadFile</kbd>函数读取:</p>

<pre>df2 &lt;- h2o.uploadFile("./data/creditcard.csv")</pre>

<p>无论哪种方式，产生的结构类型不再是数据帧，而是环境。</p>

<p>像往常一样，让我们留出一部分数据用于训练，一部分用于测试。在<kbd>h2o,</kbd>中，我们可以使用<kbd>h2o.splitFrame</kbd>功能:</p>

<pre>splits &lt;- h2o.splitFrame(df, ratios=c(0.8), seed=1)<br/> train &lt;- splits[[1]]<br/> test &lt;- splits[[2]]</pre>

<p>现在，让我们来区分特征和标签，这一点很快就会派上用场:</p>

<pre>label &lt;- "Class"<br/> features &lt;- setdiff(colnames(train), label)</pre>

<p>我们已经准备好开始训练我们的自动编码器:</p>

<pre>autoencoder &lt;- h2o.deeplearning(x=features,<br/> training_frame = train,<br/> autoencoder = TRUE,<br/> seed = 1,<br/> hidden=c(10,2,10),<br/> epochs = 10,<br/> activation = "Tanh")</pre>

<p>有些评论是适当的。如您所料，autoencoder参数设置为true。这一次我们将使用稍微不同的体系结构，只是为了便于说明。你可以在<kbd>hidden</kbd>参数中看到，各层的结构。我们还将使用不同的激活函数。在实践中，有时使用有界激活函数是有用的，比如用tanh代替ReLu，它在数值上可能是不稳定的。</p>

<p>我们可以用与<kbd>keras</kbd>相似的方式生成重建图像:</p>

<pre># Use the predict function as before<br/>preds &lt;- h2o.predict(autoencoder, test)</pre>

<p>我们得到这样的结果:</p>

<pre>&gt; head(preds)<br/> reconstr_Time reconstr_V1 reconstr_V2 reconstr_V3 reconstr_V4 reconstr_V5 reconstr_V6 reconstr_V7<br/>1 380.1466 -0.3041237 0.2373746 1.617792 0.1876353 -0.7355559 0.3570959 -0.1331038<br/>2 1446.0211 -0.2568674 0.2218221 1.581772 0.2254702 -0.6452812 0.4204379 -0.1337738<br/>3 1912.0357 -0.2589679 0.2212748 1.578886 0.2171786 -0.6604871 0.4070894 -0.1352975<br/>4 1134.1723 -0.3319681 0.2431342 1.626862 0.1473913 -0.8192215 0.2911475 -0.1369512<br/>5 1123.6757 -0.3194054 0.2397288 1.619868 0.1612631 -0.7887480 0.3140728 -0.1362253<br/>6 1004.4545 -0.3589335 0.2508191 1.643208 0.1196120 -0.8811920 0.2451117 -0.1380364</pre>

<p>从现在开始，我们可以像以前一样继续。但是，<kbd>h2o</kbd>有一个内置的函数，<kbd>h2o.anomaly</kbd>，它简化了我们的部分工作。</p>

<p>我们可以做的另一个简化是不分别导入<kbd>ggplot2</kbd>和<kbd>dplyr</kbd>，我们可以导入<kbd>tidyverse</kbd>包，将这些(和其他)对数据操作有用的包引入我们的环境:</p>

<p class="mce-root">我们调用这个函数，进行一些格式化，使行名成为列本身，并为真正的类添加标签:</p>

<pre>library(tidyverse)<br/>anomaly &lt;- h2o.anomaly(autoencoder, test) %&gt;%<br/>as.data.frame() %&gt;%<br/>tibble::rownames_to_column() %&gt;%<br/>mutate(Class = as.vector(test[, 31]))</pre>

<p>让我们计算平均均方误差:</p>

<pre># Type coercion useful for plotting later<br/>anomaly$Class &lt;- as.factor(anomaly$Class)<br/>mean_mse &lt;- anomaly %&gt;%<br/> group_by(Class) %&gt;%<br/> summarise(mean = mean(Reconstruction.MSE))</pre>

<p>最后，根据重建误差可视化我们的测试数据:</p>

<pre>anomaly$Class &lt;- as.factor(anomaly$Class)<br/>mean_mse &lt;- anomaly %&gt;%<br/>    group_by(Class) %&gt;%<br/>    summarise(mean = mean(Reconstruction.MSE))</pre>

<p>我们看到自动编码器做了一件不太糟糕的工作。很大比例的欺诈案例具有相对较高的重构误差，尽管它远非完美。你如何改进它？：</p>

<div><img src="img/f456dd5c-fefc-4b1c-8647-dcc27557d6df.png" style="width:48.17em;height:24.17em;"/></div>

<p>结果从我们使用H2O的架构中，我们看到autoencoder在标记欺诈案例方面做得很好，但仍有改进的余地。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">练习</h1>

                

            

            

                

<ul>

<li>在我们的信用卡欺诈示例中，我们混合使用了欺诈和非欺诈案例。在这种情况下，我们很幸运，每个案例都有正确的标签。因此，在这种情况下，只进行正常事务的重建可能更有意义。如果我们只使用非欺诈案件，破案率会提高吗？运行相同的实验，但只对训练集使用非欺诈案例。</li>

<li>使用信用卡数据集，使用来自自动编码器的重构作为分类模型的输入，与使用PCA的方式非常相似。这是否提高了分类的准确性？请注意，您可以在这个特定的数据集中这样做，因为您有关于交易类别(欺诈/非欺诈)的信息，这些信息在其他数据中可能不可用。</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Variational Autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">可变自动编码器</h1>

                

            

            

                

<p><strong>变型自动编码器</strong> ( <strong> VAE </strong>)是自动编码问题的最新成果。与学习数据的压缩表示的自动编码器不同，变分自动编码器学习产生这种数据的随机过程，而不是像我们之前对神经网络所做的那样学习本质上任意的函数。</p>

<p>VAEs也有编码器和解码器部分。编码器学习假设已经产生数据的正态分布的平均值和标准偏差。均值和标准差被称为<strong>潜在变量</strong>，因为它们不是显式观察到的，而是从数据中推断出来的。</p>

<p>VAEs的解码器部分将这些潜在空间点映射回数据中。和以前一样，我们需要一个损失函数来衡量原始输入和它们的重建之间的差异。有时会添加一个额外的术语，称为<strong>库尔贝克-莱布勒散度，</strong>或简称为KL散度。KL散度粗略地计算了一个概率分布与另一个概率分布的差异。增加KL散度，迫使后验分布类似于先验分布。反过来，这有助于学习更好的数据表示并减少过度拟合。</p>

<p>与自动编码器不同，VAEs有一个坚实的概率基础，所以你得到的分数实际上是一个观察值成为异常值的概率。在自动编码器中，我们得到的分数没有这样的解释，因此截止值或阈值的选择完全依赖于人类专家的输入，并且是严格特定的数据。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image reconstruction using VAEs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用VAEs的图像重建</h1>

                

            

            

                

<p class="mce-root">我们的第一个例子将使用MNIST数据来说明变分自动编码器的使用。</p>

<p>发展战略如下:</p>

<ul>

<li>首先，编码器网络将输入样本<em xmlns:epub="http://www.idpf.org/2007/ops"> x </em>、<em xmlns:epub="http://www.idpf.org/2007/ops">T5T7】转换成潜在空间中的两个参数，这两个参数将被表示为<em xmlns:epub="http://www.idpf.org/2007/ops"> z_mean </em>和<em xmlns:epub="http://www.idpf.org/2007/ops"> z_log_sigma </em></em></li>

<li>然后，我们从我们假设用于生成数据的潜在正态分布中随机抽取相似点<em> z <strong> </strong> </em>，作为<em>z ~ z _ mean+exp(z _ log _ sigma)*ε<strong/></em>其中ε是随机正态张量</li>

<li>一旦完成，解码器网络将这些潜在空间点<em> z </em>映射回原始输入数据</li>

</ul>

<p>我们像往常一样开始获取和预处理数据:</p>

<pre>library(keras)<br/># Switch to the 1-based indexing from R<br/>options(tensorflow.one_based_extract = FALSE)<br/>K &lt;- keras::backend()<br/>mnist &lt;- dataset_mnist()<br/>X_train &lt;- mnist$train$x<br/>y_train &lt;- mnist$train$y<br/>X_test &lt;- mnist$test$x<br/>y_test &lt;- mnist$test$y<br/># reshape<br/>dim(X_train) &lt;- c(nrow(X_train), 784)<br/>dim(X_test) &lt;- c(nrow(X_test), 784)<br/># rescale<br/>X_train &lt;- X_train / 255<br/>X_test &lt;- X_test / 255</pre>

<p>请注意附加行:</p>

<pre>K &lt;- keras::backend()</pre>

<p>这为我们提供了对张量后端的引用，Keras将在这里执行张量操作。</p>

<p>现在我们转向VAE。它将由一个大小为<kbd>2</kbd>的潜在维度和一个隐藏的<kbd>256</kbd>神经元层组成:</p>

<pre>orig_dim &lt;- 784<br/>latent_dim &lt;- 2<br/>inner_dim &lt;- 256<br/>X &lt;- layer_input(shape = c(original_dim))<br/>hidden_state &lt;- layer_dense(X, intermediate_dim, activation = "relu")<br/>z_mean &lt;- layer_dense(hidden_state, latent_dim)<br/>z_log_sigma &lt;- layer_dense(hidden_state, latent_dim)</pre>

<p>接下来，在Keras后端的帮助下，我们定义了从潜在变量生成数据的采样函数:</p>

<pre>sample_z&lt;- function(params){<br/> z_mean &lt;- params[,0:1]<br/> z_log_sigma &lt;- params[,2:3]<br/> epsilon &lt;- K$random_normal(<br/> shape = c(K$shape(z_mean)[[1]]), <br/> mean=0.,<br/> stddev=1<br/> )<br/> z_mean + K$exp(z_log_sigma/2)*epsilon<br/>}</pre>

<p>我们现在定义采样点:</p>

<pre>z &lt;- layer_concatenate(list(z_mean, z_log_sigma)) %&gt;%<br/>  layer_lambda(sample_z)</pre>

<p>是时候定义解码器了。我们创建这些层的单独实例，以便以后能够重用它们:</p>

<pre>decoder_hidden_state &lt;- layer_dense(units = intermediate_dim, activation = "relu")<br/>decoder_mean &lt;- layer_dense(units = original_dim, activation = "sigmoid")<br/>hidden_state_decoded &lt;- decoder_hidden_state(z)<br/>X_decoded_mean &lt;- decoder_mean(hidden_state_decoded)</pre>

<p>我们准备好了！我们的VAE由以下编码器和解码器组件指定:</p>

<pre># end-to-end autoencoder<br/>variational_autoencoder &lt;- keras_model(X, X_decoded_mean)<br/><br/>encoder &lt;- keras_model(X, z_mean)<br/>decoder_input &lt;- layer_input(shape = latent_dim)
decoded_hidden_state_2 &lt;- decoder_hidden_state(decoder_input)<br/>decoded_X_mean_2 &lt;- decoder_mean(decoded_hidden_state_2)<br/>generator &lt;- keras_model(decoder_input, decoded_X_mean_2)</pre>

<p>仍然需要指定自定义损失函数，因为我们添加了KL-divergence惩罚:</p>

<pre>loss_function &lt;- function(X, decoded_X_mean){<br/>  cross_entropy_loss &lt;- loss_binary_crossentropy(X, decoded_X_mean)<br/>  kl_loss &lt;- -0.5*K$mean(1 + z_log_sigma - K$square(z_mean) - K$exp(z_log_sigma), axis = -1L)<br/>  cross_entropy_loss + kl_loss<br/>}</pre>

<p>我们像往常一样编译并运行我们的算法:</p>

<pre>variational_autoencoder %&gt;% compile(optimizer = "rmsprop", loss = loss_function)<br/>history &lt;- variational_autoencoder %&gt;% fit(<br/>  X_train, X_train, <br/>  shuffle = TRUE, <br/>  epochs = 10, <br/>  batch_size = 256, <br/>  validation_data = list(X_test, X_test)<br/>)<br/>plot(history)</pre>

<p>训练完成后，我们可以看到表演(或者使用RStudio中的查看器实时跟踪表演):</p>

<div><img src="img/36dbbf3e-2d9c-4e58-a88f-1187d6306fb4.png" style="width:25.33em;height:18.00em;"/></div>

<p>在MNIST数据中的性能。</p>

<p>我们可以使用下面的代码片段来检查我们的算法的性能:</p>

<pre>library(ggplot2)<br/>preds &lt;- variational_autoencoder %&gt;% predict(X_test)<br/>error &lt;- rowSums((preds-X_test)**2)<br/>eval &lt;- data.frame(error=error, class=as.factor(y_test))<br/>eval %&gt;% <br/> group_by(class) %&gt;% <br/> summarise(avg_error=mean(error)) %&gt;% <br/> ggplot(aes(x=class,fill=class,y=avg_error))+geom_col()</pre>

<p>结果如下所示:</p>

<div><img src="img/6c6aac6a-ef80-49da-a767-3abc24746cc5.png" style="width:26.92em;height:19.17em;"/></div>

<p>重建误差看起来相当令人沮丧，因为我们显然更好地利用了自动编码器。我们如何改进这一点？一种方法是提高潜在空间的维度。在我们目前的设置中，我们的潜在空间只是二维的。但是，请注意，在质量方面没有太大损失:</p>

<div><img src="img/efed1cfc-3831-4f4a-8460-ba0662f3758a.png" style="width:18.25em;height:9.67em;"/></div>

<div><img src="img/c3b6dbc2-5820-4112-b9ec-03ead175a6e6.png" style="width:18.50em;height:9.92em;"/></div>

<p>左边是我们的VAE重建的图像，右边是原始图像(在屏幕上)。</p>

<p>而且，我们现在有了一个生成过程！这意味着我们可以自己创造数字。让我们遍历潜在空间，使用概率分布来生成我们自己的数字:</p>

<pre># Reshape original and reconstructed<br/>dim(X_test) &lt;- c(nrow(X_test),28,28)<br/>dim(preds) &lt;- c(nrow(preds),28,28)<br/>image(255*preds[1,,], col=gray.colors(3))<br/>y_test[1]<br/>image(255*X_test[1,,], col=gray.colors(3))<br/><br/>grid_x &lt;- seq(-4, 4, length.out = 3)<br/>grid_y &lt;- seq(-4, 4, length.out = 3)<br/><br/>rows &lt;- NULL<br/>for(i in 1:length(grid_x)){<br/>  column &lt;- NULL<br/>  for(j in 1:length(grid_y)){<br/>    z_sample &lt;- matrix(c(grid_x[i], grid_y[j]), ncol = 2)<br/>    column &lt;- rbind(column, predict(generator, z_sample) %&gt;% matrix(ncol = 28) )<br/>  }<br/>  rows &lt;- cbind(rows, column)<br/>}<br/>rows %&gt;% as.raster() %&gt;% plot()</pre>

<p>让我们看看VAE生成的几个数字:</p>

<div><img src="img/bf801f72-25a6-4c49-939e-84980f7d1700.png" style="width:6.67em;height:6.75em;"/></div>

<p>由VAE生成的数字。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Outlier detection in MNIST</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">MNIST的异常检测</h1>

                

            

            

                

<p>让我们再来看看MNIST的异常值检测问题。像以前一样，假设这次数字<kbd>0</kbd>是我们的异常值，我们希望能够检测到它。</p>

<p>我们像以前一样，读取和预处理数据:</p>

<pre>library(keras)<br/># Switch to the 1-based indexing from R<br/>options(tensorflow.one_based_extract = FALSE)<br/>K &lt;- keras::backend()<br/>mnist &lt;- dataset_mnist()<br/>X_train &lt;- mnist$train$x<br/>y_train &lt;- mnist$train$y<br/>X_test &lt;- mnist$test$x<br/>y_test &lt;- mnist$test$y<br/>## Exclude "0" from the training set. "0" will be the outlier<br/>outlier_idxs &lt;- which(y_train!=0, arr.ind = T)<br/>X_train &lt;- X_train[outlier_idxs,,]<br/>y_test &lt;- sapply(y_test, function(x){ ifelse(x==0,"outlier","normal")})<br/># reshape<br/>dim(X_train) &lt;- c(nrow(X_train), 784)<br/>dim(X_test) &lt;- c(nrow(X_test), 784)<br/># rescale<br/>X_train &lt;- X_train / 255<br/>X_test &lt;- X_test / 255</pre>

<p>然后我们定义我们的编码器结构。注意，我们将使用不同结构的<kbd>128</kbd>维潜空间和中间层的<kbd>256</kbd>神经元:</p>

<pre>original_dim &lt;- 784<br/>latent_dim &lt;- 2<br/>intermediate_dim &lt;- 256<br/>X &lt;- layer_input(shape = c(original_dim))<br/>hidden_state &lt;- layer_dense(X, intermediate_dim, activation = "relu")<br/>z_mean &lt;- layer_dense(hidden_state, latent_dim)<br/>z_log_sigma &lt;- layer_dense(hidden_state, latent_dim)</pre>

<p>并重写我们的<kbd>sample_z</kbd>函数，使其更易于定制:</p>

<pre>sample_z&lt;- function(params){<br/>  z_mean &lt;- params[,0:1]<br/>  z_log_sigma &lt;- params[,2:3]<br/>  epsilon &lt;- K$random_normal(<br/>    shape = c(K$shape(z_mean)[[1]]), <br/>    mean=0.,<br/>    stddev=1<br/>  )<br/>  z_mean + K$exp(z_log_sigma/2)*epsilon<br/>}</pre>

<p>然后我们来看解码器部分:</p>

<pre>z &lt;- layer_concatenate(list(z_mean, z_log_sigma)) %&gt;% <br/>  layer_lambda(sample_z)<br/>decoder_hidden_state &lt;- layer_dense(units = intermediate_dim, activation = "relu")<br/>decoder_mean &lt;- layer_dense(units = original_dim, activation = "sigmoid")<br/>hidden_state_decoded &lt;- decoder_hidden_state(z)<br/>X_decoded_mean &lt;- decoder_mean(hidden_state_decoded)</pre>

<p>最后，完整的自动编码器:</p>

<pre>variational_autoencoder &lt;- keras_model(X, decoded_X_mean)<br/>encoder &lt;- keras_model(X, z_mean)<br/>decoder_input &lt;- layer_input(shape = latent_dim)<br/>decoded_hidden_state_2 &lt;- decoder_hidden_state(decoder_input)<br/>decoded_X_mean_2 &lt;- decoder_mean(decoded_hidden_state_2)<br/>generator &lt;- keras_model(decoder_input, decoded_X_mean_2)</pre>

<p>我们用以下公式定义损失函数:</p>

<pre>loss_function &lt;- function(X, decoded_X_mean){<br/>  cross_entropy_loss &lt;- loss_binary_crossentropy(X, decoded_X_mean)<br/>  kl_loss &lt;- -0.5*K$mean(1 + z_log_sigma - K$square(z_mean) - K$exp(z_log_sigma), axis = -1L)<br/>  cross_entropy_loss + kl_loss<br/>}</pre>

<p>我们使用与之前相同的函数来训练模型:</p>

<pre>variational_autoencoder %&gt;% compile(optimizer = "rmsprop", loss = loss_function)<br/>history &lt;- variational_autoencoder %&gt;% fit(<br/>  X_train, X_train, <br/>  shuffle = TRUE, <br/>  epochs = 10, <br/>  batch_size = 256, <br/>  validation_data = list(X_test, X_test)<br/>)<br/>plot(history)</pre>

<p>培训完成后，我们会查看表现:</p>

<pre>preds &lt;- variational_autoencoder %&gt;% predict(X_test)<br/>error &lt;- rowSums((preds-X_test)**2)<br/>eval &lt;- data.frame(error=error, class=as.factor(y_test))<br/>library(dplyr)<br/>library(ggplot2)<br/>eval %&gt;% <br/> ggplot(aes(x=class,fill=class,y=error))+geom_boxplot()</pre>

<p>让我们看看每个类的重构误差:</p>

<div><img src="img/8a7e39d7-5bf9-415e-99fa-879c9c956d03.png" style="width:27.67em;height:19.67em;"/></div>

<p>使用VAE的重建误差。</p>

<p>该图建议您将阈值的重建误差设置为<kbd>5</kbd>:</p>

<pre>threshold &lt;- 5<br/>y_preds &lt;- sapply(error, function(x){ifelse(x&gt;threshold,"outlier","normal")})</pre>

<p>我们现在来看混淆矩阵:</p>

<pre>table(y_preds,y_test)<br/>         y_test<br/>y_preds normal outlier<br/>  outlier 9020     980</pre>

<p>这表明我们的方向是正确的！但在庆祝之前，我们应该看看其他分类指标，ROC曲线和曲线下面积(AUC):</p>

<pre>library(ROCR)<br/> pred &lt;- prediction(error, y_test)<br/> perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr")<br/> auc &lt;- unlist(performance(pred, measure = "auc")@y.values)<br/> auc<br/> plot(perf, col=rainbow(10))</pre>

<p>我们得到一个AUC<kbd>0.8473375</kbd>和一个合理的ROC图，如下所示，这告诉我们，我们的VAE在区分异常值<kbd>0</kbd>方面做得很好。</p>

<p>请注意，这比数字7是异常值时要好得多。这告诉我们，当一个异常的观察与通常的观察太相似时，我们需要付出额外的努力:</p>

<div><img src="img/b6f608ea-7aff-47f4-8498-a3c616bb3dfc.png" style="width:28.25em;height:15.58em;"/></div>

<p>变分自动编码器的ROC曲线。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Text fraud detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">文本欺诈检测</h1>

                

            

            

                

<p>欺诈已经成为一个超越传统交易欺诈的问题。例如，许多网站依赖用户对餐馆、酒店或旅游景点等服务的评论，这些评论以不同的方式赚钱。如果用户对这些评论失去了信任，例如，一个企业主故意搞乱他或她自己企业的好评论，那么网站将很难重新获得信任并保持盈利。因此，检测这些潜在问题非常重要。</p>

<p>自动编码器如何帮助我们做到这一点？和以前一样，这个想法是学习一个网站上的<em>正常</em>审查的表示，然后找到那些不符合<em>正常</em>审查的。文本数据的问题在于之前需要做一些处理。我们将用一个例子来说明这一点，这个例子也将作为在下一章中讨论的不同文本建模方法的动机。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From unstructured text data to a matrix</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从非结构化文本数据到矩阵</h1>

                

            

            

                

<p>文本数据的一个问题是单词和句子是杂乱的，数据挖掘算法通常不是现成的，因为它们被设计成对数据的抽象进行操作，通常是矩阵形式。所以我们需要找到一种方法将我们杂乱的文本数据表示为一个矩阵。</p>

<p>实践中最常用的矩阵表示之一是<em>单词包</em>模型。这是从文本中提取信息的一种非常简单和直观的方式。有一些注意事项，将在后面讨论。</p>

<p>文本的单词包表示包括:</p>

<ul>

<li>已知单词的词汇表</li>

<li>与这些词的存在相关的数字度量</li>

</ul>

<p>例如，假设我们有一个由三个句子组成的<em>语料库</em>(文档集合):</p>

<ul>

<li><kbd>"My sentence"</kbd></li>

<li><kbd>"Your sentence"</kbd></li>

<li><kbd>"My sentence, your sentence, our sentences"</kbd></li>

</ul>

<p>词汇表(忽略逗号)是集合<kbd>("My","sentence","sentences","Your","your","our")</kbd>。至于数值度量，一个自然的选项是计数函数。所以矩阵表示应该是:</p>

<table border="1" style="border-collapse: collapse">

<tbody>

<tr>

<td><kbd>My</kbd></td>

<td><kbd>Your</kbd></td>

<td><kbd>sentence</kbd></td>

<td><kbd>sentences</kbd></td>

<td><kbd>your</kbd></td>

<td><kbd>our</kbd></td>

</tr>

<tr>

<td>一</td>

<td>0</td>

<td>一</td>

<td>0</td>

<td>0</td>

<td>0</td>

</tr>

<tr>

<td>0</td>

<td>一</td>

<td>一</td>

<td>0</td>

<td>0</td>

<td>0</td>

</tr>

<tr>

<td>一</td>

<td>0</td>

<td>2</td>

<td>一</td>

<td>一</td>

<td>一</td>

</tr>

</tbody>

</table>

<p> </p>

<p>这是我们现在可以操作的东西。请注意，一些评论是适当的。我们忽略了逗号，这看起来没什么大不了的，但确实如此，我们将回头再讨论它。第二，我们将<kbd>"Your"</kbd>和<kbd>"your"</kbd>作为不同的单词包含在内，这可能是不可取的。请注意，从信息的角度来看，<kbd>"sentence"</kbd>和<kbd>"sentences"</kbd>也是相似的。如果我们想推断文档的内容，只保留其中一个就足够了。</p>

<p>为了处理大写问题，我们可以简单地将所有的单词先转换成小写，然后再转换成矩阵形式。通过不同的算法来处理来自<em>词根</em>的复数或其他派生单词，称为<strong>词干分析器</strong>。最常见的选择是波特的词干算法。有时做词干并不是一个好主意，这可能取决于语言和问题的上下文。</p>

<p>根据上下文，有时代词等常用词最好省略。互联网上有一系列这样的词，叫做<strong>停用词</strong>。所以在创建矩阵表示之前，你过滤掉那些词，这也降低了问题的维度。</p>

<p>对词频进行评分的一个问题是，我们将让高频词支配矩阵表示，但从信息的角度来看，这种支配可能是无用的。取而代之的是，通常使用其他的表示方法，比如TF–IDF，它代表文本文档-逆文档频率。有不同的计算方法，大致等价，如果<em> w </em>是一个单词，<em> D </em>是文档的集合，<em> d </em>是那里的一个文档(前面例子中的一个句子)，那么:</p>

<div><img class="fm-editor-equation" src="img/07924714-0a10-4bd4-a440-ffff272e523d.png" style="width:30.08em;height:2.42em;"/></div>

<p>你可以玩玩这些，例如，你可以使用特征函数(如果单词不在，0；如果在，1)或者它的对数来代替分子中单词的频率。同样，你可以考虑分母的不同可能性。</p>

<p>去掉标点符号的问题有点微妙，它与单词袋方法的主要缺点有关，意思完全丢失了。即使不考虑标点符号，像<kbd>Alice loves pizza</kbd>和<kbd>Pizza loves Alice</kbd>这样的句子也可以用相同的方式表示，但是它们有不同的含义。有了标点，我们可以得到完全相反的意思，句子<kbd>Pardon, impossible execution</kbd>和<kbd>Pardon impossible, execution</kbd>表示相反的东西。</p>

<p>上下文也丢失了，单词内部的关系也可能丢失。例如，文档<kbd>I was in Paris</kbd>和<kbd>I saw the Eiffel tower</kbd>显然是相关的，但是它们在单词包表示中看起来是正交的文档。我们将在后面的章节中讨论其中的一些问题。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From text to matrix representation — the Enron dataset</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从文本到矩阵表示——安然数据集</h1>

                

            

            

                

<p>安然电子邮件数据集包含大约500，000封由安然公司员工生成的电子邮件。它是美国联邦能源管理委员会在调查安然公司倒闭时获得的。安然公司是一家总部设在德克萨斯州休斯顿的美国能源公司，它卷入了会计欺诈丑闻，最终导致其破产。我们将使用子集作为例子，但你可以从ka ggle(<a href="https://www.kaggle.com/wcukierski/enron-email-dataset">https://www.kaggle.com/wcukierski/enron-email-dataset</a>)或卡耐基梅隆大学的计算机科学学院(<a href="https://www.cs.cmu.edu/~./enron/">https://www.cs.cmu.edu/~./enron/</a>)访问完整的数据集(50万封电子邮件)。</p>

<p class="mce-root">对于文本挖掘，我们将使用包，<kbd>tm</kbd>(<a href="https://cran.r-project.org/web/packages/tm/index.html">https://cran.r-project.org/web/packages/tm/index.html</a>)和<kbd>SnowballC</kbd>(<a href="https://cran.r-project.org/web/packages/SnowballC/index.html">https://cran . r-project . org/web/packages/snow ballc/index . html</a>)。请务必在以下时间之前安装它们:</p>

<pre>install.packages("tm")<br/>install.packages("SnowballC")</pre>

<p>我们首先在工作区中加载数据帧。我们将省略一些预处理词干，并假设您的数据帧有两列，email和responsive。如果原始数据中没有响应列(并非所有版本都有)，我们会为小样本手工标记响应列。从法律角度来说，响应意味着电子邮件是否与欺诈调查相关:</p>

<pre>df &lt;- read.csv("./data/enron.csv")<br/>names(df)<br/>[1] "email"      "responsive"</pre>

<p class="mce-root">我们加载了<kbd>tm</kbd>库，并从email列创建了一个语料库对象:</p>

<pre>library(tm)<br/>corpus &lt;- Corpus(VectorSource(df$email))</pre>

<p>我们可以使用<kbd>inspect</kbd>命令访问每封电子邮件，如下所示:</p>

<pre>inspect(corpus[[1]])</pre>

<p class="mce-root">在建模之前，对我们的数据进行了一系列转换:转换为小写字母、删除标点符号、停用词和词干:</p>

<pre>corpus &lt;- tm_map(corpus,tolower)<br/>corpus &lt;- tm_map(corpus, removePunctuation)<br/>corpus &lt;- tm_map(corpus, removeWords, stopwords("english"))<br/>corpus &lt;- tm_map(corpus, stemDocument)</pre>

<p>一旦完成，我们就可以获得文档的矩阵表示，如下所示:</p>

<pre>dtm &lt;- DocumentTermMatrix(corpus)<br/>dtm &lt;-  removeSparseTerms(dtm, 0.97)<br/>X &lt;- as.data.frame(as.matrix(dtm))<br/>X$responsive &lt;- df$responsive</pre>

<p>我们创建训练/测试分割。为此，我们可以使用库<kbd>caTools</kbd>:</p>

<pre># Train, test, split<br/>library(caTools)<br/>set.seed(42)<br/>spl &lt;- sample.split(X$responsive, 0.7)<br/>train &lt;- subset(X, spl == TRUE)<br/>test &lt;- subset(X, spl == FALSE)<br/>train &lt;- subset(train, responsive==0)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Autoencoder on the matrix representation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">矩阵表示上的自动编码器</h1>

                

            

            

                

<p>一旦我们将文本放入矩阵形式，我们就可以继续自动编码器的训练，就像前面几节一样。</p>

<p>请注意，我们的autoencoder在培训部分只会有未回复的电子邮件。这在这个只有几百个样本的数据集中非常有用。</p>

<p>一旦完成，我们创建我们的训练和测试集，像以前一样分成<kbd>X</kbd>和<kbd>y</kbd>组件:</p>

<pre>X_train &lt;- subset(train,select=-responsive)<br/>y_train &lt;- train$responsive<br/>X_test &lt;- subset(test,select=-responsive)<br/>y_test &lt;- test$responsive</pre>

<p>现在，我们准备定义我们的自动编码器。我们将只使用尺寸为<kbd>32</kbd>的内层:</p>

<pre>library(keras)<br/>input_dim &lt;- ncol(X_train)<br/>inner_layer_dim &lt;- 32<br/>input_layer &lt;- layer_input(shape=c(input_dim))<br/>encoder &lt;- layer_dense(units=inner_layer_dim, activation='relu')(input_layer)<br/>decoder &lt;- layer_dense(units=input_dim)(encoder)<br/>autoencoder &lt;- keras_model(inputs=input_layer, outputs = decoder)<br/>autoencoder %&gt;% compile(optimizer='adam', <br/>                        loss='mean_squared_error', <br/>                        metrics=c('accuracy'))</pre>

<p>然后，对于培训:</p>

<pre>X_train &lt;- as.matrix(X_train)<br/>X_test &lt;- as.matrix(X_test)<br/>history &lt;- autoencoder %&gt;% fit(<br/> X_train,X_train, <br/> epochs = 100, batch_size = 32, <br/> validation_data = list(X_test, X_test)<br/>)<br/>plot(history)</pre>

<p>我们查看测试集上的重构，并查看两个类中的错误分布:</p>

<pre># Reconstruct on the test set<br/>preds &lt;- autoencoder %&gt;% predict(X_test)<br/>error &lt;- rowSums((preds-X_test)**2)<br/>library(tidyverse)<br/>eval %&gt;% <br/> filter(error &lt; 1000) %&gt;%<br/> ggplot(aes(x=error,color=class))+geom_density()</pre>

<p>像往常一样，让我们看看每个类的误差分布，这次是用密度图:</p>

<div><img src="img/6dd733d0-32c1-4574-9600-1da33af0a79a.png" style="width:28.08em;height:19.92em;"/></div>

<p>每类重建误差的分布。</p>

<p>注意，和以前一样，我们过滤了重建误差，这有助于我们查看大部分观测值所在的尺度。我们的目标是为重建错误设置阈值，以标记为异常值(在这种情况下，这意味着该电子邮件不是普通的电子邮件通信)。从视觉上看，100似乎是一个合理的阈值，尽管我们会得到大量的误报:</p>

<pre>threshold &lt;- 100<br/>y_preds &lt;- sapply(error, function(x){ifelse(x&gt;threshold,"outlier","normal")})<br/># Confusion matrix<br/>table(y_preds,y_test)<br/>         y_test<br/>y_preds    0 1<br/> normal  142 7<br/> outlier 73 35</pre>

<p>我们在捕捉可疑邮件方面做得相当不错，代价是73次误报。在捕捉大量假阳性和忽略真阳性之间总是有一个权衡。该模型可以通过添加更多数据来改进，我们只使用了500，000封可用电子邮件中的大约800封，因此显然还有改进的空间。然而，该模型工作得相当好，如AUC值0.79和ROC图所证实的:</p>

<pre class="mce-root">library(ROCR)<br/>pred &lt;- prediction(error, y_test)<br/>perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr") <br/>auc &lt;- unlist(performance(pred, measure = "auc")@y.values) <br/>plot(perf, col=rainbow(10))<br/>auc<br/>[1] 0.7951274<br/>plot(perf, col=rainbow(10))</pre>

<p>我们看到安然数据集的ROC曲线如下。这使我们能够在一般情况下诊断我们的二元分类器模型，而不仅仅是在这种情况下:</p>

<div><img src="img/dc1a5fc4-bd75-4c51-9af1-3b05c653827d.png" style="width:21.25em;height:12.75em;"/></div>

<p>安然数据集的ROC曲线。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">练习</h1>

                

            

            

                

<ul>

<li>使用原始的安然数据集创建一个自动编码器，可以识别可疑和非可疑的电子邮件。</li>

<li>除了自动编码器，你还可以尝试使用不同的自动编码器。比较两种方法的性能。</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在本章中，我们了解到自动编码器是一种主要用于图像重建和去噪的技术，以获得数据的压缩和概括表示。我们看到它们有时也用于欺诈检测任务。离群点识别来自于测量重建误差，观察重建误差的分布，我们可以设置用于识别离群点的阈值，并学习产生数据的概率过程。因此，变分自动编码器也能够生成新数据。</p>





            



            

        

    </body>



</html></body></html>