<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Traffic Signs Recognition for Intelligent Vehicles</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">智能车辆的交通标志识别</h1>

                

            

            

                

<p><strong>卷积神经网络</strong>(<strong>CNN</strong>)在计算机视觉中非常有用，我们将把它用于另一个应用，智能车辆的交通标志检测。我们还将在本章中涵盖深度学习的几个重要概念，并将让读者接触到其他流行的深度学习框架和库。</p>

<p>我们继续我们的R深度学习之旅，研究自动驾驶汽车的核心问题之一，物体识别，具体来说，就是交通标志分类。为了避免事故和确保安全，鲁棒的交通标志分类对于实现自主驾驶至关重要。我们就从什么是自动驾驶汽车，深度学习应用于哪些方面开始。我们还将讨论深度学习如何脱颖而出，成为智能车辆中物体识别的最先进解决方案。了解背景知识后，我们将开始我们的项目，首先对数据和性能数据预处理进行探索性分析，包括Y'UV颜色编码转换。我们将首次尝试在MXNet中使用CNN，并达到99%以上的准确率。然后我们将转移到另一个强大的深度学习框架，称为TensorFlow和一个建立在它之上的API，称为Keras。</p>

<p>我们还将引入漏失技术作为一种正则化方法，类似于拉索(L1) /岭(L2)正则化。除了过度拟合，我们将演示如何利用数据扩充技术处理训练数据的缺乏。我们将通过总结防止CNN模型过度拟合的方法来结束这一章。我们将再次证明，深度学习模型消除了传统方法中采取的手动或显式特征提取步骤，而是在训练过程中自动发现输入数据下的有用模式。</p>

<p>我们将详细讨论这里提到的主题:</p>

<ul>

<li>什么是自动驾驶汽车</li>

<li>深度学习在自动驾驶汽车中是如何应用的</li>

<li>深度学习如何成为智能车辆中物体识别的最先进解决方案</li>

<li>德国交通标志识别基准数据集数据的探索性分析</li>

<li>使用MXNet包的卷积神经网络进行交通标志识别</li>

<li>喀拉斯和张量流</li>

<li>基于卷积神经网络的交通标志识别</li>

<li>通过下降减少过度拟合</li>

<li>数据扩充和常用的扩充方法</li>

<li>对小型训练集使用数据扩充</li>

<li>CNN模型中防止过拟合的方法综述</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>How is deep learning applied in self-driving cars?</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">深度学习在自动驾驶汽车中是如何应用的？</h1>

                

            

            

                

<p>自动驾驶汽车(也称为自主/自动车辆或无人驾驶汽车)是一种机器人车辆，能够在目的地之间行驶并在没有人类干预的情况下导航。为了实现自主，自动驾驶汽车使用雷达、GPS和计算机视觉等多种技术来检测和解释环境；然后他们计划到达期望目的地的适当导航路径。</p>

<p>更详细地说，以下是自动驾驶汽车的一般工作方式:</p>

<ul>

<li>该软件根据目的地、交通和道路信息规划路线，并启动汽车</li>

<li>一个<strong>光探测和测距</strong> ( <strong>激光雷达</strong>)传感器实时捕捉周围环境并创建动态3D地图</li>

<li>传感器监控横向运动，计算汽车在三维地图上的位置</li>

<li>雷达系统利用与其他交通参与者、行人或障碍物的距离信息</li>

<li>计算机视觉算法从相机中识别交通标志、交通灯和其他地标，并提供预先通知</li>

<li>算法驱动的软件分析所有的传感数据，结合来自其他来源的输入，并控制转向和制动等动作，模拟人类感知周围环境并做出决定的方式</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>How does deep learning become a state-of-the-art solution?</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">深度学习如何成为最先进的解决方案？</h1>

                

            

            

                

<p>正如我们所见，鲁棒的物体识别对于实现驾驶自主至关重要。为了避免事故和确保安全，有必要注意周围的环境、交通标志和灯光。一般来说，自动驾驶汽车中的物体识别可以总结为四项任务:</p>

<ul>

<li>物体检测，如障碍物、行人、交通标志和灯。</li>

<li>物体识别和分类。一个例子是标记交通信号灯(红色、黄色、绿色和关闭),如果在正面摄像机捕获的图像中检测到的话。然后，我们将交通参与者分为自行车、摩托车、汽车、卡车和公共汽车，当然还有交通标志的分类(我们本章的主要讨论点)。</li>

<li>目标定位，将地面图像映射到航空图像。</li>

<li>运动预测，例如，了解物体的速度或根据行人的姿势估计行人的行为和意图。</li>

</ul>

<p>在过去的二十年里，各种机器学习算法已经被应用于解决智能车辆中的物体识别问题。</p>

<p>例如，在使用运动和外观模式检测行人的<em xmlns:epub="http://www.idpf.org/2007/ops">(Viola等人，发表在国际计算机视觉杂志，63(2))中，使用了<strong xmlns:epub="http://www.idpf.org/2007/ops">AdaBoost</strong>(<strong xmlns:epub="http://www.idpf.org/2007/ops">自适应增强</strong>的缩写，它顺序纠正分类错误)分类器来检测行走的行人。</em></p>

<p>在<em>用于人体检测的方向梯度直方图</em> (Dalal和Triggs，2005年在IEEE计算机视觉和模式识别会议(CVPR)上发表)中，使用<strong>方向直方图</strong> ( <strong> HOG </strong>)技术提取有效特征，并送入<strong>支持向量机</strong> ( <strong> SVM </strong>)分类器进行人体检测。从那时起，更复杂的变体，如梯度场HOG (GF-HOG)和其他更复杂的特征提取方法被开发出来。举几个例子，我们有分区+投影，投影+ HOG等等。</p>

<p>传统的对象识别方法(显式特征提取+机器学习分类)严重依赖于手工制作的特征，例如具有HOG的梯度方向直方图、具有<strong>加速的鲁棒特征的局部关键点</strong> ( <strong> SURF </strong>)或<strong>尺度不变特征变换</strong> ( <strong> SIFT </strong>)。尽管它们在某些任务中表现良好，但设计这些特征描述符是困难的，需要大量的手动调整和实验。</p>

<p>回想一下，在前一章对手写数字进行分类时，我们求助于CNN。它首先导出低级表示、局部边缘和曲线，然后通过一系列低级表示合成更高级表示，如整体形状和轮廓。我们还得出结论，CNN非常适合开发强大而独特的功能。</p>

<p>事实上，在许多解决方案中已经证明，CNN能够有效地自动化特征提取，同时允许性能的显著提升。比如在<em xmlns:epub="http://www.idpf.org/2007/ops">无监督多阶段特征学习的行人检测</em> (Sermanet等，2013年发表在IEEE关于<strong xmlns:epub="http://www.idpf.org/2007/ops">计算机视觉和模式识别会议</strong> ( <strong xmlns:epub="http://www.idpf.org/2007/ops"> CVPR </strong>)中，首次将CNN引入行人检测问题。在<em xmlns:epub="http://www.idpf.org/2007/ops">用于精确对象检测和语义分割的丰富特征层次结构</em> (Girshick等人，2014年在IEEE会议上发表)中，提出了一种基于变体区域的CNN模型来提高性能。如今，许多最先进的对象识别方法都涉及深度学习技术，特别是CNN。一个很好的证明就是它们在http://www.cvlibs.net/datasets/kitti/eval_object.php?自动驾驶汽车KITTI Vision基准排行榜上的领先地位obj_benchmark=2d) 。</p>

<p>我希望所有这些案例都让你对CNN及其为智能车辆提供更好的物体识别解决方案的能力感到兴奋。</p>

<p>那么我们还在等什么呢？让我们继续我们的项目，交通标志识别，因为它是自动驾驶汽车中最重要的主题之一！</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Traffic sign recognition using CNN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基于CNN的交通标志识别</h1>

                

            

            

                

<p>和往常一样，我们从探索<strong>德国交通标志识别基准</strong> ( <strong> GTSRB </strong>)数据集开始，数据集位于<a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset" target="_blank">http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection = dataset</a>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Getting started with exploring GTSRB</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">开始探索GTSRB</h1>

                

            

            

                

<p>神经信息研究所实时计算机视觉研究小组编制并大量发布的GTSRB数据集最初用于对交通标志的单幅图像进行分类的竞赛。它由39，209个标记图像的训练集和12，630个未标记图像的测试集组成。训练数据集包含43个类别，即43种类型的交通标志。我们将浏览所有的课程，并为每个课程展示几个样本。</p>

<p>数据集可以通过<a href="http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip">http://benchmark . ini . rub . de/Dataset/gt SRB _ Final _ Training _ images . zip</a>下载(位于页面的下载|训练数据集部分)。解压下载的文件，会出现一个名为<kbd>Images</kbd>的文件夹，包含43个文件夹(<kbd>00000</kbd>、<kbd>00001</kbd>...最多<kbd>00042</kbd>)；它们代表43类图像。这些图像的形式如下:</p>

<ul>

<li>图像文件为<strong> PPM </strong>(便携像素图<strong>的简称</strong>)格式。</li>

<li>每个类别的图像数量范围从210到2250。所以是一个不平衡的多类分类问题。</li>

<li>每个图像包含一个交通标志。</li>

<li>图像的大小不统一，从15*15到250*250像素不等，图像不一定是正方形。</li>

<li>图像在实际标志周围包含高达10%的边框。因此，标志不一定以图像为中心。</li>

</ul>

<p>让我们从在<kbd>00000</kbd>文件夹中绘制一个样本<kbd>00000_00002.ppm</kbd>开始。</p>

<p>我们使用<kbd>pixmap</kbd>包(<a href="https://cran.r-project.org/web/packages/pixmap" target="_blank">https://cran.r-project.org/web/packages/pixmap</a>)来读取PPM文件:</p>

<pre>&gt; library('pixmap') 

&gt; image &lt;- read.pnm('GTSRB/Final_Training/Images/00000/00000_00002.ppm',cellres=1) </pre>

<p>现在我们获得一个具有属性<kbd>red</kbd>、<kbd>green</kbd>和<kbd>blue</kbd>(这是三个通道中每一个通道的像素)的<kbd>pixmapRGB</kbd>对象，以及图像的宽度和高度<kbd>size</kbd>。我们可以访问红色、绿色和蓝色通道，如下所示:</p>

<pre>&gt; red_matrix &lt;- matrix(image@red, nrow = image@size[1], ncol = image@size[2]) 

&gt; green_matrix &lt;- matrix(image@green, nrow = image@size[1], ncol = image@size[2]) 

&gt; blue_matrix &lt;- matrix(image@blue, nrow = image@size[1], ncol = image@size[2]) </pre>

<p>我们分别可视化原始图像及其三个通道:</p>

<pre>&gt; plot(image, main=sprintf("Original")) 

&gt; rotate &lt;- function(x) t(apply(x, 2, rev)) 

&gt; par(mfrow=c(1, 3)) 

&gt; image(rotate(red_matrix), col = grey.colors(255), main=sprintf("Red")) 

&gt; image(rotate(green_matrix), col = grey.colors(255), main=sprintf("Green")) 

&gt; image(rotate(blue_matrix), col = grey.colors(255), main=sprintf("Blue")) </pre>

<p>注意，这里我们重用了上一章定义的旋转函数，以便更好地查看图像。这是一幅原始图像:</p>

<div><img src="img/fbcb9032-731f-4d21-9156-9af6fec14db6.png" style="width:19.17em;height:21.58em;"/></div>

<p>下图分别显示了红色、蓝色和绿色通道的输出:</p>

<div><img src="img/4e43e65a-0e7c-405c-9201-c89befb70cb6.png"/></div>

<p>这是一个时速20公里的限速标志。</p>

<p>我们现在可以浏览43类标志，并通过定义以下函数为每种类型显示三个样本:</p>

<pre>&gt; plot_samples &lt;- function(training_path, class, num_sample){ 

+     classes &lt;- c("Speed limit (20km/h)", "Speed limit (30km/h)",  

                   "Speed limit (50km/h)", "Speed limit (60km/h)", 

+                  "Speed limit (70km/h)", "Speed limit (80km/h)",  

                   "End of speed limit (80km/h)", 

+                  "Speed limit (100km/h)", "Speed limit (120km/h)",  

                   "No passing",  

+                  "No passing for vehicles over 3.5 metric tons",  

                   "Right-of-way at the next intersection",  

+                  "Priority road", "Yield", "Stop", "No vehicles",  

                   "Vehicles over 3.5 metric tons prohibited", 

+                  "No entry", "General caution", "Dangerous curve to  

                   the left", "Dangerous curve to the right", 

+                  "Double curve", " Bumpy road", "Slippery road",  

                   "Road narrows on the right", "Road work", 

+                  "Traffic signals", "Pedestrians", "Children  

                   crossing", "Bicycles crossing",  

                   "Beware of ice/snow", 

+                  "Wild animals crossing",  

                   "End of all speed and passing limits",  

                   "Turn right ahead",  

+                  "Turn left ahead", "Ahead only",  

                   "Go straight or right", "Go straight or left",  

                   "Keep right", "Keep left", "Roundabout mandatory",  

                   "End of no passing",  

+                  "End of no passing by vehicles over 3.5 metric  

                    tons") 

+     if (class&lt;10) { 

+       path &lt;- paste(training_path, "0000", class, "/", sep="") 

+     } else { 

+       path &lt;- paste(training_path, "000", class, "/", sep="") 

+     } 

+     par(mfrow=c(1, num_sample)) 

+     # Randomly display num_sample samples 

+     all_files &lt;- list.files(path = path) 

+     title &lt;- paste('Class', class, ':', classes[class+1]) 

+     print(paste(title, "          (", length(all_files),  

                 " samples)", sep="")) 

+     files &lt;- sample(all_files, num_sample) 

+     for (file in files) { 

+       image &lt;- read.pnm(paste(path, file, sep=""), cellres=1) 

+       plot(image) 

+     } 

+     mtext(title, side = 3, line = -23, outer = TRUE) 

+ } </pre>

<p>用<kbd>class=0</kbd>调用函数:</p>

<pre>&gt; training_path &lt;- "GTSRB/Final_Training/Images/" 

&gt; plot_samples(training_path, 0, 3) 

[1] "Class 0 : Speed limit (20km/h)          (211 samples)" </pre>

<p>显示三个样本:</p>

<div><img src="img/2a58b4b4-244f-42ef-a714-e3e59e21e1a0.png" style="width:28.83em;height:9.67em;"/></div>

<p>用不同的类重复这个函数调用(或使用循环),遍历剩余的42种类型:</p>

<pre>&gt; plot_samples(training_path, 1, 3) 

[1] "Class 1 : Speed limit (30km/h)          (2221 samples)" </pre>

<p>来自类别1的三个样本显示如下:</p>

<div><img src="img/36b9bca9-7be0-4c67-a242-fab07379adbd.png" style="width:33.67em;height:11.58em;"/></div>

<pre>&gt; plot_samples(training_path, 2, 3) 

[1] "Class 2 : Speed limit (50km/h)          (2251 samples)" </pre>

<p>此处显示了来自第2类的三张图片:</p>

<div><img src="img/cb24bf24-1998-4b91-8795-df7c13dfacb8.png" style="width:35.25em;height:12.75em;"/></div>

<pre>&gt; plot_samples(training_path, 3, 3) 

[1] "Class 3 : Speed limit (60km/h)          (1411 samples)" </pre>

<p>这是3班的成绩:</p>

<div><img src="img/bf9dc681-762c-49d7-96b1-e82e883a1381.png" style="width:35.08em;height:13.17em;"/></div>

<pre>&gt; plot_samples(training_path, 4, 3) 

[1] "Class 4 : Speed limit (70km/h)          (1981 samples)" </pre>

<p>我们绘制了来自第4类的三幅图像:</p>

<div><img src="img/9b0e768c-b749-44bb-8a77-4aa0758a9937.png" style="width:33.50em;height:12.25em;"/></div>

<p class="mce-root"/>

<p class="mce-root"/>

<p>这里我们跳过其余部分，但很明显，这些图像是在各种条件下拍摄的，包括天气、光照、遮挡、旋转等等。相反，为了便于参考，我们列出了所有类型的样本量:</p>

<table border="1" style="border-collapse: collapse">

<tbody>

<tr>

<td>

<p><strong> ID </strong></p>

</td>

<td>

<p><strong>类型</strong></p>

</td>

<td>

<p><strong>样本数量</strong></p>

</td>

</tr>

<tr>

<td>

<p>0</p>

</td>

<td>

<p>速度限制(20公里/小时)</p>

</td>

<td>

<p>211</p>

</td>

</tr>

<tr>

<td>

<p>一</p>

</td>

<td>

<p>速度限制(30公里/小时)</p>

</td>

<td>

<p>2221</p>

</td>

</tr>

<tr>

<td>

<p>2</p>

</td>

<td>

<p>速度限制(50公里/小时)</p>

</td>

<td>

<p>2251</p>

</td>

</tr>

<tr>

<td>

<p>3</p>

</td>

<td>

<p>速度限制(60公里/小时)</p>

</td>

<td>

<p>1411</p>

</td>

</tr>

<tr>

<td>

<p>四</p>

</td>

<td>

<p>速度限制(70公里/小时)</p>

</td>

<td>

<p>1981</p>

</td>

</tr>

<tr>

<td>

<p>5</p>

</td>

<td>

<p>速度限制(80公里/小时)</p>

</td>

<td>

<p>1861</p>

</td>

</tr>

<tr>

<td>

<p>6</p>

</td>

<td>

<p>限速终点(80公里/小时)</p>

</td>

<td>

<p>421</p>

</td>

</tr>

<tr>

<td>

<p>七</p>

</td>

<td>

<p>速度限制(100公里/小时)</p>

</td>

<td>

<p>1441</p>

</td>

</tr>

<tr>

<td>

<p>8</p>

</td>

<td>

<p>速度限制(120公里/小时)</p>

</td>

<td>

<p>1411</p>

</td>

</tr>

<tr>

<td>

<p>9</p>

</td>

<td>

<p>禁止通行</p>

</td>

<td>

<p>1471</p>

</td>

</tr>

<tr>

<td>

<p>10</p>

</td>

<td>

<p>超过3.5公吨的车辆不得通过</p>

</td>

<td>

<p>2011</p>

</td>

</tr>

<tr>

<td>

<p>11</p>

</td>

<td>

<p>下一个十字路口的通行权</p>

</td>

<td>

<p>1321</p>

</td>

</tr>

<tr>

<td>

<p>12</p>

</td>

<td>

<p>优先道路</p>

</td>

<td>

<p>2101</p>

</td>

</tr>

<tr>

<td>

<p>13</p>

</td>

<td>

<p>产量</p>

</td>

<td>

<p>2161</p>

</td>

</tr>

<tr>

<td>

<p>14</p>

</td>

<td>

<p>停止</p>

</td>

<td>

<p>781</p>

</td>

</tr>

<tr>

<td>

<p>15</p>

</td>

<td>

<p>没有车辆</p>

</td>

<td>

<p>631</p>

</td>

</tr>

<tr>

<td>

<p>16</p>

</td>

<td>

<p>禁止超过3.5公吨的车辆通行</p>

</td>

<td>

<p>421</p>

</td>

</tr>

<tr>

<td>

<p>17</p>

</td>

<td>

<p>禁止进入</p>

</td>

<td>

<p>1111</p>

</td>

</tr>

<tr>

<td>

<p>18</p>

</td>

<td>

<p>一般注意事项</p>

</td>

<td>

<p>1201</p>

</td>

</tr>

<tr>

<td>

<p>19</p>

</td>

<td>

<p>向左的危险曲线</p>

</td>

<td>

<p>211</p>

</td>

</tr>

<tr>

<td>

<p>20</p>

</td>

<td>

<p>向右的危险曲线</p>

</td>

<td>

<p>361</p>

</td>

</tr>

<tr>

<td>

<p>21</p>

</td>

<td>

<p>双曲线</p>

</td>

<td>

<p>331</p>

</td>

</tr>

<tr>

<td>

<p>22</p>

</td>

<td>

<p>坎坷的道路</p>

</td>

<td>

<p>391</p>

</td>

</tr>

<tr>

<td>

<p>23</p>

</td>

<td>

<p>湿滑的道路</p>

</td>

<td>

<p>511</p>

</td>

</tr>

<tr>

<td>

<p>24</p>

</td>

<td>

<p>路在右边变窄了</p>

</td>

<td>

<p>271</p>

</td>

</tr>

<tr>

<td>

<p>25</p>

</td>

<td>

<p>道路工程</p>

</td>

<td>

<p>1501</p>

</td>

</tr>

<tr>

<td>

<p>26</p>

</td>

<td>

<p>交通信号</p>

</td>

<td>

<p>601</p>

</td>

</tr>

<tr>

<td>

<p>27</p>

</td>

<td>

<p>行人</p>

</td>

<td>

<p>241</p>

</td>

</tr>

<tr>

<td>

<p>28</p>

</td>

<td>

<p>儿童穿越</p>

</td>

<td>

<p>541</p>

</td>

</tr>

<tr>

<td>

<p>29</p>

</td>

<td>

<p>自行车交叉路口</p>

</td>

<td>

<p>271</p>

</td>

</tr>

<tr>

<td>

<p>30</p>

</td>

<td>

<p>当心冰/雪</p>

</td>

<td>

<p>451</p>

</td>

</tr>

<tr>

<td>

<p>31</p>

</td>

<td>

<p>野生动物穿越</p>

</td>

<td>

<p>781</p>

</td>

</tr>

<tr>

<td>

<p>32</p>

</td>

<td>

<p>所有速度和通过限制的终点</p>

</td>

<td>

<p>241</p>

</td>

</tr>

<tr>

<td>

<p>33</p>

</td>

<td>

<p>前方右转</p>

</td>

<td>

<p>690</p>

</td>

</tr>

<tr>

<td>

<p>34</p>

</td>

<td>

<p>前方左转</p>

</td>

<td>

<p>421</p>

</td>

</tr>

<tr>

<td>

<p>35</p>

</td>

<td>

<p>仅限前方</p>

</td>

<td>

<p>1201</p>

</td>

</tr>

<tr>

<td>

<p>36</p>

</td>

<td>

<p>直走还是右转</p>

</td>

<td>

<p>391</p>

</td>

</tr>

<tr>

<td>

<p>37</p>

</td>

<td>

<p>直走还是左转</p>

</td>

<td>

<p>211</p>

</td>

</tr>

<tr>

<td>

<p>38</p>

</td>

<td>

<p>靠右行驶</p>

</td>

<td>

<p>2071</p>

</td>

</tr>

<tr>

<td>

<p>39</p>

</td>

<td>

<p>靠左</p>

</td>

<td>

<p>301</p>

</td>

</tr>

<tr>

<td>

<p>40</p>

</td>

<td>

<p>强制环岛</p>

</td>

<td>

<p>361</p>

</td>

</tr>

<tr>

<td>

<p>41</p>

</td>

<td>

<p>禁止通行的结束</p>

</td>

<td>

<p>241</p>

</td>

</tr>

<tr>

<td>

<p>42</p>

</td>

<td>

<p>超过3.5公吨的车辆不得通过</p>

</td>

<td>

<p>241</p>

</td>

</tr>

</tbody>

</table>

<p> </p>

<p>显然，标志，我们的感兴趣区域 ( <strong xmlns:epub="http://www.idpf.org/2007/ops"> ROI </strong>)并不在图像的中心，不幸的是，图像的大小是变化的。因此，在对数据进行分析和分类之前，我们需要将ROI从图像中分离出来，并将其大小标准化(如大多数研究人员所做的那样，将其大小调整为32*32)。我们求助于随图像一起提供的注释。每个类文件夹包含一个注释文件，例如位于<kbd xmlns:epub="http://www.idpf.org/2007/ops">00000</kbd>的<kbd xmlns:epub="http://www.idpf.org/2007/ops">GT-00000.csv</kbd>。每个注释文件包含以下有用的字段:</p>

<ul>

<li><kbd>Filename</kbd>:图像的文件名</li>

<li><kbd>ROI.X1</kbd>:ROI边界框左上角的<em> x </em>坐标</li>

<li><kbd>ROI.Y1</kbd>:ROI边界框左上角的<em> y </em>坐标</li>

<li><kbd>ROI.X2</kbd>:ROI边界框右下角的<em> x </em>坐标</li>

<li><kbd>ROI.Y2</kbd>:ROI边界框右下角的<em> y </em>坐标</li>

</ul>

<p>下面是一个ROI示例:</p>

<div><img src="img/8664b836-d856-47b3-8837-6f85aeae23f4.png" style="width:18.25em;height:18.25em;"/></div>

<p>现在我们为原始图像定义预处理函数，包括分离ROI并将其大小调整为32*32:</p>

<pre>&gt; source("http://bioconductor.org/biocLite.R") 

&gt; biocLite("EBImage") 

&gt; library("EBImage") 

&gt; roi_resize &lt;- function(input_matrix, roi){ 

+     roi_matrix &lt;- input_matrix[roi[1, 'Roi.Y1']:roi[1, 'Roi.Y2'],  

                    roi[1, 'Roi.X1']:roi[1, 'Roi.X2']] 

+     return(resize(roi_matrix, 32, 32)) 

+ } </pre>

<p>注意，resize函数来自EBImage包:</p>

<p><a href="https://bioconductor.org/packages/release/bioc/html/EBImage.html" target="_blank">https://bio conductor . org/packages/release/bioc/html/ebimage . html</a></p>

<p>我们在第一个样本上试用了它(仅红色通道):</p>

<pre>&gt; # read annotation csv file 

&gt; annotation &lt;- read.csv(file="GTSRB/Final_Training/Images/00000/GT-00000.csv", header=TRUE, sep=";") 

&gt; roi = annotation[3, ] 

&gt; red_matrix_cropped &lt;- roi_resize(red_matrix, roi) 

&gt; par(mfrow=c(1, 2)) 

&gt; image(rotate(red_matrix), col = grey.colors(255) , main=sprintf("Original")) 

&gt; image(rotate(red_matrix_cropped), col = grey.colors(255) , main=sprintf("Preprocessed")) </pre>

<p>我们在右边得到预处理的红色通道:</p>

<div><img src="img/23bb9cb5-70e3-4060-bc1b-e0f6f2a1b9a1.png" style="width:40.83em;height:19.92em;"/></div>

<p>同样，我们可以处理其他两个通道。基于这三个渠道，如何构建特征空间？丢弃任何频道都可能导致信息丢失。简单地把它们堆在一起会导致冗余。因此，将三个频道合并为一个将是更好的解决方案。在色彩世界中，Y'UV是一种编码系统，它将亮度信息与颜色信息分开加密。它通常用作彩色图像管道和计算机图形硬件的一部分。Y'UV用三个分量表示人类对颜色的感知:Y '表示亮度，U和V表示色度。Y'UV可以通过以下方式从RGB转换:</p>

<ul>

<li>Y' = 0.299R + 0.587G + 0.114B</li>

<li>u = 0.492(B-Y’)</li>

<li>v = 0.877(R-Y’)</li>

</ul>

<p>对于我们的特征空间，我们只能取亮度通道Y’。</p>

<p>现在我们已经准备好了整个预处理的最后一部分，让我们将它们放在一起，加载并处理(ROI + resize +转换为Y’)整个带标签的数据集:</p>

<pre>&gt; load_labeled_data &lt;- function(training_path, classes){ 

+   # Initialize the pixel features X and target y 

+   X &lt;- matrix(, nrow = 0, ncol = 32*32) 

+   y &lt;- vector() 

+   # Load images from each of the 43 classes 

+   for(i in classes) { 

+     print(paste('Loading images from class', i)) 

+     if (i&lt;10) { 

+       annotation_path &lt;- paste(training_path, "0000", i, "/GT-0000",  

                                 i, ".csv", sep="") 

+       path &lt;- paste(training_path, "0000", i, "/", sep="") 

+     } else { 

+       annotation_path &lt;- paste(training_path, "000", i, "/GT-000",  

                                 i, ".csv", sep="") 

+       path &lt;- paste(training_path, "000", i, "/", sep="") 

+     } 

+     annotation &lt;- read.csv(file=annotation_path, header=TRUE,  

                             sep=";") 

+      

+     for (row in 1:nrow(annotation)) { 

+       # Read each image 

+       image_path &lt;- paste(path, annotation[row, "Filename"], sep="") 

+       image &lt;- read.pnm(image_path, cellres=1) 

+       # Parse RGB color space 

+       red_matrix &lt;- matrix(image@red, nrow = image@size[1],  

                             ncol = image@size[2]) 

+       green_matrix &lt;- matrix(image@green, nrow = image@size[1],   

                               ncol = image@size[2]) 

+       blue_matrix &lt;- matrix(image@blue, nrow = image@size[1],  

                              ncol = image@size[2]) 

+       # Crop ROI and resize 

+       red_matrix_cropped &lt;- roi_resize(red_matrix,  

                                         annotation[row, ]) 

+       green_matrix_cropped &lt;- roi_resize(green_matrix,  

                                           annotation[row, ]) 

+       blue_matrix_cropped &lt;- roi_resize(blue_matrix,  

                                          annotation[row, ]) 

+       # Convert to brightness, e.g. Y' channel 

+       x &lt;- 0.299 * red_matrix_cropped + 0.587 *  

             green_matrix_cropped + 0.114 * blue_matrix_cropped 

+       X &lt;- rbind(X, matrix(x, 1, 32*32)) 

+       y &lt;- c(y, i) 

+     } 

+      

+   } 

+    

+   return(list("x" = X, "y" = y)) 

+ } </pre>

<p>如前所示定义数据加载函数后，我们将其应用于整个原始数据集:</p>

<pre>&gt; classes &lt;- 0:42 

&gt; data &lt;- load_labeled_data(training_path, classes) </pre>

<p>请耐心等待，因为读取和处理39，209张图像可能需要几个小时。为了以防发生意外，一个好的做法是保存数据对象，以便我们可以在以后的任何时候恢复它:</p>

<pre>&gt; # Save the data object to a file 

&gt; saveRDS(data, file = "43 classes.rds") 

&gt; # Restore the data object 

&gt; data &lt;- readRDS(file = "43 classes.rds") </pre>

<p>只需快速检查即可使用的数据:</p>

<pre>&gt; data.x &lt;- data$x 

&gt; data.y &lt;- data$y 

&gt; dim(data.x) 

[1] 39209  1024 </pre>

<p>尺寸正确！</p>

<pre>&gt; summary(as.factor(data.y)) 

   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22  

 210 2220 2250 1410 1980 1860  420 1440 1410 1470 2010 1320 2100 2160  780  630  420 1110 1200  210  360  330  390  

  23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42  

 510  270 1500  600  240  540  270  450  780  240  689  420 1200  390  210 2070  300  360  240  240 </pre>

<p>正确的班级人数，他们又一次不平衡了！</p>

<p>永远不要跳过检查分类的类平衡。</p>

<p>在确保数据被正确加载和处理后，我们对特征的分布，即像素，做更多的探索性分析。作为一个例子，我们从类别1(速度限制= 1)的每幅图像中的中心4*4块(第222 <sup>个</sup>到第225 <sup>个</sup>，第254 <sup>个</sup>到第257 <sup>个</sup>，第286 <sup>个</sup>到第289 <sup>个</sup>，以及第318 <sup>个</sup>到第321 <sup>个</sup>)中取16个像素我们展示它们的直方图:</p>

<pre>&gt; central_block &lt;- c(222:225, 254:257, 286:289, 318:321) 

&gt; par(mfrow=c(2, 2)) 

&gt; for(i in c(1, 14, 20, 27)) { 

+   hist(c(as.matrix(data.x[data.y==i, central_block])),</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<pre>+        main=sprintf("Histogram for class %d", i),  

+        xlab="Pixel brightness") 

+ } </pre>

<p>生成的像素亮度直方图显示如下:</p>

<div><img src="img/877372da-92e1-4df4-8f9c-1ea1f2843f5a.png" style="width:41.83em;height:30.42em;"/></div>

<p>中心像素的亮度在这四个类别中有不同的分布。例如，类别20的大部分中心像素是暗的，因为标志(右边的危险曲线)具有穿过中心的粗黑笔划；而在14级，停车标志在中央区域附近有一个白色的笔划(<strong> O </strong>的左边部分)。从其他位置获取的像素也可以清楚地分布在不同的类别中。</p>

<p>我们刚刚进行的探索性分析有助于我们继续构建基于像素的分类模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>First solution – convolutional neural networks using MXNet</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第一种解决方案——使用MXNet的卷积神经网络</h1>

                

            

            

                

<p>我们从一个类似于上一章末尾开发的解决方案开始，CNN使用MXNet。</p>

<p>同样，我们首先使用<kbd>caret</kbd>包将数据集分成两个子集，分别用于训练(75%)和测试(25%):</p>

<pre>&gt; if (!require("caret"))  

+     install.packages("caret") 

&gt; library (caret) 

&gt; set.seed(42) 

&gt; train_perc = 0.75 

&gt; train_index &lt;- createDataPartition(data.y, p=train_perc, list=FALSE) 

&gt; train_index &lt;- train_index[sample(nrow(train_index)),] 

&gt; data_train.x &lt;- data.x[train_index,] 

&gt; data_train.y &lt;- data.y[train_index] 

&gt; data_test.x &lt;- data.x[-train_index,] 

&gt; data_test.y &lt;- data.y[-train_index] </pre>

<p>不要忘记为可重复的工作指定一个特定的随机种子。我们通常在应用CNN之前进行数据标准化。在我们的例子中，加载的原始像素已经在0到1的范围内；Y '亮度转换后，得到的像素仍然在0到1的范围内。</p>

<p>一般来说，在将我们的数据放入CNN或实际上任何神经网络或基于梯度下降的模型之前，标准化我们的数据是必要的。对于图像输入，我们通常在0到1的范围内缩放像素。</p>

<p>在准备好训练和测试集之后，我们如下构建我们的CNN模型:</p>

<div><img src="img/09b7c4d4-2a09-45ce-ba14-e847164dbb6e.png"/></div>

<p>网络从第一组卷积层、ReLu非线性层和汇集层开始。这里我们使用32个5*5卷积滤波器和一个2*2最大池滤波器:</p>

<pre>&gt; require(mxnet) 

&gt; data &lt;- mx.symbol.Variable("data") 

&gt; # first convolution 

&gt; conv1 &lt;- mx.symbol.Convolution(data=data, kernel=c(5,5),  

                                 num_filter=32) 

&gt; act1 &lt;- mx.symbol.Activation(data=conv1, act_type="relu") 

&gt; pool1 &lt;- mx.symbol.Pooling(data=act1, pool_type="max", 

+                            kernel=c(2,2), stride=c(2,2)) 

It follows with the second set of convolutional, ReLu nonlinear and pooling layer, where 64 5*5 convolutional filters and 2*2 max pooling filter are used: 

&gt; # second convolution 

&gt; conv2 &lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5),  

                                 num_filter=64) 

&gt; act2 &lt;- mx.symbol.Activation(data=conv2, act_type="relu") 

&gt; pool2 &lt;- mx.symbol.Pooling(data=act2, pool_type="max", 

+                            kernel=c(2,2), stride=c(2,2)) </pre>

<p>现在，我们已经通过检测边缘、曲线和形状提取了输入图像的丰富表示，我们继续学习完全连接的图层。但在此之前，我们需要展平来自之前卷积图层的结果特征图:</p>

<pre>&gt; flatten &lt;- mx.symbol.Flatten(data=pool2) </pre>

<p>在完全连接部分，我们应用了1，000个单元的ReLu隐藏层和43个单元的softmax层:</p>

<pre>&gt; # first fully connected layer 

&gt; fc1 &lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=1000) 

&gt; act3 &lt;- mx.symbol.Activation(data=fc1, act_type="relu") 

&gt; # second fully connected layer 

&gt; fc2 &lt;- mx.symbol.FullyConnected(data=act3, num_hidden=43) </pre>

<p>最后是softmax层，为43个类中的每一个生成输出:</p>

<pre>&gt; # softmax output 

&gt; softmax &lt;- mx.symbol.SoftmaxOutput(data=fc2, name="sm") </pre>

<p>现在网络的所有部分都组装好了。在我们开始训练模型之前，我们指定随机种子和训练设备，并将矩阵<kbd>data_train.x</kbd>整形为MXNet中卷积层所需的数组:</p>

<pre>&gt; devices &lt;- mx.cpu() 

&gt; train.array &lt;- t(data_train.x) 

&gt; dim(train.array) &lt;- c(32, 32, 1, nrow(data_train.x)) 

&gt; mx.set.seed(42) </pre>

<p>模特训练时间:</p>

<pre>&gt; model_cnn &lt;- mx.model.FeedForward.create(softmax, X=train.array,  

y=data_train.y, ctx=devices, num.round=30,  

array.batch.size=100,learning.rate=0.05, momentum=0.9,  

wd=0.00001, eval.metric=mx.metric.accuracy,            

               epoch.end.callback=mx.callback.log.train.metric(100)) </pre>

<p>使用一种设备开始训练:</p>

<pre>[1] Train-accuracy=0.184965986394558 

[2] Train-accuracy=0.824610169491525 

[3] Train-accuracy=0.949389830508475 

[4] Train-accuracy=0.968305084745763 

[5] Train-accuracy=0.983050847457628 

[6] Train-accuracy=0.988372881355934 

[7] Train-accuracy=0.990745762711866 

[8] Train-accuracy=0.993152542372882 

[9] Train-accuracy=0.992576271186442 

[10] Train-accuracy=0.994372881355933 

[11] Train-accuracy=0.99542372881356 

[12] Train-accuracy=0.995118644067798 

[13] Train-accuracy=0.99671186440678 

[14] Train-accuracy=0.999830508474576 

[15] Train-accuracy=0.999932203389831 

[16] Train-accuracy=1 

[17] Train-accuracy=1 

[18] Train-accuracy=1 

[19] Train-accuracy=1 

[20] Train-accuracy=1 

[21] Train-accuracy=1 

[22] Train-accuracy=1 

[23] Train-accuracy=1 

[24] Train-accuracy=1 

[25] Train-accuracy=1 

[26] Train-accuracy=1 

[27] Train-accuracy=1 

[28] Train-accuracy=1 

[29] Train-accuracy=1 

[30] Train-accuracy=1 </pre>

<p>我们只是用超参数来拟合我们的模型:</p>

<ul>

<li><kbd>num.round =30</kbd>:最大迭代次数</li>

<li><kbd>array.batch.size = 100</kbd>:小批量梯度下降的批量</li>

<li><kbd>learning.rate = 0.05</kbd>:学习率</li>

<li><kbd>momentum=0.9</kbd>:动量因子，决定有多少以前的速度被合并到当前速度中</li>

<li><kbd>eval.metric=mx.metric.accuracy</kbd>:使用分类精度作为评估标准</li>

<li><kbd>initializer=mx.init.uniform(0.1)</kbd>:初始权重从-0.1到0.1之间的均匀分布中随机产生，以降低权重在深度网络中爆炸和消失的几率</li>

<li><kbd>wd = 0.00001</kbd>:L2正则化的权重衰减，它为大的权重增加了惩罚，以避免过度拟合</li>

</ul>

<p>我们可以通过以下方式查看模型的结构:</p>

<pre>&gt; graph.viz(model_cnn$symbol) </pre>

<p>然后将训练好的模型应用于测试集:</p>

<pre>&gt; test.array &lt;- t(data_test.x) 

&gt; dim(test.array) &lt;- c(32, 32, 1, nrow(data_test.x)) 

&gt; prob_cnn &lt;- predict(model_cnn, test.array) 

&gt; prediction_cnn &lt;- max.col(t(prob_cnn)) - 1 </pre>

<p>我们计算混淆矩阵和分类准确度如下:</p>

<pre>&gt; cm_cnn = table(data_test.y, prediction_cnn) 

&gt; cm_cnn </pre>

<p>由此产生的混淆矩阵的前半部分:</p>

<div><img src="img/fe46d35d-b213-4040-afcc-9a9b095ba2b4.png"/></div>

<p>后半部分:</p>

<div><img src="img/9eb4dc94-8762-4744-9e4b-2e81d9c72bf1.png" style="width:37.75em;height:46.33em;"/></div>

<p>尽管由于光照变化、部分遮挡、旋转、天气条件等导致图像外观变化很大，但分类错误仅在极少数情况下发生:</p>

<pre>&gt; accuracy_cnn = mean(prediction_cnn == data_test.y) 

&gt; accuracy_cnn 

[1] 0.9930612 </pre>

<p>我们刚刚建立了一个健壮的CNN模型，它正确地分类了超过99.30%的测试符号。CNN模型通过导出低级和高级特征来有效地学习表示。它使那些手工制作的特征变得过时，因为它从标志图像中自己捕捉重要的和可区分的特征。</p>

<p>既然我们已经使用我们最喜欢的(目前为止)深度学习工具MXNet取得了巨大的成功，我们为什么不探索其他同样强大的工具呢？事实上，另一个名为Keras的深度学习API最近越来越受欢迎；它的后端TensorFlow可能是最知名的深度学习框架。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Trying something new – CNNs using Keras with TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">尝试新事物——CNN使用Keras和TensorFlow</h1>

                

            

            

                

<p>keras(<a href="https://keras.io/" target="_blank">https://keras.io/</a>)是一个用Python编写的高级深度学习API，可以运行在这三个深度学习框架的任何一个之上:TensorFlow(来自谷歌)、CNTK(来自微软)和Theano(来自加拿大蒙特利尔大学蒙特利尔学习算法研究所)。要高效解决机器学习问题，能够快速原型化想法是关键。这也是最初开发Keras的原因，以促进以下关键方面的快速实验:</p>

<ul>

<li>用户友好的API构建在多个强大的后端之上，包括TensorFlow、CNTK和Theano。</li>

<li>内置的CNN、RNN和autoencoder模型以及支持类和方法(指标、优化器、正则化器、可视化等等)，可以轻松快速地构建原型。</li>

<li>出色的模块化和可扩展性。这些允许定制网络体系结构:多输入、多输出、层共享、模型共享、基于内存的网络等等。</li>

<li>允许相同的代码在CPU和GPU上无缝运行。</li>

</ul>

<p>对于R用户来说，Keras(<a href="https://keras.rstudio.com/" target="_blank">https://keras.rstudio.com/</a>)的R接口是在2017年开发的，其被社区采用的程度逐渐增长。让我们首先从GitHub安装<kbd>keras</kbd> R包，如下所示:</p>

<pre>&gt; if (!require("keras"))  

+     devtools::install_github("rstudio/keras") 

&gt; library(keras) </pre>

<p>这还没有完成。我们需要安装Keras连接到的底层后端。默认情况下，它使用TensorFlow作为后端引擎。我们可以使用以下函数来安装TensorFlow后端:</p>

<pre>&gt; install_keras() 

Using existing virtualenv at  ~/.virtualenvs/r-tensorflow  

Upgrading pip ... 

...... 

Installation complete. </pre>

<p>在我们等待安装的同时，让我们了解更多关于TensorFlow的信息。</p>

<p>tensor flow(<a href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>)是谷歌创建的开源机器学习框架。众所周知，它用于设计、构建和训练深度学习模型，但它也可以用于一般的数值计算。在TensorFlow中，使用数据流图来描述计算，其中图中的每个节点代表一个数学运算的实例，每个边代表一个多维数据数组(所谓的张量，可以保存矩阵、向量或标量)，在该数组上执行运算。这种灵活的架构允许我们有效地执行数据处理机器学习操作，如对巨大矩阵的导数。下面是一个数据流图表示例:</p>

<div><img src="img/3f0d938d-60d7-4a25-a83e-5bd13a979962.png" style="width:17.58em;height:15.33em;"/></div>

<p>现在我们看到它的名字TensorFlow是从哪里得来的:在网络中流动的张量。</p>

<p>至此，Keras核心库以及TensorFlow后端的安装已经完成。我们继续使用基于Keras的交通标志分类解决方案。</p>

<p>首先，通过重塑训练和测试特征矩阵为Keras建模准备输入数据:</p>

<pre>&gt; x_train &lt;- data_train.x 

&gt; dim(x_train) &lt;- c(nrow(data_train.x), 32, 32, 1) 

&gt; x_test &lt;- data_test.x 

&gt; dim(x_test) &lt;- c(nrow(data_test.x), 32, 32, 1) </pre>

<p>同样，输入像素的值已经从0到1，所以我们不需要执行任何重新缩放。</p>

<p>我们还根据Keras分类模型的要求，将训练和测试目标向量(从0到42的整数)转换为二进制类矩阵(一位热编码):</p>

<pre>&gt; y_train &lt;- to_categorical(data_train.y, num_classes = 43) 

&gt; y_test &lt;- to_categorical(data_test.y, num_classes = 43) </pre>

<p>在我们开始建模之前，有一个技巧可以在r中的Keras中获得可重复的结果。它是在使用以下函数开发模型之前指定一个随机种子:</p>

<pre>&gt; use_session_with_seed(42) </pre>

<p>它采取所有已知的措施来确保从Keras会话中获得可再现的结果。</p>

<p>数据准备后定义模型的时间到了！</p>

<p>我们首先初始化Keras顺序模型，如下所示:</p>

<pre>&gt; model &lt;- keras_model_sequential() </pre>

<p>然后，我们添加第一组卷积层，ReLu非线性层和汇集层，使用与先前MXNet解决方案相同的参数(网络的其余部分使用相同的参数):</p>

<pre>&gt; model %&gt;% </pre>

<p>从输入32*32像素图像的隐藏2D卷积层开始:</p>

<pre>+   layer_conv_2d( 

+     filter = 32, kernel_size = c(5,5),  

+     input_shape = c(32, 32, 1) 

+   ) %&gt;% 

+   layer_activation("relu") %&gt;% 

+   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% </pre>

<p>注意，我们使用pipe (%&gt;%)操作符将层添加到Keras顺序模型中。</p>

<p>接下来是第二组卷积层、ReLu非线性层和池层:</p>

<pre>+   # Second hidden convolutional layer layer 

+   layer_conv_2d(filter = 64, kernel_size = c(5,5)) %&gt;% 

+   layer_activation("relu") %&gt;% 

+   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% </pre>

<p>展平先前卷积图层生成的要素地图:</p>

<pre>+   layer_flatten() %&gt;% </pre>

<p>并送入致密层:</p>

<pre>+   layer_dense(1000) %&gt;% 

+   layer_activation("relu") %&gt;% </pre>

<p>最后，连接到包含43个输出单元的softmax层:</p>

<pre>+   layer_dense(43) %&gt;% 

+   layer_activation("softmax") </pre>

<p>我们可以使用<kbd>summary()</kbd>功能来查看模型的详细信息:</p>

<pre>&gt; summary(model)</pre>

<div><img src="img/5a7ff90e-d7dd-4383-b04c-b61eb3bf750b.png" style="width:39.33em;height:33.75em;"/></div>

<p>根据构建模型的时间，层的名称可能有不同的后缀(例如，<kbd>_1</kbd>，<kbd>_2</kbd>)。</p>

<p>CNN模型的所有部分现在都组装好了。在编译模型之前，我们需要显式地指定它的优化器。在MXNet中，优化器是<kbd>mx.model.FeedForward.create</kbd>方法中的一个参数，默认值为<strong>随机梯度下降</strong> ( <strong> SGD </strong>)。在Keras中，我们使用具有相同学习速度和动力的相同优化器:</p>

<pre>&gt; opt &lt;- optimizer_sgd(lr = 0.005, momentum = 0.9)</pre>

<p>连同我们刚刚定义的优化器，交叉熵作为损失函数，分类准确度作为度量，我们编译CNN模型:</p>

<pre>&gt; model %&gt;% compile( 

+   loss = "categorical_crossentropy", 

+   optimizer = opt, 

+   metrics = "accuracy" 

+ ) </pre>

<p>我们现在可以安全地开始模特训练了。同样，使用相同的超参数，包括批量大小和迭代次数。注意，测试数据集用于模型验证，其中为每个训练迭代计算当前CNN模型的分类性能。最后但并非最不重要的一点是，在我们的例子中，混洗是不必要的，因为在原始数据分割之后，训练数据已经被混洗了:</p>

<pre>&gt; model %&gt;% fit( 

+   x_train, y_train, 

+   batch_size = 100, 

+   epochs = 30, 

+   validation_data = list(x_test, y_test), 

+   shuffle = FALSE 

+ ) 

Train on 29409 samples, validate on 9800 samples 

Epoch 1/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 2.8031 - acc: 0.2823 - val_loss: 1.1719 - val_acc: 0.6733 

Epoch 2/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.6438 - acc: 0.8372 - val_loss: 0.4079 - val_acc: 0.8891 

Epoch 3/30 

29409/29409 [==============================] - 110s 4ms/step - loss: 0.3154 - acc: 0.9217 - val_loss: 0.2623 - val_acc: 0.9336 

Epoch 4/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.1969 - acc: 0.9533 - val_loss: 0.2096 - val_acc: 0.9483 

Epoch 5/30 

29409/29409 [==============================] - 24703s 840ms/step - loss: 0.1410 - acc: 0.9682 - val_loss: 0.1715 - val_acc: 0.9604 

Epoch 6/30 

29409/29409 [==============================] - 1076s 37ms/step - loss: 0.1055 - acc: 0.9761 - val_loss: 0.1363 - val_acc: 0.9690 

Epoch 7/30 

29409/29409 [==============================] - 34344s 1s/step - loss: 0.0860 - acc: 0.9806 - val_loss: 0.1147 - val_acc: 0.9742 

Epoch 8/30 

29409/29409 [==============================] - 104s 4ms/step - loss: 0.0698 - acc: 0.9841 - val_loss: 0.1065 - val_acc: 0.9756 

Epoch 9/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0535 - acc: 0.9874 - val_loss: 0.1015 - val_acc: 0.9780 

Epoch 10/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0414 - acc: 0.9913 - val_loss: 0.0927 - val_acc: 0.9801 

Epoch 11/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0415 - acc: 0.9917 - val_loss: 0.0912 - val_acc: 0.9807 

Epoch 12/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0341 - acc: 0.9933 - val_loss: 0.1054 - val_acc: 0.9769 

Epoch 13/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0266 - acc: 0.9946 - val_loss: 0.0811 - val_acc: 0.9842 

Epoch 14/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0207 - acc: 0.9965 - val_loss: 0.0790 - val_acc: 0.9845 

Epoch 15/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0221 - acc: 0.9955 - val_loss: 0.0780 - val_acc: 0.9841 

Epoch 16/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0169 - acc: 0.9974 - val_loss: 0.0753 - val_acc: 0.9854 

Epoch 17/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0137 - acc: 0.9982 - val_loss: 0.0777 - val_acc: 0.9863 

Epoch 18/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0114 - acc: 0.9986 - val_loss: 0.0757 - val_acc: 0.9863 

Epoch 19/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0101 - acc: 0.9990 - val_loss: 0.0775 - val_acc: 0.9867 

Epoch 20/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0086 - acc: 0.9993 - val_loss: 0.0786 - val_acc: 0.9862 

Epoch 21/30 

29409/29409 [==============================] - 110s 4ms/step - loss: 0.0077 - acc: 0.9994 - val_loss: 0.0776 - val_acc: 0.9859 

Epoch 22/30 

29409/29409 [==============================] - 110s 4ms/step - loss: 0.0071 - acc: 0.9995 - val_loss: 0.0774 - val_acc: 0.9862 

Epoch 23/30 

29409/29409 [==============================] - 109s 4ms/step - loss: 0.0066 - acc: 0.9996 - val_loss: 0.0779 - val_acc: 0.9862 

Epoch 24/30 

29409/29409 [==============================] - 110s 4ms/step - loss: 0.0062 - acc: 0.9997 - val_loss: 0.0783 - val_acc: 0.9860 

Epoch 25/30 

29409/29409 [==============================] - 114s 4ms/step - loss: 0.0059 - acc: 0.9997 - val_loss: 0.0786 - val_acc: 0.9859 

Epoch 26/30 

29409/29409 [==============================] - 115s 4ms/step - loss: 0.0056 - acc: 0.9998 - val_loss: 0.0791 - val_acc: 0.9861 

Epoch 27/30 

29409/29409 [==============================] - 117s 4ms/step - loss: 0.0053 - acc: 0.9998 - val_loss: 0.0793 - val_acc: 0.9860 

Epoch 28/30 

29409/29409 [==============================] - 115s 4ms/step - loss: 0.0051 - acc: 0.9998 - val_loss: 0.0794 - val_acc: 0.9862 

Epoch 29/30 

29409/29409 [==============================] - 114s 4ms/step - loss: 0.0050 - acc: 0.9998 - val_loss: 0.0795 - val_acc: 0.9864 

Epoch 30/30 

29409/29409 [==============================] - 113s 4ms/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.0796 - val_acc: 0.9865  </pre>

<p>经过30个时期后，该模型被很好地训练，在测试集上达到98.65%的准确度。在RStudio查看器窗格中，我们还可以实时查看每个时期的分类性能:</p>

<div><img src="img/43053402-088e-466e-82e2-5305a9636821.png" style="width:36.67em;height:29.00em;"/></div>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reducing overfitting with dropout</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">通过下降减少过度拟合</h1>

                

            

            

                

<p>你可能会注意到，我们在MXNet解决方案中使用了L2正则化，它增加了对大权重的惩罚，以避免过度拟合；但是在这个Keras解决方案中我们没有这样做。这导致在测试集上的分类准确度略有不同(99.30%对98.65%)。我们将在我们的Keras解决方案中采用正规化，特别是这次<strong>辍学</strong>。</p>

<p>Dropout是最初由Geoffrey Hinton等人提出的神经网络中的正则化技术。艾尔。在2012年(<em>通过防止特征检测器的共同适应来改进神经网络<em>神经和进化计算</em>中的</em>)。顾名思义，它忽略了在训练过程中在神经网络中随机选择的一小部分神经元(可以是隐藏的，也可以是可见的)。<em>退出的</em>神经元暂时对下游神经元的激活或后向通路上神经元的权重更新没有贡献。那么，辍学技术如何能够防止过度拟合呢？</p>

<p>回想一下，在标准神经网络中，神经元在训练期间在相邻神经元之间是相互依赖的。并且针对网络内的特定上下文来调整神经元的权重，这限制了每个神经元的个体能力。这种对上下文的依赖可能会导致模型过于专门化而无法训练数据。当不考虑网络中的一些神经元时，神经元的权重对其他神经元的权重变得不那么敏感。神经元是更独立地学习有用信息的力量。训练数据的协同适应是不利的。</p>

<p>雇用辍学者很简单。在训练阶段，在一个辍学率为<em> p </em>的层中，对于每次迭代，我们随机关闭一小部分<em> p </em>的神经元。在测试阶段，我们使用所有的神经元，但是通过因子<em> q </em> = 1 - <em> p </em>来缩放它们的激活，以便考虑在训练阶段退出的激活。</p>

<p>这是一个标准的神经网络(第一幅图)和一个有漏失的网络(第二幅图):</p>

<div><img src="img/4e10722a-7e13-4918-9010-2878e9c69ff8.png" style="width:32.75em;height:21.75em;"/></div>

<div><img src="img/b6038a10-a631-48d6-819e-60faf329187e.png" style="width:33.67em;height:22.83em;"/></div>

<p>在本例中，除了隐藏图层之外，还将dropout应用于可见图层，即输入图层。</p>

<p>在实践中，辍学率通常设定在20%到50%之间。丢失率太低的层影响不大，而丢失率太高会导致欠拟合。</p>

<p>现在让我们通过使用函数<kbd>layer_dropout(p)</kbd>将dropout应用于我们的Keras解决方案。我们定义一个函数来初始化和编译一个带有dropout的CNN模型(为了重用):</p>

<pre>&gt; init_cnn_dropout &lt;- function(){ 

+     model_dropout &lt;- keras_model_sequential() 

+     model_dropout %&gt;% 

+         layer_conv_2d( 

+             filter = 32, kernel_size = c(5,5),  

+             input_shape = c(32, 32, 1) 

+         ) %&gt;% 

+         layer_activation("relu") %&gt;% 

+         layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 

+          

+         # Second hidden convolutional layer layer 

+         layer_conv_2d(filter = 64, kernel_size = c(5,5)) %&gt;% 

+         layer_activation("relu") %&gt;% 

+         # Use max pooling 

+         layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 

+         layer_dropout(0.25) %&gt;% 

+          

+         # Flatten and feed into dense layer 

+         layer_flatten() %&gt;% 

+         layer_dense(1000) %&gt;% 

+         layer_activation("relu") %&gt;% 

+         layer_dropout(0.25) %&gt;% 

+          

+         # Outputs from dense layer  

+         layer_dense(43) %&gt;% 

+         layer_activation("softmax") 

+      

+     opt &lt;- optimizer_sgd(lr = 0.005, momentum = 0.9) 

+      

+     model_dropout %&gt;% compile( 

+         loss = "categorical_crossentropy", 

+         optimizer = opt, 

+         metrics = "accuracy" 

+     ) 

+     return(model_dropout) 

+ } </pre>

<p>获取新型号:</p>

<pre>&gt; model_dropout &lt;- init_cnn_dropout() </pre>

<p>我们只是在第二个最大池层中采用25%的下降，在完全连接的隐藏层中采用25%的下降。通过调用<kbd>summary(model_dropout)</kbd>，我们可以看到分别位于第二个MaxPooling2D层和第一个密集和激活层正下方的两个dropout层。</p>

<p>继续进行模型培训:</p>

<pre>&gt; model_dropout %&gt;% fit( 

+   x_train, y_train, 

+   batch_size = 100, 

+   epochs = 30, 

+   validation_data = list(x_test, y_test), 

+   shuffle = FALSE 

+ ) 

Train on 29409 samples, validate on 9800 samples 

Epoch 1/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 3.1078 - acc: 0.1987 - val_loss: 1.4475 - val_acc: 0.6487 

Epoch 2/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.9772 - acc: 0.7337 - val_loss: 0.4570 - val_acc: 0.8934 

Epoch 3/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.5194 - acc: 0.8598 - val_loss: 0.3043 - val_acc: 0.9310 

Epoch 4/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.3606 - acc: 0.9037 - val_loss: 0.2058 - val_acc: 0.9529 

Epoch 5/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.2828 - acc: 0.9250 - val_loss: 0.1677 - val_acc: 0.9640 

Epoch 6/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.2272 - acc: 0.9406 - val_loss: 0.1424 - val_acc: 0.9707 

Epoch 7/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.1910 - acc: 0.9494 - val_loss: 0.1138 - val_acc: 0.9793 

Epoch 8/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.1560 - acc: 0.9602 - val_loss: 0.0986 - val_acc: 0.9797 

Epoch 9/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.1423 - acc: 0.9621 - val_loss: 0.0956 - val_acc: 0.9804 

Epoch 10/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.1256 - acc: 0.9663 - val_loss: 0.0814 - val_acc: 0.9841 

Epoch 11/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.1111 - acc: 0.9708 - val_loss: 0.0760 - val_acc: 0.9847 

Epoch 12/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0987 - acc: 0.9735 - val_loss: 0.0795 - val_acc: 0.9824 

Epoch 13/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0899 - acc: 0.9752 - val_loss: 0.0626 - val_acc: 0.9876 

Epoch 14/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0799 - acc: 0.9787 - val_loss: 0.0665 - val_acc: 0.9868 

Epoch 15/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0794 - acc: 0.9792 - val_loss: 0.0571 - val_acc: 0.9887 

Epoch 16/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0691 - acc: 0.9817 - val_loss: 0.0534 - val_acc: 0.9898 

Epoch 17/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0668 - acc: 0.9817 - val_loss: 0.0560 - val_acc: 0.9892 

Epoch 18/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0583 - acc: 0.9846 - val_loss: 0.0486 - val_acc: 0.9916 

Epoch 19/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0541 - acc: 0.9861 - val_loss: 0.0484 - val_acc: 0.9914 

Epoch 20/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0529 - acc: 0.9858 - val_loss: 0.0494 - val_acc: 0.9906 

Epoch 21/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0500 - acc: 0.9864 - val_loss: 0.0449 - val_acc: 0.9909 

Epoch 22/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0469 - acc: 0.9872 - val_loss: 0.0414 - val_acc: 0.9926 

Epoch 23/30 

29409/29409 [==============================] - 106s 4ms/step - loss: 0.0473 - acc: 0.9863 - val_loss: 0.0415 - val_acc: 0.9917 

Epoch 24/30 

29409/29409 [==============================] - 107s 4ms/step - loss: 0.0406 - acc: 0.9894 - val_loss: 0.0416 - val_acc: 0.9916 

Epoch 25/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0413 - acc: 0.9888 - val_loss: 0.0445 - val_acc: 0.9909 

Epoch 26/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0337 - acc: 0.9906 - val_loss: 0.0412 - val_acc: 0.9922 

Epoch 27/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0333 - acc: 0.9911 - val_loss: 0.0388 - val_acc: 0.9928 

Epoch 28/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0332 - acc: 0.9905 - val_loss: 0.0395 - val_acc: 0.9933 

Epoch 29/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0312 - acc: 0.9910 - val_loss: 0.0371 - val_acc: 0.9937 

Epoch 30/30 

29409/29409 [==============================] - 108s 4ms/step - loss: 0.0305 - acc: 0.9917 - val_loss: 0.0383 - val_acc: 0.9940 </pre>

<p>通过剔除，测试集上的预测准确率提高到99.40%。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Dealing with a small training set – data augmentation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">处理小规模训练集——数据扩充</h1>

                

            

            

                

<p>到目前为止，我们非常幸运地拥有一个足够大的训练数据集，39，209个样本中有75%的样本。这也是我们能够达到99.3%到99.4%的分类准确率的原因之一。然而，在现实中，在大多数监督学习情况下，获得大的训练集并不容易，在这些情况下，人工工作是必要的，或者数据收集和标记的成本很高。在我们的交通标志分类项目中，如果给我们的训练样本少得多，我们还能达到同样的性能吗？让我们试一试。</p>

<p>我们用39，209个样本中的10%来模拟一个小的训练集，用其余的90%来模拟一个测试集:</p>

<pre>&gt; train_perc_1 = 0.1 

&gt; train_index_1 &lt;- createDataPartition(data.y, p=train_perc_1, list=FALSE) 

&gt; train_index_1 &lt;- train_index_1[sample(nrow(train_index_1)),] 

&gt; data_train_1.x &lt;- data.x[train_index_1,] 

&gt; data_train_1.y &lt;- data.y[train_index_1] 

&gt; data_test_1.x &lt;- data.x[-train_index_1,] 

&gt; data_test_1.y &lt;- data.y[-train_index_1] 

&gt; x_train_1 &lt;- data_train_1.x 

&gt; dim(x_train_1) &lt;- c(nrow(data_train_1.x), 32, 32, 1) 

&gt; x_test_1 &lt;- data_test_1.x 

&gt; dim(x_test_1) &lt;- c(nrow(data_test_1.x), 32, 32, 1) 

&gt; y_train_1 &lt;- to_categorical(data_train_1.y, num_classes = 43) 

&gt; y_test_1 &lt;- to_categorical(data_test_1.y, num_classes = 43) </pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mceNonEditable"/>

<p>初始化一个新模型，并使其适合新的训练集:</p>

<pre>&gt; model_1 &lt;- init_cnn_dropout() 

&gt; model_1 %&gt;% fit( 

+   x_train_1, y_train_1, 

+   batch_size = 100, 

+   epochs = 1, 

+   validation_data = list(x_test_1, y_test_1), 

+   shuffle = FALSE 

+ )  </pre>

<p>对3921个样本进行训练；对35，288个样本进行验证:</p>

<pre>Epoch 1/30 

3921/3921 [==============================] - 19s 5ms/step - loss: 3.6705 - acc: 0.0594 - val_loss: 3.5191 - val_acc: 0.0592 

Epoch 2/30 

3921/3921 [==============================] - 17s 4ms/step - loss: 3.5079 - acc: 0.0681 - val_loss: 3.4663 - val_acc: 0.0529 

...... 

...... 

Epoch 29/30 

3921/3921 [==============================] - 17s 4ms/step - loss: 0.1935 - acc: 0.9462 - val_loss: 0.2760 - val_acc: 0.9381 

Epoch 30/30 

3921/3921 [==============================] - 17s 4ms/step - loss: 0.1962 - acc: 0.9431 - val_loss: 0.2772 - val_acc: 0.9393 </pre>

<p>一个只有3921个样本训练的模型，达到93.93%的准确率，已经不错了。但是我们能做得更好吗，至少接近99%，因为我们有足够的训练数据？是啊！一个解决方案是数据扩充。</p>

<p><strong>数据扩充</strong>简单来说就是扩大我们提供给监督学习模型的现有数据的规模，以补偿进一步数据收集和标记的成本。</p>

<p>有许多方法可以增加计算机视觉中的数据。最简单的方法可能是水平或垂直翻转图像。以<kbd>General caution</kbd>标志为例；我们使用Keras中的函数<kbd>flow_images_from_data()</kbd>实现翻转，如下所示。</p>

<p>装载<kbd>General caution</kbd>样品:</p>

<pre>&gt; img&lt;-image_load(paste(training_path, "00018/00001_00004.ppm", sep="")) 

&gt; img1&lt;-image_to_array(img) 

&gt; dim(img1)&lt;-c(1,dim(img1)) </pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p>我们生成一个水平翻转的图像，并将结果图像保存在我们创建的<kbd>augmented</kbd>目录中:</p>

<pre>&gt; images_iter  &lt;- flow_images_from_data(img1, , generator =  

                  image_data_generator(horizontal_flip = TRUE), 

+                 save_to_dir = 'augmented', 

+                 save_prefix = "horizontal", save_format = "png") 

&gt; reticulate::iter_next(images_iter)  </pre>

<p>翻转的符号(右)与原始图像(左)一起显示如下:</p>

<div><img src="img/34d95963-4bd2-47e1-b4e1-19cd7dadcfd9.png" style="width:31.83em;height:12.33em;"/></div>

<p>水平翻转的标志图像传达了与原始图像相同的信息。需要注意的是，翻转只在对方向不敏感的情况下起作用，比如猫和狗之间的分类，或者我们对交通灯的识别。然而，在方位很重要的情况下，例如右转和左转之间的分类，仍然可以应用小至中等程度的旋转。例如，将<em>危险曲线向右翻转</em>符号是绝对危险的，但是最多旋转20度是无害的，甚至是有益的，正如我们在下面的例子中看到的:</p>

<pre>&gt; img&lt;-image_load(paste(training_path, "00020/00002_00017.ppm", sep="")) 

&gt; img1&lt;-image_to_array(img) 

&gt; dim(img1)&lt;-c(1,dim(img1)) 

&gt; images_iter  &lt;- flow_images_from_data(img1, , generator =         

                  image_data_generator(rotation_range = 20), 

+                 save_to_dir = 'augmented', 

+                 save_prefix = "rotation", save_format = "png") 

&gt; reticulate::iter_next(images_iter) </pre>

<p>旋转后的符号(右)和原始图像(左)包含相同的信息:</p>

<div><img src="img/a9ddd530-9cc9-4533-bc4d-5220919ca1a9.png" style="width:28.92em;height:11.58em;"/></div>

<p>移位可能是最常见的增强方法。将图像水平和/或垂直移动少量像素会产生功能相同的图像。使用与前面相同的示例，我们将其水平和垂直移动最多20%的宽度和高度:</p>

<pre>&gt; images_iter  &lt;- flow_images_from_data(img1,                                      

                generator=image_data_generator(width_shift_range=0.2,                                                                         

                height_shift_range=0.2), save_to_dir = 'augmented', 

+               save_prefix = "shift", save_format = "png") 

&gt; reticulate::iter_next(images_iter) </pre>

<p>这导致右半部分的图像偏移:</p>

<div><img src="img/feecb4fe-83aa-400a-862d-3f17eaa013dd.png" style="width:30.42em;height:12.17em;"/></div>

<p>有了常见的增强方法，让我们通过最多20度的旋转和最多20%的移位来增强我们的小训练数据集(注意，我们不能应用翻转，因为一些符号，如类19、20、21、33和34，不是语义对称的):</p>

<pre>&gt; datagen &lt;- image_data_generator( 

+   rotation_range = 20, 

+   width_shift_range = 0.2, 

+   height_shift_range = 0.2, 

+   horizontal_flip = FALSE 

+ ) 

&gt; 

&gt; datagen %&gt;% fit_image_data_generator(x_train_1) 

Augmented data generator is defined and now being applied to a CNN model with function fit_generator:  

&gt; model_2 &lt;- init_cnn_dropout() 

&gt; model_2 %&gt;% fit_generator( 

+   flow_images_from_data(x_train_1, y_train_1,  

                          datagen, batch_size = 100), 

+   steps_per_epoch = as.integer(50000/100),  

+   epochs = 30,  

+   validation_data = list(x_test_1, y_test_1) 

+ ) 

Epoch 1/30 

500/500 [==============================] - 74s 149ms/step - loss: 3.4566 - acc: 0.0798 - val_loss: 3.2963 - val_acc: 0.1322 

Epoch 2/30 

500/500 [==============================] - 77s 153ms/step - loss: 3.0920 - acc: 0.1666 - val_loss: 2.1010 - val_acc: 0.4249 

...... 

...... 

Epoch 25/30 

500/500 [==============================] - 83s 166ms/step - loss: 0.1396 - acc: 0.9584 - val_loss: 0.0636 - val_acc: 0.9860 

Epoch 26/30 

500/500 [==============================] - 79s 158ms/step - loss: 0.1359 - acc: 0.9592 - val_loss: 0.0672 - val_acc: 0.9859 

Epoch 27/30 

500/500 [==============================] - 80s 160ms/step - loss: 0.1344 - acc: 0.9600 - val_loss: 0.0727 - val_acc: 0.9843 

Epoch 28/30 

500/500 [==============================] - 81s 163ms/step - loss: 0.1227 - acc: 0.9628 - val_loss: 0.0647 - val_acc: 0.9862 

Epoch 29/30 

500/500 [==============================] - 79s 158ms/step - loss:
0.1222 - acc: 0.9627 - val_loss: 0.0668 - val_acc: 0.9858 

Epoch 30/30 

500/500 [==============================] - 80s 160ms/step - loss: 0.1220 - acc: 0.9636 - val_loss: 0.0614 - val_acc: 0.9870 </pre>

<div><img src="img/efc6b5f7-7f79-4c86-9812-27b9845f18f8.png"/></div>

<p>通过数据扩充，我们可以在小训练集的情况下获得98.70%准确率的优秀结果。</p>

<p>与没有数据扩充相比，除了4.77% (93.93%到98.70%)的性能提升之外，我们还观察到每次迭代花费的时间比以前更长(大约20秒到大约80秒)。这是因为<kbd>image_data_generator()</kbd>为每次迭代实时生成小批量的增强图像数据。因此，即使在两次迭代中使用相同的样本集，扩充的数据也可能非常不同。这种设置为训练集增加了更多的变化，从而使模型更加健壮。这就是为什么数据扩充被认为是一种减少过度拟合的方法。</p>

<p>数据扩充的另一个特别有用的应用是平衡数据集。在大多数不平衡分类的情况下(如在线广告点击预测，或银行欺诈检测)，我们通常对主导类进行下采样。但是，对于小数据集来说，这可能会适得其反。另一个解决方案是从minor类增加数据。</p>

<p>最后但并非最不重要的是，还有之前没有提到的其他方法来增加图像数据。例如，重新缩放会将像素值乘以一个系数，从而改变照明条件。剪切和缩放也是有用的数据扩充转换。它们可以在<kbd>image_data_generator()</kbd>中指定。如果感兴趣，你可以尝试应用这些方法中的任何一种，看看你是否能打败98.70%。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reviewing methods to prevent overfitting in CNNs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">回顾防止CNN过度拟合的方法</h1>

                

            

            

                

<p>当模型与训练集拟合得太好，但不能推广到看不见的情况时，就会发生过度拟合。例如，CNN模型在训练集中识别特定的交通标志图像，而不是一般的模式。如果自动驾驶汽车无法在不断变化的条件下识别标志图像，例如不同的天气、照明和不同于训练集中呈现的角度，这可能是非常危险的。概括来说，我们可以做以下事情来减少过度拟合:</p>

<ul>

<li>收集更多的训练数据(如果可能和可行),以便考虑各种输入数据。</li>

<li>使用数据扩充，如果时间或成本不允许我们收集更多的数据，我们就以一种聪明的方式发明数据。</li>

<li>采用辍学，这减少了复杂的相邻神经元之间的共同适应。</li>

<li>添加套索(L1)或/和山脊(L2)惩罚，防止模型系数拟合得如此完美，以至于出现过度拟合。</li>

<li>降低网络架构的复杂性。回想一下，在上一章中，我们提到过添加隐藏层不会帮助提高模型性能，但会增加过度拟合的机会。</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6e9b3920-1b5c-448c-b620-b3dbb86cf683" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>我们刚刚完成了这个R和深度学习旅程中的第二个计算机视觉项目！通过本章，我们更加熟悉了卷积神经网络及其在MXNet中的实现，以及另一个强大的深度学习工具:Keras with TensorFlow。</p>

<p>我们从什么是自动驾驶汽车以及深度学习技术如何使自动驾驶汽车变得可行和更可靠开始。我们还讨论了深度学习如何脱颖而出，成为智能车辆中物体识别的最先进解决方案。在探索交通标志数据集之后，我们使用MXNet开发了我们的第一个CNN模型，并实现了99%以上的准确率。然后我们转移到另一个强大的深度学习框架，Keras + TensorFlow，并获得了可比的结果。</p>

<p>我们引入了剔除技术来减少过拟合。我们还学习了如何处理缺乏训练数据和利用数据增强技术，包括翻转，移位和旋转。我们最后总结了一些防止CNN模型过度拟合的方法。这是我们观察深度学习如何消除传统方法中采取的手动或显式特征提取步骤的第二个例子，而是有效地找到最佳特征集。</p>

<p>我们在这两个计算机视觉项目中练习了CNN。在下一个项目中，我们将使用完全不同类型的深度神经网络——自动编码器。</p>





            



            

        

    </body>



</html></body></html>