

# 九、实现强化学习

**强化学习** ( **RL** )最近获得了相当大的牵引力。这是一种不同于传统机器学习和深度学习技术的机器智能方法。在学习围棋、Dota 等复杂游戏方面已经达到了人类水平的表现。RL 是一个人工智能框架，其中代理通过试错来执行学习。这是一个模仿人类基本学习方式的学习过程。本章的首要目标是让你熟悉 RL 的组成部分。您将学习如何使用 r 中的各种包来实现 RL。

在本章中，我们将介绍以下配方:

*   使用 MDPtoolbox 的基于模型的 RL
*   无模型 RL
*   使用 RL 进行悬崖行走



# 使用 MDPtoolbox 的基于模型的 RL

RL 是人工智能的通用框架。它用于解决顺序决策问题。在 RL 中，给计算机一个要实现的目标，它通过从与环境的交互中学习来学习如何实现这个目标。一个典型的 RL 设置由五个组件组成，分别称为**代理**、**环境**、**动作**、**状态**和**奖励**。

在 RL 中，代理使用一组**动作** ( **A** )中的一个动作与环境进行交互。基于代理采取的动作，环境从初始状态转换到新状态，其中每个状态属于环境内的一组**状态**。这种转变从环境中产生一个反馈**奖励**信号(一个标量)。奖励是对代理表现的评估，奖励值取决于当前状态和执行的操作。这个过程是一个主体对行为和环境作出选择，对新的状态和与之相关的奖励作出响应，直到主体学习到一个最佳行为，从任何初始状态达到最终状态，使累积奖励最大化。

下图是 RL 设置的图示:

![](img/be3e424e-7dba-4aed-ae9e-0d2a198d533b.png)

![](img/9ed2e42e-dac2-459b-bc39-64a0052189af.png)可以表述如下:

![](img/f2f45a6d-1c85-4810-ac7d-08ad78ea20c2.png)

这里， *G [t]* 是累积奖励，
γ是折扣因子，
和 *t* 是时间步长

RL 设置遵循特定的假设。首先，智能体顺序地与环境相互作用；那么，第二，时空是离散的；最后，转移遵循马尔可夫性质；也就是说，环境的未来状态![](img/f8c01674-1229-4082-a468-9ed341d87eaa.png)，只取决于当前状态 *s* 。马尔可夫过程是一种无记忆的随机过程；也就是说，它是一个具有马尔可夫性质的随机状态序列。这是一个为决策过程建模的框架。**马尔可夫决策过程** ( **MDP** )指定了一种数学结构来寻找 RL 问题的解决方案。

它是由五个部分组成的元组—(S，A，P，R，γ):

*   **S** :状态集合，![](img/c1fe1aca-6ea2-4bb0-a7cd-b9490cd15ec1.png)。
*   **A** :一组动作，![](img/56a6e606-0c24-45a7-a6db-8551a0a19669.png)。
*   **P** :转移概率![](img/910c477c-71f7-4e7f-995c-7e032d77c050.png)，是在![](img/cb772df4-546f-418e-af56-f22866a8d69f.png)状态下采取动作![](img/88273da5-7252-49d1-814b-4ee10f3c3319.png)后到达![](img/e7bb7ed5-c9a6-40c6-9c3e-fe79dc75ad30.png)的概率。
*   **R** :奖励函数![](img/ea48ab26-c285-4d01-81ff-b3588626bc82.png)，使用动作![](img/ed564300-186e-4cd5-a8c0-d08225cb609a.png)从![](img/c281b351-7a6f-4832-8fb7-4af5afa82745.png)移动到![](img/1949fc0e-dfc1-4286-b1db-1f1a24fec760.png)时的期望奖励。
*   **![](img/9d88101e-80b8-4ed0-9f7a-5af5cdc16d53.png)** :折扣系数。z

使用 MDP，我们可以找到一个策略![](img/383da270-68e8-462c-b548-0649505ef099.png)，它用贴现因子![](img/2903960e-b5fb-4001-95e0-58ce2a063f7f.png)最大化预期长期回报![](img/3933cbf6-2168-4ecb-93c5-b6cbfb5adf6d.png)(它定义了未来回报的适用贴现)。策略定义了代理应该根据当前状态采取的最佳操作。它将动作映射到状态。从一个状态 *s* 开始，并遵循一个策略![](img/c6c206cc-bb6e-4d0d-b41c-8b89bcb82833.png)来估计代理的长期回报的函数被称为价值函数。

有两种类型的价值函数:

*   **状态值**函数， **![](img/15bcf353-28d0-41a9-8cd1-74a77b63253c.png)** :对于 MDP，它是从状态 *s* 开始并遵循策略![](img/b4ca8e02-4c1e-4452-9c3f-9853ed603c67.png)的代理的期望收益:

![](img/a3f9e91b-641c-414d-bda2-83b57096fd38.png)

*   **动作值**函数， **![](img/aba50990-5f52-4456-92d9-64ca76d635b7.png)** :对于 MDP，它是代理人从状态 *s* 开始，采取动作 *a* ，然后遵循策略![](img/a174b73c-9fa8-482f-93d5-fa5e44a8f0a8.png)的期望收益:

![](img/ee0585cd-7820-4d71-9b0d-ee74e6c4ddac.png)

在所有可能的价值函数中，存在一个**最优价值函数** ![](img/1c7b930c-f5c9-44a3-a055-b75c305a7102.png)，它对所有状态产生最高的期望回报。对应于最优值函数的策略被称为**最优策略**。

![](img/38f70e66-666f-43ee-991d-647802374da5.png)和![](img/e361271b-0b1a-428c-a72e-cfc486cc1b7c.png)可以表示如下:

![](img/ceb3eb6a-6f66-4c42-8212-85420d6498f1.png)

![](img/4a20786d-7d1d-4883-98bc-e1840c615ed9.png)

通过最大化超过![](img/4166cad1-64d4-4379-b5a0-6cca0e9ceb1d.png)可以找到最优策略。最佳策略可以描述如下:

![](img/1957e405-5e8e-4299-91d0-e4605428de91.png)

利用贝尔曼方程，我们可以找到最优值函数。**贝尔曼期望方程**将价值函数定义为从当前状态转移 *s* ，使用动作 *a* ，以及从下一个状态转移*s’*的期望回报的总和:

![](img/1d23264f-8b8b-4bcf-9559-5b19b4e6a5cc.png)
![](img/1310c566-8ade-4c21-a12c-d3e2c3c09f2b.png)

用于![](img/91383f66-0beb-48c7-95f3-54ff0820ed6b.png)和![](img/c84c5922-1f6b-4de0-893d-55f7bc81458d.png)的**贝尔曼最优方程**如下所示:

![](img/5318ae64-2363-4908-992a-89875212e058.png)

![](img/8c09a216-8bb8-448e-8409-ee35745d6ca6.png)

此外，最佳状态值和动作值函数通过贝尔曼最优方程递归关联，如下式所示:

![](img/ad5e2850-92b8-4d65-995c-534ebce4ec7e.png)

由此，我们得到以下结论:

![](img/efadc984-5c8c-466d-80d4-1638919bacc3.png)

求解贝尔曼最优性方程有很多方法，比如值迭代、策略迭代、SARSA、Q-learning 等。RL 技术可以分为基于模型的方法和无模型的方法。基于模型的算法依赖于提供状态转移概率的环境的显式模型，以及 MDP 形式的环境表示。这些 MDP 可以用各种算法求解，比如值迭代，策略迭代。

另一方面，无模型算法不依赖于任何关于代表问题的环境的明确知识。相反，他们试图学习基于代理与环境的动态交互的最优策略。在这个菜谱中，我们将使用基于模型的策略迭代算法来解决 RL 问题。



# 做好准备

在这个食谱中，我们将解决一个网格导航问题。下图是导航网格的图示。它代表一个导航矩阵，其中每个状态都被分配了一个标签。矩阵中的每个单元代表一种状态，总共有四种状态。代理应该从任意随机起始状态导航到最终目标状态，4。代理只能通过网格中的开口在状态之间移动，而不能离开网格壁。在每个状态下，代理可以执行可用动作集中的任何动作；也就是说，它们可以向上、向下、向左或向右移动。当进入目标状态时，代理人获得 100 的奖励，其他每增加一步都要付出-1 的惩罚。

在下图中，可能的状态为{1，2，3，4}，可能的动作集为{上、下、左、右}:

![](img/ba827ce6-0f16-411b-b54f-6b045dca48f9.png)

对于基于模型的 RL 实现，我们将使用`MDPtoolbox`库。

让我们导入`MDPtoolbox`库:

```r
library(MDPtoolbox)
```

`MDPtoolbox`库包含许多与离散时间 MDP 解析相关的函数。



# 怎么做...

在*准备就绪*部分，我们定义了我们的 RL 问题。我们知道，要解决基于模型的 RL 问题，我们需要一个转移概率矩阵和一个回报矩阵:

1.  让我们从定义转移概率矩阵开始。我们从定义每个状态下所有行为的概率开始。每行中概率的总和等于 1:

```r
# Up 
up=matrix(c( 0.9, 0.1, 0, 0,
 0.2, 0.7, 0.1, 0,
 0, 0, 0.1, 0.9,
 0, 0, 0, 1),
 nrow=4,ncol=4,byrow=TRUE)

# Down 
down=matrix(c(0.1, 0, 0, 0.9,
 0, 0.8, 0.2, 0,
 0, 0.2, 0.8, 0,
 0, 0, 0.8, 0.2),
 nrow=4,ncol=4,byrow=TRUE)

# Left 
left=matrix(c(1, 0, 0, 0,
 0.9, 0.1, 0, 0,
 0, 0.8, 0.2, 0,
 0, 0, 0, 1),
 nrow=4,ncol=4,byrow=TRUE)

# Right 
right=matrix(c(0.1, 0.9, 0, 0,
 0.1, 0.2, 0.7, 0,
 0, 0, 0.9, 0.1,
 0, 0, 0, 1),
 nrow=4,ncol=4,byrow=TRUE)
```

现在，让我们将所有操作放在一个列表中:

```r
actions = list(up=up, down=down, left=left, right=right)
actions
```

以下屏幕截图显示了每个动作的状态动作矩阵:

![](img/bbf27212-0c3c-4f7c-8a87-c42919ed3711.png)

2.  现在，让我们定义奖励和惩罚。唯一的回报是进入状态 4；其他每一步都会招致-1 的惩罚:

```r
rewards=matrix(c( -1, -1, -1, -1,
 -1, -1, -1, -1,
 -1, -1, -1, -1,
 100, 100, 100, 100),
 nrow=4,ncol=4,byrow=TRUE)

rewards
```

以下截图显示了奖励矩阵:

![](img/05320bf6-2f5a-4f39-bcea-ce501495107a.png)

3.  现在，我们可以使用`mdp_policy_iteration()`函数来解决这个问题。该函数将转移概率和奖励矩阵以及折扣因子作为输入:

```r
solved_MDP=mdp_policy_iteration(P=actions, R=rewards, discount = 0.2)
solved_MDP
```

以下截图显示了解决问题的结果:

![](img/b8d504f5-8763-4647-8b97-3ec5d345f44e.png)

让我们来看看策略迭代算法给出的策略:

```r
solved_MDP$policy 
names(actions)[solved_MDP$policy]
```

以下截图显示了针对我们问题的策略:

![](img/2917e6fe-ffcc-48bd-9b20-d486ae0a6c84.png)

上一个屏幕截图中描述的策略让我们在相应的状态 1、2、3、4 中采取最佳行动。例如，状态 1 的最佳行动是向下，状态 2 是向左，状态 3 是向上，状态 4 是向上。我们可以使用以下代码获得每一步的值:

```r
solved_MDP$V
```

下面的屏幕截图显示了我们策略每一步的价值:

![](img/27ad3a5d-9658-4940-b0b3-769bc92f5ed6.png)

在这里，我们可以看到在最后一步，我们的策略的值是 **125** 。



# 它是如何工作的...

在*步骤* *1* 中，我们为每个动作定义了动作概率矩阵。我们可以将此解释为使用一个动作从当前状态转换到下一个状态的概率。假设代理处于状态 **2** 并试图向左**走**，代理有 90%的概率会转换到状态 1。

下图显示了**左**动作的转移概率矩阵:

![](img/cf5e62fc-a04e-4cae-a05f-4af3cd09671c.png)

在*步骤* *2* 中，我们定义了一个奖励矩阵；也就是说，从当前状态转换到下一个状态时给予代理的标量奖励。

下图显示了奖励矩阵:

![](img/abfa4095-9db7-45bb-93be-36f379b00d85.png)

在最后一步中，我们通过使用策略迭代算法来求解贴现 MDP 来解决 RL 问题。`mdp_policy_iteration()`函数返回 **V** ，即**最优值函数***；*策略，即最优策略；`iter`，为迭代次数；还有`time`，就是程序占用的 CPU 时间。当两个连续的策略相同时，策略迭代算法停止。我们还可以通过向`max_iter`参数传递一个值来指定迭代的次数。



# 还有更多...

`MDPtoolbox`包还提供了**值迭代**算法的实现，这样我们就可以求解一个 MDP。下面的代码块演示了同样的情况:

```r
mdp_value_iteration(P=actions, R=rewards, discount = 0.2)
```

以下屏幕截图显示了最佳策略的详细信息:

![](img/8f2515ac-d664-45e9-afa6-a03cbb981c5a.png)

价值迭代法给出的最优策略是{2，3，1，1}。最后一步的值接近我们在策略迭代方法中得到的值。



# 无模型 RL

在之前的配方中，*使用 MDPtoolbox* 的基于模型的 RL，我们遵循基于模型的方法来解决 RL 问题。随着状态和动作空间的增长，基于模型的方法变得不切实际。另一方面，无模型强化算法依赖于代理与代表当前问题的环境的试错交互。在这个配方中，我们将使用一种无模型的方法，通过 r 中的`ReinforcementLearning`包来实现 RL。这个包利用了一种流行的无模型算法，称为 **Q-learning** 。它是一种非策略算法，因为它同时探索环境和利用当前知识。

Q-learning 保证收敛到最优策略，但要实现这一点，它依赖于代理与其环境之间的持续交互，这使得它的计算量很大。这种算法期待下一个状态，并观察该状态下所有可能行动的最大可能回报。然后，它利用这一知识以特定的学习速率α更新当前状态中相应动作的动作值信息。该算法试图学习一个称为 Q 函数的最佳评估函数，该函数将每个状态和动作对映射到一个值。它表示为 Q: S × A => V，其中 V 是在状态![](img/ce403408-d222-41d9-bfbd-76d394ae2c5b.png)中执行的动作![](img/ee214e18-5d3c-4920-9f6a-7bafdc370f9e.png)的未来奖励值。

下面是 Q 学习算法的伪代码:

1.  将表中所有状态-动作对(s，a)的值初始化为 0，![](img/7f562841-dad6-47cd-b70e-539ae3cbda79.png)。
2.  观察当前状态，![](img/95a03b7b-8ca6-4e50-914f-32322ba117cb.png)。
3.  重复直到收敛:

*   选择一个动作， **![](img/8c6f5ddc-ae69-49e5-9e0e-8b16dbf30eb2.png)** ，并应用它。
*   获得即时奖励，![](img/46736c46-ebfc-4302-9464-faec3ff6041a.png)。
*   移动到新状态，![](img/2629bb85-80b3-4fe9-b318-ac8ffb664244.png)。
*   根据以下公式更新![](img/99b84206-c5a4-42d2-9249-0cbfc5c3ed22.png)的表格条目:

![](img/17efba54-5e79-4108-b0dc-99ac8bc37075.png)

*   移到下一个状态，![](img/fb4e6004-dca1-4c66-9053-d24c9921e389.png)。现在，![](img/06aad29c-a36b-47ba-b95e-22f7455ca8f4.png)变成当前状态，![](img/d0b61e9a-c86f-498c-a882-42400c48e773.png)。



# 做好准备

在这个配方中，我们将使用`ReinforcementLearning` 包，它执行无模型 RL。

让我们导入`ReinforcementLearning`包:

```r
library(ReinforcementLearning)
```

在这个菜谱中，我们将使用 MDPtoolbox 处理上一节中使用的相同导航示例*。在这种情况下，我们不会有任何预先确定的输入数据，我们将使用无模型方法来解决问题。代理将与代表问题的环境动态交互，并生成状态-动作转换元组。环境的结构特定于手头的问题。一个**环境**通常是一个随机的有限状态机，代表任何特定问题中的操作规则。它根据奖励和惩罚向代理提供关于其行为的反馈。*

以下是某个环境的一些通用伪代码:

```r
environment <- function(state, action) {
 ...
 return(list("NextState" = newState,"Reward" = reward))
 }
```

在下一节中，我们将创建一个导航网格环境，并训练一个代理使用无模型 RL 在网格中导航。



# 怎么做...

让我们以编程方式为我们的问题创建一个环境:

1.  我们首先定义状态和一组动作:

```r
states <- c("1", "2", "3", "4")
actions <- c("up", "down", "left", "right")
cat("The states are:",states)
cat('\n')
cat("The actions are:",actions)
```

2.  接下来，我们创建一个函数，它将为我们的示例定义一个自定义环境:

```r
gridExampleEnvironment <- function(state, action) {
 next_state <- state
 if (state == state("1") && action == "down") next_state <- state("4")
 if (state == state("1") && action == "right") next_state <- state("2")
 if (state == state("2") && action == "left") next_state <- state("1")
 if (state == state("2") && action == "right") next_state <- state("3")
 if (state == state("3") && action == "left") next_state <- state("2")
 if (state == state("3") && action == "up") next_state <- state("4")
 if (next_state == state("4") && state != state("4")) {
 reward <- 100
 } else {
 reward <- -1
 }
out <- list("NextState" = next_state, "Reward" = reward)
return(out)
}

print(gridExampleEnvironment)
```

以下屏幕截图提供了对环境的描述:

![](img/3ed49322-b9e4-44ef-bbe8-a9068e48aea4.png)

3.  现在，我们使用`sampleExperience()`函数以状态转移元组的形式生成一些样本经验数据。该函数将状态、动作、迭代和环境作为输入参数:

```r
# Let us generate 1000 iterations
sequences <- sampleExperience(N = 1000, env = gridExampleEnvironment, states = states, actions = actions)
head(sequences,6)
```

下面的屏幕截图显示了示例数据的前几条记录:

![](img/fb0132c9-110a-4773-866e-388a7784d344.png)

4.  使用我们在上一步中生成的样本经验数据，我们可以使用`ReinforcementLearning()`函数来解决我们的问题:

```r
solver_rl <- ReinforcementLearning(sequences, s = "State", a = "Action", r = "Reward", s_new = "NextState")

print(solver_rl)
```

下面的屏幕截图显示了州行动表，以及针对我们的问题的策略和总体奖励。 **X1** 、 **X2** 、 **X3** 、 **X4** 分别代表状态 1、2、3、4:

![](img/ef2db0e7-811c-44d0-8547-714ed8999ade.png)

在这里，我们可以看到我们在最后一次迭代中的总回报是`11423`。



# 它是如何工作的...

在*步骤 1* 中，我们为这个问题定义了一组可能的状态和动作。为了使用无模型 RL，我们需要创建一个模拟环境行为的函数。在*步骤 2* 中，我们通过创建一个名为`gridExampleEnvironment()`的函数将问题公式化，该函数将一个状态-动作对作为输入，并生成下一个状态和相关奖励的列表。在*步骤 3* 中，我们使用`sampleExperience()`函数通过查询我们在前面步骤中创建的环境来生成动态状态-动作转换元组。这个函数的输入参数是样本数、环境函数以及状态和动作集。该函数返回一个数据帧，其中包含来自环境的经验观察序列。

一旦生成了观察序列数据，代理就会基于该数据学习最优策略。为了实现这一点，在*步骤 4* 中，我们使用了`ReinforcementLearning()`功能。我们可以向这个函数传递更多的参数来定制代理的学习行为。

论据如下:

*   `alpha`:这是学习率α，在 0 和 1 之间变化。该参数值越高，学习速度越快。
*   `gamma`:这是折扣系数γ，可以设置为 0 到 1 之间的任意值。它决定了未来奖励的重要性。较低的 gamma 值会使代理人短视，只考虑眼前的回报，而较高的 gamma 值会使代理人努力争取更大的长期回报。
*   `epsilon`:该参数 epsilon，ε，决定了ε-贪婪动作选择中的探索机制，可以设置在 0 到 1 之间。
*   `iter`:该参数表示代理通过训练数据集的重复学习迭代次数。默认情况下，它设置为 1。

我们看到学习过程的结果包含状态-动作表；即每个状态-动作对的 Q 值和在每个状态下具有最佳可能动作的最优策略。此外，我们还得到了策略的整体奖励。



# 请参见

要了解更多关于其他 RL 算法的信息，比如已经在街机学习环境中实现的 SARSA 和 GQ，请访问[https://arxiv.org/pdf/1410.8620.pdf](https://arxiv.org/pdf/1410.8620.pdf)。



# 使用 RL 进行悬崖行走

到目前为止，您应该已经了解了 RL 的框架。在这个菜谱中，我们将在 RL 中实现`gridworld`环境的真实应用。这个问题可以表示为一个 4x12 大小的网格。剧集从左下方的状态开始，目标状态位于网格的右下方。向左、向右、向上和向下是任何状态下唯一可能的动作。网格下部标记为 *C* 的状态是悬崖。任何进入这些状态的转变都会招致-100 的高负奖励，并使代理立即回到开始状态。对于目标状态， *G* ，奖励为 0，而对于除目标状态和悬崖之外的所有转换，奖励为-1。

下图显示了悬崖行走问题的导航矩阵:

![](img/6850415d-e859-4134-869b-23f35bd364dc.png)

让我们继续使用 RL 来解决这个导航问题。



# 做好准备

在这个菜谱中，我们将使用`reinforcelearn`包从名为 **cliff walking** 的内置环境中获取数据。这个环境是从`gridworld`环境继承来的。

我们将使用`ReinforcementLearning`包执行无模型 RL:

```r
library(reinforcelearn)
library(ReinforcementLearning)
```

在下一节中，我们将创建一个代表悬崖行走问题的环境。



# 怎么做...

让我们创造一个环境来代表我们在悬崖上行走的问题:

1.  让我们从使用`makeEnvironment()`函数加载悬崖行走环境开始:

```r
env = makeEnvironment("cliff.walking")
env
```

以下截图是对悬崖行走环境的描述:

![](img/75b439ef-ece0-4266-af8e-75962638b76a.png)

2.  接下来，我们创建一个函数，该函数将使用随机动作查询环境，并获取观察序列数据:

```r
# Creating the function to query the environment
sequences <- function(iter,env){
 actions <- env$actions
 data <- data.frame(matrix(ncol = 4, nrow = 0))
 colnames(data) <- c("State", "Action", "Reward","NextState")
 env$reset()
 for(i in 1:iter){
 current_state <- env$state
 current_action <- floor(runif(1,0,4))
 current_reward <- env$step(current_action)$reward
 next_state_iter <- env$step(current_action)$state
 iter_data <- cbind("State" = current_state,"Action" = current_action,"Reward"=current_reward,"NextState" = next_state_iter)
 data <- rbind(data,iter_data)
 if(env$done == "TRUE"){
 break;
 }
 }
 return(data) 
}
```

现在，让我们从前面代码块中定义的函数中获取数据:

```r
iter <- 1000
observations = sequences(iter,env)
cols.name <- c("State","Action","NextState")
observations[cols.name] <- sapply(observations[cols.name],as.character)
sapply(observations, class)

# Displaying first 20 records
head(observations,20)
```

下面的屏幕截图显示了生成的数据中的一些记录:

![](img/f2951fc5-4fe2-4816-acd9-da0f5380bdc9.png)

3.  使用我们在上一步中生成的样本观察数据，我们可以使用`ReinforcementLearning()`函数来解决我们的问题。我们通过指定一个`control`对象来定制代理的学习行为，在这里我们为学习速率`alpha`、折扣因子`gamma`和探索贪婪度`epsilon`设置参数选择:

```r
control <- list(alpha = 0.2, gamma = 0.4, epsilon = 0.1)

# Perform RL
model <- ReinforcementLearning(data = observations, s = "State", a = "Action", r = "Reward",
 s_new = "NextState", iter = 1, control = control)
```

现在，我们打印已学习的状态-动作表，它包含每个状态-动作对的 Q 值:

```r
print(model)
```

下表显示了每个状态-动作对的 Q 值表:

![](img/5c8ffc50-a1ea-41f7-8d7c-aa3101f1ed8a.png)

Q 值表中给出了最佳策略。



# 它是如何工作的...

在*步骤* *1* 中，我们使用`reinforcelearn`库中的`makeEnvironment()`函数创建了**悬崖漫步**环境。这种环境属于`gridworld`阶层。在*步骤 2* 中，我们创建了一个定制的函数来查询悬崖行走环境并获取样本观测数据。`env()`函数的`step()`方法将一个动作作为输入参数，并返回一个带有状态、奖励和完成的列表作为输出。一旦生成了观察序列数据，我们就使用`ReinforcementLearning()`函数让代理在最后一步学习基于该数据的最优策略。



# 还有更多...

在许多 RL 问题中，探索制定最优策略的行为可能是昂贵的。**体验回放**是一种用于让代理重用过去体验的技术。这种技术通过重放已经观察到的状态转换作为环境中的新观察来实现快速收敛。体验重放需要由状态、动作和奖励组成的样本序列作为输入数据。这些转换使代理学习状态-动作函数和输入数据中所有状态的最优策略。这个策略也可以应用于验证目的或者迭代地改进当前策略。要在 R 中实现体验重放，您需要将现有的 RL 模型作为参数传递给`ReinforcementLearning()`函数。

让我们从悬崖行走环境中获取 100 个新的数据样本:

```r
new_observations = sequences(100,env) 
cols.name <- c("State","Action","NextState")
new_observations[cols.name] <- sapply(new_observations[cols.name],as.character)
sapply(new_observations, class)
head(new_observations)
```

以下截图显示了新观测数据中的一些记录:

![](img/9426e0f0-8b52-4efa-bb9c-2b6b55106749.png)

现在，我们提供我们现有的 RL 模型，这是我们在*如何做中创建的...*此配方的一部分，作为更新现有策略的一个参数。

下面的屏幕截图显示了在实现体验重放之后，每个状态-动作对的 Q 值表:

![](img/8c74e137-e9e8-4b5b-ad0c-2b12b0981fa1.png)

在前面的屏幕截图中，我们可以看到，与之前的策略相比，更新后的策略获得了更高的总体回报。