<html><head/><body>


    
        <title>Recurrent Neural Networks in Action</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">运行中的递归神经网络</h1>
                
            
            
                
<p>序列数据是顺序很重要的数据，例如音频、视频和语音。由于数据的性质，学习序列数据是模式识别领域中最具挑战性的问题之一。在处理顺序数据时，序列各部分之间的相关性及其变化的长度进一步增加了复杂性。随着序列模型和算法的出现，例如<strong>递归神经网络</strong> ( <strong> RNN </strong>)、<strong>长短期记忆模型</strong> ( <strong> LSTM </strong>)、以及<strong>门控递归单元</strong> ( <strong> GRU </strong>)，序列数据建模正被用于多种应用，例如序列分类、序列生成、语音到文本的转换等等。</p>
<p>在序列分类中，目标是预测序列的类别，而在序列生成中，我们基于输入序列生成新的输出序列。在这一章中，你将学习如何使用不同风格的RNN实现序列分类和生成，以及时间序列预测。</p>
<p>在本章中，我们将介绍以下配方:</p>
<ul>
<li>基于神经网络的情感分类</li>
<li>使用LSTMs生成文本</li>
<li>基于GRUs的时间序列预测</li>
<li>实现双向递归神经网络</li>
</ul>
<p class="mce-root"/>


            

            
        
    






    
        <title>Sentiment classification using RNNs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基于神经网络的情感分类</h1>
                
            
            
                
<p class="mce-root">RNN是一个独特的网络，因为它有记忆输入的能力。这种能力使它非常适合处理序列数据的问题，例如时间序列预测、语音识别、机器翻译以及音频和视频序列预测。在RNNs中，数据以这样一种方式遍历，即在每个节点，网络从当前和以前的输入中学习，随着时间的推移共享权重。这就像在每一步执行相同的任务，只是不同的输入减少了我们需要学习的参数总数。</p>
<p class="mce-root">例如，如果激活函数是<em> tanh </em>，那么递归神经元的权重是<img class="fm-editor-equation" src="img/5ae2a862-cc87-4f13-b851-bc4cce2b912b.png" style="width:2.17em;height:1.17em;"/>，输入神经元的权重是<img class="fm-editor-equation" src="img/f9f04865-b258-4222-a6c7-f6554a3d71b8.png" style="width:1.92em;height:1.00em;"/>。这里，我们可以将在时间<em> t </em>的状态<img class="fm-editor-equation" src="img/86aefc36-beed-4fb3-ba23-04e266017376.png" style="width:0.75em;height:1.25em;"/>的方程写成如下:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/3ca25f02-619b-4ee6-904c-4d6127878808.png" style="width:14.83em;height:1.42em;"/></p>
<p class="mce-root">每个输出的梯度取决于当前和先前时间步长的计算。例如，为了计算在<em> t=6 </em>处的梯度，我们需要反向传播五个步骤并添加梯度。这就是所谓的<strong>穿越时间</strong> ( <strong> BPTT </strong>)。在BPTT过程中，当迭代训练样本时，我们修改权重以减少错误。</p>
<p class="mce-root">rnn可以通过它支持的不同体系结构处理各种输入和输出类型的数据。主要的如下:</p>
<ul>
<li><strong>一对多:</strong>一个输入映射到一个序列，多个步骤作为输出。这方面的一个例子是音乐生成:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1099 image-border" src="img/9421d892-109d-43f8-b5e3-e192375b63f3.png" style="width:11.33em;height:17.25em;"/></p>
<ul>
<li><strong>多对一</strong>:输入序列映射到一个类别或数量预测。情感分类就是一个例子:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1100 image-border" src="img/17f054e9-d264-4e0b-a6e3-d11f1fbd177d.png" style="width:9.58em;height:14.50em;"/></p>
<ul>
<li><strong>多对多</strong>:输入序列映射到输出序列。这方面的一个例子是语言翻译:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1101 image-border" src="img/f9c4d7f1-7c68-41b4-a453-62bf56c84e4c.png" style="width:10.17em;height:15.92em;"/></p>
<p>在这个菜谱中，我们将建立一个RNN模型，对电影评论的观点进行分类。</p>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>在这一部分，我们将使用IMDb数据集，它包含电影评论和与之相关的情感。我们可以从<kbd>keras</kbd>库中导入数据集。这些评论被预处理并编码为一系列单词索引。这些单词通过它们在数据集中的总频率来索引；例如，单词index <em> 8 </em>，指的是数据中第8个<sup>第</sup>个最频繁出现的单词。</p>
<p>现在，让我们导入<kbd>keras</kbd>库和<kbd>imdb</kbd>数据集:</p>
<pre>library(keras)<br/>imdb &lt;- dataset_imdb(num_words = 1000)</pre>
<p>让我们将数据分为训练集和测试集:</p>
<pre>train_x &lt;- imdb$train$x<br/>train_y &lt;- imdb$train$y<br/>test_x &lt;- imdb$test$x<br/>test_y &lt;- imdb$test$y</pre>
<p>现在，我们可以看看训练和测试数据中的评论数量:</p>
<pre># number of samples in train and test set<br/>cat(length(train_x), 'train sequences\n')<br/>cat(length(test_x), 'test sequences')</pre>
<p>在这里，我们可以看到在训练集和测试集中各有一个<kbd>25000</kbd>评论:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/15e5632d-6892-42d1-bfbd-cfcad9e8a487.png" style="width:10.08em;height:2.17em;"/></p>
<p>让我们也看看训练数据的结构:</p>
<pre>str(train_x)</pre>
<p>以下屏幕截图显示了训练数据中预测变量的描述:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1211 image-border" src="img/88afcbc3-e17c-48b2-8a5b-ae0ea2411459.png" style="width:22.75em;height:22.17em;"/></p>
<p>同样，我们将看看培训标签的结构:</p>
<pre>str(train_y)</pre>
<p>下面的屏幕截图显示了训练数据中目标变量的描述:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1212 image-border" src="img/542ed6a9-ec15-482b-992e-68fabc4d8790.png" style="width:19.50em;height:1.92em;"/></p>
<p>在这里，我们可以看到我们的训练集是一个评论和情感标签的列表。我们来看看第一篇评论，以及里面的字数:</p>
<pre>train_x[[1]]<br/>cat("Number of words in the first review is",length(train_x[[1]]))</pre>
<p>下面的屏幕截图显示了编码形式的第一篇评论:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b802a9d1-7650-466d-b831-5fa95a36a286.png" style="width:48.67em;height:9.33em;"/></p>
<p>请注意，当我们导入数据集时，我们将参数<kbd>num_words</kbd>的值设置为<kbd>1000</kbd>。这意味着只有前1000个常用词被保存在编码评论中。为了验证这一点，让我们看看评论列表中的最大编码值:</p>
<pre>cat("Maximum encoded value in train ",max(sapply(train_x, max)),"\n")<br/>cat("Maximum encoded value in test ",max(sapply(test_x, max)))</pre>
<p>执行前面的代码为我们提供了训练和测试数据中的最大编码值。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<p>现在我们已经熟悉了这些数据，让我们更详细地看一下:</p>
<ol>
<li>让我们为<kbd>imdb</kbd>数据导入word index:</li>
</ol>
<pre style="padding-left: 60px">word_index = dataset_imdb_word_index()</pre>
<p style="padding-left: 60px">我们可以使用下面的代码来查看单词index的头部:</p>
<pre style="padding-left: 60px">head(word_index)<br/></pre>
<p style="padding-left: 60px">在这里，我们可以看到有一个键-值对列表，其中键是单词，值是它映射到的整数:</p>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1213 image-border" src="img/35303a33-a1e4-4095-aa26-62da0a1aef21.png" style="width:4.58em;height:16.08em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">让我们也来看看单词索引中独特单词的数量:</p>
<pre style="padding-left: 60px">length((word_index))</pre>
<p style="padding-left: 60px">在这里，我们可以看到单词索引中有88，584个唯一单词:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1102 image-border" src="img/e8b42ddf-78d1-42d7-a13a-a4f8043f376b.png" style="width:4.42em;height:2.08em;"/></p>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">现在，我们创建一个单词index的键值对的反向列表。我们将使用这个列表来解码IMDb数据集中的评论:</li>
</ol>
<pre style="padding-left: 60px">reverse_word_index &lt;- names(word_index)<br/>names(reverse_word_index) &lt;- word_index<br/>head(reverse_word_index)</pre>
<p style="padding-left: 60px">在这里，我们可以看到反转单词索引列表是一个键-值对列表，其中键是整数索引，值是关联单词:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1214 image-border" src="img/eca520f5-eb64-462f-91d7-5c4527503971.png" style="width:8.00em;height:8.17em;"/></p>
<ol start="3">
<li class="CDPAlignLeft CDPAlign">现在，我们解码第一篇评论。请注意，字编码偏移3，因为0、1、2分别保留用于填充、序列开始和词汇外字:</li>
</ol>
<pre style="padding-left: 60px">decoded_review &lt;- sapply(train_x[[1]], function(index) {<br/><br/>  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index -3)]]<br/>  if (!is.null(word)) word else "?"<br/><br/>})

cat(decoded_review)</pre>
<p style="padding-left: 60px">以下屏幕截图显示了第一篇评论的解码版本:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1103 image-border" src="img/60faea1b-75aa-47b0-9d8a-f34a77e3a9e8.png" style="width:74.75em;height:13.00em;"/></p>
<ol start="4">
<li class="CDPAlignLeft CDPAlign">让我们填充所有的序列，使它们长度一致:</li>
</ol>
<pre style="padding-left: 60px">train_x &lt;- pad_sequences(train_x, maxlen = 80)<br/>test_x &lt;- pad_sequences(test_x, maxlen = 80)<br/>cat('x_train shape:', dim(train_x), '\n')<br/>cat('x_test shape:', dim(test_x), '\n')</pre>
<p style="padding-left: 60px">所有序列都被填充到长度<kbd>80</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1104 image-border" src="img/8cfc4eb2-56eb-494e-8f0f-95c008e3656a.png" style="width:12.67em;height:2.92em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">现在，我们来看看填充后的第一篇评论:</p>
<pre style="padding-left: 60px">train_x[1,]</pre>
<p style="padding-left: 60px">在这里，您可以看到填充后的评论只有80个索引:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1105 image-border" src="img/98fe757e-3a08-4a75-8d0c-052dcd146234.png" style="width:72.75em;height:5.33em;"/></p>
<ol start="5">
<li class="CDPAlignLeft CDPAlign">现在，我们构建情感分类模型并查看其摘要:</li>
</ol>
<pre style="padding-left: 60px">model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>  layer_embedding(input_dim = 1000, output_dim = 128) %&gt;%<br/>  layer_simple_rnn(units = 32) %&gt;%<br/>  layer_dense(units = 1, activation = 'sigmoid')

summary(model)<br/></pre>
<p style="padding-left: 60px">以下是对该模型的描述:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1106 image-border" src="img/bc38496f-c386-4d26-96d4-569d024517a2.png" style="width:39.83em;height:12.92em;"/></p>
<ol start="6">
<li class="CDPAlignLeft CDPAlign">现在，我们编译模型并训练它:</li>
</ol>
<pre style="padding-left: 60px"># compile model
model %&gt;% compile(
  loss = 'binary_crossentropy',<br/>  optimizer = 'adam',<br/>  metrics = c('accuracy')
)

# train model
model %&gt;% fit(<br/>  train_x,train_y,<br/>  batch_size = 32,<br/>  epochs = 10,<br/>  validation_split = .2
)<br/></pre>
<ol start="7">
<li>最后，我们根据测试数据评估模型的性能，并打印指标:</li>
</ol>
<pre style="padding-left: 60px">scores &lt;- model %&gt;% evaluate(<br/>  test_x, test_y,<br/>  batch_size = 32
)

cat('Test score:', scores[[1]],'\n')<br/>
cat('Test accuracy', scores[[2]])</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了测试数据的性能指标:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1107 image-border" src="img/2101b0f5-cd4a-4cce-afbd-5bb0d9f869e9.png" style="width:12.33em;height:2.83em;"/></p>
<p>通过这样做，我们在测试数据上实现了大约71%的准确率。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在这个例子中，我们使用了来自<kbd>keras</kbd>库的IMDb评论的内置数据集。我们加载了数据的训练和测试分区，并查看了这些数据分区的结构。我们看到数据被映射到一个特定的整数值序列，每个整数代表字典中的一个特定单词。这本字典有丰富的单词集合，根据每个单词在语料库中的使用频率进行排列。由此，我们可以看到字典是一个键-值对的列表，键代表单词，值代表单词在字典中的索引。为了丢弃不经常使用的单词，我们提供了1000的阈值；也就是说，我们只保留了训练数据集中前1000个最常用的单词，而忽略了其余的单词。然后，我们转移到数据处理部分。</p>
<p class="mce-root">在<em>步骤1 </em>中，我们为IMDb数据集导入了单词index。在这个单词索引中，数据中的单词按照数据集中的总频率进行编码和索引。在<em>步骤2 </em>中，我们创建了一个单词index的逆序键值对列表，用于从一系列编码整数中将句子解码回原始版本。在<em>步骤3 </em>中，我们展示了如何重新生成一个样本审查。<br/>在<em>步骤4 </em>中，我们准备了数据，以便将其输入到模型中。因为我们不能直接将整数列表传递到模型中，所以我们将它们转换成统一形状的张量。为了使所有评论的长度一致，我们可以采用以下两种方法之一:</p>
<ul>
<li class="mce-root"><strong>一键编码</strong>:这将把序列转换成相同长度的张量。矩阵的大小将是<em>字数*评论数</em>。这种方法计算量很大。</li>
<li class="mce-root"><strong>填充评论</strong>:或者，我们可以填充所有的序列，使它们都具有相同的长度。这将创建一个形状为<em>num _ examples * max _ length</em>的整数张量。max_length 参数用于限制我们希望在所有评论中保留的最大字数。</li>
</ul>
<p class="mce-root">因为第二种方法占用的内存和计算量较少，所以我们选择了第二种方法；也就是说，我们将序列填充到最大长度80。</p>
<p class="mce-root">在<em>步骤5 </em>中，我们定义了一个顺序的Keras模型并配置了它的层。第一层是嵌入层，用于从我们的数据中生成单词序列的上下文，并提供有关相关特征的信息。在嵌入中，单词由密集向量表示来表示。每个向量代表单词到连续向量空间的投影，然后从文本中学习，并基于特定单词周围的单词。单词在向量空间中的位置被称为它的<strong>嵌入</strong>。当我们做嵌入时，我们根据一些潜在的因素来表示每个评论。比如<em>辉煌</em>这个词可以用一个向量来表示；比方说，[.32，. 02，. 48，. 21，. 56，. 15]。当我们使用大规模数据集时，这在计算上是高效的，因为它降低了维数。嵌入的向量也在深度神经网络的训练过程中得到更新，这有助于在多维空间中识别相似的单词。单词嵌入也反映了单词在语义上是如何相互关联的。例如，像<strong>说话</strong>和<strong>说话</strong>这样的词可以被认为是相关的，就像<strong>游泳</strong>与<strong>游泳</strong>相关一样。</p>
<p class="mce-root">下图显示了单词嵌入的图形表示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1108 image-border" src="img/593240cf-718c-49cc-ab2f-c1af6eb19614.png" style="width:20.92em;height:17.08em;"/></p>
<p class="mce-root">嵌入层通过指定三个参数来定义:</p>
<ul>
<li class="mce-root"><kbd>input_dim</kbd>:这是文本数据中词汇的大小。在我们的例子中，文本数据是一个被编码为0-999之间的值的整数。由于这个原因，词汇表的大小是1000个单词。</li>
<li class="mce-root"><kbd>output_dim</kbd>:这是单词将要嵌入的向量空间的大小。我们将其指定为128。</li>
<li class="mce-root"><kbd>input_length</kbd>:这是输入序列的长度，我们为Keras模型的任何输入层定义。<br/>在下一层，我们定义了一个有32个隐藏单元的简单RNN模型。如果<em> n </em>是输入维度的数量，并且<em> d </em>是RNN层中隐藏单元的数量，那么可训练参数的数量可以由下面的等式给出:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/bf4d0b2d-7cb3-433a-a389-520477318aaf.png" style="width:7.58em;height:1.33em;"/></p>
<p class="mce-root">最后一层与单个输出节点紧密相连。这里，我们使用sigmoid激活函数，因为这是一个二元分类任务。在<em>步骤6 </em>中，我们编译了模型。我们将<kbd>binary_crossentropy</kbd>指定为损失函数，因为我们正在处理二进制分类，而将<strong> adam </strong>指定为优化器。然后，我们用20%的验证分割来训练我们的模型。最后，在最后一步中，我们评估了模型的测试准确性，以了解我们的模型在测试数据上的表现。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<p>到现在为止，你已经知道BPTT是如何在RNN工作的了。我们反向遍历网络，计算每次迭代中相对于权重的误差梯度。在反向传播过程中，随着我们靠近网络的早期层，这些梯度变得太小，从而使这些层中的神经元学习非常慢。对于一个精确的模型，对早期层进行精确的训练是至关重要的，因为这些层负责从输入中学习简单的模式，并相应地将相关信息传递给后续层。当我们训练在层内具有更多依赖性的巨大网络时，rnn经常面临这个挑战。这个挑战被称为<strong>消失梯度问题</strong>，它使得网络学习速度太慢。这也意味着结果并不尽如人意。通常建议使用RELU激活函数来避免大型网络中的消失梯度问题。处理这个问题的另一个非常常见的方法是使用一个<strong>长短期记忆</strong> ( <strong> LSTM </strong>)模型。我们将在下一个食谱中讨论这一点。</p>
<p class="mce-root">rnn遇到的另一个挑战是<strong>爆炸梯度问题</strong>。在这种情况下，我们可以看到较大的梯度值，这反过来使模型学习速度过快和不准确。在某些情况下，由于计算中的数值溢出，梯度也可能变成<kbd>NaN</kbd>。当这种情况发生时，网络中的权重会在训练的较短时间内大幅增加。防止这个问题最常用的补救措施是<strong>梯度削波</strong>，它可以防止梯度增加超过指定的阈值。</p>


            

            
        
    






    
        <title>See also</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">请参见</h1>
                
            
            
                
<p>要了解更多关于递归神经网络正则化的信息，请访问<a href="https://arxiv.org/pdf/1409.2329.pdf">https://arxiv.org/pdf/1409.2329.pdf</a>。</p>
<p class="mce-root"/>


            

            
        
    






    
        <title>Text generation using LSTMs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用LSTMs生成文本</h1>
                
            
            
                
<p>递归神经网络在正确携带信息方面面临困难，特别是当大型网络中的层之间存在长顺序依赖性时。<strong>长短期记忆</strong>网络，通常被称为<strong> LSTM </strong>网络，是能够学习长期依赖关系的RNNs的扩展，被广泛用于深度学习，以避免RNNs面临的消失梯度问题。LSTMs通过门控机制对抗消失梯度，并且能够向单元状态移除或添加信息。这种细胞状态由闸门仔细调节，闸门控制通过它们的信息。LSTMs有三种门:输入、输出和遗忘。遗忘门控制着我们想要将多少来自前一状态的信息传递给下一个单元。输入状态定义了对于当前输入，<em>x<sub>t</sub>T7】我们想要将多少新计算状态的信息传递给后续状态，而输出门定义了我们想要将多少内部状态信息传递给下一个状态。</em></p>
<p>下图显示了LSTM网络架构的图示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1215 image-border" src="img/44f7ca72-ced0-4776-ae1f-2c52f780d932.png" style="width:62.75em;height:23.75em;"/></p>
<p>在这个菜谱中，我们将实现一个用于序列预测的LSTM模型(在这个例子中是多对一)。该模型将根据之前的单词序列预测某个单词的出现。这就是所谓的文本生成。</p>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>在这个例子中，我们将使用<em> Jack and Jill </em>童谣作为我们的源文本，以便我们可以建立一个语言模型。我们将创建一个包含押韵的文本文件，并将其保存在目录中。我们的语言模型将两个单词作为输入来预测下一个单词。</p>
<p>我们将首先导入所需的库并读取我们的文本文件:</p>
<pre>library(keras)<br/>library(readr)<br/>library(stringr)<br/><br/>data &lt;- read_file("data/rhyme.txt") %&gt;% str_to_lower()</pre>
<p>在NLP中，我们将数据称为语料库。语料库是大量文本的集合。让我们看看我们的语料库:</p>
<pre>data</pre>
<p>下面的截图显示了我们语料库中的文本:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1b363237-9a2c-47c0-a800-a9d99d37f329.png" style="width:72.75em;height:3.75em;"/></p>
<p>我们将使用前面截图中的文本来生成序列。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<div><div><div><p>到目前为止，我们已经将一个语料库导入到R环境中。为了建立一个语言模型，我们需要把它转换成一个整数序列。让我们开始做一些数据预处理:</p>
<ol>
<li>首先，我们定义我们的记号赋予器。我们稍后将使用它将文本转换成整数序列:</li>
</ol>
</div>
</div>
</div>
<div><div><div><pre style="padding-left: 60px">tokenizer = text_tokenizer(num_words = 35,char_level = F)<br/>tokenizer %&gt;% fit_text_tokenizer(data)</pre></div>
</div>
</div>
<p style="padding-left: 60px">让我们来看看语料库中独特单词的数量:</p>
<pre style="padding-left: 60px">cat("Number of unique words", length(tokenizer$word_index))</pre>
<p style="padding-left: 60px">我们的语料库中有37个独特的单词。要查看词汇表的前几条记录，我们可以使用以下命令:</p>
<pre style="padding-left: 60px">head(tokenizer$word_index)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">让我们使用之前定义的记号赋予器将语料库转换成整数序列:</p>
<pre style="padding-left: 60px">text_seqs &lt;- texts_to_sequences(tokenizer, data)<br/>str(text_seqs)</pre>
<p style="padding-left: 60px">下图显示了返回序列的结构:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1216 image-border" src="img/eff06b50-cac9-4591-82b7-b8ed22088e90.png" style="width:21.58em;height:2.50em;"/></p>
<p style="padding-left: 60px">在这里，我们可以看到<kbd>texts_to_sequences()</kbd>返回一个列表。让我们把它转换成一个向量，并打印出它的长度:</p>
<pre style="padding-left: 60px">text_seqs &lt;- text_seqs[[1]]<br/>length(text_seqs)</pre>
<p style="padding-left: 60px">我们语料库的长度是48:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/79c55644-a466-42b6-8028-1dc974f6ee5d.png" style="width:1.92em;height:1.83em;"/></p>
<ol start="2">
<li>现在，让我们将文本序列转换为输入(特征)和输出(标签)序列，其中输入将是两个连续单词的序列，输出将是序列中出现的下一个单词:</li>
</ol>
<pre style="padding-left: 60px">input_sequence_length &lt;- 2<br/>feature &lt;- matrix(ncol = input_sequence_length)<br/>label &lt;- matrix(ncol = 1)

for(i in seq(input_sequence_length, length(text_seqs))){
    if(i &gt;= length(text_seqs)){
        break()
    }
    start_idx &lt;- (i - input_sequence_length) +1
    end_idx &lt;- i +1
    new_seq &lt;-  text_seqs[start_idx:end_idx]
    feature &lt;- rbind(feature,new_seq[1:input_sequence_length])
    label &lt;- rbind(label,new_seq[input_sequence_length+1])
}
feature &lt;- feature[-1,]
label &lt;- label[-1,]<br/><br/>paste("Feature")<br/>head(feature)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了我们制定的功能序列:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1110 image-border" src="img/732c6646-c846-4d60-86a8-1a47bd3cc855.png" style="width:5.17em;height:14.67em;"/></p>
<p style="padding-left: 60px">让我们看看我们创建的标签序列:</p>
<pre style="padding-left: 60px">paste("label")
head(label)</pre>
<p style="padding-left: 60px">以下屏幕截图显示了前几个标签序列:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1111 image-border" src="img/d5cfb13f-8d1d-4f75-972d-e43d8573e6a2.png" style="width:3.75em;height:10.92em;"/></p>
<p style="padding-left: 60px">让我们对我们的标签进行一次性编码，并查看我们的特性和标签的尺寸:</p>
<pre style="padding-left: 60px">label &lt;- to_categorical(label,num_classes = tokenizer$num_words )</pre>
<p style="padding-left: 60px">在这里，我们可以看到我们的特征和标签数据的尺寸:</p>
<pre style="padding-left: 60px">cat("Shape of features",dim(feature),"\n")
cat("Shape of label",length(label))</pre>
<p style="padding-left: 60px">以下截图显示了我们的特征和标签序列的尺寸:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1112 image-border" src="img/a992ad4a-511c-478e-8cc9-10104f568579.png" style="width:11.50em;height:2.00em;"/></p>
<ol start="3">
<li>现在，我们创建一个文本生成模型并打印其摘要:</li>
</ol>
<pre style="padding-left: 60px">model &lt;- keras_model_sequential()
model %&gt;%
    layer_embedding(input_dim = tokenizer$num_words,output_dim = 10,input_length = input_sequence_length) %&gt;%
    layer_lstm(units = 50) %&gt;%
    layer_dense(tokenizer$num_words) %&gt;%
    layer_activation("softmax")

summary(model)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了模型的概要:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1113 image-border" src="img/517f80a5-a04b-4b84-8916-d19a2c5b4001.png" style="width:40.58em;height:15.08em;"/></p>
<p style="padding-left: 60px">接下来，我们编译模型并对其进行训练:</p>
<pre style="padding-left: 60px"># compile
model %&gt;% compile(
    loss = "categorical_crossentropy", 
    optimizer = optimizer_rmsprop(lr = 0.001),
    metrics = c('accuracy')
)

# train
model %&gt;% fit(
  feature, label,
#   batch_size = 128,
  epochs = 500
)</pre>
<ol start="4">
<li>在下面的代码块中，我们实现了一个从语言模型生成序列的函数:</li>
</ol>
<pre style="padding-left: 60px">generate_sequence &lt;-function(model, tokenizer, input_length, seed_text, predict_next_n_words){
    input_text &lt;- seed_text
    for(i in seq(predict_next_n_words)){
        encoded &lt;- texts_to_sequences(tokenizer,input_text)[[1]]
        encoded &lt;- pad_sequences(sequences = list(encoded),maxlen = input_length,padding = 'pre')
        yhat &lt;- predict_classes(model,encoded, verbose=0)
        next_word &lt;- tokenizer$index_word[[as.character(yhat)]]
        input_text &lt;- paste(input_text, next_word)
    }
    return(input_text)
}</pre>
<p style="padding-left: 60px">现在，我们可以使用自定义函数<kbd>generate_sequence()</kbd>，从整数序列生成文本:</p>
<pre style="padding-left: 60px">seed_1 = "Jack and"
cat("Text generated from seed 1: " ,generate_sequence(model,tokenizer,input_sequence_length,seed_1,11),"\n ")
seed_2 = "Jack fell"
cat("Text generated from seed 2: ",generate_sequence(model,tokenizer,input_sequence_length,seed_2,11))</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了模型根据输入文本生成的文本:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d0c8bee1-453b-4f79-b89a-5f3b55af080d.png" style="width:49.17em;height:2.75em;"/></p>
<p>由此可见，我们的模型在预测序列方面做得很好。</p>
<p class="mce-root"/>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>要构建任何语言模型，我们都需要清理输入文本并将其分解成标记。记号是单个的单词，将文本分解成不同的单词称为记号化。默认情况下，<kbd>keras</kbd>记号赋予器将语料库拆分成记号列表(“”用于将句子拆分成单词)，删除所有标点符号，将单词转换成小写，并基于输入文本构建内部词汇表。标记器生成的词汇表是一个索引列表，其中的单词根据它们在数据集中的总频率进行索引。在这个食谱中，我们看到，在童谣中，“和”是出现频率最高的词，而“上”是出现频率第五高的词。总共有37个独特的词。</p>
<p>在<em>步骤1 </em>中，我们将语料库转换成一个整数序列。请注意，<kbd>text_tokenizer()</kbd>的<kbd>num_words</kbd>参数根据词频定义了要保留的最大字数。这意味着只有前“n”个频繁单词被保留在编码序列中。在<em>步骤2 </em>中，我们为我们的语料库准备了特征和标签。</p>
<p>在<em>步骤3 </em>中，我们定义了我们的LSTM神经网络。首先，我们初始化一个序列模型，然后向它添加一个嵌入层。嵌入层将输入特征空间转换成具有“d”维的潜在特征；在我们的例子中，它将其转化为128个潜在特征。接下来，我们添加了50个单位的LSTM层。单词预测是一个分类问题，我们从词汇表中预测下一个单词。由于这一点，我们添加了一个密集层，其单位等于具有softmax激活功能的词汇表中的单词数。</p>
<p>在<em>步骤4 </em>中，我们定义了一个从给定的两个单词的初始集合生成文本的函数。我们的模型从原来的前两个单词预测下一个单词。在我们的示例中，初始种子是“Jack and ”,预测单词是“jill ”,因此创建了一个三个单词的序列。在下一次迭代中，我们使用句子的最后两个单词“and jill”，并预测下一个单词“got”。该函数继续生成文本，直到我们生成的单词等于参数<kbd>predict_next_n_words</kbd>的值。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<div><div><div><p>在处理NLP应用程序时，我们从文本数据中构造有意义的特征。我们可以使用许多技术来构建这些特征，例如计数矢量化、二进制矢量化、<strong>术语频率-逆文档频率</strong> ( <strong> tf-idf </strong>)、单词嵌入等等。下面的代码块演示了如何使用R中的<kbd>keras</kbd>库为各种NLP应用构建一个tf-idf特征矩阵:</p>
</div>
</div>
</div>
<pre>texts_to_matrix(tokenizer, input, mode = c("tfidf"))</pre>
<p>其他可用的模式包括<em>二进制</em>、<em>计数</em>和<em>频率</em>。</p>


            

            
        
    






    
        <title>See also</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">请参见</h1>
                
            
            
                
<ul>
<li>欲了解更多有关编码器-解码器网络堆叠rnn/lstm的信息，请访问<a href="https://cs224d.stanford.edu/reports/Lambert.pdf">https://cs224d.stanford.edu/reports/Lambert.pdf</a>。</li>
<li>要了解更多关于word2vec类神经网络的信息，请访问<a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf">http://mccormickml . com/assets/word 2 vec/Alex _ Minnaar _ word 2 vec _ Tutorial _ Part _ I _ The _ Skip-Gram _ model . pdf</a>。</li>
</ul>


            

            
        
    






    
        <title>Time series forecasting using GRUs</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">基于GRUs的时间序列预测</h1>
                
            
            
                
<p>与LSTMs不同，gru不使用存储单元来控制信息流，并且可以直接利用所有隐藏状态。GRUs不使用单元状态，而是使用隐藏状态来传递信息。GRUs通常比其他基于记忆的神经网络训练得更快，因为它们需要训练的参数更少，张量运算更少，并且可以在更少的数据下很好地工作。格鲁有两个大门。这些被称为<em>复位门</em>和<em>更新门</em>。复位门用于确定如何将新的输入与先前的存储器相结合，而更新门确定从先前的状态保留多少信息。如果将此与LSTMs进行比较，GRU中的更新门与LSTM中的输入和遗忘门相当。它决定添加或删除什么信息。GRUs还合并了单元格状态和隐藏状态，使得生成的模型很简单。</p>
<p>下图是GRU的图示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1114 image-border" src="img/d0723d86-d223-4805-81a6-e28534f337b0.png" style="width:19.92em;height:15.08em;"/></p>
<p>在这个食谱中，我们将使用GRUs建立一个模型来预测洗发水的销售。</p>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>对于这个配方，我们需要在构建模型之前分析数据的趋势。</p>
<p>首先，让我们导入<kbd>keras</kbd>库:</p>
<pre>library(keras)</pre>
<p>在这个食谱中，我们将使用洗发水的销售数据，这些数据可以从本书的GitHub存储库中下载。该数据集包含三年内洗发水的月销售额，由36行组成。原始数据集归功于Makridakis、Wheelwright和Hyndman (1998年):</p>
<pre>data = read.table("data/shampoo_sales.txt",sep = ',')<br/>data &lt;- data[-1,]<br/>rownames(data) &lt;- 1:nrow(data)<br/>colnames(data) &lt;- c("Year_Month","Sales")<br/>head(data)</pre>
<p>下面的屏幕截图显示了数据中的一些记录:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7f2419b1-9108-4eae-8c0c-c63bb9359d78.png" style="width:9.92em;height:12.58em;"/></p>
<p>让我们分析一下数据的<kbd>Sales</kbd>列中的趋势:</p>
<pre># Draw a line plot to show the trend of data<br/>library(ggplot2)<br/>q = ggplot(data = data, aes(x = Year_Month, y = Sales,group =1))+ geom_line()<br/>q = q+theme(axis.text.x = element_text(angle = 90, hjust = 1))<br/>q</pre>
<p class="mce-root">下图让我们了解了数据的趋势:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/860bb48a-0381-4502-b5ec-2c9bde3481c1.png" style="width:40.00em;height:37.92em;"/></p>
<p>在这里，我们可以看到数据有增加的趋势。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<div><div><p class="text_cell_render rendered_html">让我们转到数据处理部分:</p>
</div>
</div>
<div><ol>
<li class="prompt input_prompt">首先，我们检查数据中<kbd>Sales</kbd>列的数据类型:</li>
</ol>
</div>
<pre style="padding-left: 60px">class(data$Sales)</pre>
<p style="padding-left: 60px">注意，在数据中，<kbd>Sales</kbd>列的数据类型是factor。为了在我们的分析中使用它，我们需要将它设为数值数据类型:</p>
<pre style="padding-left: 60px">data$Sales &lt;- as.numeric(as.character(data$Sales))<br/>class(data$Sales)</pre>
<p style="padding-left: 60px">现在，<kbd>Sales</kbd>列的类已经被更改为数值数据类型。</p>
<ol start="2">
<li>为了实现时间序列预测，我们需要将数据转换成平稳数据。我们可以使用<kbd>diff()</kbd>函数来实现这一点，它将计算迭代差。我们将参数differences的值作为1传递，因为我们希望差异有一个滞后<kbd>1</kbd>:</li>
</ol>
<pre style="padding-left: 60px">data_differenced = diff(data$Sales, differences = 1)<br/>head(data_differenced)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了差异后的一段数据:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/573a05e3-e76c-47c7-aadf-ad0d2d55b296.png" style="width:13.17em;height:1.17em;"/></p>
<p class="text_cell_render rendered_html"/>
<ol start="3">
<li>接下来，我们创建一个监督数据集，以便我们可以应用GRU。我们通过在序列中创建一个阶数为1的滞后来转换<kbd>data_differenced</kbd>序列；也就是说，作为输入的时间(t-1)的值将具有作为输出的时间<em> t </em>的值:</li>
</ol>
<pre style="padding-left: 60px">data_lagged = c(rep(NA, 1), data_differenced[1:(length(data_differenced)-1)])<br/>data_preprocessed = as.data.frame(cbind(data_lagged,data_differenced))<br/>colnames(data_preprocessed) &lt;- c( paste0('x-', 1), 'x')<br/>data_preprocessed[is.na(data_preprocessed)] &lt;- 0<br/>head(data_preprocessed)</pre>
<p style="padding-left: 60px">以下是我们的监督数据集的外观:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f633428f-eebe-4d8c-a182-d07e4183a32b.png" style="width:7.33em;height:12.08em;"/></p>
<ol start="4">
<li>现在，我们需要将数据分成训练集和测试集。在时间序列问题中，我们不能对数据进行随机抽样，因为数据的顺序很重要。因此，我们需要拆分数据，将系列的前70%作为训练数据，其余30%作为测试数据:</li>
</ol>
<pre style="padding-left: 60px">N = nrow(data_preprocessed)<br/>n = round(N *0.7, digits = 0)<br/>train = data_preprocessed[1:n, ]<br/>test = data_preprocessed[(n+1):N,]<br/>print("Training data snapshot :")<br/>head(train)<br/>print("Testing data snapshot :")<br/>head(test)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了训练数据集中的一些记录:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1115 image-border" src="img/237ce800-4cef-4da5-924c-17188f247110.png" style="width:15.25em;height:13.33em;"/></p>
<p style="padding-left: 60px">下面的屏幕截图显示了测试数据集中的一些记录:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1116 image-border" src="img/b0b44a01-2e10-4085-88f8-abb85ec9f373.png" style="width:14.83em;height:13.50em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>接下来，我们在将要使用的激活函数的范围内对数据进行归一化。由于我们选择<em> tanh </em>作为激活函数，其范围从-1到+1，我们使用<strong>最小-最大归一化</strong>来缩放数据。这里，我们对训练数据进行标准化:</li>
</ol>
<pre style="padding-left: 60px">scaling_data = function(train, test, feature_range = c(0, 1)) {<br/> x = train<br/> fr_min = feature_range[1]<br/> fr_max = feature_range[2]<br/> std_train = ((x - min(x) ) / (max(x) - min(x) ))<br/> std_test = ((test - min(x) ) / (max(x) - min(x) ))<br/> <br/> scaled_train = std_train *(fr_max -fr_min) + fr_min<br/> scaled_test = std_test *(fr_max -fr_min) + fr_min<br/> <br/> return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )<br/> <br/>}<br/><br/>Scaled = scaling_data(train, test, c(-1, 1))<br/>y_train = Scaled$scaled_train[, 2]<br/>x_train = Scaled$scaled_train[, 1]<br/><br/>y_test = Scaled$scaled_test[, 2]<br/>x_test = Scaled$scaled_test[, 1]</pre>
<p style="padding-left: 60px">然后，我们编写一个函数，将预测值恢复到原始比例。我们将在预测值时使用该函数:</p>
<pre style="padding-left: 60px">## inverse-transform<br/>invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){<br/> min = scaler[1]<br/> max = scaler[2]<br/> t = length(scaled)<br/> mins = feature_range[1]<br/> maxs = feature_range[2]<br/> inverted_dfs = numeric(t)<br/> <br/> for( i in 1:t){<br/> X = (scaled[i]- mins)/(maxs - mins)<br/> rawValues = X *(max - min) + min<br/> inverted_dfs[i] &lt;- rawValues<br/> }<br/> return(inverted_dfs)<br/>}</pre>
<ol start="6">
<li>现在，我们定义模型并配置层。我们将数据重塑为3D格式，以便将其输入模型:</li>
</ol>
<pre style="padding-left: 60px"># Reshaping the input to 3-dimensional<br/>dim(x_train) &lt;- c(length(x_train), 1, 1)<br/><br/># specify required arguments<br/>batch_size = 1 <br/>units = 1 <br/><br/>model &lt;- keras_model_sequential() <br/>model%&gt;%<br/> layer_gru(units, batch_input_shape = c(batch_size, dim(x_train)[2], dim(x_train)[3]),      stateful=TRUE)%&gt;%<br/> layer_dense(units = 1)</pre>
<p style="padding-left: 60px">我们来看看模型的<kbd>summary</kbd>:</p>
<pre style="padding-left: 60px">summary(model)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了模型的描述:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/73aac092-58a4-4908-940c-6ddad3a0ab9f.png" style="width:41.75em;height:12.50em;"/></p>
<p style="padding-left: 60px">接下来，我们编译模型:</p>
<pre style="padding-left: 60px">model %&gt;% compile(<br/> loss = 'mean_squared_error',<br/> optimizer = optimizer_adam( lr= 0.01, decay = 1e-6 ), <br/> metrics = c('accuracy')<br/>)</pre>
<ol start="7">
<li>现在，在每个时期，我们将训练数据放入模型并重置状态。我们为<kbd>50</kbd>时代训练模型:</li>
</ol>
<pre style="padding-left: 60px">for(i in 1:50 ){<br/> model %&gt;% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)<br/> model %&gt;% reset_states()<br/>}</pre>
<ol start="8">
<li>最后，我们预测测试数据集的值，并使用<kbd>inverse_scaling</kbd>函数将预测值按比例缩小到原始比例:</li>
</ol>
<pre style="padding-left: 60px">scaler = Scaled$scaler<br/>predictions = vector()<br/><br/>for(i in 1:length(x_test)){<br/> X = x_test[i]<br/> dim(X) = c(1,1,1)<br/> yhat = model %&gt;% predict(X, batch_size=batch_size)<br/> # invert scaling<br/> yhat = invert_scaling(yhat, scaler, c(-1, 1))<br/> # invert differencing<br/> yhat = yhat + data$Sales[(n+i)]<br/> # store<br/> predictions[i] &lt;- yhat<br/>}</pre>
<p style="padding-left: 60px">让我们看看测试数据的<kbd>predictions</kbd>:</p>
<pre style="padding-left: 60px">predictions</pre>
<p style="padding-left: 60px">以下屏幕截图显示了测试数据集的预测值:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/02c45d74-9352-4e9a-998f-41497d4f56c0.png" style="width:69.17em;height:3.17em;"/></p>
<p>从测试数据的预测值中，我们可以推断出该模型做得相当不错。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在<em>步骤1 </em>中，我们检查了要预测其值的列的数据类型；这里，我们使用了<kbd>Sales</kbd>列。我们将其数据类型更改为数字。在<em>步骤2 </em>中，我们将输入数据转换成静态数据。这样做是为了消除数据中与时间相关的成分。我们看到我们的输入数据有增加的趋势。在时间序列预测中，建议在建立模型之前去掉趋势分量。以后可以将这些趋势添加回预测值，这样我们就可以在原始范围内进行预测。在本例中，我们通过用1阶差分数据来消除趋势；也就是说，我们从当前的观测值中减去先前的观测值。</p>
<p class="mce-root"/>
<p>在使用LSTM和GRU等算法时，我们需要以监督的形式输入数据；即以预测器和目标变量的形式。在时间序列问题中，我们处理这样的数据，其中时间(t-k)的值充当任何<em> k </em>步进滞后数据集的时间<em> t </em>的值的输入。在我们的示例中，<em> k </em>等于1，因此在<em>步骤3 </em>中，我们通过将当前数据转换为阶为1的滞后数据来创建滞后数据集。通过这样做，我们看到了数据中的X=t-1和Y=t模式。我们创建的滞后数据集系列充当预测变量。<br/>在<em>步骤4 </em>中，我们将数据分成训练和测试数据集。随机抽样不允许我们在时间序列数据中保持观察值的顺序。因此，我们拆分数据，同时保持观察顺序不变。为此，我们将前70%的数据作为训练数据，其余30%作为测试数据。<em> n </em>代表分割点，是训练的最后一个指标，而<em> n+1 </em>代表测试数据的起始指标。在<em>步骤5 </em>中，我们将数据标准化。gru期望数据在网络所使用的激活函数的范围内。由于我们使用了<em> tanh </em>作为激活函数，它给出了(-1，1)范围内的输出，因此，我们也在(-1，1)范围内缩放了训练和测试数据集。在这种情况下，我们使用最小-最大缩放:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ac860f57-3714-4419-8f60-75689fa6e2e8.png" style="width:12.00em;height:3.50em;"/></p>
<p>应该在训练数据集上计算缩放系数值，并将其应用于缩放测试数据集。这样做是为了避免实验中由于来自测试数据集的任何种类的知识而产生的偏差。这就是为什么在这里，训练数据的最小和最大值被用作缩放训练和测试数据以及预测值的参考。我们还创建了一个名为<kbd>invert_scaling</kbd>的函数来反向缩放缩放值，并将它们映射回原始比例。</p>
<p>GRUs期望数据具有特定的格式[ <kbd>batch_size</kbd>、<kbd>timesteps</kbd>、<kbd>features</kbd> ]。<kbd>batch_size</kbd>定义了每批输入模型的观察值的数量。<kbd>timesteps</kbd>表示模型在历史数据中进行预测时需要回顾的时间步数。在本例中，我们将其设置为1。<kbd>features</kbd>参数表示我们将使用的预测值的数量，在我们的例子中是1。在<em>步骤6 </em>中，我们按照要求的格式对输入数据进行了整形，并将其输入到GRU层。请注意，我们指定了参数<kbd>stateful = TRUE</kbd>,以便批次中索引为<em> i </em>的每个样本的最后状态将用作下一批次中索引为<em> i </em>的样本的初始状态。</p>
<p>这假设了不同连续批次中的样本之间的一对一映射。<kbd>units</kbd>参数表示输出空间的维度。因为我们处理的是预测连续值，所以在本例中我们采用了单位<em> =1 </em>。一旦我们定义了模型，我们就编译它。我们指定<kbd>mean_squared_error</kbd>为损失函数，adam为学习率为0.01的优化算法。我们使用准确性作为衡量标准来评估模型的性能。接下来，我们看了一下模型的概要。</p>
<p>假设我们有以下内容:</p>
<ul>
<li><em> f </em>为一个单元中<strong>前馈神经网络</strong> ( <strong> FFNN </strong>)的数量(在GRU为3)</li>
<li>h是隐藏单元的尺寸</li>
<li><em> i </em>是输入的尺寸或大小</li>
</ul>
<p>由于每个FFNN都有<em> h(h+i) + h </em>个参数，我们可以如下计算GRU中训练参数的数量:</p>
<p class="CDPAlignCenter CDPAlign"><em>num _ params = f×[h(h+I)+h]；GRU的f = 3</em></p>
<p class="CDPAlignLeft CDPAlign">在<em>步骤7 </em>中，对于每个时期，我们将训练数据拟合到模型中。我们将<kbd>shuffle</kbd>参数的值指定为false，以避免在构建模型时对训练数据进行任何形式的洗牌。这样做是为了防止观测值之间的时间依赖性。我们还通过<kbd>reset_states()</kbd>功能在每个时期后重置网络状态。由于我们指定了<kbd>stateful = True</kbd>参数的值，我们需要在每个时期后手动重置LSTM的状态，以便为下一个时期进行干净的设置。<br/>在最后一步中，我们预测了测试数据集的值。为了将预测值缩放回原始比例，我们使用了在<em>步骤5 </em>中定义的<kbd>inverse_scaling</kbd>函数。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<p>在处理大型数据集时，我们经常在训练深度学习模型时耗尽内存。R中的<kbd>keras</kbd>库提供了各种生成器实用函数，这些函数在训练过程中动态生成批量训练数据。它还提供了创建批量时态数据的实用函数。下面的代码创建了一个监督形式的数据，类似于我们在<em>中创建的内容..</em>该配方的一部分，使用发电机实用程序:</p>
<pre># importing required libraries
library(reticulate)
library(keras)

# generating dummy data
data =  seq(from = 1,to = 10)

# timseries generator
gen = timeseries_generator(data = data,targets = data,length = 1,batch_size = 5)

# Print first batch
iter_next(as_iterator(gen))</pre>
<p>下面的屏幕截图显示了第一批生成器。在这里，我们可以看到一个生成器对象产生了两个序列的列表；第一个序列是特征向量，而第二个序列是相应的标签向量:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1117 image-border" src="img/a9d3ea9a-f054-4357-a2df-bc854e79789e.png" style="width:7.75em;height:8.75em;"/></p>
<p>下面的代码为时序数据实现了一个自定义生成器。它给了我们定义<kbd>lookback</kbd>的灵活性；也就是我们要用多少过去的值来预测未来的值或者序列。<kbd>future_steps</kbd>定义了要预测的未来时间步数:</p>
<pre>generator &lt;- function (data,lookback =3 ,future_steps = 3,batch_size = 3 ){
    new_data = data
    for(i in seq(1,3)){
        data_lagged = c(rep(NA, i), data[1:(length(data)-i)])
        new_data = cbind(data_lagged,new_data)
    }    
    targets = new_data[future_steps:length(data),(ncol(new_data)-(future_steps-1)):ncol(new_data)]
    gen = timeseries_generator(data = data[1:(length(data)-(future_steps-1))],targets = targets,length = lookback,batch_size = batch_size)
}<br/><br/>cat("First batch of generator:")
iter_next(as_iterator(generator(data = data,lookback = 3,future_steps = 2)))</pre>
<p>下面的屏幕截图显示了自定义生成器生成的第一批:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1118 image-border" src="img/ff2250f0-d217-4efa-8b10-02b111bfb49c.png" style="width:13.00em;height:10.67em;"/></p>
<p>在这里，我们可以看到，在列表的第一个索引处，我们有<kbd>lookback</kbd>个数据点，而第二个索引包含相应的未来时间步长。</p>


            

            
        
    






    
        <title>See also</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">请参见</h1>
                
            
            
                
<p class="document-title">要了解如何利用LSTMs获取具有多个季节模式的时间序列数据，请访问<a href="https://arxiv.org/pdf/1909.04293.pdf">https://arxiv.org/pdf/1909.04293.pdf</a>。</p>


            

            
        
    






    
        <title>Implementing bidirectional recurrent neural networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">实现双向递归神经网络</h1>
                
            
            
                
<p>双向递归神经网络是RNNs的扩展，其中输入数据以正常和反向时间顺序馈入两个网络。从两个网络接收的输出在每个时间步骤中使用各种合并模式进行合并，如求和、级联、乘法和求平均值。双向rnn主要用于整个语句或文本的上下文依赖于整个序列而不仅仅是线性解释的挑战。由于其长梯度链，双向rnn的训练成本很高。</p>
<p>下图是双向RNN的图示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1119 image-border" src="img/9306805e-1cea-4633-ae7e-9849a8bcf186.png" style="width:39.67em;height:15.75em;"/></p>
<p>在这个菜谱中，我们将为IMDb评论的情感分类实现一个双向RNN。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<p>在本节中，我们将使用IMDb评论数据集。其数据准备步骤与使用RNNs  <em> </em>部分的<em>情感分类相同。这意味着我们可以直接进入模型构建部分:</em></p>
<ol>
<li>让我们从实例化我们的顺序模型开始:</li>
</ol>
<pre>model &lt;- keras_model_sequential()</pre>
<ol start="2">
<li>现在，我们添加一些层到我们的模型并打印它的摘要:</li>
</ol>
<pre style="padding-left: 60px">model %&gt;%
  layer_embedding(input_dim = 2000, output_dim = 128) %&gt;% 
  bidirectional(layer_simple_rnn(units = 32),merge_mode = "concat") %&gt;% 
  layer_dense(units = 1, activation = 'sigmoid')

summary(model)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了模型的概要:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1217 image-border" src="img/1c6bead3-dda8-48f0-bf71-5987c3e1cb0c.png" style="width:40.58em;height:13.00em;"/></p>
<ol start="3">
<li>让我们编译和训练我们的模型:</li>
</ol>
<pre style="padding-left: 60px"># compile model
model %&gt;% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy")
)

# train model
model %&gt;% fit(
  train_x,train_y,
  batch_size = 32,
  epochs = 10,
  validation_split = .2
)</pre>
<ol start="4">
<li>让我们评估模型并打印指标:</li>
</ol>
<pre style="padding-left: 60px">scores &lt;- model %&gt;% evaluate(
  test_x, test_y,
  batch_size = 32
)

cat('Test score:', scores[[1]],'\n')
cat('Test accuracy', scores[[2]])</pre>
<p>下面的屏幕截图显示了测试数据的性能指标:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/406c6b32-8a78-4cf7-898f-f0f348678449.png" style="width:13.75em;height:2.50em;"/></p>
<p>我们在测试数据上获得了75%的准确率。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在建立模型之前，我们需要准备数据。如果想了解更多关于数据预处理部分的内容，可以参考本章的<em>使用RNNs进行情感分类</em>一节。</p>
<p>在<em>步骤1 </em>中，我们实例化了一个Keras序列模型。在<em>步骤2 </em>中，我们向顺序模型添加了层。首先，我们增加了一个嵌入层，降低了输入特征空间的维数。然后，我们在双向包装器中添加了一个简单的RNN，其中<kbd>merge_mode</kbd>等于<kbd>concat</kbd>。合并模式定义了如何组合前向和后向rnn的输出。其他模式包括<kbd>sum</kbd>、<kbd>mul</kbd>、<kbd>ave</kbd>和<kbd>NULL</kbd>。最后，我们添加了一个具有一个隐藏单元的密集层，并使用sigmoid作为激活函数。</p>
<p>在<em>步骤3 </em>中，我们用<kbd>binary_crossentropy</kbd>作为损失函数来编译模型，因为我们正在解决一个二元分类问题。我们为此使用了<kbd>adam</kbd>优化器。然后，我们在训练数据集上训练我们的模型。在<em>步骤4 </em>中，我们评估了我们的模型的测试准确性，以查看我们的模型在测试数据上的表现。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<p>虽然双向rnn是一种先进的技术，但使用它们有一些限制。由于双向rnn在正方向和负方向上运行，它们非常慢，因为梯度具有很长的依赖链。</p>
<p>此外，双向rnn还用于非常特殊的应用，例如填充缺失的单词、机器翻译等等。该算法的另一个主要问题是，由于内存带宽限制的计算，训练起来很有挑战性。</p>


            

            
        
    


</body></html>