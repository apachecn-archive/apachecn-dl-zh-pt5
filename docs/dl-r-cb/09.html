<html><head/><body>


    
        <title>Implementing Reinforcement Learning</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">实施强化学习</h1>
                
            
            
                
<p class="mce-root"><strong>强化学习</strong> ( <strong> RL </strong>)最近获得了相当大的牵引力。这是一种不同于传统机器学习和深度学习技术的机器智能方法。在学习围棋、Dota等复杂游戏方面已经达到了人类水平的表现。RL是一个人工智能框架，其中代理通过试错来执行学习。这是一个模仿人类基本学习方式的学习过程。本章的首要目标是让你熟悉RL的组成部分。您将学习如何使用r中的各种包来实现RL。</p>
<p>在本章中，我们将介绍以下配方:</p>
<ul>
<li>使用MDPtoolbox的基于模型的RL</li>
<li>无模型RL</li>
<li>使用RL进行悬崖行走</li>
</ul>


            

            
        
    






    
        <title>Model-based RL using MDPtoolbox</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用MDPtoolbox的基于模型的RL</h1>
                
            
            
                
<p class="CDPAlignLeft CDPAlign">RL是人工智能的通用框架。它用于解决顺序决策问题。在RL中，给计算机一个要实现的目标，它通过从与环境的交互中学习来学习如何实现这个目标。一个典型的RL设置由五个组件组成，分别称为<strong>代理</strong>、<strong>环境</strong>、<strong>动作</strong>、<strong>状态</strong>和<strong>奖励</strong>。</p>
<p class="CDPAlignLeft CDPAlign">在RL中，代理使用一组<strong>动作</strong> ( <strong> A </strong>)中的一个动作与环境进行交互。基于代理采取的动作，环境从初始状态转换到新状态，其中每个状态属于环境内的一组<strong>状态</strong>。这种转变从环境中产生一个反馈<strong>奖励</strong>信号(一个标量)。奖励是对代理表现的评估，奖励值取决于当前状态和执行的操作。这个过程是一个主体对行为和环境作出选择，对新的状态和与之相关的奖励作出响应，直到主体学习到一个最佳行为，从任何初始状态达到最终状态，使累积奖励最大化。</p>
<p class="CDPAlignLeft CDPAlign">下图是RL设置的图示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1174 image-border" src="img/be3e424e-7dba-4aed-ae9e-0d2a198d533b.png" style="width:25.83em;height:13.25em;"/></p>
<p><img class="fm-editor-equation" src="img/9ed2e42e-dac2-459b-bc39-64a0052189af.png" style="width:1.00em;height:0.92em;"/>可以表述如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f2f45a6d-1c85-4810-ac7d-08ad78ea20c2.png" style="width:8.75em;height:4.67em;"/></p>
<p class="CDPAlignCenter CDPAlign">这里，<em> G <sub> t </sub> </em>是累积奖励，<br/> γ是折扣因子，<br/>和<em> t </em>是时间步长</p>
<p class="CDPAlignLeft CDPAlign">RL设置遵循特定的假设。首先，智能体顺序地与环境相互作用；那么，第二，时空是离散的；最后，转移遵循马尔可夫性质；也就是说，环境的未来状态<img class="fm-editor-equation" src="img/f8c01674-1229-4082-a468-9ed341d87eaa.png" style="width:1.17em;height:1.50em;"/>，只取决于当前状态<em> s </em>。马尔可夫过程是一种无记忆的随机过程；也就是说，它是一个具有马尔可夫性质的随机状态序列。这是一个为决策过程建模的框架。<strong>马尔可夫决策过程</strong> ( <strong> MDP </strong>)指定了一种数学结构来寻找RL问题的解决方案。</p>
<p class="CDPAlignLeft CDPAlign">它是由五个部分组成的元组—(S，A，P，R，γ):</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong> S </strong>:状态集合，<img class="fm-editor-equation" src="img/c1fe1aca-6ea2-4bb0-a7cd-b9490cd15ec1.png" style="width:2.92em;height:1.17em;"/>。</li>
<li class="CDPAlignLeft CDPAlign"><strong> A </strong>:一组动作，<img class="fm-editor-equation" src="img/56a6e606-0c24-45a7-a6db-8551a0a19669.png" style="width:3.00em;height:1.08em;"/>。</li>
<li class="CDPAlignLeft CDPAlign"><strong> P </strong>:转移概率<img class="fm-editor-equation" src="img/910c477c-71f7-4e7f-995c-7e032d77c050.png" style="width:1.92em;height:1.42em;"/>，是在<img class="fm-editor-equation" src="img/cb772df4-546f-418e-af56-f22866a8d69f.png" style="width:0.67em;height:0.92em;"/>状态下采取动作<img class="fm-editor-equation" src="img/88273da5-7252-49d1-814b-4ee10f3c3319.png" style="width:0.75em;height:0.92em;"/>后到达<img class="fm-editor-equation" src="img/e7bb7ed5-c9a6-40c6-9c3e-fe79dc75ad30.png" style="width:1.00em;height:1.25em;"/>的概率。</li>
<li class="CDPAlignLeft CDPAlign"><strong> R </strong>:奖励函数<img class="fm-editor-equation" src="img/ea48ab26-c285-4d01-81ff-b3588626bc82.png" style="width:2.00em;height:1.33em;"/>，使用动作<img class="fm-editor-equation" src="img/ed564300-186e-4cd5-a8c0-d08225cb609a.png" style="width:0.83em;height:0.92em;"/>从<img class="fm-editor-equation" src="img/c281b351-7a6f-4832-8fb7-4af5afa82745.png" style="width:0.75em;height:1.00em;"/>移动到<img class="fm-editor-equation" src="img/1949fc0e-dfc1-4286-b1db-1f1a24fec760.png" style="width:1.17em;height:1.50em;"/>时的期望奖励。</li>
<li class="CDPAlignLeft CDPAlign"><strong> <img class="fm-editor-equation" src="img/9d88101e-80b8-4ed0-9f7a-5af5cdc16d53.png" style="width:0.75em;height:1.08em;"/> </strong>:折扣系数。z</li>
</ul>
<p>使用MDP，我们可以找到一个策略<img class="fm-editor-equation" src="img/383da270-68e8-462c-b548-0649505ef099.png" style="width:2.08em;height:1.33em;"/>，它用贴现因子<img class="fm-editor-equation" src="img/2903960e-b5fb-4001-95e0-58ce2a063f7f.png" style="width:0.67em;height:0.92em;"/>最大化预期长期回报<img class="fm-editor-equation" src="img/3933cbf6-2168-4ecb-93c5-b6cbfb5adf6d.png" style="width:3.00em;height:1.42em;"/>(它定义了未来回报的适用贴现)。策略定义了代理应该根据当前状态采取的最佳操作。它将动作映射到状态。从一个状态<em> s </em>开始，并遵循一个政策<img class="fm-editor-equation" src="img/c6c206cc-bb6e-4d0d-b41c-8b89bcb82833.png" style="width:2.17em;height:1.42em;"/>来估计代理的长期回报的函数被称为价值函数。</p>
<p>有两种类型的价值函数:</p>
<ul>
<li><strong>状态值</strong>函数，<strong> <img class="fm-editor-equation" src="img/15bcf353-28d0-41a9-8cd1-74a77b63253c.png" style="width:2.58em;height:1.33em;"/> </strong>:对于MDP，它是从状态<em> s </em>开始并遵循策略<img class="fm-editor-equation" src="img/b4ca8e02-4c1e-4452-9c3f-9853ed603c67.png" style="width:2.08em;height:1.33em;"/>的代理的期望收益:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a3f9e91b-641c-414d-bda2-83b57096fd38.png" style="width:11.08em;height:1.42em;"/></p>
<ul>
<li><strong>动作值</strong>函数，<strong> <img style="font-size: 1em;color: #333333;width:4.08em;height:1.42em;" class="fm-editor-equation" src="img/aba50990-5f52-4456-92d9-64ca76d635b7.png"/> </strong>:对于MDP，它是代理人从状态<em> s </em>开始，采取动作<em> a </em>，然后遵循策略<img style="font-size: 1em;color: #333333;width:1.92em;height:1.25em;" class="fm-editor-equation" src="img/a174b73c-9fa8-482f-93d5-fa5e44a8f0a8.png"/>的期望收益:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ee0585cd-7820-4d71-9b0d-ee74e6c4ddac.png" style="width:12.50em;height:1.42em;"/></p>
<p class="CDPAlignLeft CDPAlign">在所有可能的价值函数中，存在一个<strong>最优价值函数</strong> <img class="fm-editor-equation" src="img/1c7b930c-f5c9-44a3-a055-b75c305a7102.png" style="width:2.83em;height:1.33em;"/>，它对所有状态产生最高的期望回报。对应于最优值函数的策略被称为<strong>最优策略</strong>。</p>
<p><img class="fm-editor-equation" src="img/38f70e66-666f-43ee-991d-647802374da5.png" style="width:2.67em;height:1.25em;"/>和<img class="fm-editor-equation" src="img/e361271b-0b1a-428c-a72e-cfc486cc1b7c.png" style="width:2.67em;height:1.25em;"/>可以表示如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ceb3eb6a-6f66-4c42-8212-85420d6498f1.png" style="width:29.08em;height:2.00em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/4a20786d-7d1d-4883-98bc-e1840c615ed9.png" style="width:29.92em;height:1.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root">通过最大化超过<img class="fm-editor-equation" src="img/4166cad1-64d4-4379-b5a0-6cca0e9ceb1d.png" style="width:3.92em;height:1.33em;"/>可以找到最优策略。最佳策略可以描述如下:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1957e405-5e8e-4299-91d0-e4605428de91.png" style="width:10.75em;height:1.83em;"/></p>
<p>利用贝尔曼方程，我们可以找到最优值函数。<strong>贝尔曼期望方程</strong>将价值函数定义为从当前状态转移<em> s </em>，使用动作<em> a </em>，以及从下一个状态转移<em>s’</em>的期望回报的总和:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1d23264f-8b8b-4bcf-9559-5b19b4e6a5cc.png" style="width:14.58em;height:1.67em;"/> <br/> <img class="fm-editor-equation" src="img/1310c566-8ade-4c21-a12c-d3e2c3c09f2b.png" style="width:18.67em;height:1.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">用于<img class="fm-editor-equation" src="img/91383f66-0beb-48c7-95f3-54ff0820ed6b.png" style="width:1.58em;height:1.25em;"/>和<img class="fm-editor-equation" src="img/c84c5922-1f6b-4de0-893d-55f7bc81458d.png" style="width:1.58em;height:1.08em;"/>的<strong>贝尔曼最优方程</strong>如下所示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5318ae64-2363-4908-992a-89875212e058.png" style="width:17.83em;height:2.92em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8c09a216-8bb8-448e-8409-ee35745d6ca6.png" style="width:20.25em;height:3.08em;"/></p>
<p>此外，最佳状态值和动作值函数通过贝尔曼最优方程递归关联，如下式所示:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ad5e2850-92b8-4d65-995c-534ebce4ec7e.png" style="width:11.58em;height:2.00em;"/></p>
<p>由此，我们得到以下结论:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/efadc984-5c8c-466d-80d4-1638919bacc3.png" style="width:19.33em;height:2.92em;"/></p>
<p>求解贝尔曼最优性方程有很多方法，比如值迭代、策略迭代、SARSA、Q-learning等。RL技术可以分为基于模型的方法和无模型的方法。基于模型的算法依赖于提供状态转移概率的环境的显式模型，以及MDP形式的环境表示。这些MDP可以用各种算法求解，比如值迭代，策略迭代。</p>
<p>另一方面，无模型算法不依赖于任何关于代表问题的环境的明确知识。相反，他们试图学习基于代理与环境的动态交互的最优策略。在这个菜谱中，我们将使用基于模型的策略迭代算法来解决RL问题。</p>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>在这个食谱中，我们将解决一个网格导航问题。下图是导航网格的图示。它代表一个导航矩阵，其中每个状态都被分配了一个标签。矩阵中的每个单元代表一种状态，总共有四种状态。代理应该从任意随机起始状态导航到最终目标状态，4。代理只能通过网格中的开口在状态之间移动，而不能离开网格壁。在每个状态下，代理可以执行可用动作集中的任何动作；也就是说，它们可以向上、向下、向左或向右移动。当进入目标状态时，代理人获得100的奖励，其他每增加一步都要付出-1的惩罚。</p>
<p>在下图中，可能的状态为{1，2，3，4}，可能的动作集为{上、下、左、右}:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ba827ce6-0f16-411b-b54f-6b045dca48f9.png" style="width:22.33em;height:15.58em;"/></p>
<p>对于基于模型的RL实现，我们将使用<kbd>MDPtoolbox</kbd>库。</p>
<p>让我们导入<kbd>MDPtoolbox</kbd>库:</p>
<pre>library(MDPtoolbox)</pre>
<p><kbd>MDPtoolbox</kbd>库包含许多与离散时间MDP解析相关的函数。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<p>在<em>准备就绪</em>部分，我们定义了我们的RL问题。我们知道，要解决基于模型的RL问题，我们需要一个转移概率矩阵和一个回报矩阵:</p>
<ol>
<li>让我们从定义转移概率矩阵开始。我们从定义每个状态下所有行为的概率开始。每行中概率的总和等于1:</li>
</ol>
<pre style="padding-left: 60px"># Up <br/>up=matrix(c( 0.9, 0.1, 0, 0,<br/> 0.2, 0.7, 0.1, 0,<br/> 0, 0, 0.1, 0.9,<br/> 0, 0, 0, 1),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/># Down <br/>down=matrix(c(0.1, 0, 0, 0.9,<br/> 0, 0.8, 0.2, 0,<br/> 0, 0.2, 0.8, 0,<br/> 0, 0, 0.8, 0.2),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/># Left <br/>left=matrix(c(1, 0, 0, 0,<br/> 0.9, 0.1, 0, 0,<br/> 0, 0.8, 0.2, 0,<br/> 0, 0, 0, 1),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/># Right <br/>right=matrix(c(0.1, 0.9, 0, 0,<br/> 0.1, 0.2, 0.7, 0,<br/> 0, 0, 0.9, 0.1,<br/> 0, 0, 0, 1),<br/> nrow=4,ncol=4,byrow=TRUE)</pre>
<p style="padding-left: 60px" class="mce-root">现在，让我们将所有操作放在一个列表中:</p>
<pre style="padding-left: 60px">actions = list(up=up, down=down, left=left, right=right)<br/>actions</pre>
<p style="padding-left: 60px">以下屏幕截图显示了每个动作的状态动作矩阵:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1175 image-border" src="img/bbf27212-0c3c-4f7c-8a87-c42919ed3711.png" style="width:7.67em;height:29.50em;"/></p>
<ol start="2">
<li>现在，让我们定义奖励和惩罚。唯一的回报是进入状态4；其他每一步都会招致-1的惩罚:</li>
</ol>
<pre style="padding-left: 60px">rewards=matrix(c( -1, -1, -1, -1,<br/> -1, -1, -1, -1,<br/> -1, -1, -1, -1,<br/> 100, 100, 100, 100),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/>rewards</pre>
<p style="padding-left: 60px">以下截图显示了奖励矩阵:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/05320bf6-2f5a-4f39-bcea-ce501495107a.png" style="width:9.25em;height:6.92em;"/></p>
<ol start="3">
<li>现在，我们可以使用<kbd>mdp_policy_iteration()</kbd>函数来解决这个问题。该函数将转移概率和奖励矩阵以及折扣因子作为输入:</li>
</ol>
<pre style="padding-left: 60px">solved_MDP=mdp_policy_iteration(P=actions, R=rewards, discount = 0.2)<br/>solved_MDP</pre>
<p style="padding-left: 60px">以下截图显示了解决问题的结果:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1176 image-border" src="img/b8d504f5-8763-4647-8b97-3ec5d345f44e.png" style="width:23.50em;height:12.08em;"/></p>
<p style="padding-left: 60px">让我们来看看策略迭代算法给出的策略:</p>
<pre style="padding-left: 60px">solved_MDP$policy <br/>names(actions)[solved_MDP$policy]</pre>
<p style="padding-left: 60px">以下截图显示了针对我们问题的策略:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1177 image-border" src="img/2917e6fe-ffcc-48bd-9b20-d486ae0a6c84.png" style="width:11.75em;height:5.25em;"/></p>
<p style="padding-left: 60px">上一个屏幕截图中描述的策略让我们在相应的状态1、2、3、4中采取最佳行动。例如，状态1的最佳行动是向下，状态2是向左，状态3是向上，状态4是向上。我们可以使用以下代码获得每一步的值:</p>
<pre style="padding-left: 60px">solved_MDP$V</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了我们政策每一步的价值:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1178 image-border" src="img/27ad3a5d-9658-4940-b0b3-769bc92f5ed6.png" style="width:26.58em;height:1.42em;"/></p>
<p>在这里，我们可以看到在最后一步，我们的策略的值是<strong> 125 </strong>。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在<em>步骤</em> <em> 1 </em>中，我们为每个动作定义了动作概率矩阵。我们可以将此解释为使用一个动作从当前状态转换到下一个状态的概率。假设代理处于状态<strong> 2 </strong>并试图向左<strong>走</strong>，代理有90%的概率会转换到状态1。</p>
<p>下图显示了<strong>左</strong>动作的转移概率矩阵:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1179 image-border" src="img/cf5e62fc-a04e-4cae-a05f-4af3cd09671c.png" style="width:20.75em;height:14.83em;"/></p>
<p>在<em>步骤</em> <em> 2 </em>中，我们定义了一个奖励矩阵；也就是说，从当前状态转换到下一个状态时给予代理的标量奖励。</p>
<p>下图显示了奖励矩阵:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1180 image-border" src="img/abfa4095-9db7-45bb-93be-36f379b00d85.png" style="width:13.25em;height:13.75em;"/></p>
<p>在最后一步中，我们通过使用策略迭代算法来求解贴现MDP来解决RL问题。<kbd>mdp_policy_iteration()</kbd>函数返回<strong> V </strong>，即<strong>最优值函数</strong><em>；</em>政策，即最优政策；<kbd>iter</kbd>，为迭代次数；还有<kbd>time</kbd>，就是程序占用的CPU时间。当两个连续的策略相同时，策略迭代算法停止。我们还可以通过向<kbd>max_iter</kbd>参数传递一个值来指定迭代的次数。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<p><kbd>MDPtoolbox</kbd>包还提供了<strong>值迭代</strong>算法的实现，这样我们就可以求解一个MDP。下面的代码块演示了同样的情况:</p>
<pre>mdp_value_iteration(P=actions, R=rewards, discount = 0.2)</pre>
<p>以下屏幕截图显示了最佳策略的详细信息:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1181 image-border" src="img/8f2515ac-d664-45e9-afa6-a03cbb981c5a.png" style="width:38.67em;height:24.33em;"/></p>
<p>价值迭代法给出的最优策略是{2，3，1，1}。最后一步的值接近我们在策略迭代方法中得到的值。</p>


            

            
        
    






    
        <title>Model-free RL</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">无模型RL</h1>
                
            
            
                
<p>在之前的配方中，<em>使用MDPtoolbox </em>的基于模型的RL，我们遵循基于模型的方法来解决RL问题。随着状态和动作空间的增长，基于模型的方法变得不切实际。另一方面，无模型强化算法依赖于代理与代表当前问题的环境的试错交互。在这个配方中，我们将使用一种无模型的方法，通过r中的<kbd>ReinforcementLearning</kbd>包来实现RL。这个包利用了一种流行的无模型算法，称为<strong> Q-learning </strong>。它是一种非策略算法，因为它同时探索环境和利用当前知识。</p>
<p>Q-learning保证收敛到最优策略，但要实现这一点，它依赖于代理与其环境之间的持续交互，这使得它的计算量很大。这种算法期待下一个状态，并观察该状态下所有可能行动的最大可能回报。然后，它利用这一知识以特定的学习速率α更新当前状态中相应动作的动作值信息。该算法试图学习一个称为Q函数的最佳评估函数，该函数将每个状态和动作对映射到一个值。它表示为Q: S × A =&gt; V，其中V是在状态<img class="fm-editor-equation" src="img/ce403408-d222-41d9-bfbd-76d394ae2c5b.png" style="width:2.75em;height:1.08em;"/>中执行的动作<img class="fm-editor-equation" src="img/ee214e18-5d3c-4920-9f6a-7bafdc370f9e.png" style="width:3.00em;height:1.08em;"/>的未来奖励值。</p>
<p>下面是Q学习算法的伪代码:</p>
<ol>
<li>将表中所有状态-动作对(s，a)的值初始化为0，<img class="fm-editor-equation" src="img/7f562841-dad6-47cd-b70e-539ae3cbda79.png" style="width:3.58em;height:1.42em;"/>。</li>
<li>观察当前状态，<img class="fm-editor-equation" src="img/95a03b7b-8ca6-4e50-914f-32322ba117cb.png" style="width:0.58em;height:0.75em;"/>。</li>
<li>重复直到收敛:</li>
</ol>
<ul>
<li style="padding-left: 30px">选择一个动作，<strong> <img class="fm-editor-equation" src="img/8c6f5ddc-ae69-49e5-9e0e-8b16dbf30eb2.png" style="width:0.67em;height:0.75em;"/> </strong>，并应用它。</li>
<li style="padding-left: 30px">获得即时奖励，<img style="font-size: 1em;color: #333333;width:1.92em;height:1.25em;" class="fm-editor-equation" src="img/46736c46-ebfc-4302-9464-faec3ff6041a.png"/>。</li>
<li style="padding-left: 30px">移动到新状态，<img class="fm-editor-equation" src="img/2629bb85-80b3-4fe9-b318-ac8ffb664244.png" style="width:0.92em;height:1.17em;"/>。</li>
<li style="padding-left: 30px">根据以下公式更新<img class="fm-editor-equation" src="img/99b84206-c5a4-42d2-9249-0cbfc5c3ed22.png" style="width:3.33em;height:1.33em;"/>的表格条目:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/17efba54-5e79-4108-b0dc-99ac8bc37075.png" style="width:25.67em;height:1.92em;"/></p>
<ul>
<li style="padding-left: 30px">移到下一个状态，<img class="fm-editor-equation" src="img/fb4e6004-dca1-4c66-9053-d24c9921e389.png" style="width:0.92em;height:1.17em;"/>。现在，<img class="fm-editor-equation" src="img/06aad29c-a36b-47ba-b95e-22f7455ca8f4.png" style="width:0.92em;height:1.25em;"/>变成当前状态，<img class="fm-editor-equation" src="img/d0b61e9a-c86f-498c-a882-42400c48e773.png" style="width:0.67em;height:0.83em;"/>。</li>
</ul>
<p class="mce-root"/>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>在这个配方中，我们将使用<kbd>ReinforcementLearning</kbd> <strong> </strong>包，它执行无模型RL。</p>
<p>让我们导入<kbd>ReinforcementLearning</kbd>包:</p>
<pre>library(ReinforcementLearning)</pre>
<p>在这个菜谱中，我们将使用MDPtoolbox 处理上一节中使用的相同导航示例<em>。在这种情况下，我们不会有任何预先确定的输入数据，我们将使用无模型方法来解决问题。代理将与代表问题的环境动态交互，并生成状态-动作转换元组。环境的结构特定于手头的问题。一个<strong>环境</strong>通常是一个随机的有限状态机，代表任何特定问题中的操作规则。它根据奖励和惩罚向代理提供关于其行为的反馈。</em></p>
<p>以下是某个环境的一些通用伪代码:</p>
<pre>environment &lt;- function(state, action) {<br/> ...<br/> return(list("NextState" = newState,"Reward" = reward))<br/> }</pre>
<p>在下一节中，我们将创建一个导航网格环境，并训练一个代理使用无模型RL在网格中导航。</p>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<p>让我们以编程方式为我们的问题创建一个环境:</p>
<ol>
<li>我们首先定义状态和一组动作:</li>
</ol>
<pre style="padding-left: 60px">states &lt;- c("1", "2", "3", "4")<br/>actions &lt;- c("up", "down", "left", "right")<br/>cat("The states are:",states)<br/>cat('\n')<br/>cat("The actions are:",actions)</pre>
<ol start="2">
<li>接下来，我们创建一个函数，它将为我们的示例定义一个自定义环境:</li>
</ol>
<pre style="padding-left: 60px">gridExampleEnvironment &lt;- function(state, action) {<br/> next_state &lt;- state<br/> if (state == state("1") &amp;&amp; action == "down") next_state &lt;- state("4")<br/> if (state == state("1") &amp;&amp; action == "right") next_state &lt;- state("2")<br/> if (state == state("2") &amp;&amp; action == "left") next_state &lt;- state("1")<br/> if (state == state("2") &amp;&amp; action == "right") next_state &lt;- state("3")<br/> if (state == state("3") &amp;&amp; action == "left") next_state &lt;- state("2")<br/> if (state == state("3") &amp;&amp; action == "up") next_state &lt;- state("4")<br/> if (next_state == state("4") &amp;&amp; state != state("4")) {<br/> reward &lt;- 100<br/> } else {<br/> reward &lt;- -1<br/> }<br/>out &lt;- list("NextState" = next_state, "Reward" = reward)<br/>return(out)<br/>}<br/><br/>print(gridExampleEnvironment)</pre>
<p style="padding-left: 60px">以下屏幕截图提供了对环境的描述:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1182 image-border" src="img/3ed49322-b9e4-44ef-bbe8-a9068e48aea4.png" style="width:30.42em;height:16.17em;"/></p>
<ol start="3">
<li>现在，我们使用<kbd>sampleExperience()</kbd>函数以状态转移元组的形式生成一些样本经验数据。该函数将状态、动作、迭代和环境作为输入参数:</li>
</ol>
<pre style="padding-left: 60px"># Let us generate 1000 iterations<br/>sequences &lt;- sampleExperience(N = 1000, env = gridExampleEnvironment, states = states, actions = actions)<br/>head(sequences,6)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了示例数据的前几条记录:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1190 image-border" src="img/fb0132c9-110a-4773-866e-388a7784d344.png" style="width:16.50em;height:12.92em;"/></p>
<ol start="4">
<li>使用我们在上一步中生成的样本经验数据，我们可以使用<kbd>ReinforcementLearning()</kbd>函数来解决我们的问题:</li>
</ol>
<pre style="padding-left: 60px">solver_rl &lt;- ReinforcementLearning(sequences, s = "State", a = "Action", r = "Reward", s_new = "NextState")<br/><br/>print(solver_rl)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了州行动表，以及针对我们的问题的政策和总体奖励。<strong> X1 </strong>、<strong> X2 </strong>、<strong> X3 </strong>、<strong> X4 </strong>分别代表状态1、2、3、4:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1183 image-border" src="img/ef2db0e7-811c-44d0-8547-714ed8999ade.png" style="width:26.75em;height:16.92em;"/></p>
<p>在这里，我们可以看到我们在最后一次迭代中的总回报是<kbd>11423</kbd>。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在<em>步骤1 </em>中，我们为这个问题定义了一组可能的状态和动作。为了使用无模型RL，我们需要创建一个模拟环境行为的函数。在<em>步骤2 </em>中，我们通过创建一个名为<kbd>gridExampleEnvironment()</kbd>的函数将问题公式化，该函数将一个状态-动作对作为输入，并生成下一个状态和相关奖励的列表。在<em>步骤3 </em>中，我们使用<kbd>sampleExperience()</kbd>函数通过查询我们在前面步骤中创建的环境来生成动态状态-动作转换元组。这个函数的输入参数是样本数、环境函数以及状态和动作集。该函数返回一个数据帧，其中包含来自环境的经验观察序列。</p>
<p>一旦生成了观察序列数据，代理就会基于该数据学习最优策略。为了实现这一点，在<em>步骤4 </em>中，我们使用了<kbd>ReinforcementLearning()</kbd>功能。我们可以向这个函数传递更多的参数来定制代理的学习行为。</p>
<p>论据如下:</p>
<ul>
<li><kbd>alpha</kbd>:这是学习率α，在0和1之间变化。该参数值越高，学习速度越快。</li>
<li><kbd>gamma</kbd>:这是折扣系数γ，可以设置为0到1之间的任意值。它决定了未来奖励的重要性。较低的gamma值会使代理人短视，只考虑眼前的回报，而较高的gamma值会使代理人努力争取更大的长期回报。</li>
<li><kbd>epsilon</kbd>:该参数epsilon，ε，决定了ε-贪婪动作选择中的探索机制，可以设置在0到1之间。</li>
<li><kbd>iter</kbd>:该参数表示代理通过训练数据集的重复学习迭代次数。默认情况下，它设置为1。</li>
</ul>
<p>我们看到学习过程的结果包含状态-动作表；即每个状态-动作对的Q值和在每个状态下具有最佳可能动作的最优策略。此外，我们还得到了政策的整体奖励。</p>


            

            
        
    






    
        <title>See also</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">请参见</h1>
                
            
            
                
<p>要了解更多关于其他RL算法的信息，比如已经在街机学习环境中实现的SARSA和GQ，请访问<a href="https://arxiv.org/pdf/1410.8620.pdf">https://arxiv.org/pdf/1410.8620.pdf</a>。</p>
<p class="mce-root"/>


            

            
        
    






    
        <title>Cliff walking using RL</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用RL进行悬崖行走</h1>
                
            
            
                
<p>到目前为止，您应该已经了解了RL的框架。在这个菜谱中，我们将在RL中实现<kbd>gridworld</kbd>环境的真实应用。这个问题可以表示为一个4x12大小的网格。剧集从左下方的状态开始，目标状态位于网格的右下方。向左、向右、向上和向下是任何状态下唯一可能的动作。网格下部标记为<em> C </em>的状态是悬崖。任何进入这些状态的转变都会招致-100的高负奖励，并使代理立即回到开始状态。对于目标状态，<em> G </em>，奖励为0，而对于除目标状态和悬崖之外的所有转换，奖励为-1。</p>
<p>下图显示了悬崖行走问题的导航矩阵:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1184 image-border" src="img/6850415d-e859-4134-869b-23f35bd364dc.png" style="width:29.25em;height:15.08em;"/></p>
<p>让我们继续使用RL来解决这个导航问题。</p>


            

            
        
    






    
        <title>Getting ready</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">做好准备</h1>
                
            
            
                
<p>在这个菜谱中，我们将使用<kbd>reinforcelearn</kbd>包从名为<strong> cliff walking </strong>的内置环境中获取数据。这个环境是从<kbd>gridworld</kbd>环境继承来的。</p>
<p>我们将使用<kbd>ReinforcementLearning</kbd>包执行无模型RL:</p>
<div><div><div><div><div><div><pre class=" CodeMirror-line">library(reinforcelearn)<br/>library(ReinforcementLearning)</pre></div>
</div>
</div>
</div>
</div>
</div>
<p>在下一节中，我们将创建一个代表悬崖行走问题的环境。</p>
<p class="mce-root"/>


            

            
        
    






    
        <title>How to do it...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">怎么做...</h1>
                
            
            
                
<p class="inner_cell"/>
<div><p>让我们创造一个环境来代表我们在悬崖上行走的问题:</p>
<ol>
<li>让我们从使用<kbd>makeEnvironment()</kbd>函数加载悬崖行走环境开始:</li>
</ol>
<pre style="padding-left: 60px">env = makeEnvironment("cliff.walking")<br/>env</pre>
<p style="padding-left: 60px">以下截图是对悬崖行走环境的描述:</p>
</div>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1185 image-border" src="img/75b439ef-ece0-4266-af8e-75962638b76a.png" style="width:41.83em;height:39.33em;"/></p>
<ol start="2">
<li>接下来，我们创建一个函数，该函数将使用随机动作查询环境，并获取观察序列数据:</li>
</ol>
<pre style="padding-left: 60px"># Creating the function to query the environment<br/>sequences &lt;- function(iter,env){<br/> actions &lt;- env$actions<br/> data &lt;- data.frame(matrix(ncol = 4, nrow = 0))<br/> colnames(data) &lt;- c("State", "Action", "Reward","NextState")<br/> env$reset()<br/> for(i in 1:iter){<br/> current_state &lt;- env$state<br/> current_action &lt;- floor(runif(1,0,4))<br/> current_reward &lt;- env$step(current_action)$reward<br/> next_state_iter &lt;- env$step(current_action)$state<br/> iter_data &lt;- cbind("State" = current_state,"Action" = current_action,"Reward"=current_reward,"NextState" = next_state_iter)<br/> data &lt;- rbind(data,iter_data)<br/> if(env$done == "TRUE"){<br/> break;<br/> }<br/> }<br/> return(data) <br/>}</pre>
<p style="padding-left: 60px">现在，让我们从前面代码块中定义的函数中获取数据:</p>
<pre style="padding-left: 60px">iter &lt;- 1000<br/>observations = sequences(iter,env)<br/>cols.name &lt;- c("State","Action","NextState")<br/>observations[cols.name] &lt;- sapply(observations[cols.name],as.character)<br/>sapply(observations, class)<br/><br/># Displaying first 20 records<br/>head(observations,20)</pre>
<p style="padding-left: 60px">下面的屏幕截图显示了生成的数据中的一些记录:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1186 image-border" src="img/f2951fc5-4fe2-4816-acd9-da0f5380bdc9.png" style="width:18.17em;height:37.75em;"/></p>
<ol start="3">
<li>使用我们在上一步中生成的样本观察数据，我们可以使用<kbd>ReinforcementLearning()</kbd>函数来解决我们的问题。我们通过指定一个<kbd>control</kbd>对象来定制代理的学习行为，在这里我们为学习速率<kbd>alpha</kbd>、折扣因子<kbd>gamma</kbd>和探索贪婪度<kbd>epsilon</kbd>设置参数选择:</li>
</ol>
<pre style="padding-left: 60px">control &lt;- list(alpha = 0.2, gamma = 0.4, epsilon = 0.1)<br/><br/># Perform RL<br/>model &lt;- ReinforcementLearning(data = observations, s = "State", a = "Action", r = "Reward",<br/> s_new = "NextState", iter = 1, control = control)</pre>
<p style="padding-left: 60px">现在，我们打印已学习的状态-动作表，它包含每个状态-动作对的Q值:</p>
<pre style="padding-left: 60px">print(model)</pre>
<p style="padding-left: 60px">下表显示了每个状态-动作对的Q值表:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1187 image-border" src="img/5c8ffc50-a1ea-41f7-8d7c-aa3101f1ed8a.png" style="width:31.08em;height:26.17em;"/></p>
<p>Q值表中给出了最佳策略。</p>


            

            
        
    






    
        <title>How it works...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">它是如何工作的...</h1>
                
            
            
                
<p>在<em>步骤</em> <em> 1 </em>中，我们使用<kbd>reinforcelearn</kbd>库中的<kbd>makeEnvironment()</kbd>函数创建了<strong>悬崖漫步</strong>环境。这种环境属于<kbd>gridworld</kbd>阶层。在<em>步骤2 </em>中，我们创建了一个定制的函数来查询悬崖行走环境并获取样本观测数据。<kbd>env()</kbd>函数的<kbd>step()</kbd>方法将一个动作作为输入参数，并返回一个带有状态、奖励和完成的列表作为输出。一旦生成了观察序列数据，我们就使用<kbd>ReinforcementLearning()</kbd>函数让代理在最后一步学习基于该数据的最优策略。</p>


            

            
        
    






    
        <title>There's more...</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">还有更多...</h1>
                
            
            
                
<p>在许多RL问题中，探索制定最优策略的行为可能是昂贵的。<strong>体验回放</strong>是一种用于让代理重用过去体验的技术。这种技术通过重放已经观察到的状态转换作为环境中的新观察来实现快速收敛。体验重放需要由状态、动作和奖励组成的样本序列作为输入数据。这些转换使代理学习状态-动作函数和输入数据中所有状态的最优策略。这个策略也可以应用于验证目的或者迭代地改进当前策略。要在R中实现体验重放，您需要将现有的RL模型作为参数传递给<kbd>ReinforcementLearning()</kbd>函数。</p>
<p>让我们从悬崖行走环境中获取100个新的数据样本:</p>
<pre>new_observations = sequences(100,env) <br/>cols.name &lt;- c("State","Action","NextState")<br/>new_observations[cols.name] &lt;- sapply(new_observations[cols.name],as.character)<br/>sapply(new_observations, class)<br/>head(new_observations)</pre>
<p>以下截图显示了新观测数据中的一些记录:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1191 image-border" src="img/9426e0f0-8b52-4efa-bb9c-2b6b55106749.png" style="width:19.92em;height:22.17em;"/></p>
<p class="mce-root">现在，我们提供我们现有的RL模型，这是我们在<em>如何做中创建的...</em>此配方的一部分，作为更新现有策略的一个参数。</p>
<p>下面的屏幕截图显示了在实现体验重放之后，每个状态-动作对的Q值表:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1189 image-border" src="img/8c74e137-e9e8-4b5b-ad0c-2b12b0981fa1.png" style="width:39.08em;height:41.17em;"/></p>
<p>在前面的屏幕截图中，我们可以看到，与之前的策略相比，更新后的策略获得了更高的总体回报。</p>


            

            
        
    


</body></html>