

# 六、LSTM、GRU 和高级 RNN

概观

在本章中，我们将研究和实现普通**循环神经网络** ( **RNN** )的高级模型和变体，它们克服了 RNNs 的一些实际缺点，是目前表现最好的深度学习模型之一。我们将从理解普通 RNNs 的缺点开始，看看**长短期记忆**的新颖想法如何克服它们。然后我们将看到并实现一个基于**门控循环单元**的模型。我们也将使用双向和堆叠的 rnn，并探索基于注意力的模型。到本章结束时，你将已经在情感分类任务中建立并评估了这些模型的性能，并亲自观察了选择不同模型的利弊。

# 简介

假设您正在处理一部手机的产品评论，您的任务是将评论中的情绪分为积极或消极。你看到一篇评论说:*“这款手机没有出色的摄像头，没有令人惊叹的生动显示屏，没有出色的电池续航时间，没有出色的连接能力，也没有其他让它成为最佳的出色功能。”*现在，当你阅读这篇文章时，你可以很容易地识别出评论中的情绪是负面的，尽管存在许多积极的短语，如*“出色的电池续航时间”*和*“使其成为最好的”*。你要明白，文本开头出现的术语*“not”*否定了后面的所有内容。

到目前为止，我们创建的模型能够识别这种情况下的情绪吗？可能不会，因为如果您的模型没有意识到句子开头的术语*【not】*很重要，需要在几个术语之后与输出紧密连接，它们将无法正确识别情感。不幸的是，这是普通 rnn 的一个主要缺点。

在前一章中，我们看了几种处理序列的深度学习方法，即一维卷积和 RNNs。我们看到，rnn 是非常强大的模型，为我们处理不同的序列任务提供了极大的灵活性。我们看到的普通 rnn 已经过大量研究。现在，我们将看看一些建立在 RNNs 之上的方法，以创建新的、强大的模型来克服 RNNs 的缺点。我们将看看 LSTM，GRUs，堆叠和双向 lstm，以及基于注意力的模型。我们将这些模型应用于情感分类任务，从而将*第 4 章，文本嵌入深度学习*和*第 5 章，序列深度学习*中讨论的概念结合在一起。

# 长期依赖/影响

我们在上一节看到的手机评论样本是一个长期依赖/影响的例子——一个序列中的术语/值会对许多后续术语/值的评估产生影响。考虑下面的例子，你需要在空白处填写一个缺少的国家名称:*“在德国一所顶尖大学批准她攻读牙科硕士学位后，Hina 非常兴奋地开始了她职业生涯的这个新阶段，并与国际接轨，她迫不及待地等到月底预订了飞往 ____ 的航班。”*

当然，正确答案是**德国**，要得出这个答案，你需要理解出现在句首的术语“德国”对句尾结果的重要性。这是另一个长程依赖的例子。下图显示了答案“*德*”对出现在句子早期的术语“*德*”的长程依赖关系:

![Figure 6.1: Long-range dependence
](img/B15385_06_01.jpg)

图 6.1:长程相关性

为了获得最佳结果，我们需要能够处理长期依赖关系。在深度学习模型和 RNNs 的背景下，这将意味着学习(或错误的反向传播)需要在许多时间步骤内平稳有效地发生。这说起来容易做起来难，主要是因为渐变消失的问题。

# 消失梯度问题

训练标准前馈深度神经网络的最大挑战之一是消失梯度问题(如第 2 章*神经网络*所述)。随着模型的层数越来越多，将误差反向传播回初始层变得越来越困难。靠近输出的层将快速“学习”/更新，但是当误差传播到初始层时，其值将大大减小，并且对初始层的参数影响很小或没有影响。

对于 RNNs，这个问题变得更加复杂，因为参数不仅需要沿深度更新，还需要按时间步长更新。如果我们在输入中有 100 个时间步长(这并不罕见，尤其是在处理文本时)，网络需要将误差(在第 100 个时间步长计算)一直传播回第一个时间步长。对于普通的 rnn 来说，这个任务可能有点难以处理。这就是 RNN 变种派上用场的地方。

注意

训练深度网络的另一个实际问题是爆炸梯度问题，梯度值变得非常高，高到系统无法表示。这个问题有一个相当简单的解决方法，叫做“**渐变裁剪**”，这意味着限制渐变的值。

# 用于文本分类的序列模型

在*第 5 章，序列的深度学习*中，我们了解到 rnn 在序列建模任务中表现非常好，并且在文本相关任务中提供高性能。在本章中，我们将在情感分类任务中使用普通的 RNNs 和 RNNs 的变体:处理输入序列并预测情感是积极的还是消极的。

在此任务中，我们将使用 IMDb 评论数据集。该数据集包含 50，000 条电影评论及其情感——25，000 条用于训练的高度极性电影评论和 25，000 条用于测试的电影评论。使用该数据集的几个原因如下:

*   用一个命令加载 Keras(标记化版本)非常方便。
*   数据集通常用于测试新的方法/模型。这将有助于您轻松地将结果与其他方法进行比较。
*   数据中较长的序列(IMDb 综述可能会很长)有助于我们更好地评估 RNNs 变体之间的差异。

让我们从使用普通 RNNs 构建我们的第一个模型开始，然后将未来的模型性能与普通 RNN 的性能进行比较。让我们从数据预处理和格式化模型开始。

## 加载数据

注意

确保在同一个 Jupyter 笔记本上完成本章中的所有练习和示例代码。请注意，本节中的代码将加载数据集。为了确保接下来的所有练习和示例代码都能正常工作，请确保不要跳过这一部分。你可以在[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)获得练习的完整代码。

首先，您需要启动一个新的 Jupyter 笔记本，并从 Keras 数据集导入`imdb`模块。请注意，除非另有说明，本章剩余部分的代码和练习应在同一个 Jupyter 笔记本中继续:

```py
from tensorflow.keras.datasets import imdb
```

导入模块后，导入数据集(标记化并分成训练集和测试集)就像运行`imdb.load_data`一样简单。我们需要提供的唯一参数是我们希望使用的词汇量。回想一下，词汇表大小是我们希望在建模过程中考虑的唯一术语的总数。当我们指定一个词汇规模时，我们使用数据中最大的词汇。这里，我们将为我们的模型指定 8000 的词汇大小(任意选择；您可以根据需要进行修改)并使用`load_data`方法加载数据，如下所示:

```py
vocab_size = 8000
(X_train, y_train), (X_test, y_test) = imdb.load_data\
                                       (num_words=vocab_size)
```

让我们检查一下`X_train`变量，看看我们在处理什么。让我们打印出它的类型和组成元素的类型，并看看其中的一个元素:

```py
print(type(X_train))
print(type(X_train[5]))
print(X_train[5])
```

我们将看到以下输出:

```py
<class 'numpy.ndarray'>
<class 'list'>
[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 
 2, 32, 85, 156, 45, 40, 
 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 
 3804, 8, 4, 226, 65,
 12, 43, 127, 24, 2, 10, 10]
```

`X_train`变量是一个`numpy`数组——数组的每个元素都是一个列表，代表单个评论的文本。文本中的术语以数字标记而不是原始标记的形式出现。这是一种非常方便的格式。

下一步是定义我们将使用的序列的长度上限，并将所有序列限制在定义的最大长度内。我们将使用`200`——在本例中是任意选择——快速开始建模过程的`200`步骤，这样网络不会变得太重，并且因为`200`时间步骤足以展示不同的 RNN 方法。让我们定义一下`maxlen`变量:

```py
maxlen = 200
```

下一步是使用 Keras 的`pad_sequences`工具让所有的序列长度相同。

注意

***** 理想情况下，我们会分析序列的长度，并确定一个覆盖大多数评论的序列。我们将在本章末尾的活动中执行这些步骤，其中我们不仅会使用当前章节的想法，还会使用*第 4 章“文本嵌入的深度学习”*和*第 5 章“序列的深度学习”*中的想法，将所有这些都集中在一个活动中。

## 暂存和预处理我们的数据

Keras 中的`sequences`模块中的`pad_sequences`实用程序帮助我们将所有序列转换到指定的长度。如果输入序列比指定的长度短，实用程序会用保留标记填充序列(表示空白/缺失)。如果输入序列长于指定长度，实用程序会截断序列以限制它。在下面的例子中，我们将把`pad_sequences`实用程序应用于我们的测试和训练数据集:

```py
from tensorflow.keras import preprocessing
X_train = preprocessing.sequence.pad_sequences\
          (X_train, maxlen=maxlen)
X_test = preprocessing.sequence.pad_sequences\
         (X_test, maxlen=maxlen)
```

为了理解这些步骤的结果，让我们来看看训练数据中特定实例的输出:

```py
print(X_train[5])
```

处理后的实例如下:

![Figure 6.2: Result of pad_sequences
](img/B15385_06_02.jpg)

图 6.2:填充序列的结果

我们可以看到结果开头有大量的`0`。您可能已经推断出，这是由`pad_sequence`实用程序完成的填充，因为输入序列比`200`短。序列开头的填充是该实用工具的默认行为。对于小于指定限制的序列，缺省情况下，截断从左侧开始，即保留最后的`200`项。输出中的所有实例现在都有了`200`术语。数据集现在可以进行建模了。

注意

该实用程序的默认行为是填充序列的开头并从左侧截断。这些可能是重要的超参数。如果您认为前几项对预测最重要，您可能希望通过将"`truncating`"参数指定为"`post`"来截断最后几项。类似地，要在序列末尾填充，可以将“`padding`”设置为“`post`”。

# 嵌入层

在*第 4 章，文本嵌入的深度学习*中，我们讨论了我们不能将文本直接输入神经网络，因此需要良好的表示。我们讨论过嵌入(低维、密集的向量)是表示文本的一种很好的方式。为了将嵌入传递到神经网络的层中，我们需要使用嵌入层。

嵌入层的功能是双重的:

*   对于任何输入术语，执行查找并返回其词嵌入/向量
*   在训练中，学习这些单词的嵌入

关于查找的部分很简单——词嵌入被存储为一个`V × D`维度的矩阵，其中`V`是词汇量(考虑的唯一术语的数量),而`D`是每个向量的长度/维度。下图说明了嵌入层。输入项“`life`”被传递到嵌入层，嵌入层执行查找并返回相应的长度为`D`的向量。这个向量是术语`life`的表示，被输入到隐藏层。

我们在训练预测模型的同时学习这些嵌入是什么意思？词嵌入不是通过使用诸如`word2vec`之类的算法来学习的吗，该算法试图根据上下文术语来预测中心单词(还记得我们在*第 4 章，文本嵌入的深度学习*中讨论的 CBOW 架构吗)？嗯，是也不是:

![Figure 6.3: Embedding layer
](img/B15385_06_03.jpg)

图 6.3:嵌入层

`word2vec`方法的目标是学习捕捉术语含义的表示法。因此，基于上下文预测目标单词是实现目标的完美公式。在我们的例子中，目标是不同的——我们希望学习能帮助我们最好地预测文本中情感的表达。那么，学习明确地对我们的目标起作用的表征是有意义的。

嵌入层始终是模型中的第一层。您可以根据自己的选择使用任何架构(在我们的例子中是 RNNs)。我们随机初始化向量，本质上是嵌入层中的权重。当模型训练时，权重以更好地预测结果的方式更新。然后，学习到的权重以及单词向量被调整到该任务。这是一个非常有用的步骤——当您可以将通用表示调整到您的任务时，为什么还要使用它们呢？

Keras 中的嵌入层有两个主要参数:

*   `input_dim`:词汇中唯一术语的数量，即词汇的大小
*   `output_dim`:嵌入的维度/词向量的长度

`input_dim`参数需要被设置为所使用的词汇量。`output_dim`参数指定了每一项的嵌入向量的长度。

请注意，Keras 中的嵌入层还允许您在嵌入层中使用自己指定的权重矩阵。这意味着你可以在嵌入层中使用预先训练的嵌入(比如`GloVe`，或者甚至是你在不同模型中训练的嵌入)。`GloVe`模型已经在数十亿个令牌上进行了训练，利用这种强大的通用表示可能会很有用。

注意

如果您使用预先训练的嵌入，您还可以选择使它们在您的模型中可训练——本质上，使用`GloVe`嵌入作为起点，并针对您的任务对它们进行微调。这是文本迁移学习的一个很好的例子。

# 建立平原 RNN 模型

在下一个练习中，我们将使用简单的 RNNs 构建我们的第一个情感分类任务模型。下图描述了我们将使用的模型架构，它展示了模型将如何处理一个输入句子"`Life is good`"，术语"`Life`"出现在时间步`T=0`，"`good`"出现在时间步`T=2`。该模型将逐个处理输入，使用嵌入层来查找将被传递到隐藏层的词嵌入。当在时间步`T=2`处理最后一项`good`时，将完成分类。我们将使用 Keras 来构建和训练我们的模型:

![Figure 6.4: Architecture using an embedding layer and RNN
](img/B15385_06_04.jpg)

图 6.4:使用嵌入层和 RNN 的架构

## 练习 6.01:构建并训练一个用于情感分类的 RNN 模型

在本练习中，我们将构建并训练一个用于情感分类的 RNN 模型。最初，我们将定义递归层和预测层的架构，并且我们将评估模型在测试数据上的性能。我们将添加嵌入层和一些辍学，并完成模型定义添加 RNN 层，辍学，和一个密集层完成。然后，我们将检查测试数据预测的准确性，以评估模型的泛化能力。按照以下步骤完成本练习:

1.  Let's begin by setting the seed for `numpy` and `tensorflow` random number generation, to get, to the best extent possible, reproducible results. We'll import `numpy` and `tensorflow` and set the seed using the following commands:

    ```py
    import numpy as np
    import tensorflow as tf
    np.random.seed(42)
    tf.random.set_seed(42)
    ```

    注意

    尽管我们已经为`numpy`和`tensorflow`设定了种子以获得可重复的结果，但仍有更多的原因导致变化，因此您可能会得到与我们不同的结果。这适用于我们从现在开始使用的所有模型。虽然您看到的值可能不同，但您看到的输出应该与我们的基本一致。如果模型的性能非常不同，您可能需要调整历元的数量-原因是神经网络中的权重是随机初始化的，因此您和我们的起点可能会略有不同，当训练不同数量的历元时，我们可能会达到类似的位置。

2.  现在，让我们继续导入所有必需的包和层，并使用以下命令初始化一个名为`model_rnn`的顺序模型:

    ```py
    from tensorflow.keras.models import Sequential from tensorflow.keras.layers \ import SimpleRNN, Flatten, Dense, Embedding, \ SpatialDropout1D, Dropout model_rnn = Sequential()
    ```

3.  Now, we need to specify the embedding layer. The `input_dim` parameter needs to be set to the `vocab_size` variable. For the `output_dim` parameter, we'll choose `32`. Recall from *Chapter 4, Deep Learning for Text – Embeddings*, that this is a hyperparameter and you may want to experiment with this to get better results. Let's specify the embedding layer and use dropout (to minimize overfitting) using the following commands:

    ```py
    model_rnn.add(Embedding(vocab_size, output_dim=32))
    model_rnn.add(SpatialDropout1D(0.4))
    ```

    请注意，这里使用的 dropout 是`SpatialDropout1D`–这个版本执行与常规 dropout 层相同的功能，但它不是删除单个元素，而是删除整个一维特征图(在我们的例子中是向量)。

4.  给模型添加一个带有`32`神经元的`SimpleRNN`层(任意选择；要调整的另一个超参数):

    ```py
    model_rnn.add(SimpleRNN(32))
    ```

5.  接下来，用`40%` dropout 添加一个 dropout 层(再次，任意选择):

    ```py
    model_rnn.add(Dropout(0.4))
    ```

6.  添加带有`sigmoid`激活功能的密集层，完成模型架构。这是进行预测的输出层:

    ```py
    model_rnn.add(Dense(1, activation='sigmoid'))
    ```

7.  Compile the model and view the model summary:

    ```py
    model_rnn.compile(loss='binary_crossentropy', \
                      optimizer='rmsprop', metrics=['accuracy'])
    model_rnn.summary()
    ```

    模型总结如下:

    ![Figure 6.5: Summary of the plain RNN model
    ](img/B15385_06_05.jpg)

    图 6.5:简单 RNN 模型总结

    我们可以看到有`258,113`个参数，大部分存在于嵌入层。这是因为词嵌入是在训练过程中学习的，所以我们正在学习嵌入矩阵，它具有维度`vocab_size(8000) × output_dim(32)`。

    让我们继续对模型进行训练(使用我们观察到的超参数来提供该数据和架构的最佳结果)。

8.  Fit the model on the train data with a batch size of `128` for `10` epochs (both of these are hyperparameters that you can tune). Use a validation split of `0.2` – monitoring this will give us a sense of the model performance on unseen data:

    ```py
    history_rnn = model_rnn.fit(X_train, y_train, \
                                batch_size=128, \
                                validation_split=0.2, \
                                epochs = 10)
    ```

    最后五个时期的训练输出如下。根据您的系统配置，此步骤所需的时间可能会比我们这里所用的时间多或少:

    ![Figure 6.6: Training the plain RNN model – the final five epochs
    ](img/B15385_06_06.jpg)

    图 6.6:训练普通 RNN 模型——最后五个纪元

    从训练输出可以看出，验证准确率达到了 86%左右。让我们在测试集上进行预测，并检查模型的性能。

9.  Make predictions on the test data using the `predict_classes` method of the model and use the `accuracy_score` method from `sklearn`:

    ```py
    y_test_pred = model_rnn.predict_classes(X_test)
    from sklearn.metrics import accuracy_score
    print(accuracy_score(y_test, y_test_pred))
    ```

    测试的准确度如下:

    ```py
    0.85128
    ```

    我们可以看到模型做了一个体面的工作。我们使用了一个简单的架构，有`32`个神经元，使用的词汇量只有`8000`。调整这些和其他超参数可能会得到更好的结果，我们鼓励你这样做。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。

    你也可以在[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在这个练习中，我们看到了如何建立一个基于 RNN 的文本模型。我们看到了如何使用嵌入层来为手头的任务导出单词向量。这些词向量是每个输入词的表示，被传递到 RNN 层。我们已经看到，即使一个简单的架构也能给我们带来好的结果。现在，让我们讨论如何使用这个模型来预测新的、看不见的评论。

# 根据看不见的数据做出预测

现在，您已经在一些数据上训练了您的模型，并评估了它在测试数据上的性能，接下来的事情是学习如何使用这个模型来预测新数据的情绪。毕竟，这就是该模型的目的——能够预测该模型以前看不到的数据的情绪。本质上，对于任何以原始文本形式出现的新评论，我们都应该能够对其情绪进行分类。

其中的关键步骤是创建一个流程/管道，将原始文本转换为预测模型能够理解的格式。这意味着新文本需要经过与用于训练模型的文本数据完全相同的预处理步骤。用于预处理的函数需要返回任何输入原始文本的格式化文本。该功能的复杂性取决于对列车数据执行的步骤。如果标记化是唯一执行的预处理步骤，那么函数只需要执行标记化。

我们的模型(`model_rnn`)是在 IMDb 评论上训练的，这些评论被标记化，降低了大小写，删除了标点符号，有一个定义的词汇大小，并被转换成一系列的索引。我们为 RNN 模型准备数据的函数/管道需要执行相同的步骤。让我们努力创建我们自己的函数。首先，让我们使用下面的代码创建一个名为“`inp_review`”的新变量，其中包含文本“*a excellent movie*”。这是包含原始审阅文本的变量:

```py
inp_review = "An excellent movie!"
```

文中的情绪是积极的。如果模型运行得足够好，它应该会预测情绪是积极的。

首先，我们必须将该文本标记为其组成术语，规范其大小写，并删除标点符号。为此，我们需要使用以下代码从 Keras 导入`text_to_word_sequence`实用程序:

```py
from tensorflow.keras.preprocessing.text \
import text_to_word_sequence
```

为了检查它是否如我们预期的那样工作，我们可以将它应用到`inp_review`变量，如下面的代码所示:

```py
text_to_word_sequence(inp_review)
```

标记化的句子将如下:

```py
['an', 'excellent', 'movie']
```

我们可以看到它正如预期的那样工作——大小写被规范化，句子被标记化，标点符号从输入文本中被删除。下一步是为数据使用定义好的词汇表。这需要使用 TensorFlow 在加载数据时使用的相同词汇。词汇表和术语到索引的映射可以使用来自`imdb`模块的`get_word_index`方法加载(我们用来加载代码)。以下代码可用于将词汇表加载到名为`word_map`的字典中:

```py
word_map = imdb.get_word_index()
```

该字典包含原始评论数据中可用的大约 88.6 K 个术语的映射。我们加载词汇大小为`8000`的数据，从而使用映射中的第一个`8000`索引。让我们用有限的词汇创建映射，以便我们可以使用训练数据使用的相同术语/索引。我们将通过对索引上的`word_map`变量进行排序并挑选第一个`8000`术语来将映射限制到`8000`术语，如下所示:

```py
vocab_map = dict(sorted(word_map.items(), \
                 key=lambda x: x[1])[:vocab_size])
```

vocab 映射将是一个字典，包含词汇表中`8000`术语的索引映射术语。使用这种映射，我们将通过对每个术语执行查找并返回相应的索引，将标记化的句子转换成一系列术语索引。使用下面的代码，我们将定义一个接受原始文本的函数，对其应用`text_to_word_sequence`实用程序，从`vocab_map`执行查找，并返回相应的整数序列:

```py
def preprocess(review):
    inp_tokens = text_to_word_sequence(review)
    seq = []
    for token in inp_tokens:
        seq.append(vocab_map.get(token))
    return seq
```

我们可以将该函数应用于`inp_review`变量，如下所示:

```py
preprocess(inp_review)
```

输出如下所示:

```py
[32, 318, 17]
```

这是对应于原始文本的术语索引序列。请注意，现在数据的格式与我们加载的 IMDb 数据的格式相同。这个指数序列可以输入到 RNN 模型(使用`predict_classes`方法)来对情绪进行分类，如下面的代码所示。如果模型运行得足够好，它应该预测情绪是积极的:

```py
model_rnn.predict_classes([preprocess(inp_review)])
```

输出预测是`1`(正)，正如我们所料:

```py
array([[1]])
```

让我们将该函数应用于另一个原始文本评论，并将其提供给模型进行预测。让我们更新`inp_review`变量，使其包含文本"`Don't watch this movie – poor acting, poor script, bad direction.`"评论中的情绪是负面的。我们预计模型会对其进行如下分类:

```py
inp_review = "Don't watch this movie"\
             " - poor acting, poor script, bad direction."
```

让我们将我们的预处理函数应用于`inp_review`变量，并使用以下代码进行预测:

```py
model_rnn.predict_classes([preprocess(inp_review)])
```

预测是`0`，如下图所示:

```py
array([[0]])
```

预测的情绪是负面的，正如我们对模型行为的预期。

我们以函数的形式将这个管道应用于单个评论，但是您可以非常容易地将它应用于整个评论集合，以使用该模型进行预测。现在，您可以使用我们训练的 RNN 模型对任何新评论的观点进行分类了。

注意

我们在这里构建的管道是专门针对这个数据集和模型的。这不是一个可用于任何模型预测的通用处理函数。使用的词汇、完成的清理、模型学习的模式——这些都是特定于这个任务和数据集的。对于任何其他模型，您需要相应地创建您的管道。

更高级的方法也可以用于为其他模型制作处理流水线。根据数据、预处理步骤以及模型部署位置的设置，管道可能会有所不同。所有这些因素也会影响您可能希望在模型构建过程中包含的步骤。因此，我们鼓励您在开始整个建模过程时就开始考虑这些方面。

我们看到了如何使用经过训练的 RNN 模型对未知数据进行预测，从而让我们了解端到端流程。在下一节中，我们将开始研究 RNNs 的变体。到目前为止，我们讨论的与实现相关的思想适用于所有后续的模型。

# LSTMs、gru 和其他变体

普通 RNNs 背后的想法是非常强大的，并且该架构已经显示出巨大的前景。由于这个原因，研究人员已经对 RNNs 的架构进行了实验，以找到克服一个主要缺点(消失梯度问题)的方法，并利用 RNNs 的能力。这导致了 LSTMs 和 gru 的发展，它们现在实际上已经取代了 rnn。事实上，这些天来，当我们提到 RNNs 时，我们通常指 LSTMs、gru 或它们的变体。

这是因为这些变体是专门为处理消失梯度问题和学习长程相关性而设计的。在大多数序列建模任务中，这两种方法都显著优于普通的 RNNs，对于长序列，这种差异尤其大。名为*使用用于统计机器翻译的 RNN 编码器-解码器学习短语表示*(可在[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)获得)的论文对普通 rnn、LSTMs 和 gru 的性能进行了实证分析。这些方法是如何克服普通 rnn 的缺点的？我们将在下一节详细讨论 LSTMs 时理解这一点。

## lstm

让我们思考一下这个问题。了解了普通 RNN 的建筑，我们如何调整它，或者可以做些什么来捕捉长期的影响？我们不能增加更多的层；这肯定会适得其反，因为每增加一层都会使问题复杂化。Sepp Hochreiter 和 Jurgen Schmidhuber 在 1997 年提出的一个想法(在 https://pubmed.ncbi.nlm.nih.gov/9377276 可用)是使用一个不通过激活的显式值(状态)。如果我们有一个细胞(对应于普通 RNNs 的一个神经元)值自由流动，而不是通过激活，这个值可能有助于我们建模长期依赖。这是 LSTM 的第一个关键区别——显式细胞状态。

单元状态可以被认为是在多个时间步长上识别和存储信息的一种方式。本质上，我们将一些值识别为网络的长期记忆，帮助我们更好地预测输出，并根据需要尽可能长地保留这些值。

但是我们如何调节这种细胞状态的流动呢？我们如何决定何时更新值以及更新多少？为此，Hochreiter 和 Schmidhuber 提出使用*门控机制*作为一种方式来调节如何以及何时更新单元状态的值。这是 LSTM 的另一个关键区别。自由流动的细胞状态，加上调节机制，使 LSTM 在较长的序列上表现得非常好，并为其提供了所有的预测能力。

注意

对 LSTM 内部运作和相关数学的详细论述超出了本书的范围。对于那些有兴趣进一步阅读的人来说，https://packt.live/3gL42Ib 的[是一个很好的参考，它提供了对 LSTMs 的很好的视觉理解。](https://packt.live/3gL42Ib)

让我们来理解 LSTM 工作背后的直觉。下图显示了 LSTM 单元的内部结构。除了通常的输出，即隐藏状态，LSTM 单元还输出一个单元“状态”。隐藏状态保存短期记忆，而细胞状态保存长期记忆:

![Figure 6.7: The LSTM cell
](img/B15385_06_07.jpg)

图 6.7:LSTM 细胞

这种内部视图可能令人生畏，这就是为什么我们要看一个更抽象的视图，如图 6.8 中的*所示。首先要注意的是，发生在单元状态上的唯一运算是两个线性运算——一个乘法和一个加法。单元状态不通过任何激活函数。这就是为什么我们说细胞状态自由流动。这种自由流动的设置也被称为“恒定误差转盘”——你不需要记住这个名字。*

`FORGET`模块的输出乘以单元状态。因为这个模块的输出在`0`和`1`之间(由 sigmoid 激活来模拟)，所以这个与单元状态的乘积将调节有多少先前的单元状态将被遗忘。如果`FORGET`模块输出`0`，则之前的单元状态被完全遗忘；而对于输出`1`，单元状态被完全保留。注意，`FORGET`门的输入是来自前一时间步(`h` t-1)的隐藏层的输出和当前时间步的新输入，`x` t(对于网络中的深层，这可能是来自前一隐藏层的输出):

![Figure 6.8: Abstracted view of the LSTM cell
](img/B15385_06_08.jpg)

图 6.8:LSTM 单元的抽象视图

在上图中，我们可以看到单元格状态乘以`FORGET`块的结果后，下一个决定是单元格状态更新多少。这来自于`UPDATE`模块的输出，该输出被添加(注意加号)到已处理的单元状态。这样，已处理的单元状态被更新。这就是对先前单元状态`(C` t-1 `)`执行的所有操作，以给我们新的单元状态`(C` t `)`作为输出。这就是细胞的长期记忆是如何被调节的。单元格还需要更新隐藏状态。该操作发生在`OUTPUT`块中，与普通 RNN 中的更新非常相似。唯一的区别是，显式单元状态乘以 sigmoid 的输出，形成最终的隐藏状态，`h` t。

现在我们已经了解了各个区块/门，让我们在下面的详细图上看到它们的标记。这将阐明这些门控机制如何共同调节 LSTM 中的信息流:

![Figure 6.9: The LSTM cell explained
](img/B15385_06_09.jpg)

图 6.9:LSTM 细胞解释

为了使这个例子更具体，让我们看看下图，了解单元格状态是如何更新的。我们可以假设先前的单元状态`(C` t-1 `)`是`5`。这个值应该传播多少由`FORGET`门的输出决定。`FORGET`门的输出值乘以先前的单元状态`C` t-1。在这种情况下，遗忘模块的输出是`0.5`，导致`2.5`作为被处理的单元状态被通过。该值(`2.5`)然后遇到来自`UPDATE`门的加法。由于`-0.8`的`UPDATE`门输出值，相加的结果是`1.7`。这是最终的、更新的单元状态，`C` t，其被传递到下一个时间步骤:

![Figure 6.10: LSTM cell state update example
](img/B15385_06_10.jpg)

图 6.10: LSTM 单元状态更新示例

# LSTM 中的参数

LSTMs 建立在普通 rnn 上。如果您简化了 LSTM，删除了所有的门，只保留了用于隐藏状态更新的双曲正切函数，您将得到一个简单的 RNN。信息(在时间`t`的新输入数据和在时间`t-1` ( `x` t 和`h` t-1)的先前隐藏状态)在 LSTM 中通过的激活次数是它在普通 RNN 中通过次数的四倍。激活在遗忘门中应用一次，在更新门中应用两次，在输出门中应用一次。因此，LSTM 中权重/参数的数量是普通 RNN 中参数数量的四倍。

在*第 5 章*、*序列深度学习、*RNN 中的*参数一节中，我们计算了一个普通 RNN 中的参数数量，看到我们已经有了相当多的参数可以使用(`n` 2 `+ nk + nm`，其中`n`是隐含层神经元的数量，`m`是输入的数量，`k`是输出层的维度)。使用 LSTMs，我们看到这个数字是这个的四倍。不用说，LSTM 中有很多参数，这不一定是件好事，尤其是在处理较小的数据集时。*

## 练习 6.02:基于 LSTM 的情感分类模型

在本练习中，我们将构建一个简单的基于 LSTM 的模型，根据我们的数据预测情绪。我们将继续使用之前使用的相同设置(即，单元数量、嵌入尺寸、漏失等)。因此，你必须在同一个 Jupyter 笔记本上继续这个练习。按照以下步骤完成本练习:

1.  从 Keras 导入 LSTM 层`layers` :

    ```py
    from tensorflow.keras.layers import LSTM
    ```

2.  实例化序列模型，添加具有适当尺寸的嵌入层，并添加 40%的空间漏失:

    ```py
    model_lstm = Sequential() model_lstm.add(Embedding(vocab_size, output_dim=32)) model_lstm.add(SpatialDropout1D(0.4))
    ```

3.  添加一个带有`32`单元格的 LSTM 图层:

    ```py
    model_lstm.add(LSTM(32))
    ```

4.  Add the dropout (`40%` dropout) and dense layers, compile the model, and print the model summary:

    ```py
    model_lstm.add(Dropout(0.4))
    model_lstm.add(Dense(1, activation='sigmoid'))
    model_lstm.compile(loss='binary_crossentropy', \
                       optimizer='rmsprop', metrics=['accuracy'])
    model_lstm.summary()
    ```

    模型总结如下:

    ![Figure 6.11: Summary of the LSTM model
    ](img/B15385_06_11.jpg)

    图 6.11:LSTM 模型概述

    从模型摘要中我们可以看到，LSTM 层的参数个数是`8320`。快速检查可以确认，这正好是我们在*练习 6.01、* *建立和训练用于情感分类的 RNN 模型*中看到的普通 RNN 层中参数数量的四倍，这符合我们的预期。接下来，让我们在训练数据上拟合模型。

5.  Fit on the training data for `5` epochs (this gives us the best result for the model) with a batch size of `128`:

    ```py
    history_lstm = model_lstm.fit(X_train, y_train, \
                                  batch_size=128, \
                                  validation_split=0.2, \
                                  epochs=5)
    ```

    培训过程的输出如下:

    ![Figure 6.12: LSTM training output
    ](img/B15385_06_12.jpg)

    图 6.12: LSTM 培训产出

    请注意，训练 LSTM 比训练普通 rnn 花费的时间要长得多。再次，考虑到 LSTM 的架构和参数的数量，这是意料之中的。此外，请注意，验证精度明显高于普通 RNN。让我们根据准确性分数来检查测试数据的性能。

6.  Make predictions on the test set and print the accuracy score:

    ```py
    y_test_pred = model_lstm.predict_classes(X_test)
    print(accuracy_score(y_test, y_test_pred))
    ```

    精确度打印如下:

    ```py
    0.87032
    ```

    我们得到的准确度(87%)比使用普通 RNNs 得到的准确度(85.1%)有了显著的提高。看起来细胞状态的额外参数和额外预测能力对我们的任务很有用。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。

    你也可以在 https://packt.live/2Oa2trm 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2Oa2trm)

在这个练习中，我们看到了如何使用 LSTMs 对文本进行情感分类。训练时间明显更长，参数数量也更多。但是最终，即使这个简单的架构(没有任何超参数调整)也比普通的 RNN 给出了更好的结果。鼓励您进一步调整超参数，以充分利用强大的 LSTM 架构。

# LSTM 对平原 RNNs

我们看到 LSTMs 建立在普通 rnn 之上，其主要目标是解决消失梯度问题，以便能够对长程相关性进行建模。下图告诉我们，普通 RNN 只通过隐藏状态(短期记忆)，而 LSTM 通过隐藏状态和显式单元状态(长期记忆)，从而赋予它更大的能力。因此，当术语“`good`”在 LSTM 中被处理时，循环层也传递持有长期记忆的细胞状态:

![Figure 6.13: Plain RNNs (left) and LSTMs (right)
](img/B15385_06_13.jpg)

图 6.13:普通 rnn(左)和 LSTMs(右)

在实践中，这是否意味着你总是需要一个 LSTM？这个问题的答案，就像数据科学尤其是深度学习中的大多数问题一样，是“视情况而定”。为了理解这些考虑，我们需要理解 LSTMs 与普通 rnn 相比的优点和缺点。

**lst ms 的优势:**

*   更强大，因为它使用了更多的参数和明确的单元状态
*   更好地模拟长期依赖关系

**lstm 的缺点:**

*   更多参数
*   需要更多的时间来训练
*   更容易过度拟合

如果你有很长的序列要处理，LSTM 将是一个不错的选择。如果您有一个小数据集，并且您正在处理的序列很短(<10), then you're probably okay to use a plain RNN, owing to there being a lower number of parameters (although you could also try LSTMs, making sure to use regularization to avoid overfitting). A larger dataset with long sequences would probably extract the most out of powerful models such as LSTMs. Note that training LSTMs is computationally expensive and time-consuming, so if you have an extremely large dataset, training LSTMs may not be the most practical approach. Of course, all these statements should serve merely as guidance – the best approach would be what works best for your data and your task.

# 门控循环单元)

在上一节中，我们看到 LSTMs 有许多参数，似乎比常规 RNN 复杂得多。你可能想知道，所有这些明显的复杂情况真的有必要吗？LSTM 能否在不丧失显著预测能力的情况下简化一点？研究人员对此也有过一段时间的疑惑，2014 年，Kyunghyun Cho 和他们的团队在他们关于机器翻译的论文([https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078))中提出了 GRU 作为 LSTMs 的替代品。

gru 是 LSTM 的简化形式，旨在减少参数数量，同时保留 LSTM 的功能。在围绕语音建模和语言建模的任务中，GRUs 提供与 LSTMs 相同的性能，但是具有更少的参数和更快的训练时间。

GRU 中的一个主要简化是省略了显式单元状态。考虑到自由流动的细胞状态赋予了 LSTM 的力量，这听起来有些违反直觉，对吗？真正赋予 LSTMs 所有权力的是细胞状态的自由流动性质，而不是细胞状态本身？事实上，如果细胞状态也受到激活的影响，LSTMs 可能不会取得成功:

![Figure 6.14: Gated Recurrent Unit
](img/B15385_06_14.jpg)

图 6.14:门控循环单元

因此，自由流动的价值观是关键的差异化理念。格鲁保留了这个想法，允许隐藏状态自由流动。让我们看一下上图来理解这意味着什么。GRUs 允许隐藏状态自由通过。另一种看待这个问题的方式是，GRUs 有效地将单元状态的概念(如在 LSTMs 中)引入了隐藏状态。

然而，我们仍然需要调节隐藏状态的流动，所以我们仍然有门。GRUs 将遗忘门和更新门合并成一个更新门。为了理解这背后的动机，考虑一下——如果我们忘记了一个细胞状态，并且没有更新它，我们到底在做什么？也许有一个单一的更新操作的优点。这是架构中的第二个主要区别。

这两个变化的结果是，GRUs 使数据通过三次激活，而不是 LSTMs 中的四次，从而减少了参数的数量。虽然 gru 的参数数量仍然是普通 RNN 的三倍，但它们的参数数量是 LSTMs 的 75%，这是一个受欢迎的变化。我们仍然有信息在网络中自由流动，这应该允许我们对长期依赖关系进行建模。

让我们看看一个基于 GRU 的模型如何执行我们的情感分类任务。

## 练习 6.03:基于 GRU 的情感分类模型

在本练习中，我们将构建一个简单的基于 GRU 的模型来预测数据中的情绪。我们将继续使用之前使用的相同设置(即，单元数量、嵌入尺寸、漏失等)。在模型中使用 GRUs 代替 LSTMs，就像在添加层时用“`GRU`”替换“`LSTM`”一样简单。按照以下步骤完成本练习:

1.  从 Keras `layers` :

    ```py
    from tensorflow.keras.layers import GRU
    ```

    导入`GRU`图层
2.  实例化序列模型，添加适当尺寸的嵌入层，添加 40%的空间漏失:

    ```py
    model_gru = Sequential() model_gru.add(Embedding(vocab_size, output_dim=32)) model_gru.add(SpatialDropout1D(0.4))
    ```

3.  添加一个有 32 个像元的 GRU 图层。将`reset_after`参数设置为`False`(这是一个次要的 TensorFlow 2 实现细节，以便与普通 rnn 和 LSTMs 的实现保持一致):

    ```py
    model_gru.add(GRU(32, reset_after=False))
    ```

4.  Add the dropout (40%) and dense layers, compile the model, and print the model summary:

    ```py
    model_gru.add(Dropout(0.4))
    model_gru.add(Dense(1, activation='sigmoid'))
    model_gru.compile(loss='binary_crossentropy', \
                      optimizer='rmsprop', metrics=['accuracy'])
    model_gru.summary()
    ```

    模型总结如下:

    ![Figure 6.15: Summary of the GRU model
    ](img/B15385_06_15.jpg)

    图 6.15:GRU 模型概述

    从 GRU 模型的总结中，我们可以看到 GRU 层的参数个数是`6240`。您可以检查一下，这正好是我们在*练习 6.01、* *构建和训练用于情感分类的 RNN 模型*中看到的普通 RNN 层中的参数数量的三倍，以及`0.75`乘以我们在*练习 6.02、* *中看到的基于 LSTM 的情感分类模型*中看到的 LSTM 层的参数数量，这也符合我们的预期。接下来，让我们在训练数据上拟合模型。

5.  Fit on the training data for four epochs (which gives us the best result):

    ```py
    history_gru = model_gru.fit(X_train, y_train, \
                                batch_size=128, \
                                validation_split=0.2, \
                                epochs = 4)
    ```

    培训过程的输出如下:

    ![Figure 6.16: GRU training output
    ](img/B15385_06_16.jpg)

    图 6.16: GRU 培训产出

    请注意，训练 gru 也比普通 rnn 花费更长时间，但比 LSTMs 更快。验证精度优于平原 RNN，似乎接近 LSTM。让我们看看这个模型在测试数据上表现如何。

6.  Make predictions on the test set and print the accuracy score:

    ```py
    y_test_pred = model_gru.predict_classes(X_test)
    accuracy_score(y_test, y_test_pred)
    ```

    精确度打印如下:

    ```py
    0.87156
    ```

    我们可以看到，GRU 模型的精确度(87.15%)与 LSTM 模型的精确度(87%)非常相似，并且高于普通的 RNN 模型。这是很重要的一点——gru 是 LSTMs 的简化，旨在用更少的参数提供类似的精度。我们在这里的练习证明了这一点。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。

    你也可以在 https://packt.live/2Oa2trm 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2Oa2trm)

在这个练习中，我们看到了如何使用 GRUs 对文本进行情感分类。训练时间略低于 LSTM 模型，参数数量也较少。甚至这种简单的结构(没有任何超参数调整)也给出了比简单的 RNN 模型更好的结果，并且给出了类似于 LSTM 模型的结果。

## LSTM 对 GRU

那么，你应该选择哪一个呢？LSTM 有更多的参数和明确的细胞状态，旨在存储长期记忆。GRU 具有更少的参数，这意味着更快的训练，并且还具有自由流动的单元状态，以允许它对远程依赖性进行建模。

Junyoung Chung、Yoshua Bengio 和他们的团队在 2014 年对音乐建模和语音建模任务进行的实证评估(在[https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)可用)显示，LSTMs 和 GRUs 都明显优于普通的 rnn。他们还发现 gru 在性能方面与 LSTMs 不相上下。他们指出，调整超参数，如层大小，可能比在 LSTM 和格鲁之间做出选择更重要

2018 年，Gail Weiss，Yoav Goldberg 和他们的团队证明并得出结论，LSTMs 在需要无界计数的任务中表现优于 GRUs，即需要处理任意长度序列的任务。谷歌大脑团队在 2018 年也表明，在机器翻译方面，LSTMs 的性能优于 GRUs。这让我们认为 LSTMs 带来的额外功率在某些应用中可能非常有用。

# 双向 RNNs

我们刚刚看到的 RNN 模型——lst ms、GRUs——确实非常强大，并且在序列处理任务方面提供了非常好的结果。现在，让我们讨论如何使它们更加强大，以及在深度学习中产生惊人成功的方法，你已经听说过了。

让我们从双向 RNNs 的概念开始。该思想适用于 RNNs 的所有变体，包括但不限于 LSTMs 和 gru。双向 rnn 在两个方向上处理序列，允许网络具有关于序列的前向和后向信息，为其提供更丰富的上下文:

![Figure 6.17: Bidirectional LSTM
](img/B15385_06_17.jpg)

图 6.17:双向 LSTM

双向模型实质上采用了两个并行的 rnn，一个作为“**正向层**，另一个作为“**反向层**”。如上图所示，转发层按照元素的顺序处理序列。对于句子“*命好*”，前向层会先处理术语“*命*”，后接“*是*”，再接“*命好*”——与通常的 RNN 层没有区别。后向层颠倒了这个顺序——它先处理“好”，然后是“是”，最后是“生活”。在每一步，前向和后向层的状态被连接以形成输出。

什么样的任务从这种架构中受益最大？从上下文的两个方面来看，有助于解决手头这个术语的任何模糊之处。当我们读到像“*星星*”这样的陈述时，我们不确定我们读到的是什么“*星星*”——是天上的星星还是电影明星？但是，当我们看到后面出现的术语并读到“*电影首映式上的明星*”时，我们确信这句话是关于电影明星的。从这种设置中受益最大的任务是机器翻译、词性标注、命名实体识别和单词预测任务等。双向 rnn 也显示了一般文本分类任务的性能增益。让我们将基于双向 LSTM 的模型应用于我们的情感分类任务。

## 练习 6.04:基于双向 LSTM 的情感分类模型

在本练习中，我们将使用双向 LSTMs 来预测数据中的情绪。我们将使用 Keras 的双向包装器在 LSTMs 上创建双向层(您可以通过简单地在包装器中用`GRU`替换`LSTM`来创建双向 GRU 模型)。按照以下步骤完成本练习:

1.  从 Keras `layers`导入`Bidirectional`层。这一层本质上是一个包装器，你可以用它来包装其他 rnn:

    ```py
    from tensorflow.keras.layers import Bidirectional
    ```

2.  实例化序列模型，添加适当尺寸的嵌入层，并添加 40%的空间漏失:

    ```py
    model_bilstm = Sequential() model_bilstm.add(Embedding(vocab_size, output_dim=32)) model_bilstm.add(SpatialDropout1D(0.4))
    ```

3.  给带有`32`单元格的 LSTM 图层添加`Bidirectional`包装:

    ```py
    model_bilstm.add(Bidirectional(LSTM(32)))
    ```

4.  Add the dropout (40%) and dense layers, compile the model, and print the model summary:

    ```py
    model_bilstm.add(Dropout(0.4))
    model_bilstm.add(Dense(1, activation='sigmoid'))
    model_bilstm.compile(loss='binary_crossentropy', \
                         optimizer='rmsprop', metrics=['accuracy'])
    model_bilstm.summary()
    ```

    摘要如下:

    ![Figure 6.18: Summary of the bidirectional LSTM model
    ](img/B15385_06_18.jpg)

    图 6.18:双向 LSTM 模型总结

    注意前面屏幕截图中显示的模型参数。毫不奇怪，双向 LSTM 层有`16640`个参数——是 LSTM 层(`8320`个参数)在*练习 6.02，基于 LSTM 的情感分类模型*中的参数数量的两倍。这是平原 RNN 参数的八倍。接下来，让我们在训练数据上拟合模型。

5.  Fit the training data for four epochs with a batch size of `128`:

    ```py
    history_bilstm = model_bilstm.fit(X_train, y_train, \
                                      batch_size=128, \
                                      validation_split=0.2, \
                                      epochs = 4)
    ```

    培训的结果如下:

    ![Figure 6.19: Bidirectional LSTM training output
    ](img/B15385_06_19.jpg)

    图 6.19:双向 LSTM 训练输出

    注意，正如我们所预料的，训练双向 lstm 比常规 lstm 要长得多，比普通 rnn 长几倍。验证的准确性似乎更接近 LSTM 的准确性。

6.  Make predictions on the test set and print the accuracy score:

    ```py
    y_test_pred = model_bilstm.predict_classes(X_test)
    accuracy_score(y_test, y_test_pred)
    ```

    精确度如下:

    ```py
    0.877
    ```

我们在这里得到的精确度(87.7%)比 LSTM 模型的精确度(87%)有一点点提高。同样，您可以进一步调整超参数，以便从这个强大的架构中获取最大收益。请注意，与 LSTM 模型相比，我们的参数数量是它的两倍，是 RNN 平原的八倍。处理大型数据集可能会使性能差异更大。

注意

要访问该特定部分的源代码，请参考 https://packt.live/31ZPO2g 的。

你也可以在[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 堆叠 rnn

现在，让我们看看另一种从 rnn 获取更多功率的方法。在本章中我们所看到的所有模型中，我们对 RNN 层使用了一个单独的层(平原 RNN、LSTM 或 GRU)。更深入，即增加更多层，通常有助于我们建立前馈网络，以便我们可以在更深的层中学习更复杂的模式/特征。在循环网络中尝试这个想法是有价值的。事实上，堆叠的 rnn 似乎给了我们更多的预测能力。

下图说明了一个简单的两层堆叠 LSTM 模型。堆叠 RNN 仅仅意味着将一个 RNN 层的输出馈送到另一个 RNN 层。RNN 层可以输出序列(即每个时间步长的输出),并且这些序列可以像任何输入序列一样被馈送到后续的 RNN 层。就通过代码实现而言，堆叠 RNN 就像从一层返回序列一样简单，并将其作为输入提供给下一个 RNN 层，即紧接的下一层:

![Figure 6.20: Two-layer stacked RNN
](img/B15385_06_20.jpg)

图 6.20:两层堆叠的 RNN

让我们通过在情感分类任务中使用它来看看堆叠的 RNN (LSTM)的运行情况。

## 练习 6.05:基于堆叠 LSTM 的情感分类模型

在本练习中，我们将通过叠加两个 LSTM 层来预测数据中的情绪，从而“深入”了解 RNN 的架构。对于其他层，我们将继续使用之前练习中使用的相同设置(单元数、嵌入尺寸、落差等)。按照以下步骤完成本练习:

1.  实例化序列模型，添加适当尺寸的嵌入层，添加 40%的空间漏失:

    ```py
    model_stack = Sequential() model_stack.add(Embedding(vocab_size, output_dim=32)) model_stack.add(SpatialDropout1D(0.4))
    ```

2.  添加一个带有`32`单元格的 LSTM 图层。确保在 LSTM 层中指定`return_sequences`为`True`。这将返回 LSTM 在每个时间步的输出，然后可以传递到下一个 LSTM 层:

    ```py
    model_stack.add(LSTM(32, return_sequences=True))
    ```

3.  添加另一个带有`32`单元格的 LSTM 图层。这一次，你不需要返回序列。您可以将`return_sequences`选项指定为`False`或者完全跳过它(默认值为`False` ):

    ```py
    model_stack.add(LSTM(32, return_sequences=False))
    ```

4.  Add the dropout (50% dropout; this is higher since we're building a more complex model) and dense layers, compile the model, and print the model summary:

    ```py
    model_stack.add(Dropout(0.5))
    model_stack.add(Dense(1, activation='sigmoid'))
    model_stack.compile(loss='binary_crossentropy', \
                        optimizer='rmsprop', \
                        metrics=['accuracy'])
    model_stack.summary()
    ```

    摘要如下:

    ![Figure 6.21: Summary of the stacked LSTM model
    ](img/B15385_06_21.jpg)

    图 6.21:堆叠 LSTM 模型总结

    请注意，堆叠 LSTM 模型与双向模型具有相同数量的参数。让我们根据训练数据来拟合模型。

5.  Fit the model on the training data for four epochs:

    ```py
    history_stack = model_stack.fit(X_train, y_train, \
                                    batch_size=128, \
                                    validation_split=0.2, \
                                    epochs = 4)
    ```

    培训的结果如下:

    ![Figure 6.22: Stacked LSTM training output
    ](img/B15385_06_22.jpg)

    图 6.22:堆叠的 LSTM 训练输出

    训练堆叠 lstm 比训练双向 lstm 花费更少的时间。验证精度似乎接近双向 LSTM 模型。

6.  Make predictions on the test set and print the accuracy score:

    ```py
    y_test_pred = model_stack.predict_classes(X_test)
    accuracy_score(y_test, y_test_pred)
    ```

    精确度打印如下:

    ```py
    0.87572
    ```

`87.6%`的精度是对 LSTM 模型(`87%`)的改进，实际上与双向模型(`87.7%`)的精度相同。考虑到我们正在处理一个相当小的数据集，这在某种程度上是常规 LSTM 模型性能的重大改进。数据集越大，就越能从这些复杂的架构中获益。尝试调整超参数，以便充分利用这个强大的架构。

注意

要访问该特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。

你也可以在 https://packt.live/2Oa2trm 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 汇总所有模型

在这一章中，我们研究了 rnn 的不同变体——从普通 rnn 到 LSTMs 到 gru。我们还研究了使用 rnn 的双向方法和堆叠方法。现在是一个很好的时机来全面地看待事物，并在模型之间进行比较。让我们看一下下表，该表从参数、训练时间和性能(即我们数据集的准确性水平)方面比较了五个模型:

![Figure 6.23: Comparing the five models
](img/B15385_06_23.jpg)

图 6.23:五种模型的比较

注意

正如本章前面提到的，在处理实际元素时，您可能会获得与上面显示的不同的值；然而，你获得的测试精度应该与我们的基本一致。如果模型的性能差异很大，您可能需要调整历元的数量。

普通的 rnn 在参数上是最低的，并且具有最低的训练时间，但是在所有模型中具有最低的精确度。这符合我们的预期——我们正在处理长度为 200 个字符的序列，我们知道不要对普通 rnn 期望太高，门控 rnn(lstm，GRUs)更合适。事实上，LSTMs 和 gru 确实比普通 rnn 表现好得多。但是精度是以明显更高的训练时间和几倍的参数为代价的，这使得这些模型更容易过拟合。

堆叠和使用双向处理的方法似乎在预测能力方面提供了增量益处，但是这是以显著更高的训练时间和数倍的参数为代价的。堆叠和双向方法为我们提供了最高的准确性，即使是在这个小数据集上。

虽然性能结果特定于我们的数据集，但我们在这里看到的性能渐变是相当普遍的。堆叠和双向模型出现在当今的许多解决方案中，在各种任务中提供最先进的结果。对于更大的数据集和更长的序列，我们预计模型性能的差异会更大。

# 注意力模型

注意力模型是由 Dzmitry Bahdanau、KyungHyun Cho 和 Yoshua Bengio 于 2015 年末在其有影响力的开创性论文([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473))中首次提出的，该论文展示了英语到法语翻译的最新成果。从那以后，这个想法被用于许多序列处理任务，并取得了巨大的成功，注意力模型也变得越来越流行。虽然详细的解释和数学处理超出了本书的范围，但让我们理解这个想法背后的直觉，这个想法被深度学习领域的许多大腕认为是我们序列建模方法的一个重大发展。

注意力背后的直觉可以通过一个例子得到最好的理解，这个例子来自于注意力被开发的任务——翻译。当一个新手在不同语言间翻译一个长句时，他们不会一口气翻译完整个句子。它们将原句分解成更小的、易于管理的组块，从而为每个组块依次生成翻译。对于每一个组块，都有一个对翻译任务最重要的部分，也就是说，你最需要注意的地方:

![Figure 6.24: Idea of attention simplified
](img/B15385_06_24.jpg)

图 6.24:注意力概念的简化

上图显示了一个简单的例子，我们将句子“`Azra is moving to Berlin`”翻译成法语。法语翻译是，“`Azra déménage à Berlin`”。为了获得法语翻译中的第一个术语，“`Azra`”，我们需要主要关注原始句子中的第一个术语(用浅灰色线强调)，可能还需要关注第二个术语(用深灰色线强调)——这些术语具有更高的重要性(权重)。句子的其余部分不相关。同样，为了在输出中生成术语“`déménage`”，我们需要注意术语“`is`”和“`moving`”。每个项对输出项的重要性被表示为权重。这就是所谓的**对准**。

这些排列可以在下图中看到，该图来源于原始论文([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473))。它很好地展示了模型对输出中每一项最重要的内容。网格中单元格的颜色越浅，表示该列中相应输入术语的权重越高。我们可以看到，对于输出术语“`marin`”，模型正确地将“`marine`”识别为需要注意的最重要的输入术语。同样，它也将“`environment`”确定为“`environnement`”、“`known`”、“`connu`”等最重要的术语。很整洁，不是吗？

![Figure 6.25: The alignment learned by the model
](img/B15385_06_25.jpg)

图 6.25:模型学习的对齐

虽然注意力模型最初是为翻译任务设计的，但该模型已经成功地应用于各种其他任务。也就是说，注意到注意力模型有非常多的参数。这些模型通常用于双向 LSTM 图层，并为重要性值添加额外的权重。大量的参数使模型更容易过度拟合，这意味着它们将需要更大的数据集来利用它们的能力。

# RNNs 的更多变体

在这一章中我们已经看到了 RNNs 的很多变体——涵盖了所有突出的变体和即将出现的主要变体(就流行度而言)。序列建模及其相关架构是一个热门的研究领域，我们看到每年都有大量的发展。许多变体旨在制造更轻的模型，参数更少，不像当前的 rnn 那样需要大量硬件。**发条 RNNs**(**cwrns**)是最近的发展，显示出巨大的成功。也有**层次注意力网络**，建立在注意力的想法上，但最终也建议你不应该使用 rnn 作为构建模块。这个令人兴奋的领域正在发生很多事情，所以请睁大你的眼睛，竖起你的耳朵，等待下一个伟大的想法。

## 活动 6.01:亚马逊产品评论的情感分析

到目前为止，我们已经研究了 RNNs 的变体，并使用它们从 IMDb 数据集预测电影评论的情绪。在本活动中，我们将在亚马逊产品评论上构建一个情感分类模型。该数据包含几类产品的评论。可在 https://snap.stanford.edu/data/web-Amazon.html 获得的原始数据集非常庞大；因此，我们为此活动抽取了 50，000 条评论。

注意

可以在[https://packt.live/3iNTUjN](https://packt.live/3iNTUjN)找到被分成训练集和测试集的采样数据集。

这项活动将把我们在本章讨论的概念和方法，以及在*第 4 章，文本嵌入的深度学习*和*第 5 章，序列的深度学习*中讨论的概念和方法结合在一起。您将首先执行详细的文本清理，并进行预处理，为深度学习模型做好准备。您还将使用嵌入来表示文本。对于预测部分，您将使用堆叠的 LSTMs(两层)和两个密集层。

为了方便起见(也为了便于理解)，您还将利用 TensorFlow (Keras)的`Tokenizer` API 将清理后的文本转换成相应的序列。`Tokenizer`结合了`NLTK`的分词器和`vectorizer` ( `CountVectorizer` / `TfIdfVectorizer`)的功能，首先对文本进行分词，然后从数据集中学习词汇。让我们通过使用以下命令创建一些玩具数据来看看它的运行情况:

```py
sents = ["life is good", "good life", "good"]
```

使用以下命令，可以导入、实例化和匹配玩具数据:

```py
tok = Tokenizer()
tok.fit_on_texts(sents)
```

一旦词汇已经在玩具数据(为每个术语学习的索引)上被训练，我们可以将输入文本转换成术语的相应索引序列。让我们使用记号赋予器的`texts_to_sequences`方法将玩具数据转换成相应的索引序列:

```py
tok.texts_to_sequences(sents)
```

我们将得到以下输出:

```py
[[2, 3, 1], [1, 2], [1]]
```

现在，数据格式与我们在本章中使用的 IMDb 数据集的格式相同，可以用类似的方式进行处理。

至此，您已经准备好开始了。以下是完成本活动需要遵循的高级步骤:

1.  读入训练和测试集的数据文件(`Amazon_reviews_train.csv`和`Amazon_reviews_test.csv`)。检查数据集的形状，并打印出训练数据中的前五个记录。
2.  为了处理方便，将原始文本和训练集和测试集的标签分开。打印培训文本中的前两个评论。您应该有以下四个变量:`train_raw`包含训练数据的原始文本，`train_labels`包含训练数据的标签，`test_raw`包含测试数据的原始文本，`test_labels`包含测试数据的标签。
3.  使用 NLTK 的`word_tokenize`规范化案例并标记测试和训练文本(当然是在导入之后——提示:使用 list comprehension 以获得更干净的代码)。打印来自训练数据的第一次检查，以检查标记化是否有效。如果您以前没有使用过 tokenizer，请从 NLTK 下载`punkt`。
4.  从字符串模块导入`stopwords`(内置于 NLTK)和标点符号。定义一个函数(`drop_stop`)从任何输入的标记化句子中删除这些标记。如果你以前没用过，从 NLTK 下载`stopwords`。
5.  使用定义的功能(`drop_stop`)，从列车和测试文本中删除多余的停用词。打印已处理列车文本的第一次检查，以检查功能是否有效。
6.  使用 NLTK 中的`Porter Stemmer`,为训练和测试数据提取标记。
7.  为每个训练和文本评论创建字符串。这将有助于我们使用 Keras 中的实用程序来创建和填充序列。创建`train_texts`和`test_texts`变量。打印已处理的训练数据的第一次检查以确认它。
8.  从文本的 Keras 预处理实用程序(`keras.preprocessing.text`)，导入`Tokenizer`模块。定义一个词汇大小`10000`并用这个词汇实例化标记器。
9.  在列车文本上安装标记器。这就像`CountVectorizer`在*第四章，文本嵌入的深度学习*中所做的一样，并训练词汇。拟合后，在训练集和测试集上使用记号化器的`texts_to_sequences`方法为它们创建序列。打印列车数据中第一次检查的顺序。
10.  我们需要找到模型中要处理的序列的最佳长度。从训练集中获取评论的长度，并绘制长度直方图。
11.  现在数据的格式与我们在本章中使用的 IMDb 数据的格式相同。使用序列长度`100`(定义`maxlen = 100`变量)，使用 Keras 预处理实用程序(`keras.preprocessing.sequence`)中`sequence`模块的`pad_sequences`方法，将训练和测试数据的序列限制为`100`。检查训练数据结果的形状。
12.  要构建模型，从 Keras ( `embedding`、`spatialdropout`、`LSTM`、`dropout`和`dense`)导入所有必要的层，并导入`Sequential`模型。初始化`Sequential`模型。
13.  添加一个`32`为向量大小的嵌入图层(`output_dim`)。增加一个`40%`的空间落差。
14.  建立一个有`2`层和`64`个单元的堆叠 LSTM 模型。用`40%` dropout 添加一个 dropout 层。
15.  添加一个具有激活`relu`的`32`神经元的密集层，然后是一个`50%`脱落层，接着是另一个激活`relu`的`32`神经元的密集层，接着是另一个脱落`50%`的脱落层。
16.  添加一个带有激活`sigmoid`的单个神经元的最终密集层，并编译模型。打印模型摘要。
17.  用`20%`验证分割和`128`批量大小拟合训练数据的模型。为`5`时代而训练。
18.  使用模型的`predict_classes`方法对测试集进行预测。使用`scikit-learn`中的`accuracy_score`方法，计算测试装置的精度。同样，打印出混淆矩阵。

有了前面的参数，你应该得到大约`86%`的精度。通过一些超参数调整，您应该能够获得更高的精度。

注意

本活动的详细步骤以及解决方案和附加注释在第 416 页提供。

# 总结

在这一章中，我们首先理解了简单的 RNNs 不适用于非常大的序列的原因——主要原因是消失梯度问题，这使得建模长程相关性不切实际。我们认为 LSTM 是一个对长序列表现非常好的更新，但是它相当复杂并且有大量的参数。GRU 是一个很好的替代方案，它是对 LSTM 的简化，适用于较小的数据集。

然后，我们开始寻找通过使用双向 rnn 和 rnn 堆叠层从这些 rnn 提取更多功率的方法。我们还讨论了注意力机制，这是一种重要的新方法，可以在翻译中提供最先进的结果，但也可以用于其他序列处理任务。所有这些都是非常强大的模型，它们改变了执行多项任务的方式，并形成了产生最先进结果的模型的基础。随着该领域的积极研究，我们预计随着更多新的变体和架构的发布，情况只会变得更好。

既然我们已经讨论了各种强大的建模方法，在下一章，我们将准备讨论深度学习领域中一个非常有趣的话题，它使人工智能具有创造力—**生成对抗网络**。