

# 1.深度学习的构建模块

介绍

在这一章中，将向您介绍深度学习及其与人工智能和机器学习的关系。我们还将了解一些重要的深度学习架构，如多层感知器、卷积神经网络、递归神经网络和生成对抗网络。随着我们的进展，我们将获得 TensorFlow 框架的实践经验，并使用它来实现一些线性代数运算。最后，我们将介绍优化器的概念。我们将通过利用它们解决一个二次方程来理解它们在深度学习中的作用。到本章结束时，你会很好地理解什么是深度学习，以及如何用 TensorFlow 编程。

# 简介

你刚从你的年度假期回来。作为一个狂热的社交媒体用户，你正忙着将照片上传到你最喜欢的社交媒体应用程序。当照片上传后，你会注意到应用程序会自动识别你的脸，并在照片上立刻给你贴上标签。事实上，即使在集体照中也是如此。即使在一些光线不好的照片中，你也会注意到该应用程序在大多数情况下都正确地标记了你。该应用程序如何学习如何做到这一点？

为了识别照片中的人，该应用程序需要关于这个人的面部结构、骨骼结构、眼睛颜色和许多其他细节的准确信息。但当你使用那个照片应用程序时，你不必把所有这些细节明确地输入到应用程序中。你所做的只是上传你的照片，应用程序自动开始在照片中识别你。app 是怎么知道这些细节的？

当你上传你的第一张照片到应用程序时，应用程序会要求你标记自己。当你手动标记自己时，该应用程序会自动“学习”它需要了解的关于你面部的所有信息。然后，每当你上传一张照片，该应用程序就会使用它学习到的信息来识别你的身份。当你在应用程序错误标记你的照片中手动标记你自己时，情况会有所改善。

由于**深度学习**(**D1**)的力量，应用程序能够在最少的人工干预下学习新的细节并自我完善。深度学习是**人工智能** ( **AI** )的一部分，它通过从标记数据中识别模式来帮助机器学习。但是等一下，这不就是**机器学习** ( **ML** )做的吗？那深度学习和机器学习的区别是什么？人工智能、机器学习和深度学习等领域的融合点是什么？让我们快速浏览一下。

## 人工智能、机器学习和深度学习

人工智能是计算机科学的一个分支，旨在开发能够模拟人类智能的机器。简而言之，人类的智能可以解释为基于我们五种感官——视觉、听觉、触觉、嗅觉和味觉——的输入做出的决定。人工智能不是一个新领域，自 20 世纪 50 年代以来一直很流行。从那以后，在这个领域里出现了多次狂喜和痛苦的浪潮。随着计算、数据可用性和对理论基础的更好理解的长足进步，21 世纪见证了人工智能的复兴。机器学习和深度学习是 AI 的子领域，并且越来越多地互换使用。

下图描述了 AI、ML 和 DL 之间的关系:

![Figure 1.1: Relationship between AI, ML, and DL
](img/B15385_01_01.jpg)

图 1.1:AI、ML 和 DL 之间的关系

## 机器学习

机器学习是人工智能的子集，它通过识别数据中的模式和提取推理来执行特定的任务。然后，从数据中得出的推论被用来预测看不见的数据的结果。机器学习在解决特定任务的方法上不同于传统的计算机编程。在传统的计算机编程中，我们编写并执行特定的业务规则和启发式规则来获得期望的结果。然而，在机器学习中，规则和启发式规则并没有明确地写出来。这些规则和试探法是通过提供数据集来学习的。为学习规则和启发而提供的数据集被称为**训练数据集**。整个学习和推断的过程叫做**训练**。

学习规则和试探法是使用不同的算法来完成的，这些算法使用统计模型来实现这个目的。这些算法利用数据的许多表示进行学习。每个这样的数据表示被称为一个**示例**。一个例子中的每个元素被称为一个**特征**。下面是一个著名的虹膜数据集的例子([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris))。该数据集是基于不同特征(如萼片和花瓣的长度和宽度)的不同鸢尾花物种的代表:

![Figure 1.2: Sample data from the IRIS dataset
](img/B15385_01_02.jpg)

图 1.2:来自 IRIS 数据集的样本数据

在前面的数据集中，每行数据代表一个示例，每列数据代表一个要素。机器学习算法利用这些特征从数据中得出推论。模型的准确性以及预测的结果在很大程度上取决于数据的特征。如果提供给机器学习算法的特征是问题陈述的良好表示，则获得良好结果的机会很高。机器学习算法的一些例子是*线性回归*、*逻辑回归*、*支持向量机*、*随机森林*和 *XGBoost* 。

尽管传统的机器学习算法对许多用例都很有用，但它们在很大程度上依赖于功能的质量来获得更好的结果。创建功能是一项耗时的艺术，需要大量的领域知识。然而，即使拥有全面的领域知识，在将该知识转换为派生特征，从而封装数据生成过程的细微差别方面仍然存在限制。此外，随着机器学习解决的问题越来越复杂，特别是随着非结构化数据(图像、语音、文本等)的出现，几乎不可能创建表示复杂函数的特征，而这些复杂函数又会生成数据。因此，经常需要找到一种不同的方法来解决复杂的问题；这就是深度学习发挥作用的地方。

## 深度学习

深度学习是机器学习的一个子集，也是一种叫做人工神经网络(ann)的算法的扩展。神经网络并不是一个新现象。神经网络创建于 20 世纪 40 年代上半叶。神经网络的发展受到了人脑工作原理的启发。从那以后，这个领域经历了几次起伏。重新燃起对神经网络的热情的一个决定性时刻是该领域的中坚分子杰弗里·辛顿(Geoffrey Hinton)引入了一种称为反向传播的算法。正因如此，Hinton 被广泛认为是‘深度学习的教父’。我们将在第 2 章、*神经网络中深入讨论神经网络。*

具有多个(深度)层的人工神经网络是深度学习的核心。深度学习模型的一个定义特征是它们从输入数据中学习特征的能力。与需要创建特征的传统机器学习不同，深度学习擅长跨多个层学习不同层次的特征。比方说，我们正在使用深度学习模型来检测人脸。模型的初始层将学习人脸的低级近似，如人脸的边缘，如图*图 1.3* 所示。随后的每一层都采用较低层的特征，并将它们组合在一起，形成更复杂的特征。在人脸检测的情况下，如果初始层已经学会检测边缘，后续层将把这些边缘放在一起，形成人脸的一部分，如鼻子或眼睛。这个过程在每一个连续的层中继续，最后一层生成人脸图像:

![Figure 1.3: Deep learning model for detecting faces
](img/B15385_01_03.jpg)

图 1.3:检测人脸的深度学习模型

注意

前面的图片来源于流行的研究论文:李，洪拉克&格罗斯，罗杰&兰格纳特，拉杰什& Ng，安德鲁。(2011).*卷积深度信念网络分层表示的无监督学习。* Commun。ACM。54.95-103.10.1145/2001269.2001295.

深度学习技术在过去十年中取得了长足的进步。有不同的因素导致了深度学习技术的指数级增长。首先是大量数据的可用性。随着互联设备网络的不断增加，数字时代产生了大量数据，尤其是非结构化数据。这反过来又推动了深度学习技术的大规模采用，因为它们非常适合处理大型非结构化数据。

导致深度学习兴起的另一个主要因素是计算基础设施的长足进步。拥有大量层和数百万个参数的深度学习模型需要强大的计算能力。价格合理的**图形处理单元**(**GPU**)和**张量处理单元** ( **TPUs** )等计算层的进步导致了深度学习的大规模采用。

为了建立和实现深度学习模型，开源不同的框架也加速了深度学习的普及。2015 年，谷歌大脑团队开源了 TensorFlow 框架，自此 TensorFlow 发展成为最受欢迎的深度学习框架之一。其他可用的主要框架有 PyTorch、MXNet 和 Caffe。我们将在本书中使用张量流框架。

在我们深入研究深度学习的构建模块之前，让我们通过一个快速演示来展示深度学习模型的威力。您不需要了解本演示中出现的任何代码。只需按照说明操作，你就能快速了解深度学习的基本功能。

## 利用深度学习对图像进行分类

在接下来的练习中，我们将对一幅比萨饼图像进行分类，并将得到的类文本转换成语音。为了对图像进行分类，我们将使用预先训练好的模型。文本到语音的转换将使用一个免费的 API 来完成，这个 API 叫做**谷歌文本到语音** ( **gTTS** )。在开始之前，让我们先了解一下本演示的一些关键构建模块。

### 预训练模型

训练深度学习模型需要大量的计算基础设施和时间，以及大数据集。然而，为了帮助研究和学习，深度学习社区也提供了在大型数据集上训练的模型。这些预先训练的模型可以被下载并用于预测，或者可以用于进一步的训练。在本演示中，我们将使用名为`ResNet50`的预训练模型。该型号与 Keras 套装一起提供。这个预先训练好的模型可以预测我们日常生活中遇到的 1000 种不同类别的物体，如鸟类、动物、汽车等。

### 谷歌文本到语音转换 API

谷歌已经将其文本到语音的算法用于有限的用途。我们将使用这种算法将预测的文本转换成语音。

### 演示的必备包

为了让这个演示运行，您需要在您的机器上安装以下软件包:

*   张量流 2.0
*   克拉斯
*   gTTS

请参考*前言*了解安装前两个包的过程。练习中将展示如何安装 gtt。让我们深入了解一下演示。

## 练习 1.01:图像和语音识别演示

在本练习中，我们将使用深度学习模型演示图像识别和语音到文本的转换。此时，您将无法理解代码的每一行。这个后面会解释。目前，只需执行代码，并了解使用 TensorFlow 构建深度学习和 AI 应用程序有多简单。按照以下步骤完成本练习:

1.  打开一个 Jupyter 笔记本，命名为*练习 1.01。*关于如何启动 Jupyter 笔记本的详细信息，请参考前言。
2.  Import all the required libraries:

    ```
    from tensorflow.keras.preprocessing.image import load_img
    from tensorflow.keras.preprocessing.image import img_to_array
    from tensorflow.keras.applications.resnet50 import ResNet50
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.applications.resnet50 \
    import preprocess_input
    from tensorflow.keras.applications.resnet50 \
    import decode_predictions
    \ ) to split the logic across multiple lines. When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line.
    ```

    以下是我们将要导入的包的简要描述:

    `load_img`:将图像载入 Jupyter 笔记本

    `img_to_array`:将图像转换成 NumPy 数组，这是 Keras 所需的格式

    `preprocess_input`:将输入转换成模型可接受的格式

    `decode_predictions`:将模型预测的数字输出转换为文本标签

    `Resnet50`:这是预先训练好的图像分类模型

3.  Create an instance of the pre-trained `Resnet` model:

    ```
    mymodel = ResNet50()
    ```

    下载时，您应该会收到类似以下内容的消息:

    ![Figure 1.4: Loading Resnet50
    ](img/B15385_01_04.jpg)

    图 1.4:加载 Resnet50

    `Resnet50`是预先训练好的图像分类模型。对于第一次使用的用户，将模型下载到您的环境中需要一些时间。

4.  Download an image of a pizza from the internet and store it in the same folder that you are running the Jupyter Notebook in. Name the image `im1.jpg`.

    注意

    你也可以从这个链接下载我们正在使用的图片:[https://packt.live/2AHTAC9](https://packt.live/2AHTAC9)

5.  Load the image to be classified using the following command:

    ```
    myimage = load_img('im1.jpg', target_size=(224, 224))
    ```

    如果你将图像存储在另一个文件夹中，必须给出图像所在位置的完整路径，以代替`im1.jpg`命令。例如，如果图像存储在`D:/projects/demo`中，代码应该如下:

    ```
    myimage = load_img('D:/projects/demo/im1.jpg', \
                       target_size=(224, 224))
    ```

6.  Let's display the image using the following command:

    ```
    myimage
    ```

    上述命令的输出如下所示:

    ![Figure 1.5: Output displayed after loading the image
    ](img/B15385_01_05.jpg)

    图 1.5:加载图像后显示的输出

7.  将图像转换成一个`numpy`数组，因为模型期望它采用这种格式:

    ```
    myimage = img_to_array(myimage)
    ```

8.  将图像重塑为四维格式，因为这是模型所期望的:

    ```
    myimage = myimage.reshape((1, 224, 224, 3))
    ```

9.  通过运行`preprocess_input()`功能:

    ```
    myimage = preprocess_input(myimage)
    ```

    准备提交图像
10.  运行预测:

    ```
    myresult = mymodel.predict(myimage)
    ```

11.  预测产生一个需要转换成文本格式的相应标签的数字:

    ```
    mylabel = decode_predictions(myresult)
    ```

12.  接下来，键入以下代码来显示标签:

    ```
    mylabel = mylabel[0][0]
    ```

13.  Print the label using the following code:

    ```
    print("This is a : " + mylabel[1])
    ```

    如果到目前为止您已经正确地遵循了这些步骤，那么输出将如下所示:

    ```
    This is a : pizza
    ```

    该模型成功地确定了我们的形象。很有趣，不是吗？在接下来的几个步骤中，我们将进一步把这个结果转换成语音。

    小费

    虽然我们在这里使用了一个比萨饼的图像，但是您可以在这个模型中使用任何图像。我们建议你用不同的图片多次尝试这个练习。

14.  准备要转换成语音的文本:

    ```
    sayit="This is a "+mylabel[1]
    ```

15.  安装将文本转换为语音所需的`gtts`包。这可以在 Jupyter 笔记本中实现，如下:

    ```
    !pip install gtts
    ```

16.  Import the required libraries:

    ```
    from gtts import gTTS
    import os
    ```

    前面的代码将导入两个库。一个是`gTTS`，即 Google Text-to-Speech，这是一个基于云的开源 API，用于将文本转换为语音。另一个是用于播放结果音频文件的`os`库。

17.  Call the `gTTS` API and pass the text as a parameter:

    ```
    myobj = gTTS(text=sayit)
    ```

    注意

    运行上述步骤时，您需要在线。

18.  Save the resulting audio file. This file will be saved in the home directory where the Jupyter Notebook is being run.

    ```
    myobj.save("prediction.mp3")
    ```

    注意

    您也可以通过在名称前包含绝对路径来指定保存路径；比如`(myobj.save('D:/projects/prediction.mp3')`。

19.  Play the audio file:

    ```
    os.system("prediction.mp3")
    ```

    如果您正确地遵循了前面的步骤，您将会听到单词`This is a pizza`的发音。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ZPZx8B](https://packt.live/2ZPZx8B)。

    你也可以在 https://packt.live/326cRIu 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/326cRIu)

在这个练习中，我们学习了如何使用 TensorFlow 中的几行代码，通过利用公开可用的模型来构建深度学习模型。现在你已经尝到了深度学习的滋味，让我们继续前进，了解深度学习的不同组成部分。

## 深度学习模型

大多数流行的深度学习模型的核心是人工神经网络，它们受到我们对大脑如何工作的知识的启发。即使没有一个单一的模型可以被称为完美的，不同的模型在不同的场景中表现得更好。在接下来的章节中，我们将了解一些最著名的模型。

### 多层感知器

**多层感知器** ( **MLP** )是神经网络的基本类型。MLP 也称为前馈网络。下图显示了一个 MLP:

![Figure 1.6: MLP representation
](img/B15385_01_06.jpg)

图 1.6: MLP 表示法

MLP(或任何神经网络)的基本构件之一是神经元。网络由连接到连续层的多个神经元组成。在最基本的层面上，MLP 将由输入图层、隐藏图层和输出图层组成。输入层将具有与输入数据相等的神经元。每个输入神经元将与隐藏层的所有神经元连接。最终的隐藏图层将连接到输出图层。MLP 是一个非常有用的模型，可以在各种分类和回归问题上试用。MLP 的概念将在第二章、*神经网络中详细介绍。*

### 卷积神经网络

卷积神经网络(CNN)是一类深度学习模型，主要用于图像识别。当我们讨论 MLP 时，我们看到一层中的每个神经元都与下一层中的每个其他神经元相连。然而，CNN 采用不同的方法，并且不求助于这种完全连接的架构。取而代之的是，CNN 从图像中提取局部特征，然后将这些特征提供给后续层。

CNN 在 2012 年声名鹊起，当时一种名为 AlexNet 的架构赢得了一场名为 **ImageNet 大规模视觉识别挑战赛** **(ILSVRC)** 的顶级比赛。ILSVRC 是一个大规模的计算机视觉竞赛，来自全球的团队争夺最佳计算机视觉模型的奖项。通过 2012 年题为*使用深度卷积神经网络的 ImageNet 分类*([https://papers . nips . cc/paper/4824-ImageNet-class ification-with-Deep-卷积神经网络](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks))的研究论文，Alex Krizhevsky 等人(多伦多大学)展示了 CNN 架构的真正力量，最终为他们赢得了 2012 年 ILSVRC 挑战赛。下图描绘了 *AlexNet* 模型的结构，这是一个 CNN 模型，其高性能使 CNN 在深度学习领域脱颖而出。虽然这个模型的结构对你来说可能看起来很复杂，但在*第 3 章*、*使用卷积神经网络进行图像分类*中，将向你详细解释这种 CNN 网络的工作原理:

![Figure 1.7: CNN architecture of the AlexNet model
](img/B15385_01_07.jpg)

图 1.7:Alex net 模型的 CNN 架构

注意

前面提到的图表来源于流行的研究论文:Krizhevsky，Alex & Sutskever，Ilya & Hinton，Geoffrey。(2012).*深度卷积神经网络的 ImageNet 分类。*神经信息处理系统。25.10.1145/3065386.

自 2012 年以来，有许多突破性的 CNN 架构扩展了计算机视觉的可能性。一些著名的架构有 ZFNet、Inception(谷歌)、VGG 和 ResNet。

使用 CNN 的一些最突出的案例如下:

*   图像识别和**光学字符识别** ( **OCR** )
*   社交媒体上的人脸识别
*   文本分类
*   自动驾驶汽车的目标检测
*   用于医疗保健的图像分析

使用深度学习的另一个巨大好处是，你不必总是从头开始构建你的模型——你可以使用其他人构建的模型，并将它们用于你自己的应用。这被称为“迁移学习”，它让你从活跃的深度学习社区中受益。

我们将把迁移学习应用于图像处理，并在*第 3 章*、*卷积神经网络图像分类*中详细了解 CNN 及其动力学。

### 递归神经网络

在传统的神经网络中，输入独立于输出。然而，在诸如语言翻译的情况下，其中存在对单词前后的单词的依赖，需要理解单词出现的序列的动态。这个问题被一类叫做**递归神经网络** ( **RNNs** )的网络解决了。RNNs 是一类深度学习网络，其中前一步的输出作为输入发送到当前步骤。RNN 的一个显著特征是隐藏层，它会记住序列中其他输入的信息。下图是 RNN 的高级表示。在*第五章、* *序列深度学习*中，你会学到更多关于这些网络内部运作的知识:

![Figure 1.8: Structure of RNNs
](img/B15385_01_08.jpg)

图 1.8:RNNs 的结构

RNN 建筑有不同的类型。其中最突出的有**长短期记忆** ( **LSTM** )和**门控循环单元** ( **GRU** )。

rnn 的一些重要用例如下:

*   语言建模和文本生成
*   机器翻译
*   语音识别
*   生成图像描述

RNNs 将在*第 5 章*、*序列深度学习*和*第 6 章、**lstm、GRUs 和高级 RNNs* 中详细介绍。

## 生成性对抗网络

**生成对抗网络** ( **GANs** )是能够生成类似于任何真实数据分布的数据分布的网络。深度学习的先驱之一 Yann LeCun 将 GANs 描述为最近十年深度学习中最有前途的想法之一。

举个例子，假设我们想从随机噪声数据中生成狗的图像。为此，我们用狗的真实图像和噪声数据来训练 GAN 网络，直到我们生成看起来像狗的真实图像的数据。下图解释了 GANs 背后的概念。在这个阶段，你可能还没有完全理解这个概念。将在*第七章*、*生成对抗网络*中详细解释。

![Figure 1.9: Structure of GANs
](img/B15385_01_09.jpg)

图 1.9:GANs 的结构

注意

前面提到的图表来源于流行的研究论文:巴里奥斯，布尔丹，科姆奇，吉尔伯特和奥鲁埃(2019)。*利用深度学习方法进行局部放电分类——近期进展综述*([https://doi.org/10.3390/en12132485](https://doi.org/10.3390/en12132485))。

gan 是一个很大的研究领域，并且有很多使用案例。GANs 的一些有用应用如下:

*   图像翻译
*   文本到图像合成
*   生成视频
*   艺术的复兴

GANs 将在*第 7 章*、*生成对抗网络*中详细介绍。

深度学习的可能性和前景是巨大的。深度学习应用已经在我们的日常生活中无处不在。一些显著的例子如下:

*   聊天机器人
*   机器人
*   智能音箱(如 Alexa)
*   虚拟助手
*   推荐引擎
*   嗡嗡声
*   自动驾驶汽车或自动驾驶车辆

这种不断扩展的可能性使它成为数据科学家武库中的一个伟大的工具集。这本书将逐步向你介绍深度学习的神奇世界，并使你擅长将其应用到现实世界的场景中。

# 张量流简介

TensorFlow 是 Google 开发的深度学习库。在写这本书的时候，TensorFlow 是迄今为止最受欢迎的深度学习库。它最初是由谷歌内部一个名为谷歌大脑团队的团队开发的，供他们内部使用，随后在 2015 年开源。Google Brain 团队开发了 Google Photos、Google Cloud 语音转文字等热门应用，这些都是基于 TensorFlow 的深度学习应用。TensorFlow 1.0 于 2017 年发布，在很短的时间内，它领先于其他现有的库，如 Caffe，Theano 和 PyTorch，成为最受欢迎的深度学习库。它被认为是行业标准，几乎每个在深度学习领域有所作为的组织都采用了它。张量流的一些主要特征如下:

*   它可以用于所有常见的编程语言，如 Python、Java 和 R
*   它可以部署在多个平台上，包括 Android 和 Raspberry Pi
*   它可以在高度分布式的模式下运行，因此具有高度的可伸缩性

经过长时间的 Alpha/Beta 发布，TensorFlow 2.0 的最终版本于 2019 年 9 月 30 日发布。TF2.0 的重点是让深度学习应用的开发更容易。让我们继续了解 TensorFlow 2.0 框架的基础知识。

**张量**

在 TensorFlow 程序中，每个数据元素都被称为张量**。张量是向量和矩阵在高维空间中的表示。张量的秩表示它的维数。用张量表示的一些常见数据形式如下。**

 ****标量**

标量是秩为 0 的张量，它只有大小。

例如，`[ 12 ]`是一个数量级为 12 的标量。

**矢量**

向量是秩为 1 的张量。

比如`[ 10 , 11, 12, 13]`。

**矩阵**

矩阵是秩为 2 的张量。

例如，`[ [10,11] , [12,13] ]`。这个张量有两行和两列。

**3 阶张量**

这是一个三维张量。例如，图像数据主要是三维张量，宽度、高度和通道数作为其三维。下面是一个三维张量的例子，即它有两行、三列和三个通道:

![Figure 1.10: Tensor with three dimensions
](img/B15385_01_10.jpg)

图 1.10:三维张量

张量的形状由一个数组表示，它表示每个维度中元素的数量。例如，如果一个张量的形状是[2，3，5]，就意味着这个张量是三维的。如果这是图像数据，这个形状意味着这个张量有两行、三列和五个通道。我们也可以从形状中得到排名。在这个例子中，张量的秩是三，因为有三个维度。下图进一步说明了这一点:

![Figure 1.11: Examples of Tensor rank and shape
](img/B15385_01_11.jpg)

图 1.11:张量秩和形状的例子

## 常量

常量用于存储在程序运行过程中不会改变或修改的值。有多种方法可以创建常数，但最简单的方法如下:

```
a = tf.constant (10)
```

这会创建一个初始化为 10 的张量。请记住，常量的值不能通过重新赋值来更新或修改。另一个例子如下:

```
s = tf.constant("Hello")
```

在这一行中，我们将一个字符串实例化为一个常量。

## 变量

变量用于存储在程序运行过程中可以更新和修改的数据。我们将在*第 2 章*、*神经网络*中对此进行更详细的讨论。创建变量有多种方法，但最简单的方法如下:

```
b=tf.Variable(20)
```

在前面的代码中，变量`b`被初始化为`20`。请注意，在 TensorFlow 中，与常数不同的是，术语`Variable`是用大写字母`V`写成的。

在程序运行过程中，变量可以被重新赋值。变量可以用来分配任何类型的对象，包括标量、向量和多维数组。以下是如何在 TensorFlow 中创建维数为 3 x 3 的数组的示例:

```
C = tf.Variable([[1,2,3],[4,5,6],[7,8,9]])
```

该变量可以初始化为 3 x 3 矩阵，如下所示:

![Figure 1.12: 3 x 3 matrix
](img/B15385_01_12.jpg)

图 1.12: 3 x 3 矩阵

现在我们知道了 TensorFlow 的一些基本概念，让我们学习如何将它们付诸实践。

### 在 TensorFlow 中定义函数

可以使用以下语法在 Python 中创建函数:

```
def myfunc(x,y,c):
    Z=x*x*y+y+c
    return Z
```

使用特殊操作符`def`初始化一个函数，后跟函数名`myfunc`和函数的参数。在前面的示例中，函数体在第二行，最后一行返回输出。

在下面的练习中，我们将学习如何使用我们之前定义的变量和常量实现一个小函数。

## 练习 1.02:实现一个数学方程

在本练习中，我们将使用 TensorFlow 求解以下数学方程:

![Figure 1.13: Mathematical equation to be solved using TensorFlow
](img/B15385_01_13.jpg)

图 1.13:使用张量流求解的数学方程

我们将使用 TensorFlow 来求解，如下所示:

```
X=3
Y=4
```

虽然有多种方法可以做到这一点，但在本练习中，我们将只探索其中一种方法。按照以下步骤完成本练习:

1.  打开一个新的 Jupyter 笔记本，将其重命名为*练习 1.02* 。
2.  使用以下命令导入 TensorFlow 库:

    ```
    import tensorflow as tf
    ```

3.  现在，让我们来解方程。为此，您需要创建两个变量，`X`和`Y`，并将它们分别初始化为给定的值`3`和`4`:

    ```
    X=tf.Variable(3) Y=tf.Variable(4)
    ```

4.  在我们的等式中，`2`的值是不变的，所以我们通过输入下面的代码将它存储为一个常量:

    ```
    C=tf.constant(2)
    ```

5.  定义将求解我们方程的函数:

    ```
    def myfunc(x,y,c):     Z=x*x*y+y+c     return Z
    ```

6.  通过将`X`、`Y`和`C`作为参数传递来调用函数。我们将把这个函数的输出存储在一个名为`result` :

    ```
    result=myfunc(X,Y,C)
    ```

    的变量中
7.  Print the result using the `tf.print()` function:

    ```
    tf.print(result)
    ```

    输出如下所示:

    ```
    42
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2ClXKjj](https://packt.live/2ClXKjj)。

    你也可以在[https://packt.live/2ZOIN1C](https://packt.live/2ZOIN1C)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在这个练习中，我们学习了如何定义和使用函数。熟悉 Python 编程的人会注意到，它与普通的 Python 代码没有太大区别。

在本章的其余部分，我们将通过查看一些基本的线性代数来做准备，并熟悉一些常见的向量运算，以便在下一章中理解神经网络会容易得多。

## 带张量流的线性代数

将在神经网络中使用的最重要的线性代数主题是矩阵乘法。在本节中，我们将解释矩阵乘法是如何工作的，然后使用 TensorFlow 的内置函数来解决一些矩阵乘法示例。这是为下一章的神经网络做准备所必需的。

矩阵乘法是如何工作的？你可能在高中时学过这个，但是让我们快速回顾一下。

假设我们必须在两个矩阵 A 和 B 之间执行矩阵乘法，其中我们有以下内容:

![Figure 1.14: Matrix A
](img/B15385_01_14.jpg)

图 1.14:矩阵 A

![Figure 1.15: Matrix B
](img/B15385_01_15.jpg)

图 1.15:矩阵 B

第一步是检查 2×3 矩阵乘以 3×2 矩阵是否可行。矩阵乘法有个前提。记住 C=R，即第一个矩阵的列数(C)应该等于第二个矩阵的行数(R)。记住顺序很重要，这就是为什么，A x B 不等于 B x A，在这个例子中，C=3，R=3。所以，乘法是可能的。

结果矩阵的行数等于 A 中的行数，列数等于 b 中的列数。因此，在这种情况下，结果将是一个 2 x 2 矩阵。

要开始将两个矩阵相乘，取 A (R1)第一行的元素和 B (C1)第一列的元素:

![Figure 1.16: Matrix A(R1)
](img/B15385_01_16.jpg)

图 1.16:矩阵 A(R1)

![Figure 1.17: Matrix B(C1)
](img/B15385_01_17.jpg)

图 1.17:矩阵 B(C1)

得到元素间乘积的和，即(1 x 7) + (2 x 9) + (3 x 11) = 58。这将是生成的 2 x 2 矩阵中的第一个元素。我们暂时称这个不完全矩阵为 D(i ):

![Figure 1.18: Incomplete matrix D(i)
](img/B15385_01_18.jpg)

图 1.18:不完整的矩阵 D(i)

对 A(R1)的第一行和 B (C2)的第二列重复此操作:

![Figure 1.19: First row of matrix A
](img/B15385_01_19.jpg)

图 1.19:矩阵 A 的第一行

![Figure 1.20: Second column of matrix B
](img/B15385_01_20.jpg)

图 1.20:矩阵 B 的第二列

得到相应元素的乘积之和，即(1 x 8) + (2 x 10) + (3 x 12) = 64。这将是结果矩阵中的第二个元素:

![Figure 1.21: Second element of matrix D(i)
](img/B15385_01_21.jpg)

图 1.21:矩阵 D(i)的第二个元素

对第二行重复同样的操作，得到最终结果:

![Figure 1.22: Matrix D
](img/B15385_01_22.jpg)

图 1.22:矩阵 D

同样的矩阵乘法可以在 TensorFlow 中使用一个名为`tf.matmul()`的内置方法来执行。需要相乘的矩阵必须作为变量提供给模型，如下例所示:

```
C = tf.matmul(A,B)
```

在前面的例子中，A 和 B 是我们要相乘的矩阵。让我们通过使用 TensorFlow 将我们手动相乘的两个矩阵相乘来练习这个方法。

## 练习 1.03:使用张量流进行矩阵乘法

在本练习中，我们将使用`tf.matmul()`方法通过`tensorflow`将两个矩阵相乘。按照以下步骤完成本练习:

1.  打开一个新的 Jupyter 笔记本，将其重命名为*练习 1.03* 。
2.  导入`tensorflow`库并创建两个变量`X`和`Y`，作为矩阵。`X`是一个 2×3 的矩阵，`Y`是一个 3×2 的矩阵:

    ```
    import tensorflow as tf X=tf.Variable([[1,2,3],[4,5,6]]) Y=tf.Variable([[7,8],[9,10],[11,12]])
    ```

3.  Print and display the values of `X` and `Y` to make sure the matrices are created correctly. We'll start by printing the value of `X`:

    ```
    tf.print(X)
    ```

    输出如下所示:

    ```
    [[1 2 3]
     [4 5 6]]
    ```

    现在，让我们打印出`Y`的值:

    ```
    tf.print(Y)
    ```

    输出如下所示:

    ```
    [[7 8]
     [9 10]
     [11 12]]
    ```

4.  Perform matrix multiplication by calling the TensorFlow `tf.matmul()` function:

    ```
    c1=tf.matmul(X,Y)
    ```

    要显示结果，打印`c1`的值:

    ```
    tf.print(c1)
    ```

    输出如下所示:

    ```
    [[58 64]
     [139 154]]
    ```

5.  Let's perform matrix multiplication by changing the order of the matrices:

    ```
    c2=tf.matmul(Y,X)
    ```

    为了显示结果，让我们打印出`c2`的值:

    ```
    tf.print(c2)
    ```

    结果输出如下。

    ```
    [[39 54 69]
     [49 68 87]
     [59 82 105]]
    ```

    请注意，由于我们更改了顺序，结果有所不同。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3eevyw4](https://packt.live/3eevyw4)。

    你也可以在 https://packt.live/2CfGGvE 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们学习了如何在 TensorFlow 中创建矩阵以及如何执行矩阵乘法。当我们创建自己的神经网络时，这将派上用场。

## 整形功能

Reshape，顾名思义，就是将张量的形状从当前的形状改变为新的形状。例如，您可以将一个 2 × 3 的矩阵整形为 3 × 2 的矩阵，如下所示:

![Figure 1.23: Reshaped matrix
](img/B15385_01_23.jpg)

图 1.23:重塑的矩阵

让我们考虑下面的 2 × 3 矩阵，我们在前面的练习中定义如下:

```
X=tf.Variable([[1,2,3],[4,5,6]])
```

我们可以使用以下代码打印矩阵的形状:

```
X.shape
```

从下面的输出中，我们可以看到我们已经知道的形状:

```
TensorShape([2, 3])
```

现在，为了将`X`重塑为 3 × 2 矩阵，TensorFlow 提供了一个名为`tf.reshape()`的便捷函数。该函数通过以下参数实现:

```
tf.reshape(X,[3,2])
```

在前面的代码中，`X`是需要整形的矩阵，`[3,2]`是`X`矩阵需要整形的新形状。

在实现神经网络时，重塑矩阵是一种方便的操作。例如，当使用 CNN 处理图像时，一个先决条件是图像必须是等级 3，也就是说，它必须具有三个维度:宽度、高度和深度。如果我们的图像是只有两个维度的灰度图像，那么`reshape`操作可以方便地添加第三个维度。在这种情况下，第三维度将是 1:

![Figure 1.24: Changing the dimension using reshape()
](img/B15385_01_24.jpg)

图 1.24:使用 reshape()更改尺寸

在上图中，我们将形状为`[5,4]`的矩阵重塑为形状为`[5,4,1]`的矩阵。在接下来的练习中，我们将使用`reshape()`函数来重塑`[5,4]`矩阵。

实现`reshape()`功能时有一些重要的考虑因素:

*   新形状中的元素总数应等于原始形状中的元素总数。例如，您可以将一个 2 × 3 矩阵(总共 6 个元素)重塑为 3 × 2 矩阵，因为新形状也有 6 个元素。但是，您不能将其重塑为 3 × 3 或 3 × 4。
*   `reshape()`功能不应与`transpose()`混淆。在`reshape()`中，矩阵元素的顺序被保留，元素以相同的顺序重新排列成新的形状。然而，在`transpose()`的情况下，行变成列，列变成行。因此元素的顺序会改变。
*   `reshape()`功能不会改变原始矩阵，除非你给它分配新的形状。否则，它只是显示新的形状，而不会实际更改原始变量。例如，假设`x`有形状【2，3】，你只需运行`tf.reshape(x,[3,2])`。当您再次检查`x`的形状时，它将保持为【2，3】。为了真正改变形状，你需要给它分配新的形状，就像这样:

    ```
    x=tf.reshape(x,[3,2])
    ```

让我们在接下来的练习中尝试在 TensorFlow 中实现`reshape()`。

## 练习 1.04:使用 TensorFlow 中的 reshape()函数重塑矩阵

在本练习中，我们将使用`reshape()`函数将一个`[5,4]`矩阵整形为`[5,4,1]`的形状。这个练习将帮助我们理解如何使用`reshape()`来改变张量的秩。按照以下步骤完成本练习:

1.  打开一个 Jupyter 笔记本，重命名为*练习 1.04* 。然后，导入`tensorflow`并创建我们想要重塑的矩阵:

    ```
    import tensorflow as tf A=tf.Variable([[1,2,3,4], \                [5,6,7,8], \                [9,10,11,12], \                [13,14,15,16], \                [17,18,19,20]])
    ```

2.  First, we'll print the variable `A` to check whether it is created correctly, using the following command:

    ```
    tf.print(A)
    ```

    输出如下所示:

    ```
    [[1 2 3 4]
     [5 6 7 8]
     [9 10 11 12]
     [13 14 15 16]
     [17 18 19 20]]
    ```

3.  Let's print the shape of `A`, just to be sure:

    ```
    A.shape
    ```

    输出如下所示:

    ```
    TensorShape([5, 4])
    ```

    目前，它的排名为 2。我们将使用`reshape()`函数将其等级改为 3。

4.  Now, we will reshape `A` to the shape [5,4,1] using the following command. We've thrown in the `print` command just to see what the output looks like:

    ```
    tf.print(tf.reshape(A,[5,4,1]))
    ```

    我们将得到以下输出:

    ```
    [[[1]
      [2]
      [3]
      [4]]
     [[5]
      [6]
      [7]
      [8]]
     [[9]
      [10]
      [11]
      [12]]
     [[13]
      [14]
      [15]
      [16]]
     [[17]
      [18]
      [19]
      [20]]]
    ```

    这和预期的一样有效。

5.  Let's see the new shape of `A`:

    ```
    A.shape
    ```

    输出如下所示:

    ```
    TensorShape([5, 4])
    ```

    我们可以看到`A`还是一样的形状。还记得我们讨论过为了保存新的形状，我们需要将它分配给自己。让我们在下一步做那件事。

6.  这里，我们将新的形状分配给`A` :

    ```
    A = tf.reshape(A,[5,4,1])
    ```

7.  Let's check the new shape of `A` once again:

    ```
    A.shape
    ```

    我们将看到以下输出:

    ```
    TensorShape([5, 4, 1])
    ```

    因此，我们不仅重塑了矩阵，还将其等级从 2 级改为 3 级。下一步，让我们打印出`A`的内容，以防万一。

8.  Let's see what `A` contains now:

    ```
    tf.print(A)
    ```

    如预期的那样，输出如下:

    ```
    [[[1]
      [2]
      [3]
      [4]]
     [[5]
      [6]
      [7]
      [8]]
     [[9]
      [10]
      [11]
      [12]]
     [[13]
      [14]
      [15]
      [16]]
     [[17]
      [18]
      [19]
      [20]]]
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3gHvyGQ](https://packt.live/3gHvyGQ)。

    你也可以在 https://packt.live/2ZdjdUY 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们看到了如何使用`reshape()`功能。使用`reshape()`，我们可以改变张量的秩和形状。我们还了解到，重塑矩阵会改变矩阵的形状，而不会改变矩阵中元素的顺序。我们学到的另一件重要的事情是，重塑维度必须与矩阵中的元素数量一致。了解了`reshape`函数之后，我们将继续学习下一个函数，Argmax。

## arg max 功能

现在，我们来了解一下`argmax`函数，这是神经网络中经常用到的函数。`Argmax`返回矩阵或张量中最大值沿特定轴的位置。必须注意，它不返回最大值，而是返回最大值的索引位置。

例如，如果`x` = `[1,10,3,5]`，那么`tf.argmax(x)`将返回 1，因为最大值(在本例中为 10)位于索引位置 1。

注意

在 Python 中，索引从 0 开始。因此，考虑前面的`x`示例，元素 1 的索引为 0，元素 10 的索引为 1，依此类推。

现在，假设我们有以下内容:

![Figure 1.25: An example matrix
](img/B15385_01_25.jpg)

图 1.25:一个示例矩阵

在这种情况下，`argmax`必须与`axis`参数一起使用。当`axis`等于 0 时，返回每列最大值的位置，如下图所示:

![Figure 1.26: The argmax operation along axis 0
](img/B15385_01_26.jpg)

图 1.26:沿 0 轴的 argmax 操作

如您所见，第一列中的最大值是 9，因此在本例中，索引将是 2。类似地，如果我们移到第二列，最大值是 5，其索引为 0。在第三列中，最大值是 8，因此索引是 1。如果我们在前面的矩阵上运行`argmax`函数，并且`axis`为 0，我们将得到以下输出:

```
[2,0,1]
```

当`axis` = 1 时，`argmax`返回每行中最大值的位置，如下所示:

![Figure 1.27: The argmax operation along axis 1
](img/B15385_01_27.jpg)

图 1.27:沿轴 1 的 argmax 操作

沿着行移动，索引 1 处有 5 个，索引 2 处有 8 个，索引 0 处有 9 个。如果我们在轴为 1 的前面的矩阵上运行`argmax`函数，我们将得到以下输出:

```
[1,2,0]
```

这样，让我们试着在矩阵上实现`argmax`。

## 练习 1.05:实现 argmax()函数

在本练习中，我们将使用`argmax`函数来查找给定矩阵中最大值沿 0 轴和 1 轴的位置。按照以下步骤完成本练习:

1.  导入`tensorflow`并创建以下矩阵:

    ```
    import tensorflow as tf X=tf.Variable([[91,12,15], [11,88,21],[90, 87,75]])
    ```

2.  Let's print `X` and see what the matrix looks like:

    ```
    tf.print(X)
    ```

    输出如下所示:

    ```
    [[91 12 15]
     [11 88 21]
     [90 87 75]]
    ```

3.  Print the shape of `X`:

    ```
    X.shape
    ```

    输出如下所示:

    ```
    TensorShape([3, 3])
    ```

4.  Now, let's use `argmax` to find the positions of the maximum values while keeping `axis` as `0`:

    ```
    tf.print(tf.argmax(X,axis=0))
    ```

    输出如下所示:

    ```
    [0 1 2]
    ```

    参考*步骤 2* 中的矩阵，我们可以看到，移动穿过列，第一列中的最大值(91)的索引是 0。类似地，沿着第二列(88)的最大值的索引是 1。最后，第三列(75)上的最大值的索引为 2。因此，我们有前面提到的输出。

5.  Now, let's change the `axis` to `1`:

    ```
    tf.print(tf.argmax(X,axis=1))
    ```

    输出如下所示:

    ```
    [0 1 0]
    ```

再次参考*步骤 2* 中的矩阵，如果我们沿着行移动，第一行的最大值是 91，在索引 0 处。类似地，第二行的最大值是 88，位于索引 1 处。最后，第三行再次位于索引 0 处，最大值为 75。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ZR5q5p](https://packt.live/2ZR5q5p)。

你也可以在 https://packt.live/3eewhNO[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/3eewhNO)

在这个练习中，我们学习了如何使用`argmax`函数找到张量给定轴上最大值的位置。当我们使用神经网络执行分类时，这将在随后的章节中使用。

## 优化者

在我们看神经网络之前，让我们学习一个更重要的概念，那就是优化器。优化器广泛用于训练神经网络，因此了解它们的应用非常重要。在本章中，让我们对优化器的概念有一个基本的介绍。您可能已经知道，机器学习的目的是找到一个将输入映射到输出的函数(及其参数)。

例如，假设数据分布的原始函数是以下形式的线性函数(线性回归):

```
Y = mX + b
```

这里，`Y`是因变量(标签)，`X`是自变量(特征)，`m`和`b`是模型的参数。用机器学习解决这个问题需要学习参数`m`和`b`，从而学习连接`X`和`Y`的函数形式。一旦学习了参数，如果给我们一个新的`X`值，我们就可以计算或预测`Y`的值。优化器正是在学习这些参数的过程中发挥作用的。学习过程包括以下步骤:

1.  假设参数`m`和`b`的一些任意随机值。
2.  利用这些假定的参数，对于给定的数据集，估计每个`X`变量的`Y`值。
3.  找出`Y`的预测值和与`X`变量相关的`Y`的实际值之间的差值。这种差异被称为**损失函数**或**成本函数**。损失的大小将取决于我们最初假设的参数值。如果假设与实际值相差甚远，那么损失将会很大。获得正确参数的方法是通过改变或变更参数的初始假设值，使损失函数最小化。这种改变参数值以降低损失函数的任务称为优化。

深度学习中使用了不同类型的优化器。一些最流行的是随机梯度下降，亚当，和 RMSprop。优化器的详细功能和内部工作方式将在*第 2 章，神经网络*中描述，但在这里，我们将看到它们如何应用于解决某些常见问题，如简单的线性回归。在本章中，我们将使用一个叫做 Adam 的优化器，它是一个非常流行的优化器。我们可以使用以下代码在 TensorFlow 中定义 Adam 优化器:

```
tf.optimizers.Adam()
```

一旦定义了优化器，我们就可以使用下面的代码来最小化损失:

```
optimizer.minimize(loss,[m,b])
```

术语`[m,b]`是在优化过程中将被改变的参数。现在，让我们使用一个优化器来训练一个使用 TensorFlow 的简单线性回归模型。

## 练习 1.06:使用优化器进行简单的线性回归

在本练习中，我们将了解如何使用优化器来训练简单的线性回归模型。我们将从假设线性方程`w*x + b`中参数`w`和`b`的任意值开始。使用优化器，我们将观察参数值如何变化以获得正确的参数值，从而映射输入值(`x`)和输出值(`y`)之间的关系。使用优化的参数值，我们将预测某些给定输入值(`x`)的输出(`y`)。完成此练习后，我们将看到由优化参数预测的线性输出非常接近输出值的真实值。按照以下步骤完成本练习:

1.  打开一个 Jupyter 笔记本，重命名为*练习 1.06* 。
2.  导入`tensorflow`，创建变量，并将其初始化为 0。这里，我们假设这两个参数的值都是零:

    ```
    import tensorflow as tf w=tf.Variable(0.0) b=tf.Variable(0.0)
    ```

3.  为线性回归模型定义一个函数。我们之前学习了如何在 TensorFlow 中创建函数:

    ```
    def regression(x):     model=w*x+b     return model
    ```

4.  以特征(`x`)和标签(`y`)的形式准备数据:

    ```
    x=[1,2,3,4] y=[0,-1,-2,-3]
    ```

5.  定义`loss`功能。在这种情况下，这是预测值和标签之差的绝对值:

    ```
    loss=lambda:abs(regression(x)-y)
    ```

6.  创建一个学习率为`.01`的`Adam`优化器实例。学习率定义了优化器应该以什么样的速度改变假设的参数。我们将在后续章节中讨论学习率:

    ```
    optimizer=tf.optimizers.Adam(.01)
    ```

7.  通过运行优化器 1000 次迭代来训练模型，以最小化损失:

    ```
    for i in range(1000):     optimizer.minimize(loss,[w,b])
    ```

8.  Print the trained values of the `w` and `b` parameters:

    ```
    tf.print(w,b)
    ```

    输出如下所示:

    ```
    -1.00371706 0.999803364
    ```

    我们可以看到，`w`和`b`参数的值已经从它们的原始值 0 改变了。这是在优化过程中完成的。这些更新的参数值将用于预测`Y`的值。

    注意

    优化过程本质上是随机的(具有随机概率分布)，您可能会得到与此处打印的值不同的`w`和`b`的值。

9.  Use the trained model to predict the output by passing in the `x` values. The model predicts the values, which are very close to the label values (`y`), which means the model was trained to a high level of accuracy:

    ```
    tf.print(regression([1,2,3,4]))
    ```

    上述命令的输出如下所示:

    ```
    [-0.00391370058 -1.00763083 -2.01134801 -3.01506495]
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3gSBs8b](https://packt.live/3gSBs8b)。

    你也可以在 https://packt.live/2OaFs7C 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们看到了如何使用优化器来训练简单的线性回归模型。在这个练习中，我们看到了最初假设的参数值是如何更新为真实值的。使用参数的真实值，我们能够得到接近实际值的预测。了解如何应用优化器将有助于您稍后训练神经网络模型。

现在我们已经看到了优化器的使用，让我们利用我们所学的知识，在下一个活动中应用优化函数来求解一个二次方程。

## 活动 1.01:使用优化器求解二次方程

在本活动中，您将使用优化器来求解以下二次方程:

![Figure 1.28: A quadratic equation
](img/B15385_01_28.jpg)

图 1.28:一个二次方程

以下是完成本活动需要遵循的高级步骤:

1.  打开一个新的 Jupyter 笔记本并导入必要的包，就像我们在前面的练习中所做的那样。
2.  初始化变量。请注意，在本例中，`x`是您需要初始化的变量。您可以将其初始化为 0 值。
3.  使用`lambda`函数构造`loss`函数。`loss`函数将是您试图求解的二次方程。
4.  使用学习率为`.01`的`Adam`优化器。
5.  针对不同的迭代运行优化器，并将损失降至最低。您可以从 1，000 开始迭代次数，然后在后续试验中增加迭代次数，直到获得您想要的结果。
6.  打印`x`的优化值。

预期产出如下:

```
4.99919891
```

请注意，虽然您的实际输出可能略有不同，但它应该是一个接近 5 的值。

注意

本活动的详细步骤，以及解决方案和附加注释在第 388 页上提供。

# 总结

这就把我们带到了本章的结尾。让我们重温一下到目前为止所学的内容。我们从研究人工智能、机器学习和深度学习之间的关系开始。然后，我们通过对图像进行分类，然后使用 Google API 实现文本到语音的转换，实现了深度学习的演示。接下来是对深度学习的不同使用案例和类型的简要描述，如 MLP、CNN、RNN 和甘斯。

在下一节中，我们介绍了 TensorFlow 框架，并了解了一些基本的构建模块，如张量及其秩和形状。我们还使用 TensorFlow 实现了不同的线性代数运算，比如矩阵乘法。在本章的后面，我们执行了一些有用的操作，比如`reshape`和`argmax`。最后，我们介绍了优化器的概念，并使用优化器实现了数学表达式的解决方案。

现在，我们已经为深度学习奠定了基础，并向您介绍了 TensorFlow 框架，为您深入了解神经网络的迷人世界奠定了基础。在下一章中，将向您介绍神经网络，在后续章节中，我们将了解更深入的深度学习概念。我们希望你喜欢这个迷人的旅程。**