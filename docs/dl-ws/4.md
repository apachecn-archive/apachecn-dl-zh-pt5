

# 四、文本嵌入的深度学习

概观

在这一章中，我们将开始研究文本的自然语言处理。我们将从使用**自然语言工具包**对原始文本数据执行文本预处理开始，我们将对原始文本进行标记，并删除标点符号和停用词。随着本章的深入，我们将实现文本表示的经典方法，比如一键编码和 **TF-lDF** 方法。本章展示了词嵌入的力量，并解释了流行的基于深度学习的嵌入方法。我们将使用**跳跃式语法**和**连续单词包**算法来生成我们自己的词嵌入。我们将探索嵌入的属性，算法的不同参数，并为短语生成向量。到本章结束时，你将能够处理文本数据，并通过使用预先训练的模型开始使用词嵌入，以及你自己的嵌入。

# 简介

当你让 Siri“*播放一首 80 年代的老歌*”时，她是如何准确知道该怎么做的？谷歌如何在几分之一秒内找到与你的不良搜索查询最相关的结果？你的翻译应用程序如何几乎瞬间将文本从德语翻译成英语？您的电子邮件客户端如何保护您并自动识别所有恶意垃圾邮件/网络钓鱼邮件？所有这些问题的答案，以及为更多令人惊奇的应用提供动力的东西，是使用自然语言处理。

到目前为止，我们已经处理了结构化的数字数据——也是数字矩阵的图像。在这一章中，我们将从谈论处理文本数据开始讨论，并解开驾驭这个非结构化信息的金矿所需的技能。我们将在本章中讨论一个关键的概念——表示，特别是使用嵌入。我们将讨论考虑事项并实现表示方法。我们将从最简单的方法开始，以词嵌入结束——这是一种非常强大的表示文本数据的方法。当结合深度学习方法时，词嵌入将帮助您在 NLP 任务中获得最先进的结果。

NLP 是一个与帮助机器理解自然(人类)语言有关的领域。如下图所示，NLP 位于语言学、计算机科学和人工智能的交叉点:

![Figure 4.1: Where NLP fits
](img/B15385_04_01.jpg)

图 4.1:NLP 的适用范围

这是一个广阔的领域——想想所有使用语言(口语和书面语)的地方。NLP 支持上图中列出的各种应用程序，包括:

*   文件分类(文本分类)
*   语言之间的翻译，比如说，从德语到英语(序列到序列学习)
*   自动分类推文或电影评论的情感(情感分析)
*   全天候即时回复您查询的聊天机器人

在我们进一步讨论之前，我们需要认识到 NLP 并不容易。考虑下面这个句子:“*这个男孩看见一个人拿着望远镜。*

谁有望远镜？那个男孩用望远镜透过望远镜看到那个人了吗？还是那个人随身带着望远镜？有一种歧义，单靠这句话是解决不了的。也许更多的背景知识会帮助我们搞清楚这一点。

让我们来看看这句话:“*拉辛说服莫汉给他自己买了一台电视机。*“电视是给谁买的——拉希姆还是莫汉？这是另一个模糊的例子，我们也许可以通过更多的上下文来解决，但同样，对于机器/程序来说，这可能是非常困难的。

让我们考虑另一个例子:“*拉希姆已经退出跳伞。*“这句话暗示拉希姆进行了相当多的跳伞运动。这句话有一个预设，机器很难推断。

语言是一个复杂的系统，它使用符号(单词/术语)并以多种方式组合它们来交流思想。理解语言并不总是很容易，原因有很多。到目前为止，歧义是最大的原因:单词在不同的上下文中可能有不同的含义。再加上潜台词，不同的视角，等等。我们永远无法确定不同的人对相同的词是否有相同的理解。一首诗可以被阅读它的人以多种方式解读，每个读者带来他们对世界的独特视角和理解，并以他们自己的方式理解这首诗。

# 自然语言处理的深度学习

深度学习的出现对许多领域产生了强烈的积极影响，NLP 也不例外。现在，你可以体会到深度学习方法给了我们前所未有的准确性，这帮助我们在许多领域取得了进步。NLP 中有几个任务从深度学习方法中获得了巨大的收益。以前，使用情感预测、机器翻译和聊天机器人的应用程序需要大量的人工干预。通过深度学习和 NLP，这些任务完全自动化，并带来令人印象深刻的性能。*图 4.2* 中显示的简单、高级视图显示了深度学习如何用于处理自然语言。深度学习不仅为我们提供了机器可以理解的自然语言的伟大表示，还提供了非常强大的建模方法，非常适合 NLP 中的任务。

![Figure 4.2: Deep learning for NLP
](img/B15385_04_02.jpg)

图 4.2:自然语言处理的深度学习

也就是说，我们需要谨慎，以避免低估让机器执行涉及人类语言和 NLP 领域的任务的难度。深度学习并没有解决 NLP 中的所有挑战，但它确实导致了 NLP 中几项任务的处理方式发生了范式转变，并帮助推进了该领域的一些应用，使原本困难的任务变得对任何人和任何人都容易实现。我们将在*第五章*、*深度学习序列*中执行其中一些。

其中一个关键任务是文本数据表示——简单地说，就是将原始文本转换成模型可以理解的内容。词嵌入构成了一种基于深度学习的方法，这种方法改变了游戏，并提供了一种非常强大的文本表示。我们将详细讨论嵌入，并在本章的后面创建我们自己的嵌入。首先，让我们动手处理一些文本，并执行一些非常重要的数据准备工作。

## 文本数据处理入门

首先，让我们将一些测试数据放入 Python。首先，我们将创建一些我们自己的玩具数据，并熟悉这些工具。然后，我们将使用刘易斯·卡罗尔的经典作品，“*爱丽丝梦游仙境*”，该作品可通过古腾堡计划([gutenberg.org](http://gutenberg.org))获得。非常方便的是，我们可以通过**自然语言工具包** ( **NLTK** )轻松访问它，这是一个用于从头开始执行 NLP 的伟大库。

注意

本章的代码实现可以在 https://packt.live/3gEgkSP[找到。本章中的所有代码必须在一个 Jupyter 笔记本中运行。](https://packt.live/3gEgkSP)

NLTK 应该随 Anaconda 发行版一起提供。如果没有，您可以通过在命令行中使用以下命令来安装 NLTK:

```
pip install nltk
```

这应该可以在 Windows 上运行。对于 macOS 和 Linux，您可以使用以下命令:

```
$ sudo pip install -U nltk
```

我们的虚拟数据可以使用下面的命令创建(我们在这里使用 Jupyter 笔记本；随意使用任何界面):

```
raw_txt = """Welcome to the world of Deep Learning for NLP! \
             We're in this together, and we'll learn together. \
             NLP is amazing, \
             and Deep Learning makes it even more fun. \
             Let's learn!"""
```

我们在`raw_txt`中有文本，它是一个字符串变量，所以现在，我们准备开始处理它。

## 文本预处理

文本预处理是指为您的主要分析/模型准备好文本数据的过程。不管你的最终目标是什么——可以是情感分析、分类、聚类或许多其他目标中的任何一个——你都需要清理原始文本数据，为分析做好准备。这是任何涉及 NLP 的应用程序的第一部分。

我们说的**清理**是什么意思，文本数据什么时候准备好？我们知道，我们在日常生活中遇到的文本数据可能非常混乱(想想社交媒体、产品评论、服务评论等等)，并且有各种不完美之处。根据手头的任务和您正在处理的数据类型，您关心的不完美之处会有所不同，而**清理**可能意味着非常不同的事情。例如，在一些应用程序中，预处理可能仅仅意味着“将句子分成单独的术语”您在这里采取的步骤能够并且将会对您的分析的最终结果产生影响。让我们更详细地讨论这个问题。

### 标记化

预处理的第一步必然是**记号化**——将原始输入文本序列分割成**记号**。简单来说，就是将原始文本分解成你想要处理的组成元素。这个记号可以是一个段落、句子、单词，甚至是一个字符。如果你想把一个段落分成句子，你可以把这个段落标记成句子。如果你想把一个句子中的单词分开，那么你可以把这个句子标记成单词。

对于我们的原始文本，首先，我们要分离句子。为此，我们在 Python 中有多个选项——这里，我们将使用 NLTK 中的 tokenize API。

注意

我们将在整本书中使用 Jupyter 笔记本，这是我们推荐的。但是，您可以随意使用任何 IDE。

在使用 API 之前，我们必须`import nltk`并下载`punkt`句子分词器。然后，我们需要导入`tokenize`库。所有这些都可以使用以下命令来完成:

```
import nltk
nltk.download('punkt')
from nltk import tokenize
```

tokenize API 具有为不同类型的数据提取不同级别的标记(句子、单词或字符)的工具(也是一个非常方便的 tweet 标记器)。这里我们将使用`sent_tokenize()`方法。`sent_tokenize()`方法将输入文本分解成句子。让我们来看看它的实际应用:

```
tokenize.sent_tokenize(raw_txt)
```

这将为我们提供以下单独的句子:

```
['Welcome to the world of Deep Learning for NLP!',
 "We're in this together, and we'll learn together.",
 'NLP is amazing, and Deep Learning makes it even more fun.',
 "Let's learn!"]
```

从输出来看，似乎`sent_tokenize()`做得相当不错。它已经正确地识别了句子的边界，并给出了我们所期望的四个句子。为了便于处理，我们将结果赋给一个变量，并检查结果的数据类型及其构成:

```
txt_sents = tokenize.sent_tokenize(raw_txt)
type(txt_sents), len(txt_sents)
```

以下是上述代码的输出:

```
(list, 4)
```

正如我们所看到的，这是一个包含四个元素的列表，其中每个元素都包含字符串形式的句子。

我们可以尝试使用`word_tokenize()`方法将句子分解成单个单词。这种方法将给定的句子分解成组成它的单词。它使用智能规则来计算单词边界。为了方便起见，我们使用列表理解(Python 中的理解是一种构造新序列的简洁方法):

```
txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]
type(txt_words), type(txt_words[0])
```

前面的命令给出了以下输出:

```
(list, list)
```

输出是预期的——结果列表的元素是列表本身，包含组成句子的单词。让我们也打印出结果的前两个元素:

```
print(txt_words[:2])
```

输出如下所示:

```
[['Welcome', 'to', 'the', 'world', 'of', 
  'Deep', 'Learning', 'for', 'NLP', '!'], 
  ['We', "'re", 'in', 'this', 'together', 
  ',', 'and', 'we', "'ll", 'learn', 'together', '.']]
```

这些句子被分成了单个的单词。我们还可以看到，像“We'll”这样的缩写已经被分解成成分，即“we”和“' ll”。所有标点符号(逗号、句号、感叹号等)都是单独的标记。如果我们希望删除它们，这对我们来说非常方便，我们稍后会这样做。

### 正火情况

另一个常见的步骤是规范化大小写——我们通常不希望“汽车”、“汽车”、“汽车”和“汽车”被视为单独的实体。为此，我们通常将所有文本转换为小写(如果我们愿意，也可以将其转换为大写)。

Python 中的所有字符串都有一个`lower()`方法，因此将一个字符串变量(`strvar`)转换成小写就像`strvar.lower()`一样简单。

注意

我们可以在开始时，在标记化之前使用它，它会像`raw_txt = raw_txt.lower()`一样简单。

我们将在标记成单个句子后，使用`lower()`方法对我们的数据进行规范化。我们将使用以下命令来实现这一点:

```
txt_sents  = [sent.lower() for sent in txt_sents]
txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]
```

让我们打印出几个句子，看看结果是什么样的:

```
print(txt_words[:2])
```

输出如下所示:

```
[['welcome', 'to', 'the', 'world', 'of', 
  'deep', 'learning', 'for', 'nlp', '!'], 
 ['we', "'re", 'in', 'this', 'together', 
  ',', 'and', 'we', "'ll", 'learn', 'together', '.']]
```

我们可以看到这次输出中所有的术语都是小写的。我们提取原始文本，将其分解成句子，规范化大小写，然后将其分解成单词。现在，我们有了我们需要的所有记号，但是我们似乎仍然有许多标点符号作为记号，我们需要去掉。让我们继续执行更多的“清理”。

### 删除标点符号

我们可以看到，数据目前将所有标点符号作为单独的标记。再次提醒你，在有些任务中，标点符号可能很重要。举个例子，在进行情感分析，也就是预测文本中的情感是积极的还是消极的时候，感叹号可以增值。对于我们的任务，让我们去掉这些，因为我们只对表示语言的术语感兴趣。要做到这一点，我们需要有一个我们想要删除的所有标点符号的列表。幸运的是，我们在 Python 的字符串库中有这样一个列表，我们可以简单地将其导入并赋给一个列表变量:

```
from string import punctuation
list_punct = list(punctuation)
print(list_punct)
```

您应该得到以下输出:

```
['!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', 
 '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', 
 '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']
```

所有常用的标点符号在这里都有。如果您想删除任何额外的标点符号，您可以简单地将它们添加到`list_punct`变量中。

我们可以定义一个函数从给定的符号列表中删除标点符号。该函数将期待一个令牌列表，它将从中删除在`list_punct`变量中可用的令牌:

```
def drop_punct(input_tokens):
    return [token for token in input_tokens \
            if token not in list_punct]
```

我们可以使用以下命令在一些虚拟令牌上测试这一点:

```
drop_punct(["let",".","us",".","go","!"])
```

我们得到以下结果:

```
['let', 'us', 'go']
```

该功能按预期工作。现在，我们需要将我们在上一节中修改的`txt_words`变量传递给我们刚刚创建的`drop_punct`函数。我们将把结果存储在一个名为`txt_words_nopunct`的新变量中:

```
txt_words_nopunct = [drop_punct(sent) for sent in txt_words]
print(txt_words_nopunct)
```

我们将获得以下输出:

```
[['welcome', 'to', 'the', 'world', 'of', 
  'deep', 'learning', 'for', 'nlp'], 
 ['we', "'re", 'in', 'this', 'together', 'and', 
  'we', "'ll", 'learn', 'together'], 
 ['nlp', 'is', 'amazing', 'and', 
  'deep', 'learning', 'makes', 'it', 'even', 'more', 'fun'], 
 ['let', "'s", 'learn']]
```

从前面的输出可以看出，我们创建的函数已经从原始文本中删除了所有标点符号。现在，没有标点符号的数据看起来更干净，但是我们仍然需要去掉非信息术语。我们将在下一节讨论这一点。

### 删除停用词

在日常语言中，我们有很多术语并没有增加很多信息/价值*。这些通常被称为“停用词”。我们可以认为这些属于两大类:

1.  **一般/功能**:这些是语言中的填充词，它们不提供大量信息，但有助于将其他信息词缝合在一起，形成有意义的句子，如“the”、“an”、“of”等。
2.  **Contextual**: These aren't general functional terms, but given the context, don't add a lot of value. If you're working with reviews of a mobile phone, where all reviews are talking about the phone, the term "phone" itself may not add a lot of information.

    注意

    *“价值”的概念随着每项任务而变化。像“the”和“and”这样的功能词对于自动文档主题分类来说可能并不重要，但是对于其他应用来说却非常重要，比如词性标注(识别动词、形容词、名词、代词等等)。

NLTK 中方便地内置了功能性停用词。我们只需要导入它们，然后将它们存储在一个变量中。一旦存储，它们就可以像任何 Python 列表一样被访问。让我们导入它们，看看我们有多少这样的单词:

```
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
list_stop = stopwords.words("english")
len(list_stop)
```

我们将看到以下输出:

```
179
```

我们可以看到，我们有 179 个内置停用词。让我们也打印其中的一些:

```
print(list_stop[:50])
```

输出如下所示:

```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 
 'you', "you're", "you've", "you'll", "you'd", 'your', 
 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 
 'himself', 'she', "she's", 'her', 'hers', 'herself', 
 'it', "it's", 'its', 'itself', 'they', 'them', 
 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 
 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 
 'was', 'were', 'be']
```

我们可以看到，这些术语中的大多数都是非常常用的“填充”术语，在语言中具有“功能”作用，并没有添加很多信息。

现在，删除停用词可以像删除标点符号一样完成。

## 练习 4.01:标记化、大小写规范化、标点符号和停用词移除

在本练习中，我们将从数据中删除停用词，并应用到目前为止我们所学的所有内容。我们将从执行标记化(句子和单词)开始；然后，我们将执行大小写规范化，然后删除标点符号和停用词。

注意

在开始这个练习之前，确保你使用的是 Jupyter 笔记本，你已经下载了`punkt`句子分词器和`stopwords`语料库，如*文本预处理*部分所示。

这次我们将保持代码简洁。我们将定义和操作`raw_txt`变量。让我们开始吧:

1.  运行以下命令导入`nltk`并从中导入`tokenize`模块:

    ```
    import nltk from nltk import tokenize
    ```

2.  定义`raw_txt`变量，使其包含文本`Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!`:

    ```
    raw_txt = """Welcome to the world of deep learning for NLP! \              We're in this together, and we'll learn together. \              NLP is amazing, \              and deep learning makes it even more fun. \              Let's learn!"""
    ```

3.  Use the `sent_tokenize()` method to separate the raw text into individual sentences and store the result in a variable. Use the `lower()` method to convert the string into lowercase before tokenizing:

    ```
    txt_sents = tokenize.sent_tokenize(raw_txt.lower())
    ```

    注意

    我们刚刚创建的`txt_sents`变量也将在本章后面用到。

4.  使用列表理解，应用`word_tokenize()`方法将每个句子分成它的组成单词:

    ```
    txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]
    ```

5.  从`string`模块导入`punctuation`并转换成列表:

    ```
    from string import punctuation stop_punct = list(punctuation)
    ```

6.  从 NLTK 导入英语的内置停用词，并保存在一个变量中:

    ```
    from nltk.corpus import stopwords stop_nltk = stopwords.words("english")
    ```

7.  创建一个包含标点符号和 NLTK 停用词的组合列表。请注意，我们可以一次性删除它们:

    ```
    stop_final = stop_punct + stop_nltk
    ```

8.  定义一个函数，该函数将从输入的句子中删除停用词和标点符号，作为标记集合提供:

    ```
    def drop_stop(input_tokens):     return [token for token in input_tokens \             if token not in stop_final]
    ```

9.  通过将函数应用于标记化的句子来移除冗余标记，并将结果存储在变量中:

    ```
    txt_words_nostop = [drop_stop(sent) for sent in txt_words]
    ```

10.  Print the first cleaned-up sentence from the data:

    ```
    print(txt_words_nostop[0])
    ```

    删除停用词后，结果将如下所示:

    ```
    ['welcome', 'world', 'deep', 'learning', 'nlp']
    ```

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/2VVNEgf 的。

    你也可以在 https://packt.live/38Gr54r 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们执行了到目前为止所学的所有清理步骤。这一次，我们合并了某些步骤，使代码更加简洁。这些是我们在处理文本数据时应该应用的一些非常常见的步骤。您可以尝试通过定义一个在所有处理步骤后返回结果的函数来进一步优化和模块化。我们鼓励您尝试一下。

到目前为止，清理过程中的步骤是去除在我们的评估中不太有用的令牌。但是，我们还可以做一些事情来使我们的数据更好——我们可以尝试使用我们对语言的理解来组合标记，识别几乎具有相同含义的标记，并进一步删除冗余。两种流行的方法是词干化和词形还原。

注意

我们在本练习中创建的变量也将在本章的后面部分使用。在进入下一个练习和活动之前，确保你先完成这个练习。

## 词干化和词形还原

“吃”、“吃”、“吃”、“吃了”——不都是同一个词的变体，都是指同一个动作吗？一般来说，在大多数文本和口语中，同一个词有多种形式。通常，我们不希望这些被认为是单独的令牌。如果查询是“红鞋子”或“红鞋子”，搜索引擎将需要返回类似的结果——否则这将是一个可怕的搜索体验。我们承认这种情况非常普遍，我们需要一种策略来处理这种情况。但是我们应该如何处理一个单词的变体呢？一个合理的方法是将它们都映射到一个公共的令牌，这样它们都被同等对待。

词干化是一种基于规则的方法，通过将单词简化为其“词干”来实现规范化。词干是单词在添加任何词缀(构成变体的成分)之前的词根。这种方法相当简单——去掉后缀得到词干。一个流行的算法是**波特词干提取**算法，它应用了一系列这样的规则:

![Figure 4.3: Examples of the Porter stemming algorithm's rule-based approach
](img/B15385_04_03.jpg)

图 4.3:波特词干算法基于规则的方法的例子

注意

波特词干算法规则的完整集合可以在[http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html)找到。

让我们看看波特词干算法的运行情况。让我们从 NLTK 中的`'stem'`模块导入`PorterStemmer`函数，并创建它的一个实例:

```
from nltk.stem import PorterStemmer
stemmer_p = PorterStemmer()
```

请注意，词干分析器作用于单个标记，而不是整个句子。让我们看看词干分析器是如何阻止单词“`driving`”的:

```
print(stemmer_p.stem("driving"))
```

输出如下所示:

```
drive
```

让我们看看如何将这个应用到整个句子中。请注意，我们必须对句子进行标记:

```
txt = "I mustered all my drive, drove to the driving school!"
```

以下代码用于标记句子并将词干分析器应用于每个词:

```
tokens = tokenize.word_tokenize(txt)
print([stemmer_p.stem(word) for word in tokens])
```

输出如下所示:

```
['I', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', 
 'the', 'drive', 'school', '!']
```

我们可以看到词干分析器已经正确地将“musterered”简化为“must ”,将“driving”简化为“drive ”,而“drive”保持不变。另外，注意词干分析器的结果不一定是有效的英语单词。

词形还原是一种更复杂的方法，它查阅字典并找到单词的有效词根形式(词汇)。当单词的词性也被提供时，词形还原效果最好——它考虑了该术语所扮演的角色，并返回适当的形式。词形还原步骤的输出总是一个有效的英语单词。然而，词形还原在计算上是非常昂贵的，为了使其工作良好，它需要词性标记，而这通常在数据中是不可用的。让我们简单看一下。首先，让我们从`nltk.stem`导入`WordNetLemmatizer`并实例化它:

```
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
```

让我们将`lemmatizer`应用于术语`ponies`:

```
lemmatizer.lemmatize("ponies")
```

以下是输出:

```
'pony'
```

对于我们的讨论，词干就足够了。词干提取的结果可能不总是有效的单词。例如，`poni`是`ponies`的词干，但不是一个有效的英语单词。此外，可能会有一些不准确，但对于映射到一个常见单词的目标来说，这种粗糙的方法工作得很好。

## 练习 4.02:提取数据的词干

在本练习中，我们将继续进行数据预处理。在上一个练习中，我们删除了停用词和标点符号。现在，我们将使用波特词干提取算法来提取记号。由于我们将使用我们之前创建的`txt_words_nostop`变量，让我们继续使用我们在*练习 4.01* 、*标记化、大小写规范化、标点符号和停用词移除*中创建的 Jupyter 笔记本。此时，变量将包含以下文本:

```
[['welcome', 'world', 'deep', 'learning', 'nlp'],
 ["'re", 'together', "'ll", 'learn', 'together'],
 ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],
 ['let', "'s", 'learn']]
```

以下是完成本练习的步骤:

1.  使用以下命令从 NLTK 导入【T7:

    ```
    from nltk.stem import PorterStemmer
    ```

2.  实例化词干分析器:

    ```
    stemmer_p = PorterStemmer()
    ```

3.  Apply the stemmer to the first sentence in `txt_words_nostop`:

    ```
    print([stemmer_p.stem(token) for token in txt_words_nostop[0]])
    ```

    当我们打印结果时，我们得到以下输出:

    ```
    ['welcome has been changed to welcom and learning to learn. This is consistent with the rules of the Porter stemming algorithm.
    ```

4.  对数据中的所有句子应用词干分析器。你可以使用循环，或者嵌套列表理解:

    ```
    txt_words_stem = [[stemmer_p.stem(token) for token in sent] \                    for sent in txt_words_nostop]
    ```

5.  Print the output using the following command:

    ```
    txt_words_stem
    ```

    输出如下所示:

    ```
    [['welcom', 'world', 'deep', 'learn', 'nlp'],
     ["'re", 'togeth', "'ll", 'learn', 'togeth'],
     ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],
     ['let', "'s", 'learn']]
    ```

看起来词干分析器已经做了大量的修改。许多单词不再有效，但仍然可以识别，这没关系。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

你也可以在[https://packt.live/38Gr54r](https://packt.live/38Gr54r)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们使用了波特词干提取算法来提取符号化数据的词干。词干处理对单个术语起作用，所以它需要在标记成术语后应用。词干化将一些术语简化为它们的基本形式，这些形式不一定是有效的英语单词。

### 超越词干化和词形还原

除了词干化和词形还原，还有许多处理单词变化的具体方法。我们有像语音散列法这样的技术来识别由发音引起的单词拼写变化。然后，有拼写纠正，以确定和纠正拼写错误。另一个潜在的步骤是缩写处理，以便*电视*和*电视*被同等对待。通过执行特定于领域的术语处理，可以进一步增强这些步骤的结果。你明白了…有很多可能的步骤，根据你的数据和应用的关键程度，你可以在你的处理中包括其中的一些。

不过，总的来说，我们一起执行的步骤很大程度上是足够的——大小写规范化、标记化、停用词和标点符号删除，然后是词干化/词条化。这些是大多数 NLP 应用程序包含的一些常见步骤。

### 使用 NLTK 下载文本语料库

到目前为止，我们已经对我们创建的虚拟数据执行了这些步骤。现在，是时候在一个更大更真实的文本上尝试我们新获得的技能了。首先，让我们获取那篇课文——刘易斯·卡罗尔的经典作品，《爱丽丝漫游奇境记》*，可以通过古腾堡计划获得，也可以通过 NLTK 获得。*

您可能需要通过 NLTK 下载`'gutenberg'`语料库。首先，使用以下命令导入 NLTK:

```
import nltk
```

然后，使用`nltk.download()`命令打开一个 app，即 **NLTK 下载器**界面(如下图截图所示):

```
nltk.download()
```

我们可以看到该应用程序有多个选项卡。点击**全集**标签页:

![Figure 4.4: NLTK Downloader
](img/B15385_04_04.jpg)

图 4.4: NLTK 下载器

在`Corpora`选项卡中，向下滚动直到到达`gutenberg`。如果状态为`not installed`，继续点击左下角的`Download`按钮。那要安装`gutenberg`文集了:

![Figure 4.5: NLTK Downloader's Corpora tab
](img/B15385_04_05.jpg)

图 4.5: NLTK 下载器的语料库选项卡

关闭界面。现在，你可以直接从 NLTK 获取一些经典文本。我们将读入文本并将其存储在一个变量中:

```
alice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')
```

文本存储在`alice_raw`中，这是一个大字符串。让我们来看看这个字符串的前几个字符:

```
alice_raw[:800]
```

输出如下所示:

```
"[Alice's Adventures in Wonderland by Lewis Carroll 1865]
  \n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was beginning
  to get very tired of sitting by her sister on the\nbank, 
  and of having nothing to do: once or twice she had peeped 
  into the\nbook her sister was reading, but it had no pictures 
  or conversations in\nit, 'and what is the use of a book,' 
  thought Alice 'without pictures or\nconversation?'
  \n\nSo she was considering in her own mind 
  (as well as she could, for the\nhot day made her feel 
  very sleepy and stupid), whether the pleasure\nof making 
  a daisy-chain would be worth the trouble of getting up 
  and\npicking the daisies, when suddenly a White Rabbit 
  with pink eyes ran\nclose by her.\n\nThere was nothing 
  so VERY remarkable in that; nor did Alice think 
  it so\nVERY much out of the way to hear the Rabbit"
```

我们可以在输出中看到原始文本，其中包含我们所期望的常见缺陷——大小写变化、停用词、标点符号等等。

我们准备好了。让我们通过一项活动来测试我们的技能。

## 活动 4.01:文本预处理“爱丽丝梦游仙境”文本

在本练习中，您将把目前为止所学的所有预处理步骤应用到一个更大的真实文本中。我们将使用存储在`alice_raw`变量中的爱丽丝梦游仙境的文本:

```
alice_raw[:800]
```

文本目前看起来是这样的:

```
"[Alice's Adventures in Wonderland by Lewis Carroll 1865]
  \n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was beginning 
  to get very tired of sitting by her sister on the\nbank, 
  and of having nothing to do: once or twice she had peeped 
  into the\nbook her sister was reading, but it had no pictures 
  or conversations in\nit, 'and what is the use of a book,' 
  thought Alice 'without pictures or\nconversation?
  '\n\nSo she was considering in her own mind 
  (as well as she could, for the\nhot day made her feel 
  very sleepy and stupid), whether the pleasure\nof making 
  a daisy-chain would be worth the trouble of getting up 
  and\npicking the daisies, when suddenly a White Rabbit 
  with pink eyes ran\nclose by her.\n\nThere was nothing 
  so VERY remarkable in that; nor did Alice think 
  it so\nVERY much out of the way to hear the Rabbit"
```

到本练习结束时，您将已经清理并标记了数据，删除了许多不完善的地方，删除了停用词和标点符号，并对数据应用了词干。

注意

在开始这个活动之前，确保您已经安装了`gutenberg`语料库并且创建了`alice_raw`变量，如前面标题为*使用 NLTK* 下载文本语料库的小节所示。

以下是您需要执行的步骤:

1.  继续在同一个 Jupyter 笔记本中，使用`'alice_raw'`变量中的原始文本。将原始文本更改为小写。
2.  给句子做记号。
3.  从`string`模块导入标点符号，从 NLTK 导入停用词。
4.  创建一个保存上下文停用词的变量，即`--`和`said`。
5.  创建停用词主列表，从标点符号、NLTK 停用词和上下文停用词中删除包含的术语。
6.  定义一个函数从任何输入句子中删除这些标记(标记化)。
7.  使用 NLTK 的`PorterStemmer`算法对结果进行词干分析。
8.  Print out the first five sentences from the result.

    注意

    本活动的详细步骤以及解决方案和附加注释在第 405 页提供。

预期的输出如下所示:

```
[['alic', "'s", 'adventur', 'wonderland', 'lewi', 'carrol', 
  '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', 
  'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', 
  'peep', 'book', 'sister', 'read', 'pictur', 'convers', 
  "'and", 'use', 'book', 'thought', 'alic', "'without", 
  'pictur', 'convers'], 
 ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', 
  'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', 
  'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', 
  'daisi', 'suddenli', 'white', 'rabbit', 
  'pink', 'eye', 'ran', 'close'], 
 ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', 
  'rabbit', 'say', "'oh", 'dear'], 
 ['oh', 'dear'], 
 ['shall', 'late']]
```

让我们看看迄今为止我们已经取得的成就和未来的发展。

到目前为止，我们已经学习了如何执行文本预处理——为我们的主要分析/模型准备文本数据的过程。我们从原始文本数据开始，这些数据可能有很多缺陷。我们学会了如何处理这些缺陷，现在我们可以轻松地处理文本数据，并为进一步的分析做好准备。这是任何 NLP 应用程序中重要的第一部分。因此，我们采用原始文本数据，并得到干净的数据作为回报。下一步是什么？

下一部分非常重要，因为它对你的分析质量有很大的影响。这就是所谓的代表。大家讨论一下。

## 文本表示注意事项

我们已经将原始输入数据处理成干净的文本。现在，我们需要将这些干净的文本转换成预测模型能够理解的东西。但是预测模型理解什么呢？它能理解不同的单词吗？它像我们一样读单词吗？它能与我们提供给它的文本一起工作吗？

到目前为止，您已经理解了模型是处理数字的。模型的输入是一串数字。它不理解图像，但它可以处理代表这些图像的矩阵和数字。对于处理图像，关键的想法是将它们转换成数字，并从中生成特征。这个想法对于文本也是一样的:我们需要将文本转换成数字，这些数字将作为模型的特征。

**表示**就是将文本转换成模型能够理解的数字/特征。听起来没什么大不了的，对吧？如果你这样认为，那么你需要考虑:输入特征对于任何建模练习都是非常重要的，而表示就是创建这些特征的过程。它对你的模型的结果有非常重要的影响，是一个你应该非常注意的过程。

那么，你是如何进行文本表示的呢？如果真有这样的东西，那么表示文本的“最佳”方式是什么？我们来讨论几种方法。

# 文本表示的经典方法

这些年来，文本表示方法有了很大的发展，神经网络和深度神经网络的出现对我们现在表示文本的方式产生了重大影响(稍后将详细介绍)。事实上，我们已经走了很长一段路:从手工制作功能到标记某个单词是否出现在文本中，再到创建强大的表示，如词嵌入。虽然有很多方法，有些比其他的更适合这个任务，但是我们将讨论几个主要的经典方法，并在 Python 中使用它们。

## 一键编码

一键编码可能是最直观的文本表示方法之一。单词的一个热编码特征是文本中存在的术语的二进制指示符。这是一种很容易解释的简单方法——一个单词的存在或不存在。为了更好地理解这一点，让我们在词干提取之前考虑一下我们的示例文本，并让我们看看对于特定的感兴趣的术语，比如说，`nlp`，一键编码是如何工作的。

让我们使用以下命令来看看文本当前的样子:

```
txt_words_nostop
```

我们可以看到文本是这样的:

```
[['welcome', 'world', 'deep', 'learning', 'nlp'],
 ["'re", 'together', "'ll", 'learn', 'together'],
 ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],
 ['let', "'s", 'learn']]
```

我们感兴趣的词是`nlp`。这是它的一键编码功能的样子:

![Figure 4.6: One-hot encoded feature for 'nlp'
](img/B15385_04_06.jpg)

图 4.6:“NLP”的一键编码特性

我们可以看到这个特征是`1`，但是只适用于有`nlp`的句子，否则就是`0`。我们可以为我们感兴趣的每个单词设置这样的指示变量。所以，如果我们对三个术语感兴趣，我们就做三个这样的特征:

![Figure 4.7: One-hot encoded features for 'nlp', 'deep', and 'learn'
](img/B15385_04_07.jpg)

图 4.7:针对“nlp”、“deep”和“learn”的一键编码功能

让我们在练习中使用 Python 来重现这一点。

## 练习 4.03:为我们的数据创建一次性编码

在本练习中，我们将重复前面的示例。目标术语是`nlp`、`deep`和`learn`。我们将使用自己的函数为这些术语创建一个热编码特性，并将结果存储在一个`numpy`数组中。

同样，我们将使用我们在*练习 4.01* 、*标记化、大小写规范化、标点和停用词移除*中创建的`txt_words_nostop`变量。所以，你需要在同一个 Jupyter 笔记本上继续这个练习。按照以下步骤完成本练习:

1.  Print out the `txt_words_nostop` variable to see what we're working with:

    ```
    print(txt_words_nostop)
    ```

    输出如下所示:

    ```
    [['welcome', 'world', 'deep', 'learning', 'nlp'], 
     ["'re", 'together', "'ll", 'learn', 'together'], 
     ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'], 
     ['let', "'s", 'learn']]
    ```

2.  用目标术语定义一个列表，即`"nlp", "deep", "learn"` :

    ```
    target_terms = ["nlp","deep","learn"]
    ```

3.  Define a function that takes in a single tokenized sentence and returns a `0` or `1` for each target term, depending on its presence in the text. Note that the length of the output is fixed at `3`:

    ```
    def get_onehot(sent):
        return [1 if term in sent else 0 for term in target_terms]
    ```

    我们正在迭代目标术语，并检查它们在输入句子中是否可用。

4.  将函数应用到我们文本中的每个句子，并将结果存储在一个变量中:

    ```
    one_hot_mat = [get_onehot(sent) for sent in txt_words_nostop]
    ```

5.  Import `numpy`, create a `numpy` `array` from the result, and print it:

    ```
    import numpy as np
    np.array(one_hot_mat)
    ```

    该数组的输出如下:

    ```
    array([[1, 1, 0],
           [0, 0, 1],
           [1, 1, 0],
           [0, 0, 1]])
    ```

我们可以看到输出包含四行，每一行对应一个句子。数组中的每一列都包含目标术语的一键编码。“learn”的值是 0，1，0，1，这与我们的预期一致。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

你也可以在 https://packt.live/38Gr54r 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们看到了如何使用一键编码从文本中生成要素。该示例使用了一系列目标术语。当你头脑中有一个非常具体的目标，我们确切地知道哪些术语是有用的时，这可能会起作用。事实上，直到几年前，人们还在大量使用这种方法，从文本中手工制作特征。在许多情况下，这是不可行的——因为我们不知道哪些术语是重要的，所以我们对大量术语(5，000、10，000 甚至更多)使用一次性编码。

另一个方面是该术语的存在/不存在对于大多数情况是否足够。我们不想包含更多的信息吗？也许是这个词的出现频率，而不仅仅是它的出现，或者甚至是其他更聪明的方法？让我们看看这是如何工作的。

## 词频

我们讨论过，一键编码仅仅表示一个术语的存在或不存在。这里一个合理的论点是，词频也很重要。可能在文档中出现次数越多的术语对文档越重要。也许用频率来表示术语比简单地用指标来表示更好。频率法很简单——对于每个术语，计算它在特定文本中出现的次数。如果文档/文本中缺少某个术语，则该术语的得分为 0。我们对词汇表中的所有术语都这样做。因此，我们的词汇中有多少单词，我们就有多少特征(这是我们可以选择的；这可以认为是一个超参数)。我们应该注意，在预处理步骤之后，我们正在处理的“*术语*是语言中可能不是有效单词的标记:

注意:

词汇表*是我们将在最终模型中使用的所有术语的超集。词汇规模是指词汇中独特术语的数量。原始文本中可能有 20，000 个唯一的术语，但选择使用最常见的 10，000 个术语；这将是有效的词汇量。*

考虑下面的图像；如果我们有 *N* 个文档，并且在我们的工作词汇表中有 *V* ( `t1, t2, t3` … `t` V)个单词，那么数据的表示将是一个维度为 *N × V* 的矩阵。

![Figure 4.8: Document-term matrix
](img/B15385_04_08.jpg)

图 4.8:文档术语矩阵

这个矩阵是我们的**文档-术语矩阵**(**DTM**)——其中每行代表一个文档，每列代表一个术语。单元格中的值可以表示某种度量(计数或任何其他度量)。在这一节中，我们将使用术语频率。

我们可以再次创建自己的函数，但是我们在`scikit-learn`中有一个非常方便的实用程序叫做`'CountVectorizer'`，我们将使用它。让我们熟悉一下它，从导入实用程序开始:

```
from sklearn.feature_extraction.text import CountVectorizer
```

向量器可以处理原始文本，也可以处理标记数据(就像我们的例子一样)。为了处理原始文本，我们将使用下面的代码，其中我们将从原始文本中创建一个带有词频的 DTM(`txt_sents`)。

在开始之前，让我们快速浏览一下这个变量的内容:

```
txt_sents
```

输出应该如下所示:

```
['welcome to the world of deep learning for nlp!',
 "we're in this together, and we'll learn together.",
 'nlp is amazing, and deep learning makes it even more fun.',
 "let's learn!"]
```

注意

如果在对“爱丽丝梦游仙境”文本进行*活动 4.01、* *文本预处理时`txt_sents`变量的内容被覆盖，您可以重新访问*练习 4.01、* *分词、大小写规范化、标点符号和停用词移除*的*步骤 3* ，并重新定义变量，使其内容与前面的输出相匹配。*

现在，让我们实例化向量器。注意，我们需要提供词汇量。这会从数据中挑选出前 *n* 项来创建矩阵:

```
vectorizer = CountVectorizer(max_features = 5)
```

我们在这里选择了五个术语。结果将包含矩阵中的五列。让我们在数据上训练(`'fit'`)向量器:

```
vectorizer.fit(txt_sents)
```

向量器现在已经学习了一个词汇表——前五个术语——并为词汇表中的每个术语创建了一个索引。让我们来看看词汇:

```
vectorizer.vocabulary_
```

前面的属性给出了以下输出:

```
{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}
```

我们可以看到哪些术语被选中(前五名)。

现在，让我们将向量器应用于数据以创建 DTM。一个小细节:向量器的结果是一个稀疏矩阵。为了查看它，我们将把它转换成一个数组:

```
txt_dtm = vectorizer.fit_transform(txt_sents)
txt_dtm.toarray()
```

看一下输出:

```
array([[0, 1, 0, 0, 0],
       [1, 0, 1, 2, 2],
       [1, 1, 0, 0, 0],
       [0, 0, 1, 0, 0]], dtype=int64)
```

第二个文档(第二行)的最后两个术语的频率为`2`。那些术语是什么？嗯，索引 3 和 4 分别是术语`'together'`和`'we'`。让我们打印出原文，看看输出是否如预期的那样:

```
txt_sents
```

输出如下所示:

```
['welcome to the world of deep learning for nlp!',
 "we're in this together, and we'll learn together.",
 'nlp is amazing, and deep learning makes it even more fun.',
 "let's learn!"]
```

这正如我们所料，看起来计数向量器工作得很好。

请注意，向量器也会对句子进行标记。如果您不希望这样，而是希望使用预处理标记(`txt_words_stem`)，您只需将一个伪标记器和预处理程序传递给`CountVectorizer`。让我们看看它是如何工作的。首先，我们创建一个函数，它什么也不做，只是返回标记化的句子/文档:

```
def do_nothing(doc):
    return doc
```

现在，我们将实例化向量器，以将该函数用作预处理器和记号赋予器:

```
vectorizer = CountVectorizer(max_features=5,
                             preprocessor=do_nothing,
                             tokenizer=do_nothing)
```

这里，我们使用来自 tokenizer 的`fit_transform()`方法在一个步骤中拟合和转换数据，然后查看结果。该方法在拟合数据时将唯一术语识别为*词汇表*，然后在转换时统计并返回每个文档中每个术语的出现次数。让我们来看看它的实际应用:

```
txt_dtm = vectorizer.fit_transform(txt_words_stem)
txt_dtm.toarray()
```

输出数组如下所示:

```
array([[0, 1, 1, 1, 0],
       [1, 0, 1, 0, 2],
       [0, 1, 1, 1, 0],
       [0, 0, 1, 0, 0]], dtype=int64)
```

我们可以看到输出与之前的结果不同。这种差异是意料之中的吗？为了理解，让我们看看向量器的词汇:

```
vectorizer.vocabulary_
```

输出如下所示:

```
{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, "'ll": 0}
```

我们在处理预处理过的数据，记得吗？我们已经删除了停用词和词干。为了确保万无一失，让我们试着打印出输入数据:

```
txt_words_stem
```

输出如下所示:

```
[['welcom', 'world', 'deep', 'learn', 'nlp'],
 ["'re", 'togeth', "'ll", 'learn', 'togeth'],
 ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],
 ['let', "'s", 'learn']]
```

我们可以看到，DTM 正在根据预处理后获得的新词汇和频率工作。

因此，这是从文本数据中生成特征的第二种方法，也就是说，使用术语的频率。在下一节中，我们将看看另一个非常流行的方法。

## TF-IDF 方法

一个词在文档中出现的频率高，是否意味着这个词对文档非常重要？不完全是。如果这个术语在所有文档中都很常见呢？在文本数据处理中采用的一个常见假设是，如果一个术语出现在所有文档中，那么它对于手头的这个特定文档来说可能不是很有区别或者很重要。似乎是个合理的假设。再一次，让我们考虑术语“ *mobile* ”的例子，当我们处理手机评论时。该术语可能会在很高比例的评论中出现。但如果你的任务是识别评论中的情绪，这个术语可能不会增加很多信息。

我们可以提高出现在文档中但在整个数据中很少出现的术语的重要性，降低出现在大多数文档中的术语的重要性。

TF-IDF 方法代表*词频-逆文档频率*，定义**逆文档频率** ( **IDF** )如下:

![Figure 4.9: Equation for TF-IDF
](img/B15385_04_09.jpg)

图 4.9:TF-IDF 方程

*n* 是文档总数，而 *df(t)* 是术语 *t* 出现的文档数。这被用作调整术语频率的因子。你可以看到它正如我们所希望的那样工作——它增加了罕见术语的重要性，降低了常见术语的重要性。注意，这个公式有变化，但是我们将坚持使用`scikit-learn`所使用的。像`CountVectorizer`一样，TF-IDF 向量器对句子进行标记并学习词汇，但它不是返回文档中某个术语的计数，而是返回调整后的(乘以 IDF)计数。

现在，让我们将这个有趣的新方法应用到我们的数据中。

## 练习 4.04:使用 TF-IDF 的文档术语矩阵

在本练习中，我们将实现从文本生成特征的第三种方法-TF-IDF。我们将使用 scikit-learn 的`TfidfVectorizer`实用程序，并为我们的原始文本数据创建 DTM。因为我们使用了本章前面创建的`txt_sents`变量，所以我们需要使用同一个 Jupyter 笔记本。变量中包含的文本目前如下所示:

```
['welcome to the world of deep learning for nlp!',
 "we're in this together, and we'll learn together.",
 'nlp is amazing, and deep learning makes it even more fun.',
 "let's learn!"]
```

注意

如果在处理“爱丽丝梦游仙境”文本的*活动 4.01* 、*文本预处理时`txt_sents`变量的内容被覆盖，您可以重新访问*练习 4.01* 、*标记化、大小写规范化、标点符号和停用词移除*的*步骤 3* ，并重新定义变量，使其内容与前面的输出相匹配。*

以下是要执行的步骤:

1.  从`scikit learn` :

    ```
    from sklearn.feature_extraction.text import TfidfVectorizer
    ```

    导入`TfidfVectorizer`工具
2.  用`5` :

    ```
    vectorizer_tfidf = TfidfVectorizer(max_features=5)
    ```

    的词汇量实例化`vectorizer`
3.  在`txt_sents` :

    ```
    vectorizer_tfidf.fit(txt_sents)
    ```

    的原始数据上拟合`vectorizer`
4.  Print out the vocabulary learned by the `vectorizer`:

    ```
    vectorizer_tfidf.vocabulary_
    ```

    经过训练的词汇表将如下所示:

    ```
    {'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}
    ```

    请注意，词汇表与计数向量器的词汇表相同。这是意料之中的。我们没有改变词汇；我们正在调整它对文档的重要性。

5.  使用经过训练的向量器转换数据:

    ```
    txt_tfidf = vectorizer_tfidf.transform(txt_sents)
    ```

6.  Print out the resulting DTM:

    ```
    txt_tfidf.toarray()
    ```

    输出如下所示:

    ```
    array([[0\.        , 1\.        , 
            0\.        , 0\.        , 0\.        ],
           [0.25932364, 0\.        , 0.25932364, 
            0.65783832, 0.65783832],
           [0.70710678, 0.70710678, 0\.        , 
            0\.        , 0\.        ],
           [0\.        , 0\.        , 1\.        , 
            0\.        , 0\.        ]])
    ```

    我们可以清楚地看到，输出值与频率不同，小于 1 的值表示与 IDF 相乘后许多值都降低了。

7.  We also need to see the IDF for each of the terms in the vocabulary to check if the factor is indeed working as we expect it to. Print out the IDF values for the terms using the `idf_` attribute:

    ```
    vectorizer_tfidf.idf_
    ```

    输出如下所示:

    ```
    array([1.51082562, 1.51082562, 1.51082562, 
           1.91629073, 1.91629073])
    ```

术语`'and'`、`'deep'`和`'learn'`的 IDF 较低，而术语`'together'`和`'we'`的 IDF 较高。这正如我们所料——术语`'together'`和`'we'`只出现在一个文档中，而其他的出现在两个文档中。因此，TF-IDF 计划确实更加重视稀有词。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

你也可以在[https://packt.live/38Gr54r](https://packt.live/38Gr54r)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在这个练习中，我们看到了如何使用 TF-IDF 方法来表示文本。通过注意到 IDF 值对于较高频率的术语较低，我们还看到了该方法如何降低较频繁术语的权重。我们最终得到了一个包含这些术语的 TF-IDF 值的 DTM。

## 总结经典方法

我们刚刚看了三种经典的文本表示方法。我们从一键编码开始，术语的特性只是简单地标记它在文档中的存在。基于计数/频率的方法试图通过使用术语在文档中的频率来增加其重要性。TF-IDF 方法试图使用该术语的“标准化”重要性值，考虑该术语在文档中的常见程度。

到目前为止，我们讨论的所有三种方法都属于“词汇袋”表示法。那么，它们为什么被称为“*袋字*”？有几个原因。第一个原因是它们没有保留令牌的顺序——一旦进入包中，术语/令牌的位置就无关紧要了。第二个原因是这种方法保留了单个术语的特性。因此，在某种程度上，对于每个文档，你都有一个“混合的令牌包”，或者简单地说，一个“*单词包*”。

所有三种方法的结果都有一个维度 *N × V* ，其中 *N* 是文档的数量， *V* 是词汇量。请注意，所有这三种表示都非常稀疏——一个典型的句子非常短(可能 20 个单词),但词汇量通常是数千个，导致 DTM 的大多数单元都是 0。这似乎不太理想。嗯，这种表示还有一些缺点，我们很快就会看到，这些缺点导致了基于深度学习的表示方法的成功。接下来我们来讨论这些想法。

# 文本的分布式表示

为什么词嵌入如此流行？为什么我们声称他们惊人的强大？是什么让他们如此特别？为了理解和欣赏词嵌入，我们需要认识到目前为止表示法的缺点。

术语“*人行道*”和“*人行道*”是同义词。你认为我们到目前为止讨论的方法能够捕获这些信息吗？那么，您可以手动将" *sidewalk* "替换为"*fowl*"，这样两者最终就有了相同的标记，但是您能对语言中所有可能的同义词都这样做吗？

术语“*热*”和“*冷*”是反义词。之前的单词袋表述抓住了这一点吗？那么“*狗*”作为“*动物*”的一种呢？*驾驶舱*作为*飞机*的一部分？区分狗叫和树叫？你能手工处理所有这些案件吗？

所有前面的例子都是术语之间的“语义关联”——简单地说，它们的意思以某种方式联系在一起。单词袋表示法不能捕捉这些。这就是分布式语义概念的由来。分布语义学的关键思想是具有相似分布的术语具有相似的含义。

给你一个快速、有趣的测验:从下面的文字中猜出术语*furby*的意思:

一个月前，我领养了一个年轻的波斯猫宝宝。像所有的毛宝宝一样，它喜欢挠背，讨厌水，但与其他毛宝宝不同的是，它在抓老鼠方面惨败。

你可能猜对了: *furbaby* 指的是一只猫。这很容易，不是吗？

但是你是怎么做到的呢？没有任何地方使用过猫这个词。你查看了“*furby*”的上下文(相关术语)，基于你对语言和世界的理解，你认为这些术语通常与猫有关。你直观地使用了这个概念:意思相似的单词出现在相似的上下文中。如果“*furby*”和“ *cat* ”出现在类似的语境中，它们的意思一定是相似的。

从一个人交的朋友就可以知道这个人说的话。”

约翰·弗斯(John Firth)的这句名言很好地抓住了这一点。它被过度使用是有原因的。让我们看看这个概念是如何应用于词嵌入的。

## 词嵌入和单词向量

词嵌入是每个术语作为低维度向量的表示。一个术语的独热编码表示也是一个向量，但是维数有几千。词嵌入/单词向量的维度要低得多，是基于分布式语义的方法的结果——本质上，这种表示抓住了具有相似含义的单词出现在相似上下文中的概念。

词向量试图捕捉术语的含义。这个想法使它们非常强大，当然，前提是它们已经被正确地创建了。有了字向量，向量运算如加/减向量和点积都是可能的，并且有一些非常有趣的意义。还有一个很好的特性是，具有相似含义的项目在空间上更接近。所有这些导致了一些惊人的结果。

一个非常有趣的结果是，单词向量可以在类比任务中表现良好。类比任务定义为以下格式的任务-“a*a*对 *b* 如同 *x* 对？”–也就是说，找到一个与 *x* 的关系与 *b* 与 *a* 的关系相同的实体。举个例子，如果你问“男人之于叔叔就像女人之于？”，结果将是“`aunt`”(稍后将详细介绍)。你还可以发现术语之间的语义规则——术语和术语集之间的关系。为了更好地理解这一点，让我们看看下图，它是基于单词向量/嵌入的:

![Figure 4.10: Semantic relationships between terms
](img/B15385_04_10.jpg)

图 4.10:术语之间的语义关系

上图显示了一些示例。向量可以具有高维度(高达 300 或甚至更多)，因此执行降维为二维以将其可视化。两个术语之间的虚线连接表示术语之间的关系。这种连接的方向很重要。在左侧面板上，我们可以看到连接`slow`和`slower`的线段平行于连接`short`和`shorter`的线段。这是什么意思？这意味着单词 embeddings 了解到`short`和`shorter`之间的关系与`slow`和`slower`之间的关系相同。同样，嵌入得知`clearer`和`clearest`之间的关系与`darker`和`darkest`之间的关系相同。很漂亮，对吧？

同样，图 4.10 的右侧*显示嵌入得知`sir`和`madam`之间的关系与`king`和`queen`之间的关系相同。嵌入还捕获了术语之间的其他类型的语义关联，我们在上一节中讨论过。是不是很神奇？*

这在我们之前讨论的方法中是不可能的。词嵌入确实是围绕术语的“含义”进行的。我们希望你已经能体会到单词向量的效用和力量。如果你还不相信，我们将很快与他们合作，并亲眼目睹这一点。

为了生成词嵌入，我们可以使用几种算法。我们将讨论两种主要的方法，并告诉你一些其他流行的方法。我们将看到如何利用分布式语义方法来导出这些词嵌入。

### word2vec

回到学校后，为了测试我们是否理解了某些术语的意思，我们的语言老师使用了一种非常流行的技巧:“*填空*”。根据它周围的单词，我们需要找出最能填补空白的单词。如果你很好地理解了意思，你就会做得很好。想想这个——这难道不是分布式语义吗？

在*‘fur baby’*的例子中，您可以预测术语`'cat'`，因为您了解它出现的上下文和术语。这个练习实际上是一个“填空”练习。你能填空只是因为你理解了“猫”的意思。

如果你能根据上下文预测一个术语，你就理解了这个术语的意思。

这个简单的想法正是`word2vec`算法背后的公式。`word2vec`算法/过程是一种预测练习，某种程度上是一种大规模的“填空”练习。简而言之，这就是算法的作用:

给定上下文单词，预测丢失的目标单词。

这就是全部了。`word2vec`算法在给定上下文的情况下预测目标单词。我们先来了解一下这些是怎么定义的。

想想这句话，“波斯猫吃鱼，讨厌洗澡”我们将上下文定义为目标词左右固定数量的术语，目标词在中间。对于我们的例子，让`'cat'`作为目标单词，让我们把目标两边的两个单词作为我们的上下文:

![Figure 4.11: "cat" as the target term
](img/B15385_04_11.jpg)

图 4.11:“猫”作为目标术语

这五个术语一起构成了一个`'window'`，目标术语位于中心，上下文术语围绕在它的周围。在这个例子中，由于我们在每一边都考虑了两个项，所以窗口大小是 2(稍后将详细介绍这些参数)。窗口是滑动的，在句子中的术语上移动。下一个窗口将把`'eats'`放在中心位置，而`'cat'`现在成为了背景的一部分:

![Figure 4.12: Windows for the target term
](img/B15385_04_12.jpg)

图 4.12:目标术语的窗口

`C1`、`C2`、`C3`和`C4`表示每个窗口的上下文。在`C3`中，“鱼”是目标词，使用术语“猫”、“吃”、“和”、“讨厌”来预测。公式是清楚的，但是模型如何学习表示法呢？接下来我们来讨论一下:

![Figure 4.13: The CBOW architecture with an example
 
](img/B15385_04_13.jpg)

图 4.13:带有示例的 CBOW 架构

上图中显示的模型使用具有单一隐藏层的神经网络。输出层用于目标项，并使用 *V* 输出进行单热编码，每个项一个——当然，预测项`'cat'`是在输出中获得`'hot'`的项。上下文术语的输入层的大小也是 *V* ，但是会触发上下文中的所有术语。隐藏层的维数为 *V x D* (其中 *D* 是向量的维数)。这个隐藏层是学习这些术语的神奇表现的地方。注意，只有一个输入层，如权重矩阵 *W* 所示。

当网络训练时，随着每个时期更好地预测目标词，隐藏层的参数也得到更新。这些参数实际上是每一项的 D 长度向量。这个术语的 D-长度向量是我们为该术语嵌入的单词。迭代完成后，我们将学习词汇表中所有术语的词嵌入。很整洁，不是吗？

我们刚刚讨论的方法是 CBOW 方法来训练单词向量。上下文是一个简单的单词包(正如我们在上一节经典方法中讨论的；顺序无所谓，记住)，故名。还有另一种流行的方法，Skip-gram 方法，它与 CBOW 方法相反，它根据中心词预测上下文词。这种方法最初可能看起来不太直观，但效果很好。我们将在本章的后面讨论 CBOW 和 Skip-gram 的结果之间的差异:

![Figure 4.14: The Skip-gram architecture
](img/B15385_04_14.jpg)

图 4.14:跳过程序架构

让我们看看 Python 中的 CBOW 方法。我们将创建我们自己的词嵌入，并评估我们是否真的可以得到我们迄今为止声称的惊人结果。

## 训练我们自己的词嵌入

在不同的包中有许多`word2vec`算法的实现。我们将使用 **Gensim** 中的实现，它是许多 NLP 任务的优秀包。word2vec 在 Gensim 中的实现接近于 *Mikolov 等人*在 2013 年[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)的原始论文。Gensim 还支持词嵌入的其他算法；稍后将详细介绍。

如果您没有安装 Gensim，可以通过在 Jupyter 笔记本中键入以下命令来安装它:

```
!pip install gensim
```

我们将使用的数据集是`text8`语料库([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html))，这是来自维基百科的第一个十亿字符。因此，它应该涵盖来自各种主题的数据，而不是特定于一个领域的数据。方便的是，Gensim 有一个实用程序(T2 API)来读入数据。让我们在从 Gensim 导入`downloader`实用程序后读入数据:

```
import gensim.downloader as api
dataset = api.load("text8")
```

此步骤下载`text8`数据，可能需要一段时间，这取决于您的互联网连接。或者，可以在这里([https://packt.live/3gKXU2D](https://packt.live/3gKXU2D))下载数据，并使用 Gensim 中的`Text8Corpus`实用程序读取，如以下代码所示:

```
from gensim.models import word2vec
dataset = word2vec.Text8Corpus("text8")
```

`text8`数据现在是可迭代的，可以简单地传递给`word2vec`算法。

在我们训练嵌入之前，为了使结果可重现，让我们使用 NumPy 将种子设置为用于随机数生成的`1`:

```
np.random.seed(1)
```

注意

虽然我们已经播下了种子，但结果的变化还有更多原因。这部分是因为您系统上的 Python 版本可能会使用内部哈希种子。使用多个内核也会导致结果不同。在任何情况下，虽然您看到的值可能不同，并且结果的顺序可能会有一些变化，但您看到的输出应该与我们的基本一致。请注意，这适用于本章中与单词向量相关的所有实用元素。

现在，让我们通过使用`word2Vec`方法来训练我们的第一个词嵌入:

```
model = word2vec.Word2Vec(dataset)
```

这可能需要一两分钟或更短时间，具体取决于您的系统。一旦完成，我们将在模型中拥有经过训练的单词向量，并可以访问多个方便的实用程序来处理这些单词向量。让我们用向量/嵌入这个词来表示一个术语:

```
print(model.wv["animal"])
```

输出如下所示:

![Figure 4.15: The embedding for "animal"
](img/B15385_04_15.jpg)

图 4.15:“动物”的嵌入

你有一系列的数字——该项的向量。让我们找出向量的长度:

```
len(model.wv["animal"])
```

向量的长度如下:

```
100
```

每一项的表示现在是长度为 100 的向量(长度是我们可以改变的超参数；我们使用默认设置开始)。任何项的向量都可以像我们之前做的那样访问。另外一个方便的工具是`most_similar()`方法，它帮助我们找到与目标术语最相似的术语。让我们来看看它的实际应用:

```
model.wv.most_similar("animal")
```

输出如下所示:

```
[('insect', 0.7598186135292053),
 ('animals', 0.729228138923645),
 ('aquatic', 0.6679497957229614),
 ('insects', 0.6522265672683716),
 ('organism', 0.6486647725105286),
 ('mammal', 0.6478426456451416),
 ('eating', 0.6435647010803223),
 ('ants', 0.6415578722953796),
 ('humans', 0.6414449214935303),
 ('feces', 0.6313734650611877)]
```

输出是一个元组列表，每个元组包含术语及其与术语“animal”的相似性得分。

我们可以在与“动物”最相似的词语中看到`insect`、`animals`、`insects`、`mammal`。这似乎是一个非常好的结果，对不对？但是相似度是如何计算的呢？单词由向量表示，向量试图捕捉意义——术语之间的相似性是它们对应的向量之间的相似性。`most_similar()`方法使用向量之间的**余弦相似度**，并返回具有最高值的项。结果中每个词对应的值是与目标词向量的余弦相似度。

余弦相似性度量在这里是合适的，因为我们期望意义相似的术语在空间上在一起。余弦相似度是向量之间角度的余弦。具有相似含义和表示的术语将具有更接近 0°的角度和更接近 1 的相似性得分，而具有完全不相关含义的术语将具有更接近 90°的角度和更接近 0°的余弦相似性。让我们看看这位模特学到了哪些与“幸福”相关的热门词汇:

```
model.wv.most_similar("happiness")
```

最相似的项目如下(最相似的项目在顶部):

```
[('humanity', 0.7819231748580933),
 ('perfection', 0.7699881792068481),
 ('pleasure', 0.7422512769699097),
 ('righteousness', 0.7402842044830322),
 ('desires', 0.7374188899993896),
 ('dignity', 0.7189303040504456),
 ('goodness', 0.7103697657585144),
 ('fear', 0.7047020196914673),
 ('mankind', 0.7046756744384766),
 ('salvation', 0.6990150213241577)]
```

人性，人类，善良，正义，和同情-我们在这里有一些生活课程。它似乎学会了许多人似乎一辈子都搞不懂的东西。记住，它只是一系列的矩阵乘法。

## 词语嵌入的语义规律

我们之前提到过，这些表征捕捉了语言中的规律，并且擅长解决简单的类比任务。向量嵌入之间的偏移量似乎捕捉到了单词之间的类比关系。所以比如*“国王”——“男人”+“女人”*预计结果是“*女王*”。让我们看看我们在`text8`语料库上训练的模型是否也理解一些规律。

这里我们将使用`most_similar()`方法，它允许我们将向量相加或相减。我们将提供`'king'`和`'woman'`作为向量来彼此相加，使用`'man'`从结果中减去，然后检查出与结果向量最相似的五项:

```
model.wv.most_similar(positive=['woman', 'king'], \
                      negative=['man'], topn=5)
```

输出如下所示:

```
[('queen', 0.6803990602493286),
 ('empress', 0.6331825852394104),
 ('princess', 0.6145625114440918),
 ('throne', 0.6131302714347839),
 ('emperor', 0.6064509153366089)]
```

排名第一的结果是`'queen'`。看起来模型捕捉到了这些规律。让我们试试另一个例子。“男人”之于“叔叔”正如“女人”之于？或者用算术的形式，最接近*大叔的向量是什么——男+女=？*

```
model.wv.most_similar(positive=['uncle', 'woman'], \
                      negative=['man'], topn=5)
```

以下是上述代码的输出:

```
[('aunt', 0.8145735263824463),
 ('grandmother', 0.8067640066146851),
 ('niece', 0.7993890643119812),
 ('wife', 0.7965766787528992),
 ('widow', 0.7914236187934875)]
```

这看起来很有效。请注意，所有前五名的结果都是女性的。所以，我们采用了`uncle`，去除了男性元素，添加了女性元素，现在我们有了一些非常好的结果。

让我们看看其他一些向量算术的例子。我们可以取两个不同术语的向量，并对它们求平均，从而得到一个短语的向量。我们自己试试吧。

注意

取单个向量的平均值只是获得短语向量的众多方法之一。变化范围从加权平均值到更复杂的数学函数。

## 练习 4.05:短语的向量

在本练习中，我们将开始为两个不同的短语`get happy`和`make merry`创建向量，取各个向量的平均值。我们会发现这些短语的表达之间有相似之处。你需要在我们本章一直使用的 Jupyter 笔记本上继续这个练习。按照以下步骤完成本练习:

1.  提取术语“ *get* 的向量，并存储在变量:

    ```
    v1 = model.wv['get']
    ```

    中
2.  提取单词“ *happy* ”的向量，并存储在变量:

    ```
    v2 = model.wv['happy']
    ```

    中
3.  创建一个向量作为两个向量的元素平均值，`(v1 + v2)/2`。这是我们整个短语“快乐起来”的向量:

    ```
    res1 = (v1+v2)/2
    ```

4.  类似地，提取术语“ *make* ”和“ *merry* ”的向量:

    ```
    v1 = model.wv['make'] v2 = model.wv['merry']
    ```

5.  通过平均各个向量来创建短语的向量:

    ```
    res2 = (v1+v2)/2
    ```

6.  Using the `cosine_similarities()` method in the model, find the cosine similarity between the two:

    ```
    model.wv.cosine_similarities(res1, [res2])
    ```

    余弦相似性如下所示:

    ```
    array([0.5798107], dtype=float32)
    ```

结果是余弦相似度约为`0.58`，为正，远高于`0`。这意味着模型认为短语“变得快乐”和“变得快乐”在意思上是相似的。不错吧？除了简单的平均，我们可以使用加权平均，或者想出更复杂的方法来组合各个向量。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

你也可以在[https://packt.live/38Gr54r](https://packt.live/38Gr54r)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在这个练习中，我们看到了如何使用向量算术来表示短语，而不是单个术语，并且我们看到了仍然可以捕捉到含义。这给我们带来了一个非常重要的教训——*词嵌入的向量运算有意义*。

这些向量算术运算处理术语的含义，产生了一些非常有趣的结果。

我们希望你现在能体会到词嵌入的力量。我们意识到这些结果仅仅来自一些矩阵乘法，并且需要一分钟在我们的数据集上训练。词嵌入几乎是不可思议的，令人惊喜的是如此简单的预测公式如何产生如此强大的表示。

当我们之前创建单词 vectors 时，我们没有太注意控件/参数。有很多，但只有一些对表示的质量有重大影响。我们现在将开始理解`word2vec`算法的不同参数，并看看改变这些参数对我们自己的影响。

### 参数的影响——向量的“大小”

`word2vec`算法的`size`参数是每一项的向量长度。默认情况下，正如我们前面看到的，这是 100。我们将尝试减少这个参数，并评估结果中的差异(如果有的话)。让我们重新训练单词 embeddings，这次用`size`作为 30:

```
model = word2vec.Word2Vec(dataset, size=30)
```

现在，让我们检查前面的类比任务，即`king - man + woman`:

```
model.wv.most_similar(positive=['woman', 'king'], \
                      negative=['man'], topn=5)
```

这将为我们提供以下输出:

```
[('emperor', 0.8314059972763062),
 ('empress', 0.8250986933708191),
 ('son', 0.8157491683959961),
 ('prince', 0.8060941696166992),
 ('archbishop', 0.8003251552581787)]
```

我们可以看到`queen`没有出现在前五个结果中。看起来通过使用一个非常低的维度，我们没有在一个术语的表示中捕获足够的信息。

### 参数的影响——“窗口大小”

`window size`参数定义上下文；具体来说，窗口大小是在构建上下文时目标术语左侧和右侧的术语数量。这个参数的效果不是很明显。一般来说，当您使用更大的窗口大小时(比如说 20 个)，顶部相似的术语似乎是与目标术语一起使用的术语，不一定具有相似的含义。另一方面，缩小窗口大小(比如说，缩小到 2)，会返回含义非常相似的顶级术语，并且在许多情况下是同义词。

## Skip-gram 对比 CBOW

通过为 Skip-gram 设置`sg = 1`，在 Skip-gram 和 CBOW 之间选择学习算法(默认为`sg = 0`，即 CBOW)。回想一下 Skip-gram 方法基于中心目标单词来预测上下文单词。这推翻了 CBOW 的公式，其中上下文单词用于预测目标单词。但是我们如何在两者之间做出选择呢？一个比另一个有什么好处？为了自己看，让我们使用 Skip-gram 训练嵌入，并将一些结果与 CBOW 的结果进行比较。首先，让我们以 CBOW 为例。首先，我们将通过不指定 size 参数，用默认的向量大小重新创建 CBOW 单词向量。作品是指艺术家/表演者的作品。我们将看到与不常用术语最相似的术语，`oeuvre`:

```
model = word2vec.Word2Vec(dataset)
model.wv.most_similar("oeuvre", topn=5)
```

下列术语是最相似的术语:

```
[('baglione', 0.7203884124755859),
 ('chateaubriand', 0.7119786143302917),
 ('kurosawa', 0.6956337690353394),
 ('swinburne', 0.6926312446594238),
 ('poetess', 0.6910216808319092)]
```

我们可以看到，大多数结果都是艺术家的名字(`swinburne`、`kurosawa`和`baglione`)或美食(chateaubriand)。前五个结果都没有接近目标术语的意思。现在，让我们使用 Skip-gram 方法重新训练我们的向量，并查看相同任务的结果:

```
model_sg = word2vec.Word2Vec(dataset, sg=1)
model_sg.wv.most_similar("oeuvre", topn=5)
```

这为我们提供了以下输出:

```
[('masterful', 0.8347533345222473),
 ('orchestration', 0.8149941563606262),
 ('mussorgsky', 0.8116796016693115),
 ('showcasing', 0.8080146312713623),
 ('lithographs', 0.805435299873352)]
```

我们可以看到，顶部的术语在含义上更接近(`masterful`、`orchestration`、`showcasing`)。因此，跳格法似乎对生僻字更有效。

为什么会这样呢？CBOW 方法通过有效地平均整个上下文单词(记住，所有上下文术语一起作为输入)来平滑大量的分布统计，而 Skip-gram 没有。当你有一个小数据集时，CBOW 所做的平滑是可取的。如果您有一个小/中等大小的数据集，并且如果您关心罕见术语的表示，那么 Skip-gram 是一个好的选择。

### 训练数据的效果

在训练你的词向量时，一个非常重要的决定是底层数据。模式和相似性将从您提供给算法的数据中学习，并且我们期望模型从来自不同域、不同种类的设置等的数据中进行不同的学习。为了理解这一点，我们从不同的上下文中加载不同的语料库，看看嵌入是如何变化的。

布朗语料库是一个通用文本集，从 15 个不同的主题中收集，使其通用(从政治到宗教，从书籍到音乐，以及许多其他主题)。它包含 500 个文本样本和大约 100 万个单词。“电影”语料库包含来自 IMDb 的电影评论数据。这两个都可以在 NLTK 中找到。

## 练习 4.06:在不同数据集上训练词向量

在这个练习中，我们将在 Brown 语料库和 IMDb 电影评论语料库上训练我们自己的单词向量。我们将评估所学表述的差异和潜在训练数据的效果。按照以下步骤完成本练习:

1.  从 NLTK 导入布朗和 IMDb 电影评论语料库:

    ```
    nltk.download('brown') nltk.download('movie_reviews') from nltk.corpus import brown, movie_reviews
    ```

2.  The corpora have a convenient method, `sent()`, to extract the individual sentences and words (tokenized sentences, which can be directly passed to the `word2vec` algorithm). Since both the corpora are rather small, use the Skip-gram method to create the embeddings:

    ```
    model_brown = word2vec.Word2Vec(brown.sents(), sg=1)
    model_movie = word2vec.Word2Vec(movie_reviews.sents(), sg=1)
    ```

    我们现在有两个嵌入，它们是在同一术语的不同上下文中学习的。让我们从 Brown 语料库的模型中查看与`money`最相似的术语。

3.  Print out the *top five terms* most similar to `money` from the model that were learned on the Brown corpus:

    ```
    model_brown.wv.most_similar('money', topn=5)
    ```

    以下是上述代码的输出:

    ```
    [('job', 0.8477444648742676),
     ('care', 0.8424298763275146),
     ('friendship', 0.8394286632537842),
     ('risk', 0.8268661499023438),
     ('permission', 0.8243911862373352)]
    ```

    我们可以看到最上面的术语是`'job'`；很公平。让我们看看这位模特从电影评论中学到了什么。

4.  Print out the top five terms most similar to `money` from the model that learned from the movie corpus:

    ```
    model_movie.wv.most_similar('money', topn=5)
    ```

    以下是热门术语:

    ```
    [('cash', 0.7299771904945374),
     ('ransom', 0.7130625247955322),
     ('record', 0.7028014063835144),
     ('risk', 0.6977001428604126),
     ('paid', 0.6940697431564331)]
    ```

排名靠前的术语是`cash`和`ransom`。考虑到电影中使用的语言，因此在电影评论中，这并不令人惊讶。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

你也可以在[https://packt.live/38Gr54r](https://packt.live/38Gr54r)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在本练习中，我们使用不同的数据集创建了单词向量，并看到相同术语的表示和所学的关联受底层数据的影响很大。所以，明智地选择你的数据。

## 使用预先训练的单词向量

到目前为止，我们已经使用我们可以访问的小数据集训练了我们自己的词嵌入。斯坦福大学 NLP 小组的人已经在词汇表中的 60 亿个标记和 40 万个术语上训练了词嵌入。单独而言，我们没有足够的资源来应对这种规模。幸运的是，斯坦福 NLP 小组已经足够仁慈地将这些经过训练的嵌入提供给公众，这样像我们这样的人就可以从他们的工作中受益。经过训练的嵌入可在手套页面([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))获得。

关于手套的一个快速提示:用于训练的方法略有不同。目标被修改以使相似的术语在空间上以稍微更明确的方式更接近地出现。你可以在 GloVe([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))的项目页面上了解细节，这里也有一个提议它的原始论文的链接。然而，最终结果在性能上与 word2vec 非常相似。

我们将从 GloVe 项目页面下载`glove.6B.zip`文件。该文件包含 50D、100D、200D 和 300D 向量。我们将在这里使用 100D 向量。请解压缩文件，并确保您的工作目录中有文本文件。经过训练的向量以文本文件的形式提供，格式略有不同。我们将使用 Gensim 中可用的`glove2word2vec`实用程序来转换成 Gensim 可以轻松加载的格式:

```
from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.w2vformat.txt'
glove2word2vec(glove_input_file, word2vec_output_file)
```

我们指定了输入和输出文件，并运行了`glove2word2vec`实用程序。顾名思义，该实用程序以 GloVe 格式接收单词向量，并将其转换为`word2vec`格式。在这之后，`word2vec`模型可以很容易地理解这些嵌入。现在，让我们从文本文件加载`keyed`单词向量(重新格式化):

```
from gensim.models.keyedvectors import KeyedVectors
glove_model = KeyedVectors.load_word2vec_format\
              ("glove.6B.100d.w2vformat.txt", binary=False)
```

完成这些后，我们就有了模型中的手套嵌入，以及 word2vec 中嵌入模型的所有便利工具。让我们来看看类似于`"money"`的顶级术语:

```
glove_model.most_similar("money", topn=5)
```

输出如下所示:

```
[('funds', 0.8508071899414062),
 ('cash', 0.848483681678772),
 ('fund', 0.7594833374023438),
 ('paying', 0.7415367364883423),
 ('pay', 0.740767240524292)]
```

最后，让我们看看这个模型在国王和王后任务中的表现:

```
glove_model.most_similar(positive=['woman', 'king'], \
                         negative=['man'], topn=5)
```

以下是上述代码的输出:

```
[('queen', 0.7698541283607483),
 ('monarch', 0.6843380928039551),
 ('throne', 0.6755737066268921),
 ('daughter', 0.6594556570053101),
 ('princess', 0.6520533561706543)]
```

既然我们在模型中有了这些嵌入，我们就可以像处理我们之前创建的嵌入一样处理它们，并且可以从更大的数据集和词汇表以及贡献组织使用的处理能力中受益。

## 嵌入中的偏差——警告

在讨论规律性和类比性时，我们看到了下面的例子:

*国王——男人+女人=王后*

嵌入通过从文本数据中学习来捕捉这些规律，这很好。我们试试类似职业的吧。让我们看看最接近*医生的术语——男人+女人*:

```
model.wv.most_similar(positive=['woman', 'doctor'], \
                      negative=['man'], topn=5)
```

关于前五个结果的输出如下:

```
[('nurse', 0.6464251279830933),
 ('child', 0.5847542881965637),
 ('teacher', 0.569127082824707),
 ('detective', 0.5451491475105286),
 ('boyfriend', 0.5403486490249634)]
```

这不是我们想要的结果。医生是男的，女的是护士？让我们试试另一个例子。这一次，让我们试试这个模型是如何看待女性与“男性”的“聪明”相对应的:

```
model.wv.most_similar(positive=['woman', 'smart'], \
                      negative=['man'], topn=5)
```

我们得到了以下五个最佳结果:

```
[('cute', 0.6156168580055237),
 ('dumb', 0.6035820245742798),
 ('crazy', 0.5834532976150513),
 ('pet', 0.582811713218689),
 ('fancy', 0.5697714686393738)]
```

我们可以看到，前几项分别是`'cute'`、`'dumb'`、`'crazy'`。那一点都不好。

这里发生了什么事？这种看似伟大的表现方式是性别歧视吗？word2vec 算法是不是性别歧视？在生成的单词向量中肯定有偏差，但是想想偏差来自哪里。在`'doctor'`用于男性的上下文中，底层数据将`'nurse'`用于女性。因此，包含偏见的是潜在的文本，而不是算法。

这个主题最近获得了极大的关注，并且正在围绕评估和消除来自学习嵌入的偏差的方法进行研究，但是一个好的方法是从一开始就避免数据中的偏差。如果你在 YouTube 评论上训练词嵌入，不要惊讶它们包含各种极端偏见。你最好避免你怀疑有偏见的文本数据。

## 其他值得注意的词嵌入方法

我们主要使用 word2vec 方法，并且简要地看了一下 GloVe 方法。虽然这些是最流行的方法，但还有一些其他方法值得一提:

**FastText** :由**脸书的 AI 研究** ( **FAIR** )实验室创建，它使用子词信息来丰富词嵌入。你可以在官方页面([https://research.fb.com/downloads/fasttext/](https://research.fb.com/downloads/fasttext/))上了解更多。

**WordRank** :将嵌入问题视为单词排序问题。它的性能在几个任务上和 word2vec 差不多。你可以在 https://arxiv.org/abs/1506.02761 了解更多。

除此之外，一些流行的库现在也提供了预先训练好的嵌入(SpaCy 就是一个很好的例子)。选择很多。我们不能在这里对这些选择做详细的处理，但是请探索这些选择。

在这一章中，我们已经讨论了很多关于表现的观点。现在，让我们借助一个活动来实现这些想法。

## 活动 4.02:爱丽丝梦游仙境的文本展示

在上一个练习中，我们对文本进行了标记化和基本的预处理。在这个活动中，我们将通过使用文本的表现方法来推进这个过程。您将从数据中创建自己的嵌入，并查看我们拥有的关系类型。您还将利用预先训练的嵌入来表示文本中的数据。

注意

请注意，您需要完成*活动 4.01* 、*对“爱丽丝梦游仙境”文本*的文本预处理，才能继续此活动。在该活动中，我们对文本执行了停用词删除。

您需要执行以下步骤:

我们将继续使用我们在*活动 4.01* 、*中用于“爱丽丝梦游仙境”文本*的相同 Jupyter 笔记本。我们将处理在该活动中获得的停用词删除步骤的结果(假设它存储在一个名为`alice_words_nostop`的变量中)。打印结果中的前三个句子。

1.  从 Gensim 导入`word2vec`并用默认参数训练你的词嵌入。
2.  找出与`rabbit`最相似的术语。
3.  使用大小为 2 的窗口，重新训练单词向量。
4.  找出与`rabbit`最相似的术语。
5.  使用窗口大小为`5`的跳格法重新训练单词向量。
6.  找出与`rabbit`最相似的术语。
7.  通过对`white`和`rabbit`的向量求平均，找到短语`white rabbit`的表示。
8.  通过对`mad`和`hatter`的向量求平均值，找到`mad hatter`的表示。
9.  找出这两个短语之间的余弦相似性。
10.  装载尺寸为 100D 的预训练手套嵌入物。
11.  查找`white rabbit`和`mad hatter`的表示。
12.  找出两个短语之间的余弦相似度。余弦相似度有变化吗？

作为这项活动的结果，我们将拥有我们自己的单词向量，这些向量已经过“爱丽丝漫游奇境记”的训练，并具有课文中可用术语的表示。

注意

本练习的详细步骤以及解决方案和附加注释在第 407 页提供。

# 摘要

在这一章中，我们首先讨论了文本数据的特性以及歧义是如何使自然语言处理变得困难的。我们讨论了处理文本有两个关键的想法——预处理和表示。我们讨论了预处理中涉及的许多任务，即清理数据并为分析做好准备。我们看到了各种消除数据缺陷的方法。

表示是下一个重要方面——我们理解了表示文本和将文本转换成数字的考虑因素。我们研究了各种方法，从经典方法开始，包括一键编码、基于计数的方法和 TF-IDF 方法。

词嵌入是一种全新的表示文本的方法，它利用了分布语义学的思想——出现在相似上下文中的术语具有相似的含义。word2vec 算法巧妙地利用了这一思想，它用公式表达了一个预测问题:在给定上下文的情况下预测一个目标单词。它使用神经网络进行预测，并在此过程中学习术语的向量表示。

我们看到，这些表示是惊人的，因为它们似乎抓住了意义，简单的算术运算给出了一些非常有趣和有意义的结果。您甚至可以使用单词向量来创建短语甚至句子/文档的表示。这为我们稍后在 NLP 的更复杂的深度学习架构中使用词嵌入奠定了基础。

在下一章中，我们将通过应用深度学习方法，如循环神经网络和一维卷积，继续探索序列。