

# 附录

# 1。深度学习的构建模块

## 活动 1.01:使用优化器求解二次方程

## 解决方案

让我们解下面的二次方程:

![Figure 1.29: Quadratic equation to be solved
](img/B15385_01_29.jpg)

图 1.29:待求解的二次方程

我们已经知道这个二次方程的解是`x=5`。

我们可以使用优化器来解决这个问题。对于优化器，`x`是变量，成本函数是左侧表达式，如下所示:

![Figure 1.30: Left-hand side expression
](img/B15385_01_30.jpg)

图 1.30:左侧表达式

优化器将找到表达式最小的`x`的值——在本例中，它是`0`。请注意，这只适用于完全平方的二次方程，比如本例。左边的表达式是一个完美的正方形，可以用下面的等式来解释:

![Figure 1.31: Perfect square
](img/B15385_01_31.jpg)

图 1.31:完美的正方形

现在，让我们看看解决这个问题的代码:

1.  打开一个新的 Jupyter 笔记本，将其重命名为 *Activity 1.01* 。
2.  进口`tensorflow` :

    ```
    import tensorflow as tf
    ```

3.  创建变量`x`并将其初始化为 0.0:

    ```
    x=tf.Variable(0.0)
    ```

4.  将`loss`函数构造为`lambda`函数:

    ```
    loss=lambda:abs(x**2-10*x+25)
    ```

5.  创建一个学习率为`.01` :

    ```
    optimizer=tf.optimizers.Adam(.01)
    ```

    的优化器实例
6.  运行优化器 10，000 次迭代。您可以从一个较小的数字开始，比如 1000，并不断增加迭代次数，直到您得到解决方案:

    ```
    for i in range(10000):     optimizer.minimize(loss,x)
    ```

7.  Print the value of `x`:

    ```
    tf.print(x)
    ```

    输出如下所示:

    ```
    4.99919891
    ```

这是我们的二次方程的解。值得注意的是，不管迭代多少次，你都不会得到一个完美的 5。

注意

要访问该特定部分的源代码，请参考 https://packt.live/3gBTFGA 的。

你也可以在 https://packt.live/2Dqa2Id 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 2。神经网络

## 活动 2.01:建立一个多层神经网络来对声纳信号进行分类

## 解决方案

让我们看看解决方案是什么样的。请记住，这是一种解决方案，但可能有多种变化:

1.  导入所有需要的库:

    ```
    import tensorflow as tf import pandas as pd from sklearn.preprocessing import LabelEncoder # Import Keras libraries from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense
    ```

2.  Load and examine the data:

    ```
    df = pd.read_csv('sonar.csv')
    df.head()
    ```

    输出是:

    ![Figure 2.37: Contents of sonar.csv
    ](img/B15385_02_37.jpg)

    图 2.37:sonar . CSV 的内容

    观察有 60 个特征，目标有两个值—岩石和矿井。

    这意味着这是一个二元分类问题。在建立神经网络之前，我们先准备好数据。

3.  Separate the features and the labels:

    ```
    X_input = df.iloc[:, :-1]
    Y_label = df['Class'].values
    ```

    在这段代码中，`X_input`是选择除了`Class`列之外的所有列，`Y_label`只是选择了`Class`列。

4.  Labels are in text format. We need to encode them as numbers before we can use them with our model:

    ```
    labelencoder_Y = LabelEncoder() 
    Y_label = labelencoder_Y.fit_transform(Y_label)
    Y_label = Y_label.reshape([208, 1])
    ```

    最后的`reshape`函数会将标签转换成矩阵格式，这是模型所期望的。

5.  Build the multilayer model with Keras:

    ```
    model = Sequential()
    model.add(Dense(300,input_dim=60, activation = 'relu'))
    model.add(Dense(200, activation = 'relu'))
    model.add(Dense(100, activation = 'relu'))
    model.add(Dense(1, activation = 'sigmoid'))
    ```

    您可以试验层和神经元的数量，但最后一层只能有一个具有 sigmoid 激活函数的神经元，因为这是一个二元分类器。

6.  设置训练参数:

    ```
    model.compile(optimizer='adam',loss='binary_crossentropy', \               metrics=['accuracy'])
    ```

7.  Train the model:

    ```
    model.fit(X_input, Y_label, epochs=30)
    ```

    截断的输出将类似于以下内容:

    ```
    Train on 208 samples
    Epoch 1/30
    208/208 [==============================] - 0s 205us/sample - 
    loss: 
      0.1849 - accuracy: 0.9038
    Epoch 2/30
    208/208 [==============================] - 0s 220us/sample – 
    loss: 
      0.1299 - accuracy: 0.9615
    Epoch 3/30
    208/208 [==============================] - 0s 131us/sample – 
    loss: 
      0.0947 - accuracy: 0.9856
    Epoch 4/30
    208/208 [==============================] - 0s 151us/sample – 
    loss: 
      0.1046 - accuracy: 0.9712
    Epoch 5/30
    208/208 [==============================] - 0s 171us/sample – 
    loss: 
      0.0952 - accuracy: 0.9663
    Epoch 6/30
    208/208 [==============================] - 0s 134us/sample – 
    loss: 
      0.0777 - accuracy: 0.9856
    Epoch 7/30
    208/208 [==============================] - 0s 129us/sample – 
    loss: 
      0.1043 - accuracy: 0.9663
    Epoch 8/30
    208/208 [==============================] - 0s 142us/sample – 
    loss: 
      0.0842 - accuracy: 0.9712
    Epoch 9/30
    208/208 [==============================] - 0s 155us/sample – 
    loss: 
      0.1209 - accuracy: 0.9423
    Epoch 10/30
    208/208 [==============================] - ETA: 0s - loss: 
      0.0540 - accuracy: 0.98 - 0s 334us/sample - los
    ```

8.  Let's evaluate the trained model and examine its accuracy:

    ```
    model.evaluate(X_input, Y_label)
    ```

    输出如下所示:

    ```
    208/208 [==============================] - 0s 128us/sample – 
    loss: 
      0.0038 - accuracy: 1.0000
     [0.003758653004367191, 1.0]
    ```

    正如你所看到的，我们已经能够成功地训练一个多层二进制神经网络，并在 30 个时期内获得 100%的准确性。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/38EMoDi](https://packt.live/38EMoDi)。

    你也可以在[https://packt.live/2W2sygb](https://packt.live/2W2sygb)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 3。基于卷积神经网络的图像分类

## 活动 3.01:基于时尚 MNIST 数据集构建一个 M 多类分类器

## 解决方案

1.  打开新的 Jupyter 笔记本。
2.  导入`tensorflow.keras.datasets.fashion_mnist` :

    ```
    from tensorflow.keras.datasets import fashion_mnist
    ```

3.  使用`fashion_mnist.load_data()`加载时尚 MNIST 数据集，并将结果保存到`(features_train, label_train), (features_test, label_test)` :

    ```
    (features_train, label_train), (features_test, label_test) = \ fashion_mnist.load_data()
    ```

4.  Print the shape of the training set:

    ```
    features_train.shape
    ```

    输出如下所示:

    ```
    (60000, 28, 28)
    ```

    训练集由尺寸为`28`乘`28`的`60000`幅图像组成。我们需要重塑它，增加渠道维度。

5.  Print the shape of the testing set:

    ```
    features_test.shape
    ```

    输出如下所示:

    ```
    (10000, 28, 28)
    ```

    测试集由尺寸为`28`乘`28`的`10000`幅图像组成。我们需要重塑它，并增加渠道维度

6.  用尺寸`(number_rows, 28, 28, 1)` :

    ```
    features_train = features_train.reshape(60000, 28, 28, 1) features_test = features_test.reshape(10000, 28, 28, 1)
    ```

    重塑训练集和测试集
7.  创建三个名为`batch_size`、`img_height`和`img_width`的变量，分别取值为`16`、`28`和`28`:

    ```
    batch_size = 16 img_height = 28 img_width = 28
    ```

8.  从`tensorflow.keras.preprocessing`导入`ImageDataGenerator`:

    ```
    from tensorflow.keras.preprocessing.image \ import ImageDataGenerator
    ```

9.  创建一个名为`train_img_gen`的`ImageDataGenerator`，并增加数据:`rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'` :

    ```
    train_img_gen = ImageDataGenerator(rescale=1./255, \                                    rotation_range=40, \                                    width_shift_range=0.1, \                                    height_shift_range=0.1, \                                    shear_range=0.2, \                                    zoom_range=0.2, \                                    horizontal_flip=True, \                                    fill_mode='nearest')
    ```

10.  创建一个名为`val_img_gen`的`ImageDataGenerator`，并重新缩放(除以 255):

    ```
    val_img_gen = ImageDataGenerator(rescale=1./255)
    ```

11.  使用`.flow()`创建一个名为`train_data_gen`的数据生成器，并从训练集中指定批量大小、特性和标签:

    ```
    train_data_gen = train_img_gen.flow(features_train, \                                     label_train, \                                     batch_size=batch_size)
    ```

12.  使用`.flow()`创建一个名为`val_data_gen`的数据生成器，并从测试集中指定批量大小、特性和标签:

    ```
    val_data_gen = train_img_gen.flow(features_test, \                                   label_test, \                                   batch_size=batch_size)
    ```

13.  将`numpy`导入为`np`，将`tensorflow`导入为`tf`，将`layers`从`tensorflow.keras` :

    ```
    import numpy as np import tensorflow as tf from tensorflow.keras import layers
    ```

    导入
14.  使用`np.random_seed()`和`tf.random.set_seed()` :

    ```
    np.random.seed(8) tf.random.set_seed(8)
    ```

    将`8`设置为`numpy`和`tensorflow`的种子
15.  Instantiate a `tf.keras.Sequential()` class into a variable called `model` with the following layers: A convolution layer with `64` kernels of shape `3`, `ReLU` as the activation function, and the necessary input dimensions; a max pooling layer; a convolution layer with `128` kernels of shape `3` and `ReLU` as the activation function; a max pooling layer; a flatten layer; a fully connected layer with `128` units and `ReLU` as the activation function; a fully connected layer with `10` units and `softmax` as the activation function.

    代码应该如下所示:

    ```
    model = tf.keras.Sequential\
            ([layers.Conv2D(64, 3, activation='relu', \
                            input_shape=(img_height, \
                                         img_width ,1)), \
                            layers.MaxPooling2D(), \
                            layers.Conv2D(128, 3, \
                                          activation='relu'), \
                            layers.MaxPooling2D(),\
                            layers.Flatten(), \
                            layers.Dense(128, \
                                         activation='relu'), \
                            layers.Dense(10, \
                                         activation='softmax')])
    ```

16.  用`0.001`作为学习率实例化一个`tf.keras.optimizers.Adam()`类，并将其保存到一个名为

    ```
    optimizer = tf.keras.optimizers.Adam(0.001)
    ```

    的变量中
17.  使用`.compile()`和`loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']` :

    ```
    model.compile(loss='sparse_categorical_crossentropy', \               optimizer=optimizer, metrics=['accuracy'])
    ```

    编译神经网络
18.  Fit the neural networks with `fit_generator()` and provide the train and validation data generators, `epochs=5`, the steps per epoch, and the validation steps:

    ```
    model.fit_generator(train_data_gen, \
                        steps_per_epoch=len(features_train) \
                                        // batch_size, \
                        epochs=5, \
                        validation_data=val_data_gen, \
                        validation_steps=len(features_test) \
                                         // batch_size)
    ```

    预期产出如下:

    ![Figure 3.30: Model training log
    ](img/B15385_03_30.jpg)

图 3.30:模型训练日志

我们在五个时期上训练我们的 CNN，并且我们分别在训练集和验证集上获得了`0.8271`和`0.8334`的准确度分数。我们的模型没有过度拟合，并取得了相当高的分数。五个历元之后，准确度仍在增加，所以如果我们继续训练它，我们可能会得到更好的结果。这是你可以自己尝试的。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ObmA8t](https://packt.live/2ObmA8t)。

你也可以在[https://packt.live/3fiyyJi](https://packt.live/3fiyyJi)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

## 活动 3.02:利用迁移学习进行水果分类

## 解决方案

1.  打开新的 Jupyter 笔记本。
2.  将`tensorflow`导入为`tf` :

    ```
    import tensorflow as tf
    ```

3.  Create a variable called `file_url` containing the link to the dataset:

    ```
    file_url = 'https://github.com/PacktWorkshops'\
               '/The-Deep-Learning-Workshop'\
               '/raw/master/Chapter03/Datasets/Activity3.02'\
               '/fruits360.zip'
    ```

    注意

    在上述步骤中，我们使用存储在[https://packt.live/3eePQ8G](https://packt.live/3eePQ8G)的数据集。如果您已经在任何其他 URL 存储了数据集，请相应地更改突出显示的路径。

4.  使用带有`'fruits360.zip', origin=file_url, extract=True`的`tf.keras.get_file`下载数据集，并将结果保存到名为`zip_dir` :

    ```
    zip_dir = tf.keras.utils.get_file('fruits360.zip', \                                   origin=file_url, \                                   extract=True)
    ```

    的变量中
5.  导入`pathlib`库:

    ```
    import pathlib
    ```

6.  使用`pathlib.Path(zip_dir).parent` :

    ```
    path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'
    ```

    创建一个名为`path`的变量，包含到`fruits360_filtered`目录的完整路径
7.  创建两个名为`train_dir`和`validation_dir`的变量，它们分别获取火车(`Training`)和验证(`Test`)文件夹的完整路径:

    ```
    train_dir = path / 'Training' validation_dir = path / 'Test'
    ```

8.  创建两个名为`total_train`和`total_val`的变量，它们将获得训练集和验证集的图像数量，即`11398`和`4752` :

    ```
    total_train = 11398 total_val = 4752
    ```

9.  从`tensorflow.keras.preprocessing`导入`ImageDataGenerator`:

    ```
    from tensorflow.keras.preprocessing.image \ import ImageDataGenerator
    ```

10.  创建一个名为`train_img_gen`的`ImageDataGenerator`并增加数据:`rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'` :

    ```
    train_img_gen = ImageDataGenerator(rescale=1./255, \                                    rotation_range=40, \                                    width_shift_range=0.1, \                                    height_shift_range=0.1, \                                    shear_range=0.2, \                                    zoom_range=0.2, \                                    horizontal_flip=True, \                                    fill_mode='nearest')
    ```

11.  创建一个名为`val_img_gen`的`ImageDataGenerator`，并进行重新调整(除以 255):

    ```
    val_img_gen = ImageDataGenerator(rescale=1./255)
    ```

12.  创建四个名为`batch_size`、`img_height`、`img_width`和`channel`的变量，分别取值为`16`、`100`、`100`和`3`:

    ```
    batch_size=16 img_height = 100 img_width = 100 channel = 3
    ```

13.  使用`.flow_from_directory()`创建一个名为`train_data_gen`的数据生成器，并指定批量大小、培训文件夹和目标大小:

    ```
    train_data_gen = train_image_generator.flow_from_directory\                  (batch_size=batch_size, \                  directory=train_dir, \                  target_size=(img_height, img_width))
    ```

14.  使用`.flow_from_directory()`创建一个名为`val_data_gen`的数据生成器，并指定批量大小、验证文件夹和目标大小:

    ```
    val_data_gen = validation_image_generator.flow_from_directory\                (batch_size=batch_size, \                directory=validation_dir, \                target_size=(img_height, img_width))
    ```

15.  导入`numpy`为`np`，`tensorflow`为`tf`，从`tensorflow.keras` :

    ```
    import numpy as np import tensorflow as tf from tensorflow.keras import layers
    ```

    导入`layers`
16.  使用`np.random_seed()`和`tf.random.set_seed()` :

    ```
    np.random.seed(8) tf.random.set_seed(8)
    ```

    将`8`设置为`numpy`和`tensorflow`的种子
17.  从`tensorflow.keras.applications`导入`VGG16`:

    ```
    from tensorflow.keras.applications import VGG16
    ```

18.  用以下参数将一个`VGG16`模型实例化为一个名为`base_model`的变量:

    ```
    base_model = VGG16(input_shape=(img_height, \                                 img_width, channel), \                                 weights='imagenet', \                                 include_top=False)
    ```

19.  使用`.trainable`属性:

    ```
    base_model.trainable = False
    ```

    将该模型设置为不可训练
20.  Print the summary of this `VGG16` model:

    ```
    base_model.summary()
    ```

    预期产出如下:

    ![Figure 3.31: Model summary
    ](img/B15385_03_31.jpg)

    图 3.31:模型摘要

    这个输出向我们展示了`VGG16`的架构。我们可以看到总共有`14,714,688`个参数，但是没有可训练的参数。这是意料之中的，因为我们已经冻结了这个模型的所有层。

21.  使用`tf.keras.Sequential()`创建一个新模型，将基础模型添加到以下层:`Flatten()`、`Dense(1000, activation='relu')`和`Dense(120, activation='softmax')`。将这个模型保存到一个名为`model` :

    ```
    model = tf.keras.Sequential([base_model, \                              layers.Flatten(), \                              layers.Dense(1000, \                                           activation='relu'), \                              layers.Dense(120, \                                           activation='softmax')])
    ```

    的变量中
22.  用`0.001`作为学习率实例化一个`tf.keras.optimizers.Adam()`类，并保存到一个名为`optimizer` :

    ```
    optimizer = tf.keras.optimizers.Adam(0.001)
    ```

    的变量中
23.  使用`.compile()`和`loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']` :

    ```
    model.compile(loss='categorical_crossentropy', \               optimizer=optimizer, metrics=['accuracy'])
    ```

    编译神经网络
24.  Fit the neural networks with `fit_generator()` and provide the train and validation data generators, `epochs=5`, the steps per epoch, and the validation steps. This model may take a few minutes to train:

    ```
    model.fit_generator(train_data_gen, \
                        steps_per_epoch=len(features_train) \
                                        // batch_size, \
                        epochs=5, \
                        validation_data=val_data_gen, \
                        validation_steps=len(features_test) \
                                         // batch_size)
    ```

    预期产出如下:

    ![Figure 3.32: Expected output
    ](img/B15385_03_32.jpg)

图 3.32:预期产出

在这里，我们使用迁移学习在 ImageNet 上定制一个预训练的`VGG16`模型，以便它适合我们的水果分类数据集。我们用我们自己的完全连接的层替换了模型的头部，并在五个时期训练这些层。我们在训练集和测试集上分别获得了`0.9106`和`0.8920`的准确率。考虑到训练这个模型所用的时间和硬件，这些是非常显著的结果。你可以尝试微调这个模型，看看你是否能取得更好的成绩。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2DsVRCl](https://packt.live/2DsVRCl)。

本节目前没有在线交互示例，需要在本地运行。

# 4。文本嵌入的深度学习

## 活动 4.01:文本预处理“爱丽丝梦游仙境”文本

## 解决方案

您需要执行以下步骤:

注意

在开始本活动之前，确保您已经定义了`alice_raw`变量，如标题为*使用 NLTK* 下载文本语料库一节中所示。

1.  把数据改成小写，分成句子:

    ```
    txt_sents = tokenize.sent_tokenize(alice_raw.lower())
    ```

2.  给句子做记号:

    ```
    txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]
    ```

3.  从`string`模块导入`punctuation`，从 NLTK:

    ```
    from string import punctuation stop_punct = list(punctuation) from nltk.corpus import stopwords stop_nltk = stopwords.words("english")
    ```

    导入`stopwords`
4.  创建一个保存上下文停用词`--`和`said` :

    ```
    stop_context = ["--", "said"]
    ```

    的变量
5.  为停用字词创建一个主列表，以删除包含标点符号、NLTK 停用字词和上下文停用字词的字词:

    ```
    stop_final = stop_punct + stop_nltk + stop_context
    ```

6.  定义一个函数，从任何输入句子中删除这些标记(标记化的):

    ```
    def drop_stop(input_tokens):     return [token for token in input_tokens \             if token not in stop_final]
    ```

7.  Remove the terms in `stop_final` from the tokenized text:

    ```
    alice_words_nostop = [drop_stop(sent) for sent in txt_words]
    print(alice_words_nostop[:2])
    ```

    前两句话是这样的:

    ```
    [['alice', "'s", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', "'and", 'use', 'book', 'thought', 'alice', "'without", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close']]
    ```

8.  Using the `PorterStemmer` algorithm from NLTK, perform stemming on the result. Print out the first five sentences of the result:

    ```
    from nltk.stem import PorterStemmer
    stemmer_p = PorterStemmer()
    alice_words_stem = [[stemmer_p.stem(token) for token in sent] \
                         for sent in alice_words_nostop]
    print(alice_words_stem[:5])
    ```

    输出如下所示:

    ```
    [['alic', "'s", 'adventur', 'wonderland', 'lewi', 'carrol', '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', 'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', 'peep', 'book', 'sister', 'read', 'pictur', 'convers', "'and", 'use', 'book', 'thought', 'alic', "'without", 'pictur', 'convers'], ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', 'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', 'daisi', 'suddenli', 'white', 'rabbit', 'pink', 'eye', 'ran', 'close'], ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', 'rabbit', 'say', "'oh", 'dear'], ['oh', 'dear'], ['shall', 'late']]
    ```

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/2VVNEgf 的。

    你也可以在 https://packt.live/38Gr54r 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

## 活动 4.02:爱丽丝梦游仙境的文本展示

## 解决方案

您需要执行以下步骤:

1.  From *Activity 4.01*, *Text Preprocessing Alice in Wonderland*, print the first three sentences from the result after stop word removal. This is the data you will work with:

    ```
    print(alice_words_nostop[:3])
    ```

    输出如下所示:

    ```
    [['alice', "'s", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', "'and", 'use', 'book', 'thought', 'alice', "'without", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close'], ['nothing', 'remarkable', 'alice', 'think', 'much', 'way', 'hear', 'rabbit', 'say', "'oh", 'dear']]
    ```

2.  从 Gensim 导入`word2vec`并用默认参数训练你的词嵌入:

    ```
    from gensim.models import word2vec model = word2vec.Word2Vec(alice_words_nostop)
    ```

3.  Find the `5` terms most similar to `rabbit`:

    ```
    model.wv.most_similar("rabbit", topn=5)
    ```

    输出如下所示:

    ```
    [('alice', 0.9963310360908508),
     ('little', 0.9956872463226318),
     ('went', 0.9955698251724243),
     ("'s", 0.9955658912658691),
     ('would', 0.9954401254653931)]
    ```

4.  使用`window`大小的`2`，重新训练单词向量:

    ```
    model = word2vec.Word2Vec(alice_words_nostop, window=2)
    ```

5.  Find the terms most similar to `rabbit`:

    ```
    model.wv.most_similar("rabbit", topn=5)
    ```

    输出如下所示:

    ```
    [('alice', 0.9491485357284546),
     ("'s", 0.9364748001098633),
     ('little', 0.9345826506614685),
     ('large', 0.9341927170753479),
     ('duchess', 0.9341296553611755)]
    ```

6.  使用 Skip-gram 方法重新训练单词向量，窗口大小为`5` :

    ```
    model = word2vec.Word2Vec(alice_words_nostop, window=5, sg=1)
    ```

7.  Find the terms most similar to `rabbit`:

    ```
    model.wv.most_similar("rabbit", topn=5)
    ```

    输出如下所示:

    ```
    [('gardeners', 0.9995723366737366),
     ('end', 0.9995588064193726),
     ('came', 0.9995309114456177),
     ('sort', 0.9995298385620117),
     ('upon', 0.9995272159576416)]
    ```

8.  通过对`white`和`rabbit` :

    ```
    v1 = model.wv['white'] v2 = model.wv['rabbit'] res1 = (v1+v2)/2
    ```

    的向量求平均值，找到短语`white rabbit`的表示
9.  通过对`mad`和`hatter` :

    ```
    v1 = model.wv['mad'] v2 = model.wv['hatter'] res2 = (v1+v2)/2
    ```

    的向量求平均值，找到`mad hatter`的表示
10.  Find the cosine similarity between these two phrases:

    ```
    model.wv.cosine_similarities(res1, [res2])
    ```

    这为我们提供了以下值:

    ```
    array([0.9996213], dtype=float32)
    ```

11.  使用格式化的键控向量加载尺寸为 100D 的预训练手套嵌入:

    ```
    from gensim.models.keyedvectors import KeyedVectors glove_model = KeyedVectors.load_word2vec_format\ ("glove.6B.100d.w2vformat.txt", binary=False)
    ```

12.  查找`white rabbit`和`mad hatter`的表示:

    ```
    v1 = glove_model['white'] v2 = glove_model['rabbit'] res1 = (v1+v2)/2 v1 = glove_model['mad'] v2 = glove_model['hatter'] res2 = (v1+v2)/2
    ```

13.  Find the `cosine` similarity between the two phrases. Has the cosine similarity changed?

    ```
    glove_model.cosine_similarities(res1, [res2])
    ```

    以下是上述代码的输出:

    ```
    array([0.4514577], dtype=float32)
    ```

在这里，我们可以看到两个短语“`mad hatter`”和“`white rabbit`”之间的余弦相似度远远低于手套模型。这是因为手套模型在其训练数据中没有像在书中那样多地看到这些术语。在这本书里，`mad`和`hatter`这两个词出现了很多次，因为它们构成了一个重要角色的名字。当然，在其他情况下，我们不会经常看到`mad`和`hatter`在一起。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。

本节目前没有在线交互示例，需要在本地运行。

# 5。序列的深度学习

## 活动 5.01:使用简单的 RNN 模型预测 IBM 的股票价格

## 解

1.  Import the necessary libraries, load the `.csv` file, reverse the index, and plot the time series (the `Close` column) for visual inspection:

    ```
    import pandas as pd, numpy as np
    import matplotlib.pyplot as plt
    inp0 = pd.read_csv("IBM.csv")
    inp0 = inp0.sort_index(ascending=False)
    inp0.plot("Date", "Close")
    plt.show()
    ```

    输出如下，收盘价绘制在 *Y 轴*上:

    ![Figure 5.40: The trend for IBM stock prices
    ](img/B15385_05_40.jpg)

    图 5.40:IBM 股票价格的趋势

2.  Extract the values for `Close` from the DataFrame as a `numpy` array and plot them using `matplotlib`:

    ```
    ts_data = inp0.Close.values.reshape(-1,1)
    plt.figure(figsize=[14,5])
    plt.plot(ts_data)
    plt.show()
    ```

    结果趋势如下，指数绘制在 *X 轴*上:

    ![Figure 5.41: The stock price data visualized
    ](img/B15385_05_41.jpg)

    图 5.41:可视化的股票价格数据

3.  Assign the final 25% data as test data and the first 75% as train data:

    ```
    train_recs = int(len(ts_data) * 0.75)
    train_data = ts_data[:train_recs]
    test_data = ts_data[train_recs:]
    len(train_data), len(test_data)
    ```

    输出如下所示:

    ```
    (1888, 630)
    ```

4.  使用`sklearn`中的`MinMaxScaler`，缩放训练和测试数据:

    ```
    from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() train_scaled = scaler.fit_transform(train_data) test_scaled = scaler.transform(test_data)
    ```

5.  Using the `get_lookback` function we defined earlier in this chapter (refer to the *Preparing the Data for Stock Price Prediction* section), get the lookback data for the train and test sets using a lookback period of 10:

    ```
    look_back = 10
    trainX, trainY = get_lookback(train_scaled, look_back=look_back)
    testX, testY = get_lookback(test_scaled, look_back= look_back)
    trainX.shape, testX.shape
    ```

    输出如下所示:

    ```
    ((1888, 10), (630, 10))
    ```

6.  从 Keras 中，导入使用普通 rnn(`SimpleRNN`、`Activation`、`Dropout`、`Dense`和`Reshape`)和 1D 卷积(Conv1D)所需的所有层。另外，从`sklearn` :

    ```
    from tensorflow.keras.models import Sequential from tensorflow.keras.layers import SimpleRNN, Activation, Dropout, Dense, Reshape, Conv1D from sklearn.metrics import mean_squared_error
    ```

    导入`mean_squared_error`指标
7.  Build a model with a 1D convolution layer (5 filters of size 3) and an RNN layer with 32 neurons. Add 25% dropout after the RNN layer. Print the model's summary:

    ```
    model_comb = Sequential()
    model_comb.add(Reshape((look_back,1), \
                            input_shape = (look_back,)))
    model_comb.add(Conv1D(5, 3, activation='relu'))
    model_comb.add(SimpleRNN(32))
    model_comb.add(Dropout(0.25))
    model_comb.add(Dense(1))
    model_comb.add(Activation('linear'))
    model.summary()
    ```

    输出如下所示:

    ![Figure 5.42: Summary of the model
    ](img/B15385_05_42.jpg)

    图 5.42:模型总结

8.  Compile the model with the `mean_squared_error` loss and the `adam` optimizer. Fit this on the train data in five epochs, with a validation split of 10% and a batch size of 1:

    ```
    model_comb.compile(loss='mean_squared_error', \
                       optimizer='adam')
    model_comb.fit(trainX, trainY, epochs=5, \
                   batch_size=1, verbose=2, \
                   validation_split=0.1)
    ```

    输出如下所示:

    ![Figure 5.43: Training and validation loss
    ](img/B15385_05_43.jpg)

    图 5.43:培训和验证损失

9.  Using the `get_model_perf` method, print the RMSE of the model:

    ```
    get_model_perf(model_comb)
    ```

    输出如下所示:

    ```
    Train RMSE: 0.03 RMSE
    Test RMSE: 0.03 RMSE
    ```

10.  Plot the predictions – the entire view, as well as the zoomed-in view:

    ```
    %matplotlib notebook
    plt.figure(figsize=[10,5])
    plot_pred(model_comb)
    ```

    我们应该会看到下面的预测图(虚线)和实际图(实线):

    ![Figure 5.44: Predictions versus actuals
    ](img/B15385_05_44.jpg)

图 5.44:预测与实际

放大后的视图如下:

![Figure 5.45: Predictions (dotted lines) versus actuals (solid lines) – detailed view
](img/B15385_05_45.jpg)

图 5.45:预测值(虚线)与实际值(实线)-详细视图

我们可以看到，该模型在捕捉更精细的模式方面做得很好，并且在预测每日股票价格方面做得非常好。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ZctArW](https://packt.live/2ZctArW)。

你也可以在 https://packt.live/38EDOEA 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 6。LSTMs、gru 和高级 rnn

## 活动 6.01:亚马逊产品评论的情感分析

## 解决方案

1.  Read in the data files for the `train` and `test` sets. Examine the shapes of the datasets and print out the top `5` records from the `train` data:

    ```
    import pandas as pd, numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline
    train_df = pd.read_csv("Amazon_reviews_train.csv")
    test_df = pd.read_csv("Amazon_reviews_test.csv")
    print(train_df.shape, train_df.shape)
    train_df.head(5)
    ```

    数据集的形状和标题如下:

    ![Figure 6.26: First five records from the train dataset
    ](img/B15385_06_26.jpg)

    图 6.26:训练数据集中的前五条记录

2.  For convenience, when it comes to processing, separate the raw text and the labels for the `train` and `test` sets. You should have `4` variables, as follows: `train_raw` comprising raw text for the train data, `train_labels` with labels for the train data, `test_raw` containing raw text for the test data, and `test_labels` comprising Labels for the test data. Print the first two reviews from the `train` text.

    ```
    train_raw = train_df.review_text.values
    train_labels = train_df.label.values
    test_raw = test_df.review_text.values
    test_labels = test_df.label.values
    train_raw[:2]
    ```

    上述代码会产生以下输出:

    ![Figure 6.27: Raw text from the train dataset
    ](img/B15385_06_27.jpg)

    图 6.27:来自训练数据集的原始文本

3.  Normalize the case and tokenize the test and train texts using NLTK's `word_tokenize` (after importing it, of course – hint: use a list comprehension for cleaner code). Download `punkt` from `nltk` if you haven't used the tokenizer before. Print the first review from the train data to check if the tokenization worked.

    ```
    import nltk
    nltk.download('punkt')
    from nltk.tokenize import word_tokenize
    train_tokens = [word_tokenize(review.lower()) \
                    for review in train_raw]
    test_tokens = [word_tokenize(review.lower()) \
                   for review in test_raw]
    print(train_tokens[0])
    ```

    标记化的数据打印如下:

    ![Figure 6.28: Tokenized review from the train dataset
    ](img/B15385_06_28.jpg)

    图 6.28:来自`train`数据集的标记化检查

4.  从字符串模块导入任何停用词(内置于 NLTK)和标点符号。定义一个函数(`drop_stop`)从任何输入的标记化句子中删除这些标记。从 NLTK 下载`stopwords`如果你以前没用过:

    ```
    from string import punctuation stop_punct = list(punctuation) nltk.download("stopwords") from nltk.corpus import stopwords stop_nltk = stopwords.words("english") stop_final = stop_punct + stop_nltk def drop_stop(input_tokens):     return [token for token in input_tokens \             if token not in stop_final]
    ```

5.  Using the defined function (`drop_stop`), remove the redundant stop words from the `train` and the `test` texts. Print the first review of the processed `train` texts to check whether the function worked:

    ```
    train_tokens_no_stop = [drop_stop(sent) \
                            for sent in train_tokens]
    test_tokens_no_stop = [drop_stop(sent) \
                           for sent in test_tokens]
    print(train_tokens_no_stop[0])
    ```

    我们将得到以下输出:

    ```
    ['stuning', 'even', 'non-gamer', 'sound', 'track', 'beautiful', 
     'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', 
     'people', 'hate', 'vid', 'game', 'music', 'played', 'game', 
     'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', 
     'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', 
     'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', 
     'impress', 'anyone', 'cares', 'listen', '^_^']
    ```

6.  Using `PorterStemmer` from NLTK, stem the tokens for both the `train` and `test` data:

    ```
    from nltk.stem import PorterStemmer
    stemmer_p = PorterStemmer()
    train_tokens_stem = [[stemmer_p.stem(token) for token in sent] \
                         for sent in train_tokens_no_stop]
    test_tokens_stem = [[stemmer_p.stem(token) for token in sent] \
                         for sent in test_tokens_no_stop]
    print(train_tokens_stem[0])
    ```

    结果应该打印如下:

    ```
    ['stune', 'even', 'non-gam', 'sound', 'track', 'beauti', 'paint', 
     'seneri', 'mind', 'well', 'would', 'recomend', 'even', 'peopl', 
     'hate', 'vid', 'game', 'music', 'play', 'game', 'chrono', 'cross', 
     'game', 'ever', 'play', 'best', 'music', 'back', 'away', 'crude', 
     'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', 
     'orchestra', 'would', 'impress', 'anyon', 'care', 'listen', '^_^']
    ```

7.  Create the strings for each of the `train` and `text` reviews. This will help us work with the utilities in Keras to create and pad the sequences. Create the `train_texts` and `test_texts` variables. Print the first review from the processed `train` data to confirm this:

    ```
    train_texts = [" ".join(txt) for txt in train_tokens_stem]
    test_texts = [" ".join(txt) for txt in test_tokens_stem]
    print(train_texts[0])
    ```

    上述代码的结果如下:

    ```
    stune even non-gam sound track beauti paint seneri mind well would recommend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^
    ```

8.  从 Keras 的文本预处理工具(`keras.preprocessing.text`，导入`Tokenizer`模块。定义一个词汇大小`10000`，并用这个词汇实例化标记器:

    ```
    from tensorflow.keras.preprocessing.text import Tokenizer vocab_size = 10000 tok = Tokenizer(num_words=vocab_size)
    ```

9.  Fit the tokenizer on the `train` texts. This works just like `CountVectorizer` did in *Chapter 4, Deep Learning for Text – Embeddings*, and trains the vocabulary. After fitting, use the `texts_to_sequences` method of the tokenizer on the `train` and `test` sets to create the sequences for them. Print the sequence for the first review in the train data:

    ```
    tok.fit_on_texts(train_texts)
    train_sequences = tok.texts_to_sequences(train_texts)
    test_sequences = tok.texts_to_sequences(test_texts)
    print(train_sequences[0])
    ```

    编码序列如下:

    ```
     [22, 514, 7161, 85, 190, 184, 1098, 283, 20, 11, 1267, 22, 
      56, 370, 9682, 114, 41, 71, 114, 8166, 1455, 114, 51, 71, 
      29, 41, 58, 182, 2931, 2153, 75, 8167, 816, 2666, 829, 719, 
      3871, 11, 483, 120, 268, 110]
    ```

10.  We need to find the optimal length of the sequences to process the model. Get the length of the reviews from the `train` set into a list and plot a histogram of the lengths:

    ```
    seq_lens = [len(seq) for seq in train_sequences]
    plt.hist(seq_lens)
    plt.show()
    ```

    长度分布如下:

    ![Figure 6.29: Histogram of text lengths
    ](img/B15385_06_29.jpg)

    图 6.29:文本长度直方图

11.  The data is now in the same format as the IMDb data we used in this chapter. Using a sequence length of `100` (define the `maxlen = 100` variable), use the `pad_sequences` method from the `sequence` module in Keras' preprocessing utilities (`keras.preprocessing.sequence`) to limit the sequences to `100` for both the `train` and `test` data. Check the shape of the result for the train data:

    ```
    maxlen = 100
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    X_train = pad_sequences(train_sequences, maxlen=maxlen)
    X_test = pad_sequences(test_sequences, maxlen=maxlen)
    X_train.shape
    ```

    形状如下:

    ```
    (25000, 100)
    ```

12.  要构建模型，从 Keras ( `embedding`、`spatial dropout`、`LSTM`、`dropout`和`dense`)导入所有必要的层，并导入`Sequential`模型。初始化`Sequential`模型:

    ```
    from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, GRU, LSTM model_lstm = Sequential()
    ```

13.  添加一个`32`为矢量大小的嵌入图层(`output_dim`)。添加一个`40%` :

    ```
    model_lstm.add(Embedding(vocab_size, output_dim=32)) model_lstm.add(SpatialDropout1D(0.4))
    ```

    的空间缺失
14.  建立一个堆叠的 LSTM 模型，每层有`2`个 T21 单元。用`40%` dropout:

    ```
    model_lstm.add(LSTM(64, return_sequences=True)) model_lstm.add(LSTM(64, return_sequences=False)) model_lstm.add(Dropout(0.4))
    ```

    添加一个 dropout 层
15.  添加一个带有激活`relu`的`32`神经元的密集层，然后是一个`50%`脱落层，接着是另一个带有激活`relu`的`32`神经元的密集层，接着是另一个带有`50%`脱落的脱落层:

    ```
    model_lstm.add(Dense(32, activation='relu')) model_lstm.add(Dropout(0.5)) model_lstm.add(Dense(32, activation='relu')) model_lstm.add(Dropout(0.5))
    ```

16.  Add a final dense layer with a single neuron with `sigmoid` `activation` and compile the model. Print the model summary:

    ```
    model_lstm.add(Dense(1, activation='sigmoid'))
    model_lstm.compile(loss='binary_crossentropy', \
                       optimizer='rmsprop', \
                       metrics=['accuracy'])
    model_lstm.summary()
    ```

    该模型的概要如下:

    ![Figure 6.30: Stacked LSTM model summary
    ](img/B15385_06_30.jpg)

    图 6.30:堆叠 LSTM 模型摘要

17.  Fit the model on the training data with a `20%` validation split and a batch size of `128`. Train for `5` `epochs`:

    ```
    history_lstm = model_lstm.fit(X_train, train_labels, \
                                  batch_size=128, \
                                  validation_split=0.2, \
                                  epochs = 5)
    ```

    我们将获得以下培训输出:

    ![Figure 6.31: Stacked LSTM model training output
    ](img/B15385_06_31.jpg)

    图 6.31:堆叠 LSTM 模型训练输出

18.  Make a prediction on the test set using the `predict_classes` method of the model. Then, print out the confusion matrix:

    ```
    from sklearn.metrics import accuracy_score, confusion_matrix
    test_pred = model_lstm.predict_classes(X_test)
    print(confusion_matrix(test_labels, test_pred))
    ```

    我们将得到以下结果:

    ```
    [[10226,  1931],
     [ 1603, 11240]]
    ```

19.  Using the `accuracy_score` method from `scikit-learn`, calculate the accuracy of the test set.

    ```
    print(accuracy_score(test_labels, test_pred))
    ```

    我们得到的精度是:

    ```
    0.85864
    ```

正如我们所看到的，准确度分数在`86%`左右，并且查看混淆矩阵(*步骤 18* 的输出),该模型很好地预测了两个类别。我们没有做任何超参数调整就获得了这种精度。您可以调整超参数以获得更高的精度。

注意

要访问该特定部分的源代码，请参考[https://packt.live/3fpo0YI](https://packt.live/3fpo0YI)。

你也可以在 https://packt.live/2Wi75QH 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 7。生成对抗网络

## 活动 7.01:为 MNIST 时装数据集实施 DCGAN

## 解决方案

1.  打开一个新的 Jupyter 笔记本，命名为`Activity 7.01`。导入以下库包:

    ```
    # Import the required library functions import numpy as np import matplotlib.pyplot as plt from matplotlib import pyplot import tensorflow as tf from tensorflow.keras.layers import Input from tensorflow.keras.initializers import RandomNormal from tensorflow.keras.models import Model, Sequential from tensorflow.keras.layers \ import Reshape, Dense, Dropout, Flatten,Activation from tensorflow.keras.layers import LeakyReLU,BatchNormalization from tensorflow.keras.layers import Conv2D, UpSampling2D,Conv2DTranspose from tensorflow.keras.datasets import fashion_mnist from tensorflow.keras.optimizers import Adam
    ```

2.  Create a function that will generate real data samples from the fashion MNIST data:

    ```
    # Function to generate real data samples
    def realData(batch):
        # Get the MNIST data 
        (X_train, _), (_, _) = fashion_mnist.load_data()
        # Reshaping the input data to include channel
        X = X_train[:,:,:,np.newaxis]
        # normalising the data to be between 0 and 1
        X = (X.astype('float32') - 127.5)/127.5
        # Generating a batch of data
        imageBatch = X[np.random.randint(0, X.shape[0], \
                                         size=batch)]
        return imageBatch
    ```

    该函数的输出是批量 MNIST 数据。请注意，我们通过减去`127.5`(最大像素值的一半)并除以相同的值来归一化输入数据。这将有助于更快地收敛解决方案。

3.  Now, let's generate a set of images from the MNIST dataset:

    ```
    # Generating a set of  sample images 
    fashionData = realData(25)
    ```

    您应该得到以下输出:

    ![Figure 7.36: Generating images from MNIST
    ](img/B15385_07_36.jpg)

    图 7.36:从 MNIST 生成图像

4.  Now, let's visualize the images with `matplotlib`:

    ```
     # for j in range(5*5):
        pyplot.subplot(5,5,j+1)
        # turn off axis 
        pyplot.axis('off') 
        pyplot.imshow(fashionData[j,:,:,0],cmap='gray_r')
    ```

    您应该会得到类似如下所示的输出:

    ![Figure 7.37: Plotted images
    ](img/B15385_07_37.jpg)

    图 7.37:绘制的图像

    从输出中，我们可以看到几个时尚文章的可视化。我们可以看到图像位于白色背景的中央。这是我们试图重现的画面。

5.  Now, let's define the function to generate inputs for the generator network. The inputs are random data points that are generated from a random uniform distribution:

    ```
    # Function to generate inputs for generator function
    def fakeInputs(batch,infeats):
        # Generate random noise data with shape (batch,input features)
        x_fake = np.random.uniform(-1,1,size=[batch,infeats])
        return x_fake
    ```

    该函数生成从随机分布中采样的假数据作为输出。

6.  Let's define the function for building the generator network:

    ```
    Activity7.01.ipynb
    # Function for the generator model
    def genModel(infeats):
        # Defining the Generator model
        Genmodel = Sequential()
        Genmodel.add(Dense(512,input_dim=infeats))
        Genmodel.add(Activation('relu'))
        Genmodel.add(BatchNormalization())
        # second layer of FC => RElu => BN layers
        Genmodel.add(Dense(7*7*64))
        Genmodel.add(Activation('relu'))
        Genmodel.add(BatchNormalization())
    The complete code for this step can be found at https://packt.live/3fpobDm
    ```

    构建发电机网络类似于构建任何 CNN 网络。在这个生成器网络中，我们将使用转置卷积方法对图像进行上采样。在这个模型中，我们可以看到转置卷积的逐步使用。初始输入从维度 100 开始，这是我们的输入特性。MNIST 数据集的维度是批量大小 x 28 x 28。因此，我们对数据进行了两次上采样，以获得批量大小为 x 28 x 28 的输出。

7.  Next, we define the function that will be used to create fake samples:

    ```
    # Function to create fake samples using the generator model
    def fakedataGenerator(Genmodel,batch,infeats):
        # first generate the inputs to the model
        genInputs = fakeInputs(batch,infeats)
        """
        use these inputs inside the generator model \
        to generate fake distribution
        """
        X_fake = Genmodel.predict(genInputs)
        return X_fake
    ```

    在这个函数中，我们只返回`X`变量。这个函数的输出是假数据集。

8.  Define the parameters that we will use in many of the functions, along with the summary of the generator network:

    ```
    # Define the arguments like batch size and input feature
    batch = 128
    infeats = 100
    Genmodel = genModel(infeats,)
    Genmodel.summary()
    ```

    您应该得到以下输出:

    ![Figure 7.38: Summary of the generative model
    ](img/B15385_07_38.jpg)

    图 7.38:创成式模型概述

    根据总结，请注意输入噪声的维度如何随着每个转置卷积运算而变化。最后，我们得到一个在维数上与真实数据集相等的输出，`( None,28 ,28,1)`。

9.  Let's use the generator function to generate a fake sample before training:

    ```
    # Generating a fake sample and printing the shape
    fake = fakedataGenerator(Genmodel,batch,infeats)
    fake.shape
    ```

    您应该得到以下输出:

    ```
    (128, 28, 28, 1)
    ```

10.  Now, let's plot the generated fake sample:

    ```
    # Plotting the fake sample
    plt.imshow(fake[1, :, :, 0], cmap='gray_r')
    ```

    您应该会得到类似如下的输出:

    ![Figure 7.39: Output of the fake sample
    ](img/B15385_07_39.jpg)

    图 7.39:假样本的输出

    这是训练前假样本的情节。训练后，我们希望像这样的样本看起来像我们在本活动中早些时候可视化的 MNIST 时装样本。

11.  Build the discriminator model as a function. The network architecture will be similar to a CNN architecture:

    ```
    Activity7.01.ipynb
    # Descriminator model as a function
    def discModel():
        Discmodel = Sequential()
        Discmodel.add(Conv2D(32,kernel_size=(5,5),strides=(2,2),\
                      padding='same',input_shape=(28,28,1)))
        Discmodel.add(LeakyReLU(0.2))
        # second layer of convolutions
        Discmodel.add(Conv2D(64, kernel_size=(5,5), strides=(2, 2), \
                      padding='same'))
        Discmodel.add(LeakyReLU(0.2))
    The full code for this step can be found at https://packt.live/3fpobDm
    ```

    在鉴别器网络中，我们已经包括了所有必要的层，例如卷积运算和`LeakyReLU`。请注意，最后一层是 sigmoid 层，因为我们希望输出样本是真实(1)还是虚假(0)的概率。

12.  Print the summary of the discriminator network:

    ```
    # Print the summary of the discriminator model
    Discmodel = discModel()
    Discmodel.summary()
    ```

    您应该得到以下输出:

    ![Figure 7.40: Discriminator model summary
    ](img/B15385_07_40.jpg)

    图 7.40:鉴别器模型总结

13.  Define the GAN model as a function:

    ```
    # Define the combined generator and discriminator model, for updating the generator
    def ganModel(Genmodel,Discmodel):
        # First define that discriminator model cannot be trained
        Discmodel.trainable = False
        Ganmodel = Sequential()
        # First adding the generator model
        Ganmodel.add(Genmodel)
        """
        Next adding the discriminator model 
        without training the parameters
        """
        Ganmodel.add(Discmodel)
        """
        Compile the model for loss to optimise the Generator model
        """
        Ganmodel.compile(loss='binary_crossentropy',\
                         optimizer = 'adam')
        return Ganmodel
    ```

    GAN 模型的结构类似于我们在*练习 7.05* 、*中开发的实现 DCGAN* 的模型。

14.  Now, it's time to invoke the GAN function:

    ```
    # Initialise the GAN model
    gan_model = ganModel(Genmodel,Discmodel)
    # Print summary of the GAN model
    gan_model.summary()
    ```

    请注意，GAN 模型的输入是之前定义的发电机模型和鉴别器模型。您应该得到以下输出:

    ![Figure 7.41: GAN model summary
    ](img/B15385_07_41.jpg)

    图 7.41: GAN 模型总结

    请注意，GAN 模型各层的参数与发生器和鉴别器模型的参数是等效的。GAN 模型只是我们之前定义的两个模型的包装。

15.  使用以下代码定义训练网络的时期数:

    ```
    # Defining the number of epochs nEpochs = 5000
    ```

16.  Now, we can start the process of training the network:

    ```
    Activity7.01.ipynb
    # Train the GAN network
    for i in range(nEpochs):
        """
        Generate samples equal to the batch size 
        from the real distribution
        """
        x_real = realData(batch)
        #Generate fake samples using the fake data generator function
        x_fake = fakedataGenerator(Genmodel,batch,infeats)
        # Concatenating the real and fake data 
        X = np.concatenate([x_real,x_fake])
        #Creating the dependent variable and initializing them as '0'
        Y = np.zeros(batch * 2)
    The complete code for this step can be found on https://packt.live/3fpobDm
    ```

    这里需要注意的是，用假样本和真样本训练鉴别器模型以及训练 GAN 模型是同时发生的。唯一的区别是 GAN 模型的训练在没有更新鉴别器模型的参数的情况下进行。另一点需要注意的是，在 GAN 内部，伪样本的标签将为 1，从而产生大量损耗项，这些损耗项将通过鉴频器网络反向传播，以更新发生器参数。我们还显示了每 50 个时期 GAN 的预测概率。在计算概率时，我们将真实数据样本和虚假数据样本结合起来，然后取预测概率的平均值。我们还保存了生成图像的副本。

    您应该会得到类似如下的输出:

    ```
    Discriminator probability:0.5276428461074829
    Discriminator probability:0.5038391351699829
    Discriminator probability:0.47621315717697144
    Discriminator probability:0.48467564582824707
    Discriminator probability:0.5270703434944153
    Discriminator probability:0.5247280597686768
    Discriminator probability:0.5282968282699585
    ```

    让我们看看在不同时期从训练过程中产生的一些图:

    ![Figure 7.42: Images generated during the training process
    ](img/B15385_07_42.jpg)

    图 7.42:训练过程中生成的图像

    从前面的图中，我们可以看到训练过程的进展。我们可以看到，到了公元 100 年，这些图大部分都是噪声。到了公元 600 年，时尚物品的形式开始变得更加明显。在 1，500 年，我们可以看到假图像看起来非常类似于时尚数据集。

    注意:

    你可以去 https://packt.live/2W1FjaI 仔细看看这些图片。

17.  Now, let's look at the images that were generated after training:

    ```
     # Images generated after training
    x_fake = fakedataGenerator(Genmodel,25,infeats)
    # Displaying the plots
    for j in range(5*5):
    pyplot.subplot(5,5,j+1)
        # turn off axis 
        pyplot.axis('off')
        pyplot.imshow(x_fake[j,:,:,0],cmap='gray_r')
    ```

    您应该会得到类似如下的输出:

    ![Figure 7.43: Images generated after the training process
    ](img/B15385_07_43.jpg)

图 7.43:训练过程后生成的图像

从训练精度级别中，您可以看到鉴别器模型的精度徘徊在. 50 范围附近，这是所需的范围。发生器的目的是创建看起来像真图像的假图像。当生成器生成看起来与真实图像非常相似的图像时，鉴别器会弄不清该图像是从真实分布还是伪分布生成的。这种现象表现在鉴频器的精度水平约为 50%,这是所希望的水平。

注意

要访问该特定部分的源代码，请参考 https://packt.live/3fpobDm 的。

本节目前没有在线交互示例，需要在本地运行。