<html><head/><body>


    
        <title>Go Live and Go Big</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">活下去，做大</h1>
                
            
            
                
<p>在这一章中，我们将了解更多关于<strong>亚马逊网络服务</strong> ( <strong> AWS </strong>)以及如何创建深度神经网络来解决视频动作识别问题。我们将向您展示如何使用多个GPU进行更快的训练。在本章的最后，我们将向您简要介绍Amazon Mechanical Turk服务，它允许我们收集标签并纠正模型的结果。</p>


            

            
        
    






    
        <title>Quick look at Amazon Web Services</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">快速浏览亚马逊网络服务</h1>
                
            
            
                
<p>亚马逊网络服务 ( <strong> AWS </strong>)是最流行的云平台之一，由Amazon.com制造。它提供许多服务，包括云计算、存储、数据库服务、内容交付和其他功能。在本节中，我们将只关注Amazon EC2上的虚拟服务器服务。Amazon EC2允许我们创建多个服务器来支持我们的模型甚至训练程序。说到为终端用户服务车型，可以阅读<a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank">第九章</a>、<em>巡航控制-自动化</em>，了解TensorFlow服务。在培训中，Amazon EC2有许多我们可以使用的实例类型。我们可以使用他们的CPU服务器来运行我们的网络机器人，从互联网上收集数据。有几种实例类型具有多个NVIDIA GPUs。</p>
<p>Amazon EC2提供了广泛的实例类型选择，以适应不同的用例。实例类型分为五类，如下所示:</p>
<ul>
<li>通用</li>
<li>计算优化</li>
<li>内存优化</li>
<li>存储优化</li>
<li>加速计算实例</li>
</ul>
<p>前四个类别最适合运行后端服务器。加速计算实例具有多个NVIDIA GPUs，可用于为模型提供服务，并利用高端GPU训练新模型。有三种类型的实例-P2、G2和F1。</p>


            

            
        
    






    
        <title>P2 instances</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">P2实例</h1>
                
            
            
                
<p>P2实例包含高性能NVIDIA K80 GPUs，每个GPU具有2，496个CUDA内核和12 GB的GPU内存。P2有三种模式，如下表所述:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>型号</strong></p>
</td>
<td>
<p><strong>图形处理器</strong></p>
</td>
<td>
<p><strong> vCPU </strong></p>
</td>
<td>
<p><strong>内存(GB) </strong></p>
</td>
<td>
<p><strong> GPU内存(GB) </strong></p>
</td>
</tr>
<tr>
<td>
<p>p2.xlarge</p>
</td>
<td>
<p>一</p>
</td>
<td>
<p>四</p>
</td>
<td>
<p>61</p>
</td>
<td>
<p>12</p>
</td>
</tr>
<tr>
<td>
<p>p 2.8x大</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>488</p>
</td>
<td>
<p>96</p>
</td>
</tr>
<tr>
<td>
<p>p 2.16大号</p>
</td>
<td>
<p>16</p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>732</p>
</td>
<td>
<p>192</p>
</td>
</tr>
</tbody>
</table>
<p>这些GPU内存大的模型最适合训练模型。有了更多的GPU内存，我们可以训练更大批量的模型和具有大量参数的神经网络。</p>


            

            
        
    






    
        <title>G2 instances</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">G2实例</h1>
                
            
            
                
<p>G2实例包含高性能NVIDIA GPUs，每个GPU具有1，536个CUDA核心和4gb GPU内存。G2有两种型号，如下表所示:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>型号</strong></p>
</td>
<td>
<p><strong>图形处理器</strong></p>
</td>
<td>
<p><strong> vCPU </strong></p>
</td>
<td>
<p><strong>内存(GB) </strong></p>
</td>
<td>
<p><strong>固态硬盘存储(GB) </strong></p>
</td>
</tr>
<tr>
<td>
<p>g 2.2x大</p>
</td>
<td>
<p>一</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>15</p>
</td>
<td>
<p>1 x 60</p>
</td>
</tr>
<tr>
<td>
<p>g 2.8 x大号</p>
</td>
<td>
<p>四</p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>60</p>
</td>
<td>
<p>2 x 120</p>
</td>
</tr>
</tbody>
</table>
<p>这些机型只有4 GB的GPU内存，所以在训练上受到限制。然而，4 GB的GPU内存通常足以为最终用户提供模型服务。最重要的因素之一是G2实例比P2实例便宜得多，这允许我们在一个负载均衡器下部署多个服务器以实现高可伸缩性。</p>


            

            
        
    






    
        <title>F1 instances</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">F1实例</h1>
                
            
            
                
<p>F1实例支持<strong>现场可编程门阵列</strong>(<strong>FPGA</strong>)。F1有两种型号，如下表所述:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>型号</strong></p>
</td>
<td>
<p><strong>图形处理器</strong></p>
</td>
<td>
<p><strong> vCPU </strong></p>
</td>
<td>
<p><strong>内存(GB) </strong></p>
</td>
<td>
<p><strong>固态硬盘存储(GB) </strong></p>
</td>
</tr>
<tr>
<td>
<p>1.2倍大</p>
</td>
<td>
<p>一</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>122</p>
</td>
<td>
<p>470</p>
</td>
</tr>
<tr>
<td>
<p>1.16倍大</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>976</p>
</td>
<td>
<p>4 x 940</p>
</td>
</tr>
</tbody>
</table>
<p>具有高内存和计算能力的FPGAs在深度学习领域非常有前景。但是TensorFlow和其他流行的深度学习库都不支持FPGAs。因此，在下一节中，我们将只讨论P2和G2实例的价格。</p>


            

            
        
    






    
        <title>Pricing</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">定价</h1>
                
            
            
                
<p>让我们在<a href="https://aws.amazon.com/emr/pricing/">https://aws.amazon.com/emr/pricing/</a>探索这些实例的定价。</p>
<p>Amazon EC2为实例提供了三种定价选项——按需实例、预约实例和现场实例:</p>
<ul>
<li>按需实例使您能够无中断地运行服务器。如果您只想在几天或几周内使用该实例，那么它是合适的。</li>
<li>保留实例使您可以选择将实例保留一年或三年，与按需实例相比有很大的折扣。如果您想运行服务器进行生产，它是合适的。</li>
<li>Spot实例为您提供了投标服务器的选项。您可以选择您愿意支付的每小时最高价格。这可以为你节省很多钱。但是，如果有人出价比你高，这些实例可以随时终止。如果您的系统可以处理中断，或者如果您只想探索服务，那么它是合适的。</li>
</ul>
<p>亚马逊提供了一个网站来计算每月的账单。你可以在http://calculator.s3.amazonaws.com/index.html看到它。</p>
<p>您可以单击“添加新行”按钮并选择实例类型。</p>
<p>在下图中，我们选择了一个p2.xlarge服务器。在撰写本文时，一个月的价格是658.80美元:</p>
<div><img class=" image-border" src="img/c034555a-aa11-4d0c-839e-89085779adfd.png"/></div>
<p>现在，单击“账单选项”栏。您将看到p2.xlarge服务器的保留实例的价格:</p>
<div><img class=" image-border" src="img/2936c039-f527-4aa6-ae23-724bfbe68789.png"/></div>
<p>还有许多其他的实例类型。我们建议您看看其他类型的服务器，选择最适合您需求的服务器。</p>
<p>在下一节中，我们将创建一个新模型，它可以使用TensorFlow执行视频动作识别。我们还将使用多个GPU来利用培训性能。</p>


            

            
        
    






    
        <title>Overview of the application</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">应用程序概述</h1>
                
            
            
                
<p>人体动作识别是计算机视觉和机器学习中一个非常有趣的问题。解决这个问题有两种流行的方法，即<strong>静止图像动作识别</strong>和<strong>视频动作识别</strong>。在静态图像动作识别中，我们可以微调来自ImageNet的预训练模型，并基于静态图像对动作进行分类。有关更多信息，您可以查看前面的章节。在这一章中，我们将创建一个可以从视频中识别人类动作的模型。在本章的最后，我们将向您展示如何使用多个GPU来加快训练过程。</p>


            

            
        
    






    
        <title>Datasets</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">数据集</h1>
                
            
            
                
<p>我们可以在培训过程中使用许多可用的数据集，如下所示:</p>
<ul>
<li>ucf 101(<a href="http://crcv.ucf.edu/data/UCF101.php">http://crcv.ucf.edu/data/UCF101.php</a>)是一个现实动作视频的动作识别数据集，有101个动作类别。101个动作类别总共有13，320个视频，这使得该数据集成为许多研究论文的绝佳选择。</li>
<li>activity net(<a href="http://activity-net.org/">http://activity-net.org/</a>)是一个用于人类活动理解的大规模数据集。有200个类别超过648小时的视频。每个类别大约有100个视频。</li>
<li>sports-1M(<a href="http://cs.stanford.edu/people/karpathy/deepvideo/">http://cs.stanford.edu/people/karpathy/deepvideo/</a>)是另一个用于体育识别的大规模数据集。总共有1133158个视频，标注了487个运动标签。</li>
</ul>
<p>在本章中，我们将使用UCF101来执行训练过程。我们还建议您尝试将本章中讨论的技术应用于大规模数据集，以充分利用多GPU训练。</p>


            

            
        
    






    
        <title>Preparing the dataset and input pipeline</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">准备数据集和输入管道</h1>
                
            
            
                
<p>UCF101数据集包含101个动作类别，如篮球投篮、弹吉他和冲浪。我们可以从http://crcv.ucf.edu/data/UCF101.php下载数据集。</p>
<p>在网站上，您需要下载名为<kbd>UCF101.rar</kbd>的文件中的UCF101数据集，以及名为<kbd>UCF101TrainTestSplits-RecognitionTask.zip</kbd>的文件中用于动作识别的训练/测试拆分。在进入下一部分之前，您需要提取数据集，在下一部分中，我们将在训练之前对视频执行预处理技术。</p>


            

            
        
    






    
        <title>Pre-processing the video for training</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">预处理用于训练的视频</h1>
                
            
            
                
<p>UCF101包含13，320个视频剪辑，帧速率和分辨率分别为25 FPS和320 x 240。所有视频剪辑都以AVI格式存储，因此在TensorFlow中使用不方便。因此，在本节中，我们将把所有视频中的视频帧提取到JPEG文件中。我们将仅以4 FPS的固定帧速率提取视频帧，以便我们可以减少网络的输入大小。</p>
<p>在我们开始实现代码之前，我们需要安装来自<a href="https://mikeboers.github.io/PyAV/installation.html">https://mikeboers.github.io/PyAV/installation.html</a>的av库。</p>
<p>首先，在<kbd>root</kbd>文件夹中创建一个名为<kbd>scripts</kbd>的Python包。然后，在<kbd>scripts/convert_ucf101.py</kbd>创建一个新的Python文件。在新创建的文件中，添加第一个代码来导入和定义一些参数，如下所示:</p>
<pre style="padding-left: 60px"> import av 
 import os 
 import random 
 import tensorflow as tf 
 from tqdm import tqdm 
 
 FLAGS = tf.app.flags.FLAGS 
 tf.app.flags.DEFINE_string( 
    'dataset_dir', '/mnt/DATA02/Dataset/UCF101', 
    'The folder that contains the extracted content of UCF101.rar' 
 ) 
 
 tf.app.flags.DEFINE_string( 
    'train_test_list_dir',   <br/> '/mnt/DATA02/Dataset/UCF101/ucfTrainTestlist', 
    'The folder that contains the extracted content of  <br/> UCF101TrainTestSplits-RecognitionTask.zip' 
 ) 
 
 tf.app.flags.DEFINE_string( 
    'target_dir', '/home/ubuntu/datasets/ucf101', 
    'The location where all the images will be stored' 
 ) 
 
 tf.app.flags.DEFINE_integer( 
    'fps', 4, 
    'Framerate to export' 
 ) 
 
 def ensure_folder_exists(folder_path): 
    if not os.path.exists(folder_path): 
        os.mkdir(folder_path) 
 
    return folder_path </pre>
<p>在前面的代码中，<kbd>dataset_dir</kbd>和<kbd>train_test_list_dir</kbd>分别是包含<kbd>UCF101.rar</kbd>和<kbd>UCF101TrainTestSplits-RecognitionTask.zip</kbd>的提取内容的文件夹的位置。<kbd>target_dir</kbd>是存储所有训练图像的文件夹。<kbd>ensure_folder_exists</kbd>是一个<kbd>utility</kbd>函数，如果文件夹不存在，它会创建一个文件夹。</p>
<p>接下来，让我们定义Python代码的<kbd>main</kbd>函数:</p>
<pre style="padding-left: 60px"> def main(_): 
    if not FLAGS.dataset_dir: 
        raise ValueError("You must supply the dataset directory with  <br/> --dataset_dir") 
 
    ensure_folder_exists(FLAGS.target_dir) 
    convert_data(["trainlist01.txt", "trainlist02.txt",  <br/> "trainlist03.txt"], training=True) 
    convert_data(["testlist01.txt", "testlist02.txt",  <br/> "testlist03.txt"], training=False) 
 
 if __name__ == "__main__": 
    tf.app.run() </pre>
<p>在<kbd>main</kbd>函数中，我们创建了<kbd>target_dir</kbd>文件夹，并调用了我们稍后将创建的<kbd>convert_data</kbd>函数。<kbd>convert_data</kbd>函数获取数据集中的训练/测试文本文件列表和一个名为training的布尔值，该布尔值指示文本文件是否用于训练过程。</p>
<p>以下是其中一个文本文件中的一些行:</p>
<pre><strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</strong>
<strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</strong>
<strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</strong></pre>
<p>文本文件的每一行包含<kbd>video</kbd>文件的路径和正确的标签。在这种情况下，我们有三个来自<kbd>ApplyEyeMakeup</kbd>类别的视频路径，这是数据集中的第一个类别。</p>
<p>这里的主要思想是，我们读取文本文件的每一行，提取JPEG格式的视频帧，并用相应的标签保存提取的文件的位置，以便进一步训练。下面是<kbd>convert_data</kbd>函数的代码:</p>
<pre style="padding-left: 60px"> def convert_data(list_files, training=False): 
    lines = [] 
    for txt in list_files: 
        lines += [line.strip() for line in  <br/> open(os.path.join(FLAGS.train_test_list_dir, txt))] 
 
    output_name = "train" if training else "test" 
 
    random.shuffle(lines) 
 
    target_dir = ensure_folder_exists(os.path.join(FLAGS.target_dir,  <br/> output_name)) 
    class_index_file = os.path.join(FLAGS.train_test_list_dir,  <br/> "classInd.txt") 
    class_index = {line.split(" ")[1].strip(): int(line.split(" ") <br/> [0]) - 1 for line in open(class_index_file)} 
 
    with open(os.path.join(FLAGS.target_dir, output_name + ".txt"),  <br/> "w") as f: 
        for line in tqdm(lines): 
            if training: 
                filename, _ = line.strip().split(" ") 
            else: 
                filename = line.strip() 
            class_folder, video_name = filename.split("/") 
 
            label = class_index[class_folder] 
            video_name = video_name.replace(".avi", "") 
            target_class_folder =  <br/> ensure_folder_exists(os.path.join(target_dir, class_folder)) 
            target_folder =  <br/> ensure_folder_exists(os.path.join(target_class_folder, video_name)) 
 
            container = av.open(os.path.join(FLAGS.dataset_dir,  <br/>            filename)) 
            frame_to_skip = int(25.0 / FLAGS.fps) 
            last_frame = -1 
            frame_index = 0 
            for frame in container.decode(video=0): 
                if last_frame &lt; 0 or frame.index &gt; last_frame +  <br/>                frame_to_skip: 
                    last_frame = frame.index 
                    image = frame.to_image() 
                    target_file = os.path.join(target_folder,  <br/>                   "%04d.jpg" % frame_index) 
                    image.save(target_file) 
                    frame_index += 1 
            f.write("{} {} {}\n".format("%s/%s" % (class_folder,  <br/>           video_name), label, frame_index)) 
 
    if training: 
        with open(os.path.join(FLAGS.target_dir, "label.txt"), "w")  <br/>        as f: 
            for class_name in sorted(class_index,  <br/>            key=class_index.get): 
                f.write("%s\n" % class_name) </pre>
<p>前面的代码很简单。我们从文本文件加载视频路径，并使用<kbd>av</kbd>库打开AVI文件。然后，我们用<kbd>FLAGS.fps</kbd>来控制每秒需要提取多少帧。您可以使用以下命令运行<kbd>scripts/convert_ucf101.py</kbd>文件:</p>
<pre><strong>python scripts/convert_ucf101.py</strong></pre>
<p>整个过程需要大约30分钟来转换所有的视频剪辑。最后，<kbd>target_dir</kbd>文件夹将包含以下文件:</p>
<pre><strong>label.txt  test  test.txt  train  train.txt</strong></pre>
<p>在<kbd>train.txt</kbd>文件中，这些行将如下所示:</p>
<pre><strong>Punch/v_Punch_g25_c03 70 43</strong>
<strong>Haircut/v_Haircut_g20_c01 33 36</strong>
<strong>BrushingTeeth/v_BrushingTeeth_g25_c02 19 33</strong>
<strong>Nunchucks/v_Nunchucks_g03_c04 55 36</strong>
<strong>BoxingSpeedBag/v_BoxingSpeedBag_g16_c04 17 21</strong></pre>
<p>这种格式可以理解为:</p>
<pre><strong>&lt;Folder location of the video&gt; &lt;Label&gt; &lt;Number of frames in the folder&gt;</strong>  </pre>
<p>有一点你必须记住，那就是<kbd>train.txt</kbd>和<kbd>test.txt</kbd>中的标签是从0到100的。然而，UCF101中的标签从1到101。这是因为TensorFlow中的<kbd>sparse_softmax_cross_entropy</kbd>函数需要类标签从0开始。</p>


            

            
        
    






    
        <title>Input pipeline with RandomShuffleQueue</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">具有RandomShuffleQueue的输入管道</h1>
                
            
            
                
<p>如果你读过<a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml">第九章</a>、<em>巡航控制-自动化</em>，你就会知道我们可以在TensorFlow中使用TextLineReader简单的逐行读取文本文件，在TensorFlow中直接使用line读取图像。然而，事情变得更加复杂，因为数据只包含文件夹位置和标签。此外，我们只想要一个文件夹中的框架子集。例如，如果帧数是30，我们只需要10帧来训练，我们将从0到20随机选择10帧。所以本章我们会用另一种机制对纯Python中的视频帧进行采样，并将选择的帧路径放入<kbd>RandomShuffleQueue</kbd>进行训练。我们还使用<kbd>tf.train.batch_join</kbd>利用多个预处理线程进行训练。</p>
<p>首先，在<kbd>root</kbd>文件夹中创建一个名为<kbd>utils.py</kbd>的新Python文件，并添加以下代码:</p>
<pre>def lines_from_file(filename, repeat=False): 
    with open(filename) as handle: 
        while True: 
            try: 
                line = next(handle) 
                yield line.strip() 
            except StopIteration as e: 
                if repeat: 
                    handle.seek(0) 
                else: 
                    raise 
 
if __name__ == "__main__": 
    data_reader = lines_from_file("/home/ubuntu/datasets/ucf101/train.txt", repeat=True) 
 
    for i in range(15): 
        print(next(data_reader)) </pre>
<p>在这段代码中，我们创建了一个名为<kbd>lines_from_file</kbd>的<kbd>generator</kbd>函数来逐行读取文本文件。我们还添加了一个<kbd>repeat</kbd>参数，以便<kbd>generator</kbd>函数可以在到达文件末尾时从头开始读取文本。</p>
<p>我们添加了一个主要部分，因此您可以尝试运行它来查看<kbd>generator</kbd>是如何工作的:</p>
<pre><strong>python utils.py</strong> </pre>
<p>现在，在<kbd>root</kbd>文件夹中创建一个名为<kbd>datasets.py</kbd>的新Python文件，并添加以下代码:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import cv2 
 import os 
 import random 
 
 from tensorflow.python.ops import data_flow_ops 
 from utils import lines_from_file 
 
 def sample_videos(data_reader, root_folder, num_samples,  <br/> num_frames): 
    image_paths = list() 
    labels = list() 
    while True: 
        if len(labels) &gt;= num_samples: 
            break 
        line = next(data_reader) 
        video_folder, label, max_frames = line.strip().split(" ") 
        max_frames = int(max_frames) 
        label = int(label) 
        if max_frames &gt; num_frames: 
            start_index = random.randint(0, max_frames - num_frames) 
            frame_paths = list() 
            for index in range(start_index, start_index +  <br/> num_frames): 
                frame_path = os.path.join(root_folder, video_folder,  <br/> "%04d.jpg" % index) 
                frame_paths.append(frame_path) 
            image_paths.append(frame_paths) 
            labels.append(label) 
    return image_paths, labels 
 
 if __name__ == "__main__": 
    num_frames = 5 
    root_folder = "/home/ubuntu/datasets/ucf101/train/" 
    data_reader =  <br/> lines_from_file("/home/ubuntu/datasets/ucf101/train.txt",  <br/> repeat=True) 
 image_paths, labels = sample_videos(data_reader,  <br/> root_folder=root_folder, 
 num_samples=3,  <br/> num_frames=num_frames) 
    print("image_paths", image_paths) 
    print("labels", labels) </pre>
<p><kbd>sample_videos</kbd>功能很好理解。它将从<kbd>lines_from_file</kbd>函数接收<kbd>generator</kbd>对象，并使用<kbd>next</kbd>函数获取所需的样本。你可以看到我们使用了一个<kbd>random.randint</kbd>方法来随机化起始帧位置。</p>
<p>您可以使用以下命令运行主要部分来查看<kbd>sample_videos</kbd>如何工作:</p>
<pre><strong>python datasets.py</strong></pre>
<p>至此，我们已经将数据集文本文件读入了<kbd>image_paths</kbd>和<kbd>labels</kbd>变量，它们是Python列表。在后面的训练例程中，我们将使用TensorFlow中的内置<kbd>RandomShuffleQueue</kbd>将<kbd>image_paths</kbd>和<kbd>labels</kbd>排入该队列。</p>
<p>现在，我们需要创建一个将在训练例程中使用的方法，以从<kbd>RandomShuffleQueue</kbd>获取数据，在多个线程中执行预处理，并将数据发送到<kbd>batch_join</kbd>函数，以创建用于训练的小批量。</p>
<p>在<kbd>dataset.py</kbd>文件中，添加以下代码:</p>
<pre style="padding-left: 60px"> def input_pipeline(input_queue, batch_size=32, num_threads=8,  <br/> image_size=112): 
    frames_and_labels = [] 
    for _ in range(num_threads): 
        frame_paths, label = input_queue.dequeue() 
        frames = [] 
        for filename in tf.unstack(frame_paths): 
            file_contents = tf.read_file(filename) 
            image = tf.image.decode_jpeg(file_contents) 
            image = _aspect_preserving_resize(image, image_size) 
            image = tf.image.resize_image_with_crop_or_pad(image,  <br/>            image_size, image_size) 
            image = tf.image.per_image_standardization(image) 
            image.set_shape((image_size, image_size, 3)) 
            frames.append(image) 
        frames_and_labels.append([frames, label]) 
 
    frames_batch, labels_batch = tf.train.batch_join( 
        frames_and_labels, batch_size=batch_size, 
        capacity=4 * num_threads * batch_size, 
    ) 
    return frames_batch, labels_batch </pre>
<p>在这段代码中，我们准备了一个名为<kbd>frames_and_labels</kbd>的数组，并使用一个带有<kbd>num_threads</kbd>迭代的for循环。这是向预处理过程添加多线程支持的一种非常方便的方式。在每个线程中，我们将从<kbd>input_queue</kbd>调用方法<kbd>dequeue</kbd>来获得一个<kbd>frame_paths</kbd>和<kbd>label</kbd>。从上一节的<kbd>sample_video</kbd>函数中，我们知道<kbd>frame_paths</kbd>是所选视频帧的列表。因此，我们使用另一个for循环来遍历每一帧。在每一帧中，我们读取、调整大小并执行图像标准化。这部分类似于<a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank">第9章</a>、<em>巡航控制-自动化</em>中的代码。在输入管道的末尾，我们添加了带有<kbd>batch_size</kbd>参数的<kbd>frames_and_labels</kbd>。返回的<kbd>frames_batch</kbd>和<kbd>labels_batch</kbd>将用于后面的训练程序。</p>
<p>最后，您应该添加下面的代码，它包含了<kbd>_aspect_preserving_resize</kbd>函数:</p>
<pre style="padding-left: 60px"> def _smallest_size_at_least(height, width, smallest_side): 
    smallest_side = tf.convert_to_tensor(smallest_side,  <br/> dtype=tf.int32) 
 
    height = tf.to_float(height) 
    width = tf.to_float(width) 
    smallest_side = tf.to_float(smallest_side) 
 
    scale = tf.cond(tf.greater(height, width), 
                    lambda: smallest_side / width, 
                    lambda: smallest_side / height) 
    new_height = tf.to_int32(height * scale) 
    new_width = tf.to_int32(width * scale) 
    return new_height, new_width 
 
 
 def _aspect_preserving_resize(image, smallest_side): 
    smallest_side = tf.convert_to_tensor(smallest_side,  <br/> dtype=tf.int32) 
    shape = tf.shape(image) 
    height = shape[0] 
    width = shape[1] 
    new_height, new_width = _smallest_size_at_least(height, width,  <br/> smallest_side) 
    image = tf.expand_dims(image, 0) 
    resized_image = tf.image.resize_bilinear(image, [new_height,  <br/> new_width], align_corners=False) 
    resized_image = tf.squeeze(resized_image) 
    resized_image.set_shape([None, None, 3]) 
    return resized_image </pre>
<p>该代码与您在<a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank">第9章</a>、<em>巡航控制-自动化</em>中使用的代码相同。</p>
<p>在下一节中，我们将创建深度神经网络架构，我们将使用它来执行101个类别的视频动作识别。</p>


            

            
        
    






    
        <title>Neural network architecture</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">神经网络体系结构</h1>
                
            
            
                
<p>在这一章中，我们将创建一个神经网络，它将接受10个视频帧的输入，并输出101个动作类别的概率。我们将基于TensorFlow中的conv3d操作创建一个神经网络。这个网络的灵感来自于D. Tran等人的工作，用3D卷积网络学习时空特征。然而，我们已经简化了模型，以便在一章中更容易解释。我们还使用了一些Tran等人没有提到的技术，比如批量归一化和丢弃。</p>
<p>现在，创建一个名为<kbd>nets.py</kbd>的新Python文件，并添加以下代码:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 from utils import print_variables, print_layers 
 from tensorflow.contrib.layers.python.layers.layers import  <br/> batch_norm 
 def inference(input_data, is_training=False): 
    conv1 = _conv3d(input_data, 3, 3, 3, 64, 1, 1, 1, "conv1") 
    pool1 = _max_pool3d(conv1, 1, 2, 2, 1, 2, 2, "pool1") 
 
    conv2 = _conv3d(pool1, 3, 3, 3, 128, 1, 1, 1, "conv2") 
    pool2 = _max_pool3d(conv2, 2, 2, 2, 2, 2, 2, "pool2") 
     
    conv3a = _conv3d(pool2, 3, 3, 3, 256, 1, 1, 1, "conv3a") 
    conv3b = _conv3d(conv3a, 3, 3, 3, 256, 1, 1, 1, "conv3b") 
    pool3 = _max_pool3d(conv3b, 2, 2, 2, 2, 2, 2, "pool3") 
     
    conv4a = _conv3d(pool3, 3, 3, 3, 512, 1, 1, 1, "conv4a") 
    conv4b = _conv3d(conv4a, 3, 3, 3, 512, 1, 1, 1, "conv4b") 
    pool4 = _max_pool3d(conv4b, 2, 2, 2, 2, 2, 2, "pool4") 
     
    conv5a = _conv3d(pool4, 3, 3, 3, 512, 1, 1, 1, "conv5a") 
    conv5b = _conv3d(conv5a, 3, 3, 3, 512, 1, 1, 1, "conv5b") 
    pool5 = _max_pool3d(conv5b, 2, 2, 2, 2, 2, 2, "pool5") 
 
    fc6 = _fully_connected(pool5, 4096, name="fc6") 
    fc7 = _fully_connected(fc6, 4096, name="fc7") 
    if is_training: 
        fc7 = tf.nn.dropout(fc7, keep_prob=0.5) 
    fc8 = _fully_connected(fc7, 101, name='fc8', relu=False) 
     
    endpoints = dict() 
    endpoints["conv1"] = conv1 
    endpoints["pool1"] = pool1 
    endpoints["conv2"] = conv2 
    endpoints["pool2"] = pool2 
    endpoints["conv3a"] = conv3a 
    endpoints["conv3b"] = conv3b 
    endpoints["pool3"] = pool3 
    endpoints["conv4a"] = conv4a 
    endpoints["conv4b"] = conv4b 
    endpoints["pool4"] = pool4 
    endpoints["conv5a"] = conv5a 
    endpoints["conv5b"] = conv5b 
    endpoints["pool5"] = pool5 
    endpoints["fc6"] = fc6 
    endpoints["fc7"] = fc7 
    endpoints["fc8"] = fc8 
         
    return fc8, endpoints 
 
 if __name__ == "__main__": 
    inputs = tf.placeholder(tf.float32, [None, 10, 112, 112, 3],  <br/> name="inputs") 
    outputs, endpoints = inference(inputs) 
 
    print_variables(tf.global_variables()) 
    print_variables([inputs, outputs]) 
    print_layers(endpoints) </pre>
<p>在<kbd>inference</kbd>功能中，我们用<kbd>call _conv3d</kbd>、<kbd>_max_pool3d</kbd>和<kbd>_fully_connected</kbd>来创建网络。对于前几章中的图像，它与CNN网络没有太大的不同。在函数的最后，我们还创建了一个名为<kbd>endpoints</kbd>的字典，它将在主节中用来可视化网络架构。</p>
<p>接下来，让我们添加<kbd>_conv3d</kbd>和<kbd>_max_pool3d</kbd>函数的代码:</p>
<pre style="padding-left: 60px"> def _conv3d(input_data, k_d, k_h, k_w, c_o, s_d, s_h, s_w, name,  <br/> relu=True, padding="SAME"): 
    c_i = input_data.get_shape()[-1].value 
    convolve = lambda i, k: tf.nn.conv3d(i, k, [1, s_d, s_h, s_w,  <br/> 1], padding=padding) 
    with tf.variable_scope(name) as scope: 
        weights = tf.get_variable(name="weights",  
 shape=[k_d, k_h, k_w, c_i, c_o], 
 regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001), 
                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        conv = convolve(input_data, weights) 
        biases = tf.get_variable(name="biases",  
 shape=[c_o], dtype=tf.float32, 
 initializer = tf.constant_initializer(value=0.0)) 
        output = tf.nn.bias_add(conv, biases) 
        if relu: 
            output = tf.nn.relu(output, name=scope.name) 
        return batch_norm(output) 
 
 
 def _max_pool3d(input_data, k_d, k_h, k_w, s_d, s_h, s_w, name,  <br/> padding="SAME"): 
    return tf.nn.max_pool3d(input_data,  
 ksize=[1, k_d, k_h, k_w, 1], 
 strides=[1, s_d, s_h, s_w, 1], padding=padding, name=name) </pre>
<p>这段代码类似于前面的章节。但是，对于图像，我们使用内置的<kbd>tf.nn.conv3d</kbd>和<kbd>tf.nn.max_pool3d</kbd>函数，而不是<kbd>tf.nn.conv2d</kbd>和<kbd>tf.nn.max_pool3d</kbd>。因此，我们需要添加<kbd>k_d</kbd>和<kbd>s_d</kbd>参数来给出关于过滤器深度的信息。此外，我们将需要从零开始训练这个网络，没有任何预先训练的模型。因此，我们需要使用<kbd>batch_norm</kbd>函数为每一层添加批量标准化。</p>
<p>让我们添加完全连接层的代码:</p>
<pre style="padding-left: 60px"> def _fully_connected(input_data, num_output, name, relu=True): 
    with tf.variable_scope(name) as scope: 
        input_shape = input_data.get_shape() 
        if input_shape.ndims == 5: 
            dim = 1 
            for d in input_shape[1:].as_list(): 
                dim *= d 
            feed_in = tf.reshape(input_data, [-1, dim]) 
        else: 
            feed_in, dim = (input_data, input_shape[-1].value) 
        weights = tf.get_variable(name="weights",  
 shape=[dim, num_output],  
 regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001),                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        biases = tf.get_variable(name="biases", 
 shape=[num_output], dtype=tf.float32, 
                                  <br/> initializer=tf.constant_initializer(value=0.0)) 
        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b 
        output = op(feed_in, weights, biases, name=scope.name) 
        return batch_norm(output) </pre>
<p>这个函数和我们在图像中使用的有点不同。首先，我们检查<kbd>input_shape.ndims</kbd>是否等于5而不是4。其次，我们将批量标准化添加到输出中。</p>
<p>最后，让我们打开<kbd>utils.py</kbd>文件并添加以下<kbd>utility</kbd>函数:</p>
<pre style="padding-left: 60px"> from prettytable import PrettyTable 
 def print_variables(variables): 
    table = PrettyTable(["Variable Name", "Shape"]) 
    for var in variables: 
        table.add_row([var.name, var.get_shape()]) 
    print(table) 
    print("") 
 
 
 def print_layers(layers): 
    table = PrettyTable(["Layer Name", "Shape"]) 
    for var in layers.values(): 
        table.add_row([var.name, var.get_shape()]) 
    print(table) 
    print("") </pre>
<p>现在我们可以运行<kbd>nets.py</kbd>来更好地理解网络架构:</p>
<pre>    <strong>python nets.py</strong></pre>
<p>在控制台结果的第一部分，您将看到如下表格:</p>
<pre>    <strong>+------------------------------------+---------------------+</strong>
    <strong>|           Variable Name            |        Shape        |</strong>
    <strong>+------------------------------------+---------------------+</strong>
    <strong>|          conv1/weights:0           |   (3, 3, 3, 3, 64)  |</strong>
    <strong>|           conv1/biases:0           |        (64,)        |</strong>
    <strong>|       conv1/BatchNorm/beta:0       |        (64,)        |</strong>
    <strong>|   conv1/BatchNorm/moving_mean:0    |        (64,)        |</strong>
    <strong>| conv1/BatchNorm/moving_variance:0  |        (64,)        |</strong>
    <strong>|               ...                  |         ...         |</strong>
    <strong>|           fc8/weights:0            |     (4096, 101)     |</strong>
    <strong>|            fc8/biases:0            |        (101,)       |</strong>
    <strong>|        fc8/BatchNorm/beta:0        |        (101,)       |</strong>
    <strong>|    fc8/BatchNorm/moving_mean:0     |        (101,)       |</strong>
    <strong>|  fc8/BatchNorm/moving_variance:0   |        (101,)       |</strong>
    <strong>+------------------------------------+---------------------+</strong> </pre>
<p>这些都是网络中<kbd>variables</kbd>的形状。正如你所看到的，三个带有文本<kbd>BatchNorm</kbd>的<kbd>variables</kbd>被添加到每一层。这些<kbd>variables</kbd>增加了网络需要学习的总参数。然而，由于我们将从零开始训练，所以在没有批量标准化的情况下训练网络将更加困难。批量规范化还提高了网络对不可见数据进行规范化的能力。</p>
<p>在控制台的第二个表中，您将看到下表:</p>
<pre>    <strong>+---------------------------------+----------------------+</strong>
    <strong>|          Variable Name          |        Shape         |</strong>
    <strong>+---------------------------------+----------------------+</strong>
    <strong>|             inputs:0            | (?, 10, 112, 112, 3) |</strong>
    <strong>| fc8/BatchNorm/batchnorm/add_1:0 |       (?, 101)       |</strong>
    <strong>+---------------------------------+----------------------+</strong></pre>
<p>这些是网络输入和输出的形状。如您所见，输入包含10个大小为(112，112，3)的视频帧，输出包含101个元素的向量。</p>
<p>在最后一个表中，您将看到各图层的输出形状如何通过网络发生变化:</p>
<pre>    <strong>+------------------------------------+-----------------------+</strong>
    <strong>|             Layer Name             |         Shape         |</strong>
    <strong>+------------------------------------+-----------------------+</strong>
    <strong>|  fc6/BatchNorm/batchnorm/add_1:0   |       (?, 4096)       |</strong>
    <strong>|  fc7/BatchNorm/batchnorm/add_1:0   |       (?, 4096)       |</strong>
    <strong>|  fc8/BatchNorm/batchnorm/add_1:0   |        (?, 101)       |</strong>
    <strong>|               ...                  |         ...           |</strong>
    <strong>| conv1/BatchNorm/batchnorm/add_1:0  | (?, 10, 112, 112, 64) |</strong>
    <strong>| conv2/BatchNorm/batchnorm/add_1:0  |  (?, 10, 56, 56, 128) |</strong>
    <strong>+------------------------------------+-----------------------+</strong></pre>
<p>在上表中，我们可以看到<kbd>conv1</kbd>层的输出与输入具有相同的大小，而<kbd>conv2</kbd>层的输出由于max pooling的影响而发生了变化。</p>
<p>现在，让我们创建一个名为<kbd>models.py</kbd>的新Python文件，并添加以下代码:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 
 def compute_loss(logits, labels): 
    labels = tf.squeeze(tf.cast(labels, tf.int32)) 
 
    cross_entropy =  <br/> tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,  <br/> labels=labels) 
    cross_entropy_loss= tf.reduce_mean(cross_entropy) 
    reg_loss =  <br/> tf.reduce_mean(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES <br/> )) 
 
    return cross_entropy_loss + reg_loss, cross_entropy_loss,  <br/> reg_loss 
 
 
 def compute_accuracy(logits, labels): 
    labels = tf.squeeze(tf.cast(labels, tf.int32)) 
    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32) 
    predicted_correctly = tf.equal(batch_predictions, labels) 
    accuracy = tf.reduce_mean(tf.cast(predicted_correctly,  <br/>    tf.float32)) 
    return accuracy 
 
 
 def get_learning_rate(global_step, initial_value, decay_steps,  <br/> decay_rate): 
    learning_rate = tf.train.exponential_decay(initial_value,  <br/>    global_step, decay_steps, decay_rate, staircase=True) 
    return learning_rate 
 
 
 def train(total_loss, learning_rate, global_step): 
    optimizer = tf.train.AdamOptimizer(learning_rate) 
    train_op = optimizer.minimize(total_loss, global_step) 
    return train_op </pre>
<p>这些函数创建计算<kbd>loss</kbd>、<kbd>accuracy</kbd>、<kbd>learning rate</kbd>的操作，并执行训练过程。这个和上一章一样，就不解释这些功能了。</p>
<p>现在，我们拥有了训练网络识别视频动作所需的所有功能。在下一节中，我们将在单个GPU上开始训练例程，并在TensorBoard上可视化结果。</p>


            

            
        
    






    
        <title>Training routine with single GPU</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">使用单个GPU的训练例程</h1>
                
            
            
                
<p>在脚本包中，创建一个名为<kbd>train.py</kbd>的新Python文件。我们将首先定义一些参数，如下所示:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import os 
 import sys 
 from datetime import datetime 
 from tensorflow.python.ops import data_flow_ops 
 
 import nets 
 import models 
 from utils import lines_from_file 
 from datasets import sample_videos, input_pipeline 
 
 # Dataset 
 num_frames = 16 
 train_folder = "/home/ubuntu/datasets/ucf101/train/" 
 train_txt = "/home/ubuntu/datasets/ucf101/train.txt" 
 
 # Learning rate 
 initial_learning_rate = 0.001 
 decay_steps = 1000 
 decay_rate = 0.7 
  
 # Training 
 image_size = 112 
 batch_size = 24 
 num_epochs = 20 
 epoch_size = 28747 
 
 train_enqueue_steps = 100 
 min_queue_size = 1000 
 
 save_steps = 200  # Number of steps to perform saving checkpoints 
 test_steps = 20  # Number of times to test for test accuracy 
 start_test_step = 50 
 
 max_checkpoints_to_keep = 2 
 save_dir = "/home/ubuntu/checkpoints/ucf101" </pre>
<p>这些参数是不言自明的。现在，我们将为培训定义一些操作:</p>
<pre style="padding-left: 60px"> train_data_reader = lines_from_file(train_txt, repeat=True) 
 
 image_paths_placeholder = tf.placeholder(tf.string, shape=(None,  <br/> num_frames), name='image_paths') 
 labels_placeholder = tf.placeholder(tf.int64, shape=(None,),  <br/> name='labels') 
 
 train_input_queue =  <br/> data_flow_ops.RandomShuffleQueue(capacity=10000, 
                                                      <br/> min_after_dequeue=batch_size, 
 dtypes= [tf.string, tf.int64], 
 shapes= [(num_frames,), ()]) 
 
 train_enqueue_op =  <br/> train_input_queue.enqueue_many([image_paths_placeholder,  <br/> labels_placeholder]) 
 
 frames_batch, labels_batch = input_pipeline(train_input_queue,   <br/> batch_size=batch_size, image_size=image_size) 
 
 with tf.variable_scope("models") as scope: 
    logits, _ = nets.inference(frames_batch, is_training=True) 
 
 total_loss, cross_entropy_loss, reg_loss =  <br/> models.compute_loss(logits, labels_batch) 
 train_accuracy = models.compute_accuracy(logits, labels_batch) 
 
 global_step = tf.Variable(0, trainable=False) 
 learning_rate = models.get_learning_rate(global_step,  <br/> initial_learning_rate, decay_steps, decay_rate) 
 train_op = models.train(total_loss, learning_rate, global_step) </pre>
<p>在这段代码中，我们从文本文件中获得一个<kbd>generator</kbd>对象。然后，我们为<kbd>image_paths</kbd>和<kbd>labels</kbd>创建两个占位符，它们将被排队到<kbd>RandomShuffleQueue</kbd>。我们在<kbd>datasets.py</kbd>中创建的<kbd>input_pipeline</kbd>函数将接收<kbd>RandomShuffleQueue</kbd>并返回一批<kbd>frames</kbd>和标签。最后，我们创建运算来计算损失、准确性和训练运算。</p>
<p>我们还想记录训练过程，并在TensorBoard中可视化。因此，我们将创建一些摘要:</p>
<pre style="padding-left: 60px"> tf.summary.scalar("learning_rate", learning_rate) 
 tf.summary.scalar("train/accuracy", train_accuracy) 
 tf.summary.scalar("train/total_loss", total_loss) 
 tf.summary.scalar("train/cross_entropy_loss", cross_entropy_loss) 
 tf.summary.scalar("train/regularization_loss", reg_loss) 
 
 summary_op = tf.summary.merge_all() 
 
 saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
 time_stamp = datetime.now().strftime("single_%Y-%m-%d_%H-%M-%S") 
 checkpoints_dir = os.path.join(save_dir, time_stamp) 
 summary_dir = os.path.join(checkpoints_dir, "summaries") 
  
 train_writer = tf.summary.FileWriter(summary_dir, flush_secs=10) 
 
 if not os.path.exists(save_dir): 
    os.mkdir(save_dir) 
 if not os.path.exists(checkpoints_dir): 
    os.mkdir(checkpoints_dir) 
 if not os.path.exists(summary_dir): 
    os.mkdir(summary_dir) </pre>
<p><kbd>saver</kbd>和<kbd>train_writer</kbd>分别负责保存检查点和总结。现在，让我们通过创建<kbd>session</kbd>并执行训练循环来完成训练过程:</p>
<pre style="padding-left: 60px"> config = tf.ConfigProto() 
 config.gpu_options.allow_growth = True 
 
 with tf.Session(config=config) as sess: 
    coords = tf.train.Coordinator() 
    threads = tf.train.start_queue_runners(sess=sess, coord=coords) 
 
    sess.run(tf.global_variables_initializer()) 
 
    num_batches = int(epoch_size / batch_size) 
 
    for i_epoch in range(num_epochs): 
        for i_batch in range(num_batches): 
            # Prefetch some data into queue 
            if i_batch % train_enqueue_steps == 0: 
                num_samples = batch_size * (train_enqueue_steps + 1) 
 
                image_paths, labels =  <br/> sample_videos(train_data_reader, root_folder=train_folder, 
                                                     <br/> num_samples=num_samples, num_frames=num_frames) 
                print("\nEpoch {} Batch {} Enqueue {}  <br/> videos".format(i_epoch, i_batch, num_samples)) 
 
                sess.run(train_enqueue_op, feed_dict={ 
                    image_paths_placeholder: image_paths, 
                    labels_placeholder: labels 
                }) 
 
            if (i_batch + 1) &gt;= start_test_step and (i_batch + 1) %  <br/> test_steps == 0: 
                _, lr_val, loss_val, ce_loss_val, reg_loss_val,  <br/> summary_val, global_step_val, train_acc_val = sess.run([ 
                    train_op, learning_rate, total_loss,  <br/> cross_entropy_loss, reg_loss, 
                    summary_op, global_step, train_accuracy 
                ]) 
                train_writer.add_summary(summary_val, <br/> global_step=global_step_val) 
  
                print("\nEpochs {}, Batch {} Step {}: Learning Rate  <br/> {} Loss {} CE Loss {} Reg Loss {} Train Accuracy {}".format( 
                    i_epoch, i_batch, global_step_val, lr_val,  <br/> loss_val, ce_loss_val, reg_loss_val, train_acc_val 
                )) 
            else: 
                _ = sess.run(train_op) 
                sys.stdout.write(".") 
                sys.stdout.flush() 
 
          if (i_batch + 1) &gt; 0 and (i_batch + 1) % save_steps ==  0: 
                saved_file = saver.save(sess, 
                                         <br/> os.path.join(checkpoints_dir, 'model.ckpt'), 
                                        global_step=global_step) 
                print("Save steps: Save to file %s " % saved_file) 
 
    coords.request_stop() 
    coords.join(threads) </pre>
<p>这段代码非常简单。我们将使用<kbd>sample_videos</kbd>函数来获取图像路径和标签的列表。然后，我们将调用<kbd>train_enqueue_op</kbd>操作将这些图像路径和标签添加到<kbd>RandomShuffleQueue</kbd>。之后，可以使用<kbd>train_op</kbd>运行训练过程，而不用<kbd>feed_dict</kbd>机构。</p>
<p>现在，我们可以通过调用<kbd>root</kbd>文件夹中的以下命令来运行训练过程:</p>
<pre><strong>export PYTHONPATH=.</strong>
<strong>python scripts/train.py</strong></pre>
<p>如果您的GPU内存对于32的批处理大小来说不够大，您可能会看到<kbd>OUT_OF_MEMORY</kbd>错误。在培训过程中，我们用<kbd>gpu_options.allow_growth</kbd>创建了一个会话，因此您可以尝试更改<kbd>batch_size</kbd>来有效地使用您的GPU内存。</p>
<p>训练过程需要几个小时才能收敛。我们将看看TensorBoard上的训练过程。</p>
<p>在您选择保存检查点的目录中，运行以下命令:</p>
<pre><strong>tensorboard --logdir .</strong></pre>
<p>现在，打开您的网络浏览器并导航至<kbd>http://localhost:6006</kbd>:</p>
<div><img class=" image-border" src="img/8bef41ea-1d59-44ee-9e19-ff375d4e6768.png"/></div>
<p class="packt_figure">一个GPU的正则化损失和总损失如下:</p>
<div><img class=" image-border" src="img/2a9a5bdf-2a3d-4a94-b80f-5baf06586d7f.png"/></div>
<p>正如您在这些图像中所看到的，训练准确性需要大约10，000步才能达到训练数据的100%准确性。这10，000步在我们的机器上花了6个小时。这可能与您的配置不同。</p>
<p>训练损耗在减少，训练时间长了可能会减少。但训练精度在一万步后几乎不变。</p>
<p>现在，让我们进入本章最有趣的部分。我们将使用多个GPU进行训练，看看这有什么帮助。</p>


            

            
        
    






    
        <title>Training routine with multiple GPU</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">多个GPU的训练例程</h1>
                
            
            
                
<p>在我们的实验中，我们将使用我们的定制机器，而不是亚马逊EC2。但是，您可以在任何带有GPU的服务器上实现相同的结果。在本节中，我们将使用两个Titan X GPUs，每个GPU上的批处理大小为32。这样，我们可以一步计算多达64个视频，而不是在单个GPU配置中计算32个视频。</p>
<p>现在，让我们在<kbd>scripts</kbd>包中创建一个名为<kbd>train_multi.py</kbd>的新Python文件。在该文件中，添加以下代码来定义一些参数:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import os 
 import sys 
 from datetime import datetime 
 from tensorflow.python.ops import data_flow_ops 
 
 import nets 
 import models 
 from utils import lines_from_file 
 from datasets import sample_videos, input_pipeline 
 
 # Dataset 
 num_frames = 10 
 train_folder = "/home/aiteam/quan/datasets/ucf101/train/" 
 train_txt = "/home/aiteam/quan/datasets/ucf101/train.txt" 
 
 # Learning rate 
 initial_learning_rate = 0.001 
 decay_steps = 1000 
 decay_rate = 0.7 
 
 # Training 
 num_gpu = 2 
 
 image_size = 112 
 batch_size = 32 * num_gpu 
 num_epochs = 20 
 epoch_size = 28747 
 
 train_enqueue_steps = 50 
 
 save_steps = 200  # Number of steps to perform saving checkpoints 
 test_steps = 20  # Number of times to test for test accuracy 
 start_test_step = 50 
 
 max_checkpoints_to_keep = 2 
 save_dir = "/home/aiteam/quan/checkpoints/ucf101" </pre>
<p>除了<kbd>batch_size</kbd>之外，这些参数与之前的<kbd>train.py</kbd>文件中的参数相同。在这个实验中，我们将使用数据并行策略来训练多个GPU。因此，我们将使用批次大小64，而不是批次大小32。然后，我们将把这一批分成两部分；每个都将由GPU处理。之后，我们将结合来自两个GPU的梯度来更新网络的权重和偏差。</p>
<p>接下来，我们将使用与之前相同的操作，如下所示:</p>
<pre style="padding-left: 60px"> train_data_reader = lines_from_file(train_txt, repeat=True) 
 
 image_paths_placeholder = tf.placeholder(tf.string, shape=(None,  <br/> num_frames), name='image_paths') 
 labels_placeholder = tf.placeholder(tf.int64, shape=(None,),  <br/> name='labels') 
 
 train_input_queue =  <br/> data_flow_ops.RandomShuffleQueue(capacity=10000, 
                                                      <br/> min_after_dequeue=batch_size, 
 dtypes= [tf.string, tf.int64], 
 shapes= [(num_frames,), ()]) 
 
 train_enqueue_op =  <br/> train_input_queue.enqueue_many([image_paths_placeholder,  <br/> labels_placeholder]) 
 
 frames_batch, labels_batch = input_pipeline(train_input_queue,  <br/> batch_size=batch_size, image_size=image_size) 
 
 global_step = tf.Variable(0, trainable=False) 
 learning_rate = models.get_learning_rate(global_step,  <br/> initial_learning_rate, decay_steps, decay_rate) 
 ``` 
 Now, instead of creating a training operation with `models.train`,  <br/> we will create a optimizer and compute gradients in each GPU. 
 ``` 
 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
 
 total_gradients = [] 
 
 frames_batch_split = tf.split(frames_batch, num_gpu) 
 labels_batch_split = tf.split(labels_batch, num_gpu) 
 for i in range(num_gpu): 
    with tf.device('/gpu:%d' % i): 
        with tf.variable_scope(tf.get_variable_scope(), reuse=(i &gt;  <br/> 0)): 
            logits_split, _ = nets.inference(frames_batch_split[i],  <br/> is_training=True) 
            labels_split = labels_batch_split[i] 
            total_loss, cross_entropy_loss, reg_loss =  <br/> models.compute_loss(logits_split, labels_split) 
            grads = optimizer.compute_gradients(total_loss) 
            total_gradients.append(grads) 
            tf.get_variable_scope().reuse_variables() 
 
 with tf.device('/cpu:0'): 
    gradients = models.average_gradients(total_gradients) 
    train_op = optimizer.apply_gradients(gradients, global_step) 
 
    train_accuracy = models.compute_accuracy(logits_split,   <br/> labels_split) </pre>
<p>梯度将在每个GPU上计算，并添加到名为<kbd>total_gradients</kbd>的列表中。最终的梯度将使用<kbd>average_gradients</kbd>在CPU上计算，我们将很快创建。然后，将通过在优化器上调用<kbd>apply_gradients</kbd>来创建训练操作。</p>
<p>现在，让我们将以下函数添加到<kbd>root</kbd>文件夹中的<kbd>models.py</kbd>文件中，以计算<kbd>average_gradient</kbd>:</p>
<pre style="padding-left: 60px"> def average_gradients(gradients): 
    average_grads = [] 
    for grad_and_vars in zip(*gradients): 
        grads = [] 
        for g, _ in grad_and_vars: 
            grads.append(tf.expand_dims(g, 0)) 
 
        grad = tf.concat(grads, 0) 
        grad = tf.reduce_mean(grad, 0) 
 
        v = grad_and_vars[0][1] 
        grad_and_var = (grad, v) 
        average_grads.append(grad_and_var) 
    return average_grads </pre>
<p>现在，回到<kbd>train_multi.py</kbd>文件，我们将创建<kbd>saver</kbd>和<kbd>summaries</kbd>操作来保存<kbd>checkpoints</kbd>和<kbd>summaries</kbd>，就像之前一样:</p>
<pre style="padding-left: 60px"> tf.summary.scalar("learning_rate", learning_rate) 
 tf.summary.scalar("train/accuracy", train_accuracy) 
 tf.summary.scalar("train/total_loss", total_loss) 
 tf.summary.scalar("train/cross_entropy_loss", cross_entropy_loss) 
 tf.summary.scalar("train/regularization_loss", reg_loss) 
  
 summary_op = tf.summary.merge_all() 
 
 saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
 time_stamp = datetime.now().strftime("multi_%Y-%m-%d_%H-%M-%S") 
 checkpoints_dir = os.path.join(save_dir, time_stamp) 
 summary_dir = os.path.join(checkpoints_dir, "summaries") 
 
 train_writer = tf.summary.FileWriter(summary_dir, flush_secs=10) 
 
 if not os.path.exists(save_dir): 
    os.mkdir(save_dir) 
 if not os.path.exists(checkpoints_dir): 
    os.mkdir(checkpoints_dir) 
 if not os.path.exists(summary_dir): 
    os.mkdir(summary_dir) </pre>
<p>最后，让我们添加训练循环来训练网络:</p>
<pre style="padding-left: 60px"> config = tf.ConfigProto(allow_soft_placement=True) 
 config.gpu_options.allow_growth = True 
 
 sess = tf.Session(config=config) 
 coords = tf.train.Coordinator() 
 threads = tf.train.start_queue_runners(sess=sess, coord=coords) 
  
 sess.run(tf.global_variables_initializer()) 
 
 num_batches = int(epoch_size / batch_size) 
 
 for i_epoch in range(num_epochs): 
    for i_batch in range(num_batches): 
        # Prefetch some data into queue 
        if i_batch % train_enqueue_steps == 0: 
            num_samples = batch_size * (train_enqueue_steps + 1) 
            image_paths, labels = sample_videos(train_data_reader,  <br/> root_folder=train_folder, 
                                                 <br/> num_samples=num_samples, num_frames=num_frames) 
            print("\nEpoch {} Batch {} Enqueue {} <br/> videos".format(i_epoch, i_batch, num_samples)) 
 
            sess.run(train_enqueue_op, feed_dict={ 
                image_paths_placeholder: image_paths, 
                labels_placeholder: labels 
            }) 
 
        if (i_batch + 1) &gt;= start_test_step and (i_batch + 1) %  <br/> test_steps == 0: 
            _, lr_val, loss_val, ce_loss_val, reg_loss_val, <br/> summary_val, global_step_val, train_acc_val = sess.run([ 
                train_op, learning_rate, total_loss, <br/> cross_entropy_loss, reg_loss, 
                summary_op, global_step, train_accuracy 
            ]) 
            train_writer.add_summary(summary_val,  <br/> global_step=global_step_val) 
 
            print("\nEpochs {}, Batch {} Step {}: Learning Rate {} <br/> Loss {} CE Loss {} Reg Loss {} Train Accuracy {}".format( 
                i_epoch, i_batch, global_step_val, lr_val, loss_val, <br/> ce_loss_val, reg_loss_val, train_acc_val 
            )) 
        else: 
            _ = sess.run([train_op]) 
            sys.stdout.write(".") 
            sys.stdout.flush() 
 
        if (i_batch + 1) &gt; 0 and (i_batch + 1) % save_steps == 0: 
            saved_file = saver.save(sess, 
                                    os.path.join(checkpoints_dir,  <br/> 'model.ckpt'), 
                                    global_step=global_step) 
            print("Save steps: Save to file %s " % saved_file) 
 
 coords.request_stop() 
 coords.join(threads) </pre>
<p>训练循环与之前的相似，除了我们在会话配置中添加了<kbd>allow_soft_placement=True</kbd>选项。如有必要，该选项将允许TensorFlow改变<kbd>variables</kbd>的位置。</p>
<p>现在，我们可以像以前一样运行训练脚本:</p>
<pre><strong>python scripts/train_multi.py</strong></pre>
<p>经过几个小时的训练后，我们可以看看TensorBoard来比较结果:</p>
<div><img height="213" width="548" class=" image-border" src="img/be48229b-8546-428e-b38f-c7663b088447.png"/></div>
<div><img height="205" width="542" class=" image-border" src="img/fcb1ec27-d394-49db-b93b-c918b587d0dc.png"/></div>
<p class="packt_figure">图04——多GPU训练过程的张量板上的图</p>
<p>正如你所看到的，在我们的计算机上，在大约四个小时内完成大约6000步后，在多个GPU上的训练达到了100%的准确率。这几乎减少了一半的训练时间。</p>
<p>现在，让我们来看看这两种培训策略是如何比较的:</p>
<div><img height="208" width="539" class=" image-border" src="img/c7041563-2279-42b8-b48d-3c5aa41d3289.png"/></div>
<div><img height="202" width="542" class=" image-border" src="img/aa1e5d15-8f4f-4c60-9a87-7d4e003dbd43.png"/></div>
<p class="packt_figure">图05——并列比较单个和多个GPU时在TensorBoard上的绘图</p>
<p>橙色线是多GPU结果，蓝色线是单GPU结果。我们可以看到，多GPU设置可以比单GPU更快地获得更好的结果。差别不是很大。但是，我们可以通过越来越多的GPU来实现更快的训练。在Amazon EC2的P1实例上，甚至有8个和16个GPU。然而，如果我们在ActivityNet或Sports 1M等大规模数据集上进行训练，在多个GPU上进行训练的好处会更好，因为单个GPU需要很长时间才能收敛。</p>
<p>在下一节中，我们将快速浏览一下亚马逊的另一项服务Mechanical Turk。</p>


            

            
        
    






    
        <title>Overview of Mechanical Turk</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">土耳其机器人概述</h1>
                
            
            
                
<p>Mechanical Turk是一项允许我们创建和管理在线人类智能任务的服务，这些任务将由人类工人完成。有许多任务人类可以比计算机做得更好。因此，我们可以利用这项服务来支持我们的机器学习系统。</p>
<p>你可以在https://www.mturk.com参观这个系统。以下是该服务的网站:</p>
<div><img class=" image-border" src="img/623738e4-e367-4e9e-92a0-333b8fe78185.png"/></div>
<p>这里有几个任务示例，您可以使用它们来支持您的机器学习系统:</p>
<ul>
<li><strong>数据集标记</strong>:你通常有很多未标记的数据，你可以使用Mechanical Turk来帮助你为你的机器学习工作流建立一个一致的地面真相。</li>
<li><strong>生成数据集</strong>:可以让工人建立大量的训练数据。例如，我们可以要求工作人员为自然语言系统创建文本翻译或聊天句子。你可以要求他们对评论的观点进行注释。</li>
</ul>
<p>除了标记之外，Mechanical Turk还可以清理杂乱的数据集，为训练、数据分类和元数据标记做好准备。你甚至可以使用这个服务让他们来判断你的系统输出。</p>


            

            
        
    






    
        <title>Summary</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>我们看了一下Amazon EC2服务，看看我们可以使用多少种服务器类型。然后，我们创建了一个神经网络，在单个GPU上执行人类视频动作识别。之后，我们应用数据并行策略来加速训练过程。最后，我们快速浏览了一下土耳其机器人服务。我们希望您可以利用这些服务，将您的机器学习系统提升到一个更高的水平。</p>


            

            
        
    


</body></html>