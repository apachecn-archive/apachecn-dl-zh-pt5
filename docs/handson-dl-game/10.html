<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding PPO</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">了解PPO</h1>

                

            

            

                

<p class="mce-root">我们已经避免太深入到更高级的<strong>近似政策优化</strong> ( <strong> PPO </strong>)算法的内部工作，甚至避免任何政策对模型的讨论。如果你还记得，PPO是OpenAI首次开发的支持ML-Agents的<strong>缩减级别</strong> ( <strong> RL </strong>)方法，是一种基于策略的算法。在这一章中，我们将看看基于策略和基于模型的RL算法之间的区别，以及Unity实现的更高级的内部工作方式。</p>

<p>以下是我们将在本章中涉及的主要主题的列表:</p>

<ul>

<li>马拉松强化学习</li>

<li>部分可观测马尔可夫决策过程</li>

<li>演员-评论家和连续行动空间</li>

<li>了解TRPO和PPO</li>

<li>用超参数调谐PPO</li>

</ul>

<p class="mce-root">本章的内容是高级的，并且假设你已经学习了前面的几个章节和练习。为了本章的目的，我们还将假设你能够毫无困难地在Unity中打开和运行一个学习环境。</p>

<p class="mce-root"/>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Marathon RL</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">马拉松RL</h1>

                

            

            

                

<p>到目前为止，我们关注的是离散的动作和情节环境，在这些环境中，代理人经常学习解决一个难题或完成一些任务。这种环境的最好例子是GridWorld，当然还有hallow/visual hallow示例，其中代理离散地选择诸如上、左、下或右之类的动作，并使用这些动作导航到某个目标。虽然这些都是很好的学习RL基本概念的环境，但是学习起来可能会很乏味，因为结果通常不是自动的，需要大量的探索。然而，在马拉松式RL环境中，代理总是通过接收控制反馈形式的奖励来学习。事实上，这种形式的RL类似于机器人和模拟的控制系统。由于这些环境以反馈的形式提供了丰富的回报，当我们改变/调整超参数时，它们为我们提供了更好的即时反馈，这将使这些类型的环境完美地满足我们自己的学习目的。</p>

<p>Unity提供了几个马拉松RL环境的例子，在写这篇文章的时候还提供了爬虫、Reacher、Walker和人形环境的例子，但是这些在将来可能会改变。</p>

<p>马拉松环境的构造是不同的，在进一步深入之前，我们可能应该理解其中的一些差异。打开Unity编辑器和您选择的Python命令窗口，设置运行<kbd>mlagents-learn</kbd>，并完成以下练习:</p>

<ol>

<li>从<kbd>Assets/ML-Agents/Examples/Crawler/Scenes</kbd>文件夹中打开<kbd>CrawlerDynamicTarget</kbd>示例场景。这个例子展示了一个具有四个可移动肢体的代理，每个肢体都有两个可以移动的关节。目标是让代理朝着某个不断变化的动态目标前进。</li>

<li>Select the DynamicPlatform | Crawler object in the Hierarchy window and take note of the Crawler Agent component and CrawlerDynamicLearning brain, as shown in the following

<p><strong>截图:</strong></p>

</li>

</ol>

<div><img src="img/45721812-4056-48f1-9b7a-cde4486d892d.png"/></div>

<p>检查爬虫代理和大脑</p>

<ol start="3">

<li>注意大脑的空间大小是如何129个向量观察和20个连续动作的。连续动作返回一个值，该值确定关节可以旋转的角度，从而允许代理学习如何将这些关节动作协调成允许其爬行到目标的运动。</li>

</ol>

<ol start="4">

<li>单击Crawler代理组件旁边的目标图标，并从上下文菜单中选择<strong>编辑脚本</strong>。</li>

<li class="mce-root">脚本打开后，向下滚动并寻找<kbd>CollectObservations</kbd>方法:</li>

</ol>

<pre style="color: black;padding-left: 60px">public override void CollectObservations()<br/>{<br/>  jdController.GetCurrentJointForces();<br/><br/>  AddVectorObs(dirToTarget.normalized);<br/>  AddVectorObs(body.transform.position.y);<br/>  AddVectorObs(body.forward);<br/>  AddVectorObs(body.up);<br/>  foreach (var bodyPart in jdController.bodyPartsDict.Values)<br/>  {<br/>    CollectObservationBodyPart(bodyPart);<br/>  }<br/>}</pre>

<ol start="6">

<li>同样，代码是用C#编写的，但是对于代理感知的输入应该是不言自明的。我们首先可以看到，代理将指向目标的方向、其向上和向前，以及来自每个身体部位的观察作为输入。</li>

<li>在场景中选择<strong>学院</strong>，确保<strong>脑</strong>配置设置为<strong>控制</strong>(学习)。</li>

<li class="mce-root">从您之前准备的命令窗口或Anaconda窗口中，如下运行<kbd>mlagents-learn</kbd>脚本:</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=crawler --train</strong></pre>

<ol start="9">

<li>培训开始后，您会很快看到代理取得了明显的进步。</li>

</ol>

<p>这个代理可以令人印象深刻地非常快速地训练，并且在接下来的部分中对于测试我们关于RL如何工作的知识将是非常有用的。您可以随意浏览和研究这个示例，但是要避免调整任何参数，因为我们将在下一节开始这样做。</p>

<p class="mce-root"/>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The partially observable Markov decision process</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">部分可观测马尔可夫决策过程</h1>

                

            

            

                

<p>回到<a href="6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml" target="_blank">第5章</a>，<em>介绍DRL </em>，我们了解到一个<strong>马尔可夫决策过程</strong> ( <strong> MDP </strong>)被用来定义一个代理用来计算一个动作/值的状态/模型。在Q-learning的例子中，我们已经看到了如何使用一个表格或网格来保存一个环境中的整个MDP，例如冰冻的池塘或网格世界。这些类型的RL是基于模型的，这意味着它们完全模拟了环境中的每个状态——例如，网格游戏中的每个方块。只不过，在大多数复杂的游戏和环境中，能够映射物理或视觉状态变成了一个部分可观测的问题，或者我们可以称之为<strong>部分可观测马尔可夫决策过程</strong> ( <strong> POMDP </strong>)。</p>

<p>POMDP定义了一个过程，在该过程中，代理从不具有其环境的完整视图，而是学习基于派生的一般策略来执行动作。这在爬虫示例中得到了很好的演示，因为我们可以看到代理只使用有限的信息学习移动——到达目标的方向。下表概述了我们通常用于RL的马尔可夫模型的定义:</p>

<table border="1" style="border-collapse: collapse;width: 100%">

<tbody>

<tr>

<td class="CDPAlignCenter CDPAlign"/>

<td class="CDPAlignCenter CDPAlign"><strong>否</strong></td>

<td class="CDPAlignCenter CDPAlign"><strong>是</strong></td>

<td/>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" rowspan="2"><strong>所有状态可观测？</strong></td>

<td class="CDPAlignCenter CDPAlign"><strong>否</strong></td>

<td class="CDPAlignCenter CDPAlign">马尔可夫链</td>

<td class="CDPAlignCenter CDPAlign">MDP</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign"><strong>是</strong></td>

<td class="CDPAlignCenter CDPAlign">隐马尔可夫模型</td>

<td class="CDPAlignCenter CDPAlign">POMDP</td>

</tr>

</tbody>

</table>

<p> </p>

<p>因为我们以行动的形式为代理提供了对其状态的控制，所以我们研究的马尔可夫模型是MDP和POMDP。同样，这些过程也经常被称为on或off模型，而如果RL算法完全知道状态，我们就称之为基于模型的过程。相反，POMDP指的是脱离模型的过程，或者我们将称之为基于策略的方法。基于策略的算法提供了更好的泛化能力，并且具有在未知或无限数量的可观察状态的环境中学习的能力。部分可观察状态的例子有走廊、可视走廊等环境，当然还有Crawler。</p>

<p>马尔可夫模型为机器学习的许多方面提供了基础，你可能会在被称为深度概率编程的更高级的深度学习方法中遇到它们的使用。Deep PPL，顾名思义，就是变分推理和深度学习方法的结合。</p>

<p class="mce-root">无模型方法通常使用有经验的缓冲区来存储一组经验，稍后将使用这些经验来学习一般策略。这个缓冲由几个超参数定义，称为<kbd>time_horizon</kbd>、<kbd>batch_size</kbd>和<kbd>buffer_size</kbd>。<strong> </strong>这里给出了从ML-Agents文档中提取的每个参数的定义:</p>

<p class="mce-root"><kbd>time_horizon</kbd>:这对应于每个代理在添加到经验缓冲区之前需要收集多少步经验。当在一集结束之前达到这个限制时，价值估计被用于从代理的当前状态预测整体预期报酬。因此，该参数在偏差较小但方差较高的估计值(长时间范围)和偏差较大但变化较小的估计值(短时间范围)之间进行权衡。在一集内有频繁奖励的情况下，或者一集太大，较小的数量可能更理想。这个数字应该足够大，以捕捉一个代理动作序列中的所有重要行为:</p>

<p>典型范围:32–2，048</p>

<ul>

<li><kbd>buffer_size</kbd>:这对应于在我们更新模型或进行任何学习之前应该收集多少经验(代理观察、行动和获得的奖励)。这应该是<kbd>batch_size</kbd>的倍数。通常，较大的<kbd>buffer_size</kbd>参数对应于更稳定的训练更新。<ul>

<li>典型范围:2048–409600</li>

</ul>

</li>

<li><kbd>batch_size</kbd>:这是梯度下降更新的一次迭代使用的经验数。这应该总是<kbd>buffer_size</kbd>参数的一部分。如果使用连续的动作空间，这个值应该很大(大约几千)。如果使用离散的动作空间，该值应该更小(十个数量级)。<ul>

<li>典型范围(连续):512–5，120</li>

</ul>

</li>

<li>

<p class="mce-root">典型范围(离散):32–512</p>

<ul>

<li>

<p class="mce-root">我们可以通过查看<kbd>CrawlerDynamicLearning</kbd>大脑配置来了解这些值是如何设置的，并对其进行更改以查看其对训练的影响。打开编辑器和一个正确配置的Python窗口，进入<kbd>CrawlerDynamicTarget</kbd>场景，然后进行以下练习:</p>

</li>

<li>

<p class="mce-root">打开位于<kbd>ML-Agents/ml-agents/config</kbd>文件夹中的<kbd>trainer_config.yaml</kbd>文件。</p>

</li>

</ul>

</li>

</ul>

<p>向下滚动到<kbd>CrawlerDynamicLearning</kbd>大脑配置部分:</p>

<ol>

<li>注意显示<kbd>time_horizon</kbd>、<kbd>batch_size</kbd>和<kbd>buffer_size</kbd>参数的突出显示的行。如果您还记得我们之前的走廊/可视走廊示例，那么<kbd>time_horizon</kbd>参数只有32或64。由于这些例子使用了离散的动作空间，我们可以为<kbd>time_horizon</kbd>设置一个低得多的值。</li>

</ol>

<ol start="2">

<li class="mce-root">将所有参数值加倍，如以下代码摘录所示:</li>

</ol>

<pre style="color: black;padding-left: 60px">CrawlerDynamicLearning:<br/>  normalize: true<br/>  num_epoch: 3<br/>  <strong>time_horizon: 1000</strong><br/><strong>  batch_size: 2024</strong><br/><strong>  buffer_size: 20240</strong><br/>  gamma: 0.995<br/>  max_steps: 1e6<br/>  summary_freq: 3000<br/>  num_layers: 3<br/>  hidden_units: 512</pre>

<ol start="3">

<li>从本质上说，我们在这里所做的是将代理用于构建其周围环境的策略的经验数量加倍。本质上，我们给代理一个更大的经验快照来训练。</li>

<li class="mce-root">像以前多次做的那样，在培训中运行代理。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>time_horizon: 2000</strong><br/><strong>batch_size: 4048</strong><br/><strong>buffer_size: 40480</strong></pre>

<ol start="5">

<li>只要您运行了之前的基本样本，就让代理接受培训。这会让你在训练表现上有一个很好的比较。</li>

<li>有一件事将立即变得显而易见，那就是代理人的训练更加稳定，这意味着代理人的平均回报将更稳定，波动更少。回想一下，我们希望避免训练跳跃、尖峰或摆动，因为这可能表明网络优化方法的收敛性差。这意味着越平缓的变化通常越好，并表明良好的训练表现。通过加倍<kbd>time_horizon</kbd>和相关参数，我们已经加倍了代理用来学习的经验数量。这反过来又有稳定训练的效果，但是您可能注意到代理花费了更长的时间来训练相同的迭代次数。</li>

<li>部分可观测RL算法分为基于策略、无模型或无模型算法，是PPO的基础。在下一节中，我们将看看RL中的改进，这些改进更好地处理了管理连续动作空间的额外复杂性。</li>

</ol>

<p>演员-评论家和连续行动空间</p>

<p>我们在研究马拉松RL或控制学习时引入的另一个复杂性是连续动作空间的引入。连续动作空间表示一个代理可以采取的一组无限可能的动作。我们的代理以前可以支持一个离散的动作，是或否，现在它必须在一个无限的动作空间中选择一些点作为每个关节的动作。这种从无限动作空间到动作的映射并不容易解决——然而，我们确实有神经网络可供我们使用，这些网络为我们提供了一个出色的解决方案，它使用的架构与我们在第三章<a xmlns:epub="http://www.idpf.org/2007/ops" href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml" target="_blank">、 <em xmlns:epub="http://www.idpf.org/2007/ops">游戏</em>中看到的g an没有什么不同。</a></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Actor-Critic and continuous action spaces</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">正如我们在关于GANs的章节中所发现的，我们可以提出一个由两个竞争网络组成的网络架构。这些相互竞争的网络将迫使每个网络通过相互竞争来学习将随机空间映射成令人信服的伪造物的最佳解决方案。类似于GAN的概念也可以应用于这种情况，称为演员-评论家模型。该模型的示意图如下:</h1>

                

            

            

                

<p>这里发生的是<strong>角色</strong>从给定<strong>状态</strong>的策略中选择一个<strong>动作</strong>。<strong>状态</strong>首先通过<strong>评判器</strong>，该评判器在给定当前<strong>状态</strong>的情况下评估最佳动作，并提供一些<strong>错误</strong>。更简单的说，<strong>评论家</strong>根据当前<strong>状态</strong>对每个动作进行批评，然后<strong>演员</strong>根据<strong>状态</strong>选择最佳动作。</p>

<p>这种动作选择的方法最早是在一种叫做<strong>决斗双Q网络</strong> ( <strong> DDQN </strong>)的算法中探索出来的。它现在是最先进的RL算法的基础。</p>

<div><img src="img/76a62795-5bcf-47ca-8854-38bec7a20d00.png" style="width:22.33em;height:21.17em;"/><br/>

<br/>

Actor-Critic architecture</div>

<p>Actor-Critic本质上是用来解决连续动作空间问题的，但是，考虑到它的性能，这种方法也已经被集成到一些高级的离散算法中。ML-Agents对连续空间使用演员-评论家模型，但对离散动作空间不使用。</p>

<p>使用Actor-Critic需要在我们的网络中使用额外的层和神经元，这是我们可以在ML-Agents中配置的。这些的超参数定义来自ML-Agents文档，如下所示:</p>

<p><kbd>num_layers</kbd>:这对应于在观察输入之后，或者在视觉观察的CNN编码之后，存在多少隐藏层。对于简单的问题，较少的层数可能训练得更快更有效。对于更复杂的控制问题，可能需要更多的层:<br/></p>

<p>典型范围:1–3</p>

<ul>

<li><kbd>hidden_units</kbd>:这些对应于神经网络的每个全连接层中有多少个单元。对于简单的问题，正确的行动是观察输入的直接组合，这应该是小的。对于操作是观察变量之间非常复杂的相互作用的问题，这个值应该更大:<ul>

<li>典型范围:32–512</li>

</ul>

</li>

</ul>

<ul>

<li>

<p class="mce-root">让我们打开一个新的ML-Agents马拉松或控制样本，看看修改这些参数对训练有什么影响。按照本练习理解向控制问题添加层和神经元(单元)的效果:</p>

<ul>

<li>从<kbd>Assets/ML-Agents/Examples/Walker/Scenes</kbd>文件夹中打开沃克场景。这个例子展示了一个行走的人形动画。</li>

</ul>

</li>

</ul>

<p>在“层次”窗口中找到并选择WalkerAgent对象，然后查看“检查器”窗口并检查代理和大脑设置，如以下屏幕截图所示:</p>

<ol>

<li>WalkerAgent和WalkerLearning属性</li>

<li>在层级窗口中选择<kbd>WalkerAcademy</kbd>，并确保<kbd>Brains</kbd>参数的控制选项已启用。</li>

</ol>

<div><img src="img/ccf7c439-29cb-4b2d-8745-f26137f898dc.png"/></div>

<p>打开位于<kbd>ML-Agents/ml-agents/config</kbd>文件夹中的<kbd>trainer_config.yaml</kbd>文件，向下滚动到<kbd>WalkerLearning</kbd>部分，如下所示:</p>

<ol start="3">

<li>注意这个例子使用了多少层和单位。它比我们在离散动作问题中使用的多还是少？</li>

<li class="mce-root">保存所有内容，并为训练设置样本。</li>

</ol>

<pre style="color: black;padding-left: 60px">WalkerLearning:<br/>    normalize: true<br/>    num_epoch: 3<br/>    time_horizon: 1000<br/>    batch_size: 2048<br/>    buffer_size: 20480<br/>    gamma: 0.995<br/>    max_steps: 2e6<br/>    summary_freq: 3000<br/>    <strong>num_layers: 3</strong><br/><strong>    hidden_units: 512</strong></pre>

<ol start="5">

<li>使用以下命令从Python控制台启动培训会话:</li>

<li>该代理可能需要相当长的时间来训练，但是尝试并等待大约100，000次迭代，以便很好地了解其训练进度。</li>

<li class="mce-root">现在，我们对演员-评论家及其在连续动作空间中的应用有了更好的理解，我们可以在下一节继续探索改变网络大小对训练这些更复杂的网络有什么影响。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=walker --train</strong></pre>

<ol start="8">

<li>扩展网络架构</li>

</ol>

<p>行动者-批评者架构增加了问题的复杂性，从而增加了解决问题所需的网络的复杂性和规模。这与我们之前研究PilotNet的情况没有什么不同，pilot net是Nvidia用于自驱动的多层CNN架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Expanding network architecture</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Expanding network architecture</h1>

                

            

            

                

<p>我们想看到的是，增加我们的网络规模对Walker例子这样的复杂例子的直接影响。打开Unity至<kbd>Walker</kbd>示例，并完成以下练习:</p>

<p class="mce-root">从正常位置打开<kbd>trainer_config.yaml</kbd>。</p>

<p>修改<kbd>WalkerLearning</kbd>配置，如下面的代码所示:</p>

<ol>

<li>设置<kbd>num_layers: 1</kbd>和<kbd>hidden_units: 128</kbd>。这些是我们用于离散动作空间问题的典型值。您可以通过查看另一个离散样本来确认这一点，例如<kbd>VisualHallwayLearning</kbd>配置，如下所示:</li>

<li class="mce-root">此示例使用的设置与我们刚刚将连续动作问题设置为的设置相同。</li>

</ol>

<pre style="color: black;padding-left: 60px">WalkerLearning:<br/>    normalize: true<br/>    num_epoch: 3<br/>    time_horizon: 1000<br/>    batch_size: 2048<br/>    buffer_size: 20480<br/>    gamma: 0.995<br/>    max_steps: 2e6<br/>    summary_freq: 3000<br/>    <strong>num_layers: 1</strong><br/><strong>    hidden_units: 128</strong></pre>

<ol start="3">

<li>完成编辑后，保存所有内容，为培训做好准备。</li>

</ol>

<pre style="padding-left: 60px">VisualHallwayLearning:<br/>    use_recurrent: false<br/>    sequence_length: 64<br/>   <strong> num_layers: 1</strong><br/><strong>    hidden_units: 128</strong><br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>

<ol start="4">

<li>使用新的<kbd>run-id</kbd>参数启动培训课程。记住练习在每次运行时改变<kbd>run-id</kbd>参数，以便更容易辨别TensorBoard中的每次运行。</li>

<li>和往常一样，让样本运行的时间和之前运行的时间一样长，以便进行比较。</li>

<li>运行此示例时，您可能会立即注意到的一件事是训练的稳定性。你可能注意到的第二件事是，训练稳定性增加了，但表现略有下降。请记住，较小的网络权重较小，通常会更稳定，训练速度也更快。但是，在这个问题中，虽然训练在网络上更稳定，并且承诺更快，但您可能会注意到训练碰壁了。代理现在受到网络大小的限制，能够更快地优化较小的网络，但没有我们以前见过的精细控制。事实上，这个代理将永远不会像第一次未改变的运行那样好，因为它现在受到较小网络的限制。这是你在为游戏/模拟建立DRL代理时需要平衡的另一个权衡。</li>

<li>在下一节中，我们将进一步研究我们称之为优势函数或那些像演员-评论家中使用的函数，并将首先探索TRPO，当然还有PPO。</li>

</ol>

<p>了解TRPO和PPO</p>

<p>在解决优化未来回报预测的RL问题时，无策略和无模型算法有许多变体。正如我们所看到的，许多算法使用了一个优势函数，比如Actor-Critic，在这里我们有问题的两个方面试图收敛到最优解。在这种情况下，优势函数试图找到最大的期望折扣奖励。TRPO和PPO通过使用一种叫做<strong>最小化最大化</strong> ( <strong> MM) </strong>算法的优化方法来做到这一点。下图显示了MM算法如何解决问题的示例:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding TRPO and PPO</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">这个图摘自惠的一系列博客，这些博客优雅地描述了MM算法以及TRPO和PPO方法的更多细节<em>。</em>来源见以下链接:(<a href="https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12" target="_blank">https://medium . com/@ Jonathan _ hui/rl-proximal-policy-optimization-PPO-explained-77f 014 EC 3 f 12</a>)。</h1>

                

            

            

                

<p>本质上，MM算法通过交互地最大化和最小化函数参数来寻找最佳对函数，直到它达到收敛解。在图中，红线表示我们要逼近的函数，蓝线表示收敛函数。当算法选择将找到解决方案的最小/最大值时，您可以看到进度。</p>

<div><img src="img/2942a112-735d-42c4-8c0b-91d658772882.png" style="width:50.50em;height:12.17em;"/><br/>

<br/>

Using the MM algorithm</div>

<p>我们在使用MM时遇到的问题是，函数逼近有时会下降，或陷入低谷。为了更好地理解这一点，让我们把这看作是用一条直线来解决爬一个不平坦的小山的问题。这种情况的一个例子如下:</p>

<p>你可以看到，只使用线性路径来尝试和导航这个相当危险的山脊，实际上是危险的。虽然危险可能不那么真实，但当使用线性方法求解MM时，这仍然是一个大问题，因为如果你只使用一条笔直的固定路径在陡峭的山脊上徒步旅行。</p>

<p class="mce-root">TRPO通过使用二次方法，并通过以信赖域的形式限制每次迭代可以采取的步骤的数量，解决了使用线性方法的问题。也就是说，算法确保每个位置都是正的和安全的。如果我们再次考虑我们的爬山例子，我们可能会认为TRPO放置了一个路径或信任区域，如下图所示:</p>

<div><img src="img/7a561b98-be5b-4d4f-bf2f-b5ac4303dd07.png" style="width:40.42em;height:30.08em;"/><br/>

<br/>

Attempting to climb a hill using linear methods</div>

<p>在前面的照片中，路径仅出于示例目的显示为一组相连的圆或区域；真实的信任路径可能或可能不更接近实际的峰或脊。不管怎样，这具有允许代理以更渐进的速度学习的效果。有了TRPO，信任区域的大小可以改变，变得更大或更小，以符合我们首选的政策趋同。TRPO的问题是实现起来相当复杂，因为它需要对一些复杂的方程进行二次求导。</p>

<p>TRPO solves the problem of using linear methods by using a quadratic method, and by  limiting the amount of steps each iteration can take in a form of trust region. That is, the algorithm makes sure that every position is positive and safe. If we consider our hill climbing example again, we may consider TRPO as placing a path or region of trust, like in the following photo:</p>

<div><img src="img/1f867818-2804-442b-9cec-fdd67792c95c.png" style="width:37.67em;height:28.08em;"/><br/>

<br/>

A trust region path up the hill</div>

<p>PPO通过在每次迭代中限制或削减两个策略之间的Kulbach-Leibler ( <strong> KL </strong>)差异来解决这个问题。KL散度衡量概率分布的差异，可通过下图进行描述:</p>

<p class="mce-root">在上图中，<strong> p(x) </strong>和<strong> q(x) </strong>分别代表衡量KL差异的不同政策。然后，该算法反过来使用这种差异度量来限制或削减迭代中可能发生的策略变化量。ML-Agents使用两个超参数，允许您控制应用于目标或函数的裁剪量，该目标或函数决定迭代中策略变化的量。如Unity文档中所述，以下是β和ε参数的定义:</p>

<p><strong> Beta </strong>:这对应于熵正则化的强度，这使得策略<em>更加随机</em>。这可确保代理在培训期间正确探索行动空间。增加该值将确保采取更多随机操作。这应该被调整，以便熵(可从TensorBoard测量)随着奖励的增加而慢慢减少。如果熵下降太快，增加β。如果熵下降太慢，则减小β:</p>

<div><img src="img/15484cb4-865c-4d5b-b788-e193fbf80aeb.png" style="width:46.42em;height:17.67em;"/><br/>

<br/>

Understanding KL divergence</div>

<p>典型范围:1e-4–1e-2</p>

<ul>

<li><strong> Epsilon </strong>:这对应于梯度下降更新过程中新旧策略之间可接受的背离阈值。将该值设置得较小会导致更稳定的更新，但也会减慢训练过程:</li>

<li style="padding-left: 30px">典型范围:0.1–0.3</li>

<li>关于这些参数要记住的关键事情是，它们控制策略从一个迭代到下一个迭代的变化速度。如果您注意到代理训练有些不稳定，将这些参数调整为较小的值可能会有所帮助。<strong>ε</strong>的默认值是<strong> .2 </strong>，而<strong>β</strong>的默认值是<strong> 1.0e-2 </strong>，但是，当然，我们会想要探究这些值如何影响训练，无论是正面的还是负面的。在下一个练习中，我们将修改这些策略更改参数，并查看它们在培训中的效果:</li>

<li style="padding-left: 30px">对于这个例子，我们将从<kbd>Assets/ML-Agents/Examples/Crawler/Scenes</kbd>文件夹中打开<kbd>CrawlerDynamic</kbd>场景。</li>

</ul>

<p class="mce-root">打开位于<kbd>ML-Agents/ml-agents/config</kbd>文件夹中的<kbd>trainer_config.yaml</kbd>文件。由于我们已经评估了该示例的性能，因此有几种方法可以恢复训练配置，并对beta和epsilon参数进行一些修改。</p>

<p class="mce-root">向下滚动到<kbd>CrawlerDynamicLearning</kbd>配置部分，修改如下:</p>

<p>我们将<kbd>epsilon</kbd>和<kbd>beta</kbd>参数修改为更高的值，这意味着训练将更不稳定。然而，如果你回想一下，这些马拉松例子通常以更稳定的方式训练。</p>

<ol>

<li>打开正确配置的Python控制台，运行以下命令启动培训:</li>

<li>像往常一样，等待一系列培训会议，以便从一个示例到下一个示例进行良好的比较。</li>

<li class="mce-root">Scroll down to the <kbd>CrawlerDynamicLearning</kbd> configuration section and modify it as follows:</li>

</ol>

<pre style="color: black;padding-left: 60px">CrawlerDynamicLearning:<br/>    normalize: true<br/>    num_epoch: 3<br/>    time_horizon: 1000<br/>    batch_size: 1024<br/>    buffer_size: 20240<br/>    gamma: 0.995<br/>    max_steps: 1e6<br/>    summary_freq: 3000<br/>    num_layers: 3<br/>    hidden_units: 512<br/>    <strong>epsilon: .1</strong><br/><strong>    beta: .1</strong> </pre>

<ol start="4">

<li>We modified the <kbd>epsilon</kbd> and <kbd>beta</kbd> parameters to higher values, meaning that the training will be less stable. If you recall, however, these marathon examples generally train in a more stable manner.</li>

<li class="mce-root">你可能会发现意想不到的是，代理似乎开始倒退，事实上，它是。发生这种情况是因为我们使那些信任区域太大(一个大的<strong>β</strong>),虽然我们允许变化率较低(1<strong>ε</strong>),但我们可以看到<strong>β</strong>值对训练更敏感。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=crawler_policy --train</strong></pre>

<ol start="6">

<li>请记住，Unity ML-Agents的实现串联使用了许多交叉特性，构成了一个强大的RL框架。在下一节中，我们将快速查看Unity最近添加的一个后发优化参数。</li>

</ol>

<p class="mce-root">广义优势估计</p>

<p class="mce-root">由于不断的研究，RL领域正在经历爆炸式的增长，推动着可能性的极限。每一点进步都会带来额外的超参数和小调整，可用于稳定和/或改善训练表现。Unity最近增加了一个名为lambda的新参数，从文档中提取的定义如下:</p>

<p><strong xmlns:epub="http://www.idpf.org/2007/ops">λ</strong>:对应于计算<strong xmlns:epub="http://www.idpf.org/2007/ops">广义优势估计</strong>(<strong xmlns:epub="http://www.idpf.org/2007/ops">https://arxiv.org/abs/1506.02438</strong>)<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://arxiv.org/abs/1506.02438" target="_blank">时使用的λ参数。这可以被认为是代理在计算更新的估价时对其当前估价的依赖程度。低值对应于更依赖于当前价值估计(可能是高偏差)，高值对应于更依赖于在环境中收到的实际回报(可能是高方差)。该参数提供了两者之间的折衷，并且正确的值可以导致更稳定的训练过程:</a></p>

<p>典型范围:0.9–0.95</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Generalized advantage estimate</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">GAE的论文描述了一个称为lambda的函数参数，该参数可用于形成奖励估计函数，并且最适合用于控制或马拉松RL任务。细节我们就不赘述了，有兴趣的读者当然要把论文拉下来自己回顾一下。但是，我们将在下一个练习中探索改变该参数如何影响控制样本，如<kbd>Walker</kbd>场景:</h1>

                

            

            

                

<p>打开Unity编辑器到<kbd>Walker</kbd>示例场景。</p>

<ul>

<li>在层次中选择学院对象，并确认场景仍为训练/学习设置。如果是，你就不用做别的了。如果场景不是为学习而设置的，你知道该怎么做。<ul>

<li>打开<kbd>trainer_config.yaml</kbd>文件，修改<kbd>WalkerLearning</kbd>如下:</li>

</ul>

</li>

</ul>

<p>注意我们如何设置<kbd>lambd</kbd>参数，并确保<kbd>num_layers</kbd>和<kbd>hidden_units</kbd>被重置为原始值。在本文中，作者描述了从<kbd>.95</kbd>到<kbd>.99</kbd>的最佳值，但这不同于Unity文档。</p>

<ol>

<li>完成编辑后保存文件。</li>

<li>打开用于培训的Python控制台设置，并使用以下命令运行它:</li>

</ol>

<ol start="3">

<li>确保您让示例运行的时间与之前一样长，以获得良好的比较。</li>

</ol>

<pre style="color: black;padding-left: 60px">WalkerLearning:<br/>    normalize: true<br/>    num_epoch: 3<br/>    time_horizon: 1000<br/>    batch_size: 2048<br/>    buffer_size: 20480<br/>    gamma: 0.995<br/>    max_steps: 2e6<br/>    summary_freq: 3000<br/>    num_layers: 3<br/>    hidden_units: 512<br/>    <strong>lambd: .99</strong></pre>

<ol start="4">

<li>经过一段时间的训练后，你会注意到，在这个例子中，代理的训练速度确实慢了25%。这个结果告诉我们的是，通过增加lambda，我们告诉代理人把更多的价值放在奖励上。现在，这可能看起来违背直觉，但是在这个例子或这种类型的环境中，代理人正在接受持续的小的积极的奖励。这导致每个奖励都变得倾斜，正如我们所看到的，倾斜训练并阻碍代理人的进步。对于感兴趣的读者来说，在走廊环境中尝试和使用lambda参数可能是一个有趣的练习，在走廊环境中，代理只收到一个积极的剧集奖励。</li>

<li>RL advantage函数有多种形式，用于解决非模型或策略驱动算法(如PPO)的许多问题。在下一节中，我们将通过自己修改和创建一个新的样本控制/马拉松学习环境来结束这一章。</li>

<li class="mce-root">学习调优PPO</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=walker_lambd --train</strong></pre>

<ol start="7">

<li>在本节中，我们将学习调整修改后的/新的控制学习环境。这将允许我们学习更多关于Unity示例的内部工作原理，但是也将向您展示如何在以后自己修改一个新的或修改过的示例。让我们首先打开Unity编辑器，以便完成以下练习:</li>

</ol>

<p>打开<kbd>Reacher</kbd>场景，设置学习，训练中运行。你现在应该可以在睡觉的时候做这部分了。像往常一样，让代理训练相当长的时间，这样你就可以建立一个基准。</p>

<p>从菜单中选择<kbd>Assets/Import Package/Custom Package</kbd>。从下载到源代码的书籍的<kbd>Chapter08</kbd>文件夹中找到<kbd>Chapter_8_Assets.unitypackage</kbd>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Learning to tune PPO </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从<kbd>Assets/HoDLG/Scenes</kbd>文件夹中打开Reacher_3_joint场景。这是修改后的场景，但是我们也将经历它的构造。</h1>

                

            

            

                

<p>首先，请注意只有一个<strong>机械手</strong>处于活动状态，但现在有三个关节，如下面的截图所示:</p>

<ol>

<li>检查代理游戏对象</li>

<li>请注意手臂现在有三个部分，新的部分称为胶囊(2)并被标识为钟摆C。关节的顺序现在混乱了，这意味着钟摆C实际上是中间的钟摆，而不是底部。</li>

<li>选择每个胶囊对象，并检查它们的配置和放置，如以下屏幕截图所示:</li>

<li class="mce-root">检查胶囊对象</li>

</ol>

<div><img src="img/e473ca06-9250-4c2f-90a8-ae6225372b74.png"/></div>

<p>请务必注意每个胶囊的可配置关节|连接的身体对象。该属性设置对象将铰接或连接到的几何体。可配置关节组件上有许多其他属性，允许您以任何形式模拟这种关节交互，甚至可能是生物形式。例如，您可能希望通过只允许某些角度的运动来使手臂的关节更像人类。同样，如果你正在设计一个运动受限的机器人，那么你也可以用这个关节组件来模拟它。</p>

<ol start="5">

<li>在这个阶段，我们可以设置并运行示例。打开并设置用于训练Python控制台或Anaconda窗口。</li>

<li>在训练中运行示例，并观察代理的进度。让代理运行足够多的迭代，以便将训练性能与基线进行比较。</li>

</ol>

<div><img src="img/909c7124-2b2d-427d-93ff-c5a7257beedb.png" style="width:40.00em;height:40.00em;"/></div>

<p>在这个阶段，我们已经启动并运行了我们的样本，我们准备开始调整新的参数来优化训练。然而，在我们这样做之前，我们将后退一步，看一看使最后一个示例成为可能所需的C#代码更改。下一节将介绍C#代码的变化，对于那些对代码不感兴趣的开发人员来说，这是可选的。如果您计划在Unity中构建自己的控制或马拉松环境，您将需要阅读下一部分。</p>

<ol start="7">

<li>控制项目所需的编码更改</li>

<li>正如我们已经提到的，这部分是可选的，是为那些对使用Unity C#构建自己的控件示例的细节感兴趣的人准备的。将来，修改这些类型的示例也可能不需要任何编码更改，这也是本节可选的另一个原因。</li>

<li>完成以下练习，了解在Reacher控制示例中添加关节所需的编码更改:</li>

</ol>

<p>在“层次”窗口中选择代理对象，然后在“检查器”窗口中，注意Reacher Agent_3组件。这是我们将要检查的修改过的脚本。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Coding changes required for control projects</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">单击Reach Agent_3组件旁边的目标图标，并从上下文菜单中选择编辑脚本。</h1>

                

            

            

                

<p>这将在您选择的C#代码编辑器中打开<kbd>ReacherAgent_3.cs</kbd>脚本。</p>

<p>声明中首先要注意的是添加了新的变量，以粗体突出显示如下:</p>

<ol>

<li>Select the Agent object in the Hierarchy window and then, in the Inspector window, note the Reacher Agent_3 component. This is the modified script that we will be inspecting.</li>

<li>添加了两个新变量<kbd>pendulumC</kbd>和<kbd>rbC</kbd>，用于保存新关节GameObject和RigidBody。现在，Unity physics中的<kbd>Rigidbody</kbd>表示可以被物理引擎移动或操纵的对象。<br/> Unity正在升级他们的物理引擎，这将改变这里的一些教学。ML-Agents的当前版本使用旧的物理系统，所以这个例子也一样。</li>

<li>下一件需要注意的重要事情是添加了额外的代理观察，如下面的<kbd>CollectObservations</kbd>方法所示:</li>

<li class="mce-root">The first thing to note under the declarations is the addition of new variables, highlighted in bold as follows:</li>

</ol>

<pre style="color: black;padding-left: 60px">public GameObject pendulumA;<br/>public GameObject pendulumB;<br/><strong>public GameObject pendulumC;</strong><br/>public GameObject hand;<br/>public GameObject goal;<br/>private ReacherAcademy myAcademy;<br/>float goalDegree;<br/>private Rigidbody rbA;<br/>private Rigidbody rbB;<br/><strong>private Rigidbody rbC;</strong><br/>private float goalSpeed;<br/>private float goalSize;</pre>

<p class="mce-root">粗体部分增加了对<kbd>pendulumC</kbd>和<kbd>rbC</kbd>的新观察，总共有另外13个矢量。回想一下，这意味着我们还需要将我们的大脑从33个向量观察值切换到46个观察值，如下面的屏幕截图所示:</p>

<ol start="5">

<li>检查更新ReacherLearning_3 brain</li>

<li>接下来，我们将目光投向<kbd>AgentAction</kbd>方法；这是Python训练器代码调用代理并告诉它做什么动作的地方，如下所示:</li>

</ol>

<pre style="padding-left: 60px">public override void CollectObservations()<br/>    {<br/>        AddVectorObs(pendulumA.transform.localPosition);<br/>        AddVectorObs(pendulumA.transform.rotation);<br/>        AddVectorObs(rbA.angularVelocity);<br/>        AddVectorObs(rbA.velocity);<br/><br/>        AddVectorObs(pendulumB.transform.localPosition);<br/>        AddVectorObs(pendulumB.transform.rotation);<br/>        AddVectorObs(rbB.angularVelocity);<br/>        AddVectorObs(rbB.velocity);<br/><br/>        <strong>AddVectorObs(pendulumC.transform.localPosition);</strong><br/><strong>        AddVectorObs(pendulumC.transform.rotation);</strong><br/><strong>        AddVectorObs(rbC.angularVelocity);</strong><br/><strong>        AddVectorObs(rbC.velocity);</strong><br/><br/>        AddVectorObs(goal.transform.localPosition);<br/>        AddVectorObs(hand.transform.localPosition);<br/>        <br/>        AddVectorObs(goalSpeed);<br/>  }</pre>

<p class="mce-root">在这个方法中，我们扩展了代码，允许代理以<kbd>rigidbody rbC</kbd>的形式移动新关节。你是否注意到新的学习大脑也增加了更多的行动空间？</p>

<ol start="7">

<li>最后，我们看一下<kbd>AgentReset</kbd>方法，看看代理如何用新分支重置自己，如下所示:</li>

</ol>

<div><img src="img/15dec040-4804-44a0-9dac-cc20311fad51.png" style="width:38.83em;height:20.75em;"/></div>

<p>Inspecting the update ReacherLearning_3 brain</p>

<ol start="8">

<li>Next, we will look to the <kbd>AgentAction</kbd> method; this is where the Python trainer code calls the agent and tells it what movements it makes, and is as follows:</li>

</ol>

<pre style="padding-left: 60px">public override void AgentAction(float[] vectorAction, string textAction)<br/>  {<br/>        goalDegree += goalSpeed;<br/>        UpdateGoalPosition();<br/><br/>        var torqueX = Mathf.Clamp(vectorAction[0], -1f, 1f) * 150f;<br/>        var torqueZ = Mathf.Clamp(vectorAction[1], -1f, 1f) * 150f;<br/>        rbA.AddTorque(new Vector3(torqueX, 0f, torqueZ));<br/><br/>        torqueX = Mathf.Clamp(vectorAction[2], -1f, 1f) * 150f;<br/>        torqueZ = Mathf.Clamp(vectorAction[3], -1f, 1f) * 150f;<br/>        rbB.AddTorque(new Vector3(torqueX, 0f, torqueZ));<br/><br/>        <strong>torqueX = Mathf.Clamp(vectorAction[3], -1f, 1f) * 150f;</strong><br/><strong>        torqueZ = Mathf.Clamp(vectorAction[4], -1f, 1f) * 150f;</strong><br/><strong>        rbC.AddTorque(new Vector3(torqueX, 0f, torqueZ));</strong><br/>    }</pre>

<ol start="9">

<li>In this method, we are extending the code to allow the agent to move the new joint in the form of <kbd>rigidbody rbC</kbd>. Did you notice that the new learning brain also added more action space?</li>

<li>Lastly, we look at the <kbd>AgentReset</kbd> method to see how the agent will reset itself with the new limb, as follows:</li>

</ol>

<pre style="padding-left: 60px">public override void AgentReset()<br/>    {<br/>        pendulumA.transform.position = new Vector3(0f, -4f, 0f) + transform.position;<br/>        pendulumA.transform.rotation = Quaternion.Euler(180f, 0f, 0f);<br/>        rbA.velocity = Vector3.zero;<br/>        rbA.angularVelocity = Vector3.zero;<br/><br/>        pendulumB.transform.position = new Vector3(0f, -10f, 0f) + transform.position;<br/>        pendulumB.transform.rotation = Quaternion.Euler(180f, 0f, 0f);<br/>        rbB.velocity = Vector3.zero;<br/>        rbB.angularVelocity = Vector3.zero;<br/><br/>        <strong>pendulumC.transform.position = new Vector3(0f, -6f, 0f) + transform.position;</strong><br/><strong>        pendulumC.transform.rotation = Quaternion.Euler(180f, 0f, 0f);</strong><br/><strong>        rbC.velocity = Vector3.zero;</strong><br/><strong>        rbC.angularVelocity = Vector3.zero;</strong><br/><br/>        goalDegree = Random.Range(0, 360);<br/>        UpdateGoalPosition();<br/><br/>        goalSize = myAcademy.goalSize;<br/>        goalSpeed = Random.Range(-1f, 1f) * myAcademy.goalSpeed;<br/><br/>        goal.transform.localScale = new Vector3(goalSize, goalSize, goalSize);<br/>    }</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root">这段代码所做的只是将手臂的位置重置到原始位置，并停止所有运动。</p>

<p class="mce-root">这涵盖了本例中唯一需要的代码更改。幸运的是，只有一个脚本需要修改。很可能在将来你根本不需要修改这些脚本。在下一节中，我们将通过调整额外的参数和引入另一种策略学习方法的训练优化来改进样本的训练。</p>

<p class="mceNonEditable">多代理策略</p>

<p class="mceNonEditable">在本节中，我们将探讨如何通过引入多个代理来训练相同的策略，从而改进基于策略或非模型的方法，如PPO。您将在本节中使用的示例练习将完全由您决定，并且应该是您熟悉和/或感兴趣的。出于我们的目的，我们将探索一个我们已经广泛研究过的样本——走廊/可视走廊。如果你已经完成了本书中的大部分练习，你应该能够适应这个例子。但是，请注意，在本练习中，我们希望使用一个设置为使用多个代理进行培训的示例。</p>

<ol start="11">

<li>以前，我们避免讨论多个代理；我们之前避免了这方面的培训，因为它可能会使模型上和模型下的讨论复杂化。现在，您已经理解了使用基于策略的方法的区别和原因，您可以更好地理解，因为我们的代理正在使用基于策略的方法，所以我们可以针对同一策略同时训练多个代理。然而，这可能会对其他训练参数和配置产生影响，正如您可以想象的那样。</li>

</ol>

<p>打开Unity编辑器进入<kbd>Hallway</kbd> / <kbd>VisualHallway</kbd>示例场景，或您选择的场景，并完成以下练习:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Multiple agent policy</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">打开Python或Anaconda控制台窗口，让它准备好进行训练。</h1>

                

            

            

                

<p class="mce-root">选择并启用走廊区域，选择区域(1)到(19)以便它们在场景中处于活动状态并可见。</p>

<p>选择每个<strong>走廊区域</strong>中的代理对象，并确保<strong>走廊代理</strong> | <strong>大脑</strong>设置为走廊学习而不是走廊玩家。这将打开所有附加的训练区域。</p>

<p>根据您以前的经验，您可能希望也可能不希望将示例修改回原始状态。回想一下，在前面的练习中，我们修改了HallwayAgent脚本，使其只扫描较小部分的角度。这可能也需要你改变大脑参数。</p>

<ol>

<li>设置场景后，保存场景和项目。</li>

<li>使用独特的<kbd>run-id</kbd>在训练中运行场景，并等待多次训练迭代。根据您的硬件，此示例的训练速度可能会慢很多，甚至更快。</li>

<li>既然我们已经为走廊环境建立了新的基线，现在我们可以确定修改一些超参数对离散动作样本有什么影响。我们将重新审视的两个参数是<kbd>num_epochs</kbd>(训练时期的数量)和<kbd>batch_size</kbd>(每个训练时期的经历)参数，我们之前在连续动作(控制)样本中看到过。在文件中，我们注意到当训练控制剂时，较大的批量是优选的。</li>

<li>在我们继续之前，让我们打开<kbd>trainer_config.yaml</kbd>文件并检查如下的走廊学习配置部分:</li>

<li>在Unity文档中，它特别提到了在增加批量时只增加历元的数量，这是为了考虑额外的训练经验。我们了解到，控制示例通常受益于较大的批量，因此也受益于较大的时段。然而，我们想要确定的最后一件事是，在一个离散动作示例中，改变<kbd>batch_size</kbd>和<kbd>num_epoch</kbd>参数的效果，其中多个代理向同一个策略提供信息并从中学习。</li>

</ol>

<ol start="6">

<li>出于本练习的目的，我们仅将<kbd>batch_size</kbd>和<kbd>num_epoch</kbd>修改为如下值:</li>

</ol>

<p>更新您正在使用的<kbd>HallwayLearning</kbd>或大脑配置，以使用以下参数:</p>

<p>我们将<kbd>num_epoch</kbd>设为10，将<kbd>batch_size</kbd>设为<kbd>1000</kbd>。正如我们之前所看到的，这些设置是控制样本的典型设置，但现在我们希望在一个离散动作示例中看到效果，其中多个代理训练相同的策略。</p>

<pre>HallwayLearning:<br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    <strong>num_epoch: 3</strong><br/>    buffer_size: 1024<br/>    <strong>batch_size: 128</strong><br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>

<p>准备用于训练的样本，准备好Python控制台并打开。</p>

<p>使用以下命令运行培训会话:</p>

<ol>

<li>注意我们是如何设置<kbd>run-id</kbd>的，使用一个助手前缀来命名迭代。我们用<kbd>e10</kbd>表示<kbd>num_epoch</kbd>参数设置为<kbd>10</kbd>，<kbd>b1000</kbd>表示<kbd>1000</kbd>的<kbd>batch_size</kbd>值。这种类型的命名方案很有帮助，我们将在本书中继续使用。</li>

</ol>

<pre style="padding-left: 60px">HallwayLearning:<br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    <strong>num_epoch: 10</strong><br/>    buffer_size: 1024<br/>    <strong>batch_size: 1000</strong><br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>

<ol start="2">

<li>代理培训时，尝试回答以下问题:</li>

<li>代理的培训比您预期的更好还是更差？</li>

<li>你认为这是为什么？</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=hallway_e10b1000 --train</strong></pre>

<ol start="5">

<li>为了了解这些问题的答案，将由您来运行示例。在下一节中，我们将看看你可以自己做的有助于理解这些复杂主题的练习。</li>

</ol>

<p>练习</p>

<ul>

<li>自己尝试以下一两个练习:</li>

<li>运行CrawlerStaticTarget示例场景，并将其性能与动态示例进行比较。</li>

</ul>

<p>将另一个对照示例中的<kbd>time_horizon</kbd>、<kbd>batch_size</kbd>和<kbd>buffer_size</kbd>大脑超参数加倍:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">对另一个对照样品进行<kbd>time_horizon</kbd>、<kbd>batch_size</kbd>和<kbd>buffer_size</kbd>的相同修改，观察组合效果。</h1>

                

            

            

                

<p>将<kbd>num_layers</kbd>和<kbd>hidden_units</kbd>大脑超参数修改为我们在控制样本中使用的值，并将它们应用于离散动作示例，如走廊示例，如以下代码所示。对训练有什么影响？</p>

<ol>

<li>更改另一个连续或离散动作实例的<kbd>num_layers</kbd>和<kbd>hidden_units</kbd>超参数，并将其与其他参数修改相结合。</li>

</ol>

<ol start="2">

<li>将离散动作示例中的λ<kbd>lambd</kbd>大脑超参数修改为值<kbd>.99</kbd>。请记住，这将产生强化奖励的效果:</li>

</ol>

<pre style="padding-left: 60px"><strong>time_horizon: 2000</strong><br/><strong>batch_size: 4048</strong><br/><strong>buffer_size: 40480</strong></pre>

<ol start="3">

<li>用关节和四肢创造你自己的控制生物。一个好的起点是使用爬虫示例并对其进行修改。</li>

<li>通过添加新的肢体或关节来修改其中一个控制样本。</li>

</ol>

<pre style="padding-left: 60px"><strong>num_layers: 3</strong><br/><strong>hidden_units: 512</strong></pre>

<ol start="5">

<li>修改步行机控制示例，为代理提供一件武器和一个目标。你必须结合沃克和Reacher例子的元素。</li>

<li>使用更改后的<kbd>num_epoch</kbd>和<kbd>batch_size</kbd>参数运行VisualHallwayLearning示例场景。结果是你期望的吗？</li>

</ol>

<pre style="padding-left: 60px"><strong>lambd: .99</strong></pre>

<ol start="7">

<li>随着我们阅读这本书的进展，这些练习可能会变得越来越乏味，尤其是如果您在一个更老更慢的系统上运行它们。但是，了解这些参数如何改变代理的训练是很重要的。</li>

<li>当与深度学习和RL实践者交谈时，他们经常会将培训的微妙之处与成为一名优秀或伟大的厨师之间的差异进行比较。一个好的厨师可能会做出美味可口、完全可以接受的饭菜，但是只有一个伟大的厨师，以及他对细节的关注，才能让你做出一顿令人难忘的大餐。</li>

<li>Modify the Walker control example to give the agent a weapon and a target. You will have to combine elements of the Walker and Reacher examples.</li>

<li>Run the VisualHallwayLearning sample scene with altered <kbd>num_epoch</kbd> and <kbd>batch_size</kbd> parameters. Are the results what you expected?</li>

</ol>

<p>摘要</p>

<p>在这一章中，通过理解基于模型的算法与基于模型和/或基于策略的算法之间的差异，我们深入了解了RL的更多内部工作原理。正如我们所了解的，Unity ML-Agents使用PPO算法，这是一种强大而灵活的策略学习模型，在训练控制时效果非常好，有时也称为马拉松RL。在学习了更多的基础知识后，我们跳到了其他RL的改进上，比如演员-评论家，或者优势训练，以及ML-Agents支持的选项。接下来，我们研究了PPO及其前身TRPO算法的发展，它们如何在基础层面上工作，以及它们如何影响训练。在这里，我们学习了如何修改一个控制样本，以在机械手上创建一个新的关节。我们通过查看如何改进多代理策略训练，再次通过调优超参数，完成了这一章。</p>

<p class="mce-root">我们已经讨论了RL的许多方面和细节，以及代理如何培训，但是我们把培训中最重要的部分——奖励——留到了下一章。在下一章，我们将研究奖励，奖励函数，以及如何模拟奖励。</p>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Summary</h1>

                

            

            

                

<p>In this chapter, we dug in and learned more of the inner workings of RL by understanding the differences between model-based versus off-model and/or policy-based algorithms. As we learned, Unity ML-Agents uses the PPO algorithm, a powerful and flexible policy learning model that works exceptionally well when training control, or what is sometimes referred to as marathon RL. After learning more basics, we jumped into other RL improvements in the form of Actor-Critic, or advantage training, and what options ML-Agents supports. Next, we looked at the evolution of PPO and its predecessor, the TRPO algorithm, how they work at a basic level, and how they affect training. This is where we learned how to modify one of the control samples to create a new joint on the Reacher arm. We finished the chapter by looking at how multi-agent policy training can be improved on, again by tuning hyperparameters.</p>

<p>We have covered many aspects and details of RL and how agents train, but we have left the most important part of training, rewards, to the next chapter. In the next chapter, we look into rewards, reward functions, and how rewards can even be simulated.</p>





            



            

        

    </body>



</html></body></html>