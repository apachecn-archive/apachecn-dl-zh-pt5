<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Convolutional and Recurrent Networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">卷积和递归网络</h1>

                

            

            

                

<p class="mce-root">人类大脑通常是我们在构建人工智能时的主要灵感和对比，也是深度学习研究人员经常寻求灵感或保证的东西。通过更详细地研究大脑及其各个部分，我们经常会发现神经子过程。神经子过程的一个例子是我们的视觉皮层，即我们大脑中负责视觉的区域。我们现在知道，我们大脑的这个区域有不同的连接方式，对输入的反应也不同。这恰好类似于我们在以前使用神经网络对图像进行分类的尝试中发现的模拟。现在，人类大脑有许多子过程，都有大脑中特定的映射区域(视觉、听觉、嗅觉、言语、味觉、触觉和记忆/时间)，但在这一章中，我们将看看我们如何通过使用称为<strong>卷积和递归网络</strong>的高级形式的深度学习来建模视觉和记忆。视觉和记忆的两个核心子过程被我们广泛用于许多任务，包括游戏，并成为许多深度学习者的研究焦点。</p>

<p>研究人员经常从大脑中寻找灵感，但他们建立的计算机模型往往不完全类似于他们的生物模型。然而，研究人员已经开始识别我们大脑内部神经网络的几乎完美的类似物。这方面的一个例子是ReLU激活函数。最近发现，当绘制时，我们大脑神经元的兴奋水平与ReLU图完全匹配。</p>

<p>在这一章中，我们将详细探讨卷积神经网络和递归神经网络。我们将看看他们如何解决在深度学习中复制精确视觉和记忆的问题。这两种新的网络或层类型是相当新的发现，但在一定程度上促成了深度学习的许多进步。本章将涵盖以下主题:</p>

<ul>

<li>卷积神经网络</li>

<li>理解卷积</li>

<li>建设自动驾驶的CNN</li>

<li>记忆和循环网络</li>

<li>用LSTMs玩石头、布、剪刀</li>

</ul>

<p>在继续之前，请确保您对前一章中概述的基础知识有相当好的理解。这包括运行安装本章所需依赖项的代码示例。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Convolutional neural networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">卷积神经网络</h1>

                

            

            

                

<p>视觉无疑是最常用的子过程。你现在正在使用它！当然，这是研究人员早期试图用神经网络模拟的东西，只是在卷积的概念被应用并用于图像分类之前，没有什么东西真正工作得很好。卷积的概念是检测(有时是分组)和隔离图像中的共同特征的思想。例如，如果你把一个熟悉的物体的照片的3/4遮盖起来给某人看，他们几乎肯定会通过只识别部分特征来识别图像。卷积以同样的方式工作，放大一幅图像，然后分离出特征供以后识别。</p>

<p>卷积的工作原理是将图像分解成其特征部分，这使得训练网络更加容易。让我们跳到一个代码示例，它从我们上一章停止的地方扩展，但现在引入了卷积。打开<kbd>Chapter_2_1.py</kbd>列表，按照以下步骤操作:</p>

<ol>

<li class="mce-root">看一看进行导入的前几行:</li>

</ol>

<pre style="color: black;padding-left: 60px">import tensorflow as tf<br/>from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras import backend as K</pre>

<ol start="2">

<li>在这个例子中，我们导入新的图层类型:<kbd>Conv2D</kbd>、<kbd>MaxPooling2D</kbd>和<kbd>UpSampling2D</kbd>。</li>

<li class="mce-root">然后，我们设置<kbd>Input</kbd>，并使用以下代码构建编码和解码的网络部分:</li>

</ol>

<pre style="color: black;padding-left: 60px">input_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format<br/><br/>x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)<br/>x = MaxPooling2D((2, 2), padding='same')(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>x = MaxPooling2D((2, 2), padding='same')(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>encoded = MaxPooling2D((2, 2), padding='same')(x)<br/><br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)<br/>x = UpSampling2D((2, 2))(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>x = UpSampling2D((2, 2))(x)<br/>x = Conv2D(16, (3, 3), activation='relu')(x)<br/>x = UpSampling2D((2, 2))(x)<br/>decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)</pre>

<ol start="4">

<li>首先要注意的是，我们现在保留图像的尺寸，在这种情况下，28 x 28像素宽，1层或通道。此示例使用灰度图像，因此只有一个颜色通道。这与之前大不相同，当时我们只是将图像分解成一个784维的向量。</li>

</ol>

<p style="padding-left: 60px">第二个要注意的是使用了<kbd>Conv2D</kbd>层或二维卷积层以及后面的<kbd>MaxPooling2D</kbd>或<kbd>UpSampling2D</kbd>层。汇集或采样图层用于收集或相反地揭示要素。请注意，当对图像进行编码时，我们如何在卷积后使用池化或下采样层，然后在对图像进行解码时使用上采样层。</p>

<ol start="5">

<li>接下来，我们用下面的代码块构建和训练模型:</li>

</ol>

<pre style="padding-left: 60px">autoencoder = Model(input_img, decoded)<br/>autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')<br/><br/>from tensorflow.keras.datasets import mnist<br/>import numpy as np<br/><br/>(x_train, _), (x_test, _) = mnist.load_data()<br/><br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) <br/>x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) <br/><br/>from tensorflow.keras.callbacks import TensorBoard<br/><br/>autoencoder.fit(x_train, x_train,<br/> epochs=50,<br/> batch_size=128,<br/> shuffle=True,<br/> validation_data=(x_test, x_test),<br/> callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])<br/><br/>decoded_imgs = autoencoder.predict(x_test)</pre>

<ol start="6">

<li>前面代码中模型的训练反映了我们在前一章结束时所做的事情，但是请注意现在训练集和测试集的选择。我们不再挤压图像，而是保留其空间属性作为卷积层的输入。</li>

<li>最后，我们用下面的代码输出结果:</li>

</ol>

<pre style="color: black;padding-left: 60px">n = 10<br/>plt.figure(figsize=(20, 4))<br/>for i in range(n):<br/>  ax = plt.subplot(2, n, i)<br/>  plt.imshow(x_test[i].reshape(28, 28))<br/>  plt.gray()<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)<br/>  ax = plt.subplot(2, n, i + n)<br/>  plt.imshow(decoded_imgs[i].reshape(28, 28))<br/>  plt.gray()<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)<br/>plt.show()</pre>

<ol start="8">

<li class="CDPAlignLeft CDPAlign">像以前一样运行代码，您会立即注意到它的训练速度大约慢了100倍。这可能需要您等待，也可能不需要，取决于您的机器；如果是这样的话，去喝一杯或三杯，或者吃一顿饭。</li>

</ol>

<p>训练我们的简单样本现在需要大量的时间，这在较旧的硬件上可能非常明显。在下一节中，我们将详细讨论如何开始监控培训课程。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Monitoring training with TensorBoard</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用TensorBoard监控训练</h1>

                

            

            

                

<p>TensorBoard本质上是一个数学图形或计算引擎，在处理数字方面表现非常好，因此我们在深度学习中使用它。该工具本身还很不成熟，但有一些非常有用的功能来监控训练练习。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>按照以下步骤开始对我们的样本进行监控培训:</p>

<ol>

<li class="mce-root">您可以通过在一个新的<strong> Anaconda </strong>或命令窗口中输入以下命令来监视培训会话，该命令窗口与您运行示例的目录/文件夹相同:</li>

</ol>

<pre style="color: black;padding-left: 60px">//first change directory to sample working folder<br/><strong>tensorboard --logdir=/tmp/autoencoder</strong></pre>

<ol start="2">

<li class="mce-root">这将启动一个TensorBoard服务器，您可以通过将浏览器导航到斜体的URL来查看输出，如您运行<kbd>TensorBoard</kbd>的窗口所示。它通常看起来如下所示:</li>

</ol>

<pre style="color: black;padding-left: 60px">TensorBoard 1.10.0 at <em><strong>http://DESKTOP-V2J9HRG:6006</strong></em> (Press CTRL+C to quit)<br/>or use<br/><strong>http://0.0.0.0:6000</strong></pre>

<ol start="3">

<li>注意，URL应该使用您的机器名，但是如果这样不行，请尝试第二种形式。如果出现提示，请确保允许端口<kbd>6000</kbd>、<kbd>6006</kbd>和/或<strong> TensorBoard </strong>应用程序通过您的防火墙。</li>

<li class="CDPAlignLeft CDPAlign">运行完示例后，您应该会看到以下内容:</li>

</ol>

<div><img class="alignnone size-medium wp-image-727 image-border" src="img/d666bad9-02b7-43c3-ae41-bee3a36635e2.png" style="width:25.00em;height:5.58em;"/><br/>

<br/>

Auto-encoding digits using convolution</div>

<ol start="5">

<li>回过头来比较这个例子和上一个例子的结果来自<a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">第1章</a>、<a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank"/> <a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank"/> <em>游戏深度学习</em>。请注意性能的提高。</li>

</ol>

<p>你立即想到的可能是，“<em>我们所经历的增加的训练时间值得吗？</em>“毕竟，解码后的图像看起来与前面的例子非常相似，而且训练速度更快，只是，请记住，我们正在通过在每次迭代中调整每个权重来缓慢训练网络权重，然后我们可以将其保存为模型。这个模型或大脑可以在不经过训练的情况下再次执行同样的任务。工程够恐怖！在我们学习本章的时候，请记住这个概念。在<a href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml" target="_blank"/> <a href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml">第三章</a>，<em> GAN for Games </em>中，我们将开始保存和移动我们的大脑模型。</p>

<p>在下一节中，我们将更深入地了解卷积的工作原理。当你第一次遇到卷积时，它可能很难理解，所以不要着急。理解它的工作原理很重要，因为我们将在后面广泛使用它。</p>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding convolution</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">理解卷积</h1>

                

            

            

                

<p><strong>卷积</strong>是一种从图像中提取特征的方法，可以让我们更容易地根据已知特征对其进行分类。在我们进入卷积之前，让我们先退后一步，理解为什么网络，以及我们对这个问题的看法，需要隔离图像中的特征。看看下面的内容；这是一个名为Sadie的狗的样本图像，应用了各种图像过滤器:</p>

<div><img src="img/fb6c51c0-04d3-40ec-9f20-5a342cefff6f.png" style="width:56.08em;height:18.58em;"/><br/>

<br/>

Example of an image with different filters applied</div>

<p>前面显示了未应用滤镜、边缘检测、像素化和发光边缘滤镜的四个不同版本。然而，在所有情况下，不管应用了什么样的滤波器，作为人类的你都可以清楚地识别出这是一张狗的图片，除了注意在边缘检测的情况下，我们已经消除了识别狗所不必要的额外图像数据。通过使用过滤器，我们可以提取我们的神经网络识别狗所需的特征。这就是卷积滤波器的全部功能，在某些情况下，其中一个滤波器可能只是简单的边缘检测。</p>

<p>卷积过滤器是定义单个数学运算的矩阵或数字核心。该过程从乘以左上角像素值开始，将矩阵运算的结果相加并设置为输出。内核以称为<strong>步幅</strong>的步长在图像上滑动，该操作如下所示:</p>

<div><img src="img/114cee40-0fd4-4978-b46d-c8d5f8a5882d.png" style="width:23.42em;height:20.67em;"/></div>

<p>应用卷积过滤器</p>

<p>在上图中，使用的步幅为1。卷积运算中应用的滤波器本质上是边缘检测滤波器。如果您查看最终操作的结果，您可以看到中间部分现在填充了OS，这大大简化了任何分类任务。我们的网络需要学习的信息越少，它们学习的速度就越快，数据也就越少。有趣的是，卷积学习滤波器、数字或权重，以便提取相关特征。这个不太明显，可能会比较混乱，我们再来过一遍。回到我们之前的例子，看看我们如何定义第一个卷积层:</p>

<pre>x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)</pre>

<p>在这一行代码中，我们将第一个卷积层定义为具有<kbd>16</kbd>个输出滤波器，这意味着该层的输出实际上是16个滤波器。然后我们将内核大小设置为<kbd>(3,3)</kbd>，它代表一个<kbd>3x3</kbd>矩阵，就像我们的例子一样。请注意，我们没有指定各种内核过滤器权重的值，因为这毕竟是网络正在训练要做的事情。</p>

<p class="mce-root">让我们看看当所有的东西都放在下图中时是什么样子:</p>

<p class="mce-root">卷积第一步的输出是特征图。一个特征图表示应用的单个卷积滤波器，并且通过应用学习的滤波器/内核来生成。在我们的例子中，第一层产生<strong> 16个内核</strong>，这些内核又产生<strong> 16个特征图</strong>；请记住，<kbd>16</kbd>的值是滤波器的数量。</p>

<p>卷积之后，我们应用池化或子采样，以便将特征收集或聚集到集合中。这种二次采样进一步创建了新的集中特征图，突出了我们正在训练的图像的重要特征。让我们回顾一下在前面的例子中我们是如何定义第一个池层的:</p>

<div><img src="img/7a13bb31-cf32-4ec2-838a-e066ef86f047.png" style="width:37.83em;height:11.50em;"/><br/>

<br/>

Full convolution operation</div>

<p>在代码中，我们使用<kbd>(2,2)</kbd>的<kbd>pool_size</kbd>进行子采样。大小表示通过宽度和高度对图像进行下采样的因子。因此，一个2 x 2大小的池将创建四个宽度和高度各为一半的要素地图。在我们的第一层卷积和合并之后，这导致总共64个特征地图。我们通过将16(卷积特征图)乘以4(汇集特征图)= 64个特征图来得到这个值。考虑在我们的简单示例中我们总共构建了多少个要素地图:</p>

<p><img class="fm-editor-equation" src="img/2061d9ed-16c1-475a-a519-bb47eff32601.png" style="width:39.42em;height:1.08em;"/></p>

<pre>x = MaxPooling2D((2, 2), padding='same')(x)</pre>

<p><img class="fm-editor-equation" src="img/d0f65917-9560-4f1b-8aae-c62ff29a9071.png" style="width:20.67em;height:1.08em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png" style="width:12.08em;height:1.00em;"/></p>

<p class="CDPAlignCenter CDPAlign">也就是65，536张4 x 4图像的特征地图。这意味着我们现在在65，536个较小的图像上训练我们的网络；对于每一幅图像，我们试图进行编码或分类。这显然是训练时间增加的原因，但也要考虑我们现在用于分类图像的额外数据量。现在，我们的网络正在学习如何识别我们图像的部分或特征，就像我们人类识别物体一样。</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png" style="width:12.08em;height:1.00em;"/></p>

<p>That is 65,536 feature maps of 4 x 4 images. This means we now train our network on 65,536 smaller images; for each image, we attempt to encode or classify. This is obviously the cause for the increased training time, but also consider the amount of extra data we are now using to classify our images. Now our network is learning how to identify parts or features of our image, just as we humans identify objects.</p>

<p class="mce-root">例如，如果你刚刚看到一只狗的鼻子，你可能会认出那是一只狗。因此，我们的样本网络现在正在识别手写数字的部分，正如我们现在所知，这极大地提高了性能。</p>

<p class="mce-root">正如我们所看到的，卷积对于识别图像非常有效，但是汇集的过程可能会对保持空间关系产生破坏性的后果。因此，当涉及到需要某种形式的空间理解的游戏或学习时，我们倾向于限制合用或完全消除。由于理解何时使用和不使用池很重要，我们将在下一节中更详细地讨论这一点。</p>

<p>建设自动驾驶的CNN</p>

<p>Nvidia在2017年创建了一个名为<strong> PilotNet </strong>的多层CNN，它能够通过向车辆显示一系列图像或视频来驾驶车辆。这是神经网络力量的一个令人信服的证明，尤其是卷积的力量。下图显示了PilotNet的神经架构:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building a self-driving CNN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">该图示出了从底部向上移动的网络输入，其中输出到单个神经元的单个输入图像的结果表示操纵方向。由于这是一个很好的例子，一些人已经发布了展示PilotNet例子的博客文章，其中一些实际上是有效的。我们将研究其中一篇博文的代码，看看如何用Keras构建类似的架构。接下来是PilotNet博客的原始图片，显示了我们的自动驾驶网络将用于训练的几种类型的图像:</h1>

                

            

            

                

<p>本例中训练的目标是输出方向盘应该转动的角度，以便保持车辆在道路上行驶。打开<kbd>Chapter_2_2.py</kbd>中的代码列表，并遵循以下步骤:</p>

<div><img src="img/dc9e1eee-fc4c-4305-9b0e-489e93ff06d6.png" style="width:20.25em;height:28.00em;"/><br/>

<br/>

PilotNet neural architecture</div>

<p>我们现在将切换到使用Keras的几个样本。虽然TensorFlow嵌入式版本的Keras为我们提供了很好的服务，但我们需要的一些功能只能在完整版本中找到。要安装Keras和其他依赖项，请打开一个shell或Anaconda窗口并运行以下命令:</p>

<div><img src="img/4cf9c76e-f157-44bb-a455-14ffb5240090.png" style="width:20.08em;height:19.92em;"/><br/>

<br/>

Example of PilotNet training images</div>

<p>在代码文件(<kbd>Chapter_2_2.py</kbd>)的开头，我们从一些导入开始，并使用以下代码加载样本数据:</p>

<ol>

<li class="mce-root">这段代码只是进行一些导入，然后从作者的源数据中下载示例驱动帧。这篇博客的原始资料由<strong>罗斯科的笔记本</strong>写在一个笔记本上，可以在<a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">https://wros Coe . github . io/keras-lane-following-auto pilot . html</a>找到。<br/> <kbd>pickle</kbd>是一个解压缩库，它解包前面清单底部的数据集<kbd>X</kbd>和<kbd>Y</kbd>中的数据。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>pip install keras</strong><br/><strong>pip install pickle</strong><br/><strong>pip install matplotlib</strong></pre>

<ol start="2">

<li>然后我们打乱帧的顺序，或者基本上随机化数据。我们经常以这种方式随机化数据，以使我们的训练更强。通过随机化数据顺序，网络需要学习图像的绝对导向值，而不是可能的相对或增量值。下面的代码会进行这种洗牌:</li>

</ol>

<pre style="padding-left: 60px">import os<br/>import urllib.request<br/>import pickle<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/><br/><em><strong>#downlaod driving data (450Mb)</strong> </em><br/>data_url = 'https://s3.amazonaws.com/donkey_resources/indoor_lanes.pkl'<br/>file_path, headers = urllib.request.urlretrieve(data_url)<br/>print(file_path)<br/><br/>with open(file_path, 'rb') as f:<br/>  X, Y = pickle.load(f)</pre>

<ol start="3">

<li>这段代码所做的就是使用<kbd>numpy</kbd>随机打乱图像帧。然后它打印出第一个混洗集合<kbd>shuffled_X</kbd>的长度，这样我们就可以确认训练数据没有丢失。</li>

<li>接下来，我们需要创建一个训练和测试数据集。训练集用于训练网络(权重)，测试集或验证集用于确认新数据或原始数据的准确性。正如我们之前看到的，这是使用监督训练或标记数据时的常见主题。我们经常把数据分成80%的训练和20%的测试。下面的代码就是这样做的:</li>

</ol>

<pre style="padding-left: 60px">import numpy as np<br/>def unison_shuffled_copies(X, Y):<br/>  assert len(X) == len(Y)<br/>  p = np.random.permutation(len(X))<br/>  return X[p], Y[p]<br/><br/>shuffled_X, shuffled_Y = unison_shuffled_copies(X,Y)<br/>len(shuffled_X)</pre>

<ol start="5">

<li>在创建了训练集和测试集之后，我们现在想要增加或扩展训练数据。在这个特殊的例子中，作者仅仅通过翻转原始图像并将它们添加到数据集来扩充数据。我们将在后面的章节中发现增加数据的许多其他方法，但这种简单有效的翻转方法是添加到您的机器学习工具带中的东西。实现这种翻转的代码如下所示:</li>

</ol>

<ol start="6">

<li>现在是最重要的部分。数据已经准备好了，现在是构建模型的时候了，如代码所示:</li>

</ol>

<pre style="padding-left: 60px">test_cutoff = int(len(X) * .8) # 80% of data used for training<br/>val_cutoff = test_cutoff + int(len(X) * .2) # 20% of data used for validation and test data<br/>train_X, train_Y = shuffled_X[:test_cutoff], shuffled_Y[:test_cutoff]<br/>val_X, val_Y = shuffled_X[test_cutoff:val_cutoff], shuffled_Y[test_cutoff:val_cutoff]<br/>test_X, test_Y = shuffled_X[val_cutoff:], shuffled_Y[val_cutoff:]<br/><br/>len(train_X) + len(val_X) + len(test_X)</pre>

<ol start="7">

<li>此时构建模型的代码应该是不言自明的。请注意架构中的变化以及代码是如何从我们之前的示例中编写出来的。还要注意突出显示的两行。第一个使用了一个新的图层类型叫做<kbd>Flatten</kbd>。这种图层类型所做的就是将2 x 2的图像展平成一个矢量，然后输入到一个标准的<kbd>Dense</kbd>隐藏的全连接图层中。第二个高亮显示的行引入了另一个新的图层类型，叫做<kbd>Dropout</kbd>。这种类型的图层需要更多的解释，在本节的最后会有更详细的介绍。</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">X_flipped = np.array([np.fliplr(i) for i in train_X])<br/>Y_flipped = np.array([-i for i in train_Y])<br/>train_X = np.concatenate([train_X, X_flipped])<br/>train_Y = np.concatenate([train_Y, Y_flipped])<br/>len(train_X)</pre>

<ol start="8">

<li>最后是训练部分，这段代码设置了这个部分:</li>

</ol>

<pre style="padding-left: 60px">from keras.models import Model, load_model<br/>from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, Flatten, Dense<br/><br/>img_in = Input(shape=(120, 160, 3), name='img_in')<br/>angle_in = Input(shape=(1,), name='angle_in')<br/><br/>x = Convolution2D(8, 3, 3)(img_in)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/>x = Convolution2D(16, 3, 3)(x)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/>x = Convolution2D(32, 3, 3)(x)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/><strong>merged = Flatten()(x)</strong><br/><br/>x = Dense(256)(merged)<br/>x = Activation('linear')(x)<br/><strong>x = Dropout(.2)(x)</strong><br/><br/>angle_out = Dense(1, name='angle_out')(x)<br/><br/>model = Model(input=[img_in], output=[angle_out])<br/>model.compile(optimizer='adam', loss='mean_squared_error')<br/>model.summary()</pre>

<ol start="9">

<li>这最后一段代码设置了一组<kbd>callbacks</kbd>来更新和控制训练。我们已经使用回调用日志更新TensorBoard服务器。在这种情况下，我们使用回调在每个检查点(epoch)后重新保存模型，并检查是否提前退出。注意我们保存模型的形式——一个<kbd>hdf5</kbd>文件。这种文件格式代表一种分层数据结构。</li>

<li>像以前一样运行代码。这个示例可能需要一段时间，所以请耐心等待。完成后，不会有输出，但要特别注意最小化损失值。</li>

</ol>

<pre style="padding-left: 60px">import os<br/>from keras import callbacks<br/><br/>model_path = os.path.expanduser('~/best_autopilot.hdf5')<br/><br/>save_best = callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=1, <br/> save_best_only=True, mode='min')<br/><br/>early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, <br/> verbose=0, mode='auto')<br/><br/>callbacks_list = [save_best, early_stop]<br/><br/>model.fit(train_X, train_Y, batch_size=64, epochs=4, validation_data=(val_X, val_Y), callbacks=callbacks_list)</pre>

<ol start="11">

<li>在你深度学习生涯的这一点上，你可能会意识到你需要更多的耐心或更好的计算机，或者支持TensorFlow的GPU。如果您想尝试后者，请随意下载并安装TensorFlow GPU库和您的操作系统所需的其他库，因为这可能会有所不同。大量的文档可以在网上找到。安装TensorFlow的GPU版本后，Keras会自动尝试使用它。如果你有一个受支持的GPU，你应该注意到性能的提高，如果没有，那么考虑购买一个。</li>

<li>虽然这个例子没有输出，但是为了简单起见，请尝试理解正在发生的事情。毕竟，这可以像一个驾驶游戏一样容易地设置，网络只需查看截图就可以驾驶车辆。我们省略了作者原始博文中的结果，但是如果你想进一步了解它的表现，请返回查看<a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">源链接</a>。</li>

</ol>

<p>作者在他的博客文章中做的一件事是使用池层，正如我们已经看到的，这在使用卷积时是非常标准的。然而，何时以及如何使用池化层现在还有点争议，需要进一步的详细讨论，这将在下一节中提供。</p>

<p>空间卷积和汇集</p>

<p>Geoffrey Hinton和他的团队最近强烈建议使用卷积合并来消除图像中的空间关系。辛顿建议使用<strong> CapsNet </strong>，或者<strong>胶囊网络</strong>。胶囊网络是一种保持数据空间完整性的池化方法。现在，这可能不是所有情况下的问题。对于手写数字，空间关系并不那么重要。然而，自动驾驶汽车或承担空间任务的网络，其中一个主要例子是游戏，在使用合用时往往表现不佳。事实上，Unity的团队在卷积后不使用池层；我们来了解一下原因。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Spatial convolution and pooling</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">汇集或下采样是一种通过收集数据的共同特征来扩充数据的方法。这样做的问题是，数据中的任何关系经常会完全丢失。下图演示了卷积映射上的<strong> MaxPooling(2，2) </strong>:</h1>

                

            

            

                

<p>即使在前面的简单图表中，您也可以很快意识到池化丢失了池化值开始所在的角(左上角、左下角、右下角和右上角)的空间关系。请注意，在几层合并之后，任何空间关系都将完全消失。</p>

<p>我们可以测试从模型中删除池层的效果，并按照以下步骤再次测试:</p>

<div><img src="img/1fa08f01-0b68-450b-9f82-d8cca463a995.png" style="width:32.17em;height:14.42em;"/><br/>

<br/>

 Max pooling at work</div>

<p>打开<kbd>Chapter_2_3.py</kbd>文件，注意我们是如何注释掉几个池层的，或者你也可以删除这些行，就像这样:</p>

<p>We can test the effect of removing pooling layers from the model and test this again by following these steps:</p>

<ol>

<li>Open the <kbd>Chapter_2_3.py</kbd> file and note how we commented out a couple of pooling layers, or you can just delete the lines as well, like so:</li>

</ol>

<pre style="padding-left: 60px">x = Convolution2D(8, 3, 3)(img_in)<br/>x = Activation('relu')(x)<br/><strong>x = MaxPooling2D(pool_size=(2, 2))(x)</strong><br/><br/>x = Convolution2D(16, 3, 3)(x)<br/>x = Activation('relu')(x)<br/><strong>#x = MaxPooling2D(pool_size=(2, 2))(x)</strong><br/><br/>x = Convolution2D(32, 3, 3)(x)<br/>x = Activation('relu')(x)<br/><strong>#x = MaxPooling2D(pool_size=(2, 2))(x)</strong></pre>

<p class="mce-root">请注意，我们没有注释掉(或删除)所有的池层，只保留了一个层。在某些情况下，您可能仍希望保留几个池化图层，以识别空间上不重要的要素。例如，当识别数字时，空间相对于整体形状来说不太重要。然而，如果我们考虑识别一张脸，那么一个人的眼睛、嘴巴等等之间的距离就是一张脸与另一张脸的区别。然而，如果你只是想识别一张脸，眼睛，嘴，等等，那么只应用池可能是完全可以接受的。</p>

<p class="mce-root">接下来，我们还增加了我们的<kbd>Dropout</kbd>层的辍学率，如下所示:</p>

<ol start="2">

<li>我们将在下一节详细探讨辍学问题。但是现在，只要意识到这个变化将对我们的模型产生更积极的影响。</li>

<li>最后，我们用下面的代码增加了<kbd>10</kbd>的纪元数:</li>

</ol>

<pre style="padding-left: 60px">x = Dropout(.5)(x)</pre>

<ol start="4">

<li>在我们之前的运行中，如果你在训练时观察损失率，你会发现上一个例子或多或少在四个时期开始收敛。因为删除池层也会减少训练数据，所以我们还需要增加历元的数量。请记住，池化或下采样会增加特征地图的数量，而更少的地图意味着网络需要更多的训练运行。如果你不是在GPU上训练，这个模型需要一段时间，所以要有耐心。</li>

<li>最后，再次运行这个例子，做一些小的修改。你会注意到的第一件事是训练时间急剧增加。请记住，这是因为我们的池层确实有助于更快的培训，但这是有成本的。这是我们允许单一池层的原因之一。</li>

</ol>

<pre style="padding-left: 60px">model.fit(train_X, train_Y, batch_size=64, <strong>epochs=10</strong>, validation_data=(val_X, val_Y), callbacks=callbacks_list)</pre>

<ol start="6">

<li>当样本运行结束时，比较我们之前运行的<kbd>Chapter_2_2.py</kbd>样本的结果。它达到你的预期了吗？</li>

<li>我们只关注这篇特别的博文，因为它展示得非常好，写得也非常好。作者显然知道他的东西，但这个例子只是表明，尽可能详细地理解这些概念的基础是多么重要。在信息泛滥的情况下，这不是一项容易的任务，但这也强化了一个事实，即开发有效的深度学习模型不是一项微不足道的任务，至少目前不是。</li>

<li>When the sample is finished running, compare the results for the <kbd>Chapter_2_2.py</kbd> sample we ran earlier. Did it do what you expected it to?</li>

</ol>

<p>既然我们理解了合并层的成本/代价，我们可以继续下一节，在这里我们跳回理解<kbd>Dropout</kbd>。这是一个你会反复使用的优秀工具。</p>

<p class="mce-root">辍学的必要性</p>

<p>现在，让我们回到我们非常需要的关于<kbd>Dropout</kbd>的讨论。我们在深度学习中使用dropout作为一种在每次迭代中随机切断层间网络连接的方式。下图中显示了一个示例，该示例显示了应用于三个网络层的下降迭代:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The need for Dropout</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">重要的是要明白，相同的连接并不总是被切断。这样做是为了让网络变得不那么专门化，而更加一般化。概括模型是深度学习中的一个常见主题，我们经常这样做，以便我们的模型可以更快地学习更广泛的问题。当然，有些时候，泛化网络会限制网络的学习能力。</h1>

                

            

            

                

<p>如果我们现在回到之前的示例，看看代码，我们会看到一个<kbd>Dropout</kbd>层被这样使用:</p>

<div><img class="alignnone size-medium wp-image-729 image-border" src="img/0588924a-6516-4bc0-8f6f-e217d96e8af1.png" style="width:25.00em;height:13.50em;"/><br/>

<br/>

Before and after dropout</div>

<p>这一行简单的代码告诉网络在每次迭代后随机地丢弃或断开50%的连接。丢弃仅适用于完全连接的层(<strong>输入</strong>-&gt;-<strong>密集</strong>-&gt;-<strong>密集</strong>)，但作为一种提高性能或精度的方式非常有用。这可能是也可能不是上一个示例中性能提高的原因。</p>

<p>在下一部分，我们将看看深度学习是如何模仿记忆子过程或时间气味的。</p>

<pre>x = Dropout(.5)(x)</pre>

<p>记忆和循环网络</p>

<p>记忆经常与<strong>递归神经网络</strong> ( <strong> RNN </strong>)联系在一起，但那并不完全是一种准确的关联。RNN实际上只对存储一系列事件有用，或者你可以称之为<strong>时间感</strong>，也可以称之为时间感。rnn通过在递归或循环中把状态保持回自身来做到这一点。这里显示了一个示例:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Memory and recurrent networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">展开递归神经网络</h1>

                

            

            

                

<p>该图显示的是一个递归神经元的内部表示，该神经元被设置为跟踪多个时间步长或迭代，其中<strong> x </strong>表示一个时间步长的输入，<strong> h </strong>表示状态。对于所有时间步长，<strong> W </strong>、<strong> U </strong>和<strong> V </strong>的网络权重保持不变，并使用称为<strong>时间反向传播</strong> ( <strong> BPTT </strong>)的技术进行训练。我们不会进入BPTT的数学，让读者自己去发现，而只是意识到循环网络中的网络权重使用成本梯度方法来优化它们。</p>

<div><img src="img/e37e1983-cc7b-4826-818a-aa84968820fb.png" style="width:43.67em;height:14.58em;"/></div>

<p>递归网络允许神经网络识别元素序列，并预测接下来通常会出现什么元素。这在预测文本、股票，当然还有游戏方面有巨大的应用。几乎所有能够从对时间或事件序列的把握中受益的活动都将从使用RNN中受益，但标准RNN除外，前面显示的类型由于梯度问题而无法预测更长的序列。我们将在下一节深入探讨这个问题和解决方案。</p>

<p>LSTM拯救的消失和爆炸渐变</p>

<p>RNN面临的问题不是渐变消失就是渐变爆炸。这是因为，随着时间的推移，我们试图最小化或降低的梯度变得如此之小或如此之大，以至于任何额外的训练都没有效果。这限制了RNN的实用性，但幸运的是，这个问题已经用<strong>长短期记忆</strong> <em> ( </em> <strong> LSTM </strong>)块纠正了，如下图所示:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Vanishing and exploding gradients rescued by LSTM</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">LSTM区块克服了消失梯度问题使用一些技术。在内部，在图中你看到一个圆圈内的<strong> x </strong>，它表示一个由激活功能控制的门。在图中，激活功能是<strong> σ </strong>和<strong> tanh </strong>。这些激活函数的工作方式很像step或ReLU，我们可以在常规网络层中使用这两个函数中的任何一个来激活。在很大程度上，我们会把LSTM当作一个黑盒，你需要记住的只是lstm克服了RNN的梯度问题，可以记住长期序列。</h1>

                

            

            

                

<p>The problem the RNN suffers from is either vanishing or exploding gradients. This happens because, over time, the gradient we try to minimize or reduce becomes so small or big that any additional training has no effect. This limits the usefulness of the RNN, but fortunately this problem was corrected with <strong>Long Short-Term Memory</strong><em> (</em><strong>LSTM</strong>) blocks, as shown in this diagram:</p>

<div><img src="img/c2cfd7cf-6e48-4993-8751-2d580fc8901f.png" style="width:47.58em;height:21.17em;"/><br/>

<br/>

Example of an LSTM block</div>

<p>LSTM blocks overcome the vanishing gradient problem using a few techniques. Internally, in the diagram where you see a <strong>x</strong> inside a circle, it denotes a gate controlled by an activation function. In the diagram, the activation functions are <strong>σ</strong> and <strong>tanh</strong>. These activation functions work much like a step or ReLU do, and we may use either function for activation in a regular network layer. For the most part, we will treat an LSTM as a black box, and all you need to remember is that LSTMs overcome the gradient problem of RNN and can remember long-term sequences.</p>

<p class="mce-root">让我们看一个工作示例，看看这两者是如何结合在一起的。打开<kbd>Chapter_2_4.py</kbd>并遵循以下步骤:</p>

<p class="mce-root">我们像往常一样从导入我们需要的各种Keras片段开始，如下所示:</p>

<p>这个例子摘自<a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/">https://machine learning mastery . com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/</a>。这是一个由杰森·布朗利博士主持的网站，他有很多很好的例子来解释LSTM和循环网络的使用。</p>

<ol>

<li>这次我们导入两个新的类，<kbd>Sequential</kbd>和<kbd>LSTM</kbd>。我们当然知道<kbd>LSTM</kbd>是干什么用的，但是<kbd>Sequential</kbd>呢？<kbd>Sequential</kbd>是一种模型形式，它以一个接一个的顺序定义层。我们以前不太担心这个细节，因为我们以前的模型都是顺序的。</li>

</ol>

<p>接下来，我们将随机种子设置为一个已知值。我们这样做是为了让我们的例子可以自我复制。在前面的例子中，您可能已经注意到，并不是所有的运行都有相同的表现。在许多情况下，我们希望我们的训练是一致的，因此我们通过使用以下代码来设置一个已知的种子值:</p>

<pre style="padding-left: 60px">import numpy<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from keras.utils import np_utils</pre>

<ol start="2">

<li>重要的是要认识到这只是设置了<kbd>numpy</kbd>随机种子值。其他库可能使用不同的随机数生成器，并需要不同的种子设置。我们将尽可能在将来找出这些不一致的地方。</li>

<li>接下来，我们需要确定我们要训练的序列；在这种情况下，我们将只使用如代码所示的<kbd>alphabet</kbd>:</li>

</ol>

<pre style="padding-left: 60px">numpy.random.seed(7)</pre>

<ol start="4">

<li>前面的代码将我们的字符序列构建为整数，并构建每个字符序列的映射。它构建了一个显示前进和后退位置的<kbd>seq_in</kbd>和<kbd>seq_out</kbd>。既然序列的长度是由<kbd>seq_length = 1</kbd>定义的，那么我们只关心字母表中的一个字母和它后面的字符。当然，你可以做更长的片段。</li>

<li>构建好序列数据后，是时候用下面的代码对数据进行整形和规范化了:</li>

</ol>

<pre style="padding-left: 60px">alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"<br/><br/>char_to_int = dict((c, i) for i, c in enumerate(alphabet))<br/>int_to_char = dict((i, c) for i, c in enumerate(alphabet))<br/><br/>seq_length = 1<br/>dataX = []<br/>dataY = []<br/><br/>for i in range(0, len(alphabet) - seq_length, 1):<br/>  seq_in = alphabet[i:i + seq_length]<br/>  seq_out = alphabet[i + seq_length]<br/>  dataX.append([char_to_int[char] for char in seq_in])<br/>  dataY.append(char_to_int[seq_out])<br/>  print(seq_in, '-&gt;', seq_out)</pre>

<ol start="6">

<li>前面代码中的第一行将数据重新整形为一个张量，其大小长度为<kbd>dataX</kbd>，步骤或序列的数量，以及要识别的特征的数量。然后我们将数据标准化。规范化数据有多种形式，但在这种情况下，我们是从0到1规范化值。然后我们对输出进行编码，以便于训练。</li>

<li>一种热门的编码方式是在有数据或响应的地方将值设置为1，在其他地方设置为零。在本例中，我们的模型输出是26个神经元，也可以用26个零来表示，每个神经元一个零，如下:<br xmlns:epub="http://www.idpf.org/2007/ops"/><strong xmlns:epub="http://www.idpf.org/2007/ops">000000000000000000<br/></strong><br xmlns:epub="http://www.idpf.org/2007/ops"/>其中每个零表示字母表中匹配的字符位置。如果我们想要表示一个字符<strong xmlns:epub="http://www.idpf.org/2007/ops"> A </strong>，我们将输出一个热编码值如下:<br xmlns:epub="http://www.idpf.org/2007/ops"/>100000000000000000000</li>

</ol>

<pre style="padding-left: 60px">X = numpy.reshape(dataX, (len(dataX), seq_length, 1))<br/># normalize<br/>X = X / float(len(alphabet))<br/># one hot encode the output variable<br/>y = np_utils.to_categorical(dataY)</pre>

<ol start="8">

<li>The first line in the preceding code reshapes the data into a tensor with a size length of <kbd>dataX</kbd>, the number of steps or sequences, and the number of features to identify. We then normalize the data. Normalizing the data comes in many forms, but in this case we are normalizing values from 0 to 1. Then we one hot encode the output for easier training.</li>

</ol>

<p>然后，我们构建模型，使用与我们之前看到的略有不同的代码形式，如下所示:</p>

<p>前面代码的关键部分是突出显示的一行，显示了<kbd>LSTM</kbd>层的构造。我们通过设置单位的数量来构建一个<kbd>LSTM</kbd>层，在这里是<kbd>32</kbd>，因为我们的序列有26个字符长，我们希望我们的单位在<kbd>2</kbd>之前被禁用。然后我们设置<kbd>input_shape</kbd>来匹配之前的张量<kbd>X</kbd>，我们创建它来保存我们的训练数据。在这种情况下，我们只是设置形状来匹配所有的字符(26)和序列长度，在这种情况下是<kbd>1</kbd>。</p>

<ol start="9">

<li>最后，我们用下面的代码输出模型:</li>

</ol>

<pre style="padding-left: 60px">model = Sequential()<br/><strong>model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))</strong><br/>model.add(Dense(y.shape[1], activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.fit(X, y, epochs=500, batch_size=1, verbose=2)<br/><br/>scores = model.evaluate(X, y, verbose=0)<br/>print("Model Accuracy: %.2f%%" % (scores[1]*100))</pre>

<ol start="10">

<li>像平常一样运行代码并检查输出。你会注意到准确率在80%左右。看看能否提高模型预测字母表中下一个序列的准确性。</li>

<li>这个简单的例子演示了LSTM块识别简单序列的基本用法。在下一节中，我们来看一个更复杂的例子:用LSTM玩石头、布、剪刀。</li>

</ol>

<pre style="padding-left: 60px">for pattern in dataX:<br/>  x = numpy.reshape(pattern, (1, len(pattern), 1))<br/>  x = x / float(len(alphabet))<br/>  prediction = model.predict(x, verbose=0)<br/>  index = numpy.argmax(prediction)<br/>  result = int_to_char[index]<br/>  seq_in = [int_to_char[value] for value in pattern]<br/>  print(seq_in, "-&gt;", result)</pre>

<ol start="12">

<li>用LSTMs玩石头、布、剪刀</li>

</ol>

<p>记忆数据序列在许多领域都有巨大的应用，不仅仅包括游戏。当然，产生一个简单、干净的例子是另一回事。幸运的是，互联网上的例子比比皆是，<kbd>Chapter_2_5.py</kbd>展示了一个用LSTM玩石头、布、剪刀的例子。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Playing Rock, Paper, Scissors with LSTMs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Playing Rock, Paper, Scissors with LSTMs</h1>

                

            

            

                

<p>Remembering sequences of data have huge applications in many areas, not the least of which includes gaming. Of course, producing a simple, clean example is another matter. Fortunately, examples abound on the internet and <kbd>Chapter_2_5.py</kbd> shows an example of using an LSTM to play Rock, Paper, Scissors.</p>

<p class="mce-root">打开示例文件，按照以下步骤操作:</p>

<p class="mce-root">这个例子来自https://github.com/hjpulkki/RPS，但是代码需要在几个地方进行调整才能为我们所用。</p>

<p>让我们像往常一样开始进口。对于此示例，请确保像我们在上一组练习中所做的那样安装Keras:</p>

<p>然后，我们设置一些常量，如下所示:</p>

<ol>

<li>然后，我们建立模型，这一次有三个LSTM层，一个用于我们序列中的每个元素(石头、布、剪刀)，像这样:</li>

</ol>

<pre style="padding-left: 60px">import numpy as np<br/>from keras.utils import np_utils<br/>from keras.models import Sequential<br/>from keras.layers import Dense, LSTM</pre>

<ol start="2">

<li>Then, we set some constants as shown:</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">EPOCH_NP = 100<br/>INPUT_SHAPE = (1, -1, 1)<br/>OUTPUT_SHAPE = (1, -1, 3)<br/>DATA_FILE = "data.txt"<br/>MODEL_FILE = "RPS_model.h5"</pre>

<ol start="3">

<li>Then, we build the model, this time with three LSTM layers, one for each element in our sequence (rock, paper and scissors), like so:</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">def simple_model(): <br/>  new_model = Sequential()<br/>  new_model.add(LSTM(output_dim=64, input_dim=1, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(Dense(64, activation='relu'))<br/>  new_model.add(Dense(64, activation='relu'))<br/>  new_model.add(Dense(3, activation='softmax'))<br/>  new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])<br/>  return new_model</pre>

<p class="mce-root">然后我们创建一个函数来从<kbd>data.txt</kbd>文件中提取数据。该文件包含使用以下代码的训练数据序列:</p>

<p class="mce-root">在这个例子中，我们按照与文件中相同的顺序，通过100个时期训练每个训练块。更好的方法是以随机顺序训练每个训练序列。</p>

<ol start="4">

<li>然后我们创建模型:</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">def batch_generator(filename): <br/>  with open('data.txt', 'r') as data_file:<br/>    for line in data_file:<br/>      data_vector = np.array(list(line[:-1]))<br/>      input_data = data_vector[np.newaxis, :-1, np.newaxis]<br/>      temp = np_utils.to_categorical(data_vector, num_classes=3) <br/>      output_data = temp[np.newaxis, 1:]<br/>      yield (input_data, output_data)</pre>

<ol start="5">

<li>使用循环训练数据，每次迭代从<kbd>data.txt</kbd>文件中提取一批数据:</li>

<li>最后，我们使用验证序列评估结果，如以下代码所示:</li>

</ol>

<pre class="mce-root" style="padding-left: 60px"># Create model<br/>np.random.seed(7)<br/>model = simple_model()</pre>

<ol start="7">

<li>像平常一样运行样本。检查最后的结果，并注意模型在预测序列时的准确性。</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">for (input_data, output_data) in batch_generator('data.txt'):<br/>  try:<br/>    model.fit(input_data, output_data, epochs=100, batch_size=100)<br/>  except:<br/>    print("error")</pre>

<ol start="8">

<li>请务必运行这个简单的例子几次，并了解LSTM层是如何设置的。请特别注意参数及其设置方式。</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">print("evaluating")<br/>validation = '100101000110221110101002201101101101002201011012222210221011011101011122110010101010101'<br/>input_validation = np.array(list(validation[:-1])).reshape(INPUT_SHAPE)<br/>output_validation = np_utils.to_categorical(np.array(list(validation[1:]))).reshape(OUTPUT_SHAPE)<br/>loss_and_metrics = model.evaluate(input_validation, output_validation, batch_size=100)<br/><br/>print("\n Evaluation results")<br/><br/>for i in range(len(loss_and_metrics)):<br/>  print(model.metrics_names[i], loss_and_metrics[i])<br/><br/>input_test = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]).reshape(INPUT_SHAPE)<br/>res = model.predict(input_test)<br/>prediction = np.argmax(res[0], axis=1)<br/>print(res, prediction)<br/><br/>model.save(MODEL_FILE)<br/>del model</pre>

<ol start="9">

<li>这就结束了我们对如何使用递归aka LSTM块来识别和预测数据序列的快速了解。当然，在本书的整个过程中，我们会多次使用这种多功能图层类型。</li>

</ol>

<p>在本章的最后一节，我们再次展示了一些鼓励你为了自己的利益而进行的练习。</p>

<p>练习</p>

<p>利用自己的时间完成下列练习，并改善自己的学习体验。提高你对材料的理解会让你成为一个更成功的深度学习者，你也会更喜欢这本书:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在<kbd>Chapter_2_1.py</kbd>的例子中，改变<kbd>Conv2D</kbd>图层使用不同的滤镜大小。再次运行该示例，看看这对训练性能和准确性有什么影响。</h1>

                

            

            

                

<p>注释掉或删除几个<kbd>Chapter_2_1.py</kbd>例子中的<kbd>MaxPooling</kbd>层和相应的<kbd>UpSampling</kbd>层。请记住，如果您删除第2层和第3层之间的池层，您同样需要删除上采样以保持一致。再次运行该示例，看看这对训练时间、准确性和性能有什么影响。</p>

<ol>

<li>使用不同的过滤器尺寸改变<kbd>Chapter_2_2.py</kbd>示例中的<strong> Conv2D </strong>层。看看这对训练有什么影响。</li>

<li>使用<strong> 2 </strong>的跨距值改变<kbd>Chapter_2_2.py</kbd>示例中的<strong> Conv2D </strong>层。为此，您可能需要查阅<strong> Keras </strong>文档。看看这对训练有什么影响。</li>

<li>通过改变池尺寸来改变<kbd>Chapter_2_2.py</kbd>示例中的<strong>最大池</strong>层。看看这对训练有什么影响。</li>

</ol>

<ol start="4">

<li>移除所有或注释掉在<kbd>Chapter_2_3.py</kbd>示例中使用的不同<strong> MaxPooling </strong>层。如果所有的池层都被注释掉，会发生什么？你现在需要增加训练次数吗？</li>

<li>在本章使用的各种示例中，更改<strong> Dropout </strong>的用法。这包括增加辍学。测试使用不同辍学水平的影响。</li>

<li>修改<kbd>Chapter_2_4.py</kbd>中的样本，使模型产生更好的精度。为了提高培训绩效，你需要做些什么？</li>

<li>在<kbd>Chapter_2_4.py</kbd>中修改样本以预测序列中的多个字符。如果您需要帮助，请返回并查看原始博客帖子以获取更多信息。</li>

<li>如果你改变<kbd>Chapter_2_5.py</kbd>例子中三个<strong> LSTM </strong>层使用的单位数量会发生什么？如果将值增加到128、32或16会怎么样？尝试这些值以了解它们的效果。</li>

<li>你可以随意扩展这些练习。试着自己写一个新的例子，即使只是一个简单的例子。学习编码的最好方法就是自己编写代码。</li>

<li>What happens if you change the number of units that the three <strong>LSTM</strong> layers use in the <kbd>Chapter_2_5.py</kbd> example? What if you increase the value to 128, 32, or 16? Try these values to understand the effect they have.</li>

</ol>

<p>Feel free to expand on these exercises on your own. Try to write a new example on your own as well, even if it is just a simple one. There really is no better way to learn to code than to write your own.</p>

<p class="mce-root">摘要</p>

<p class="mce-root">对于本章和最后一章，我们深入研究了深度学习和神经网络的核心元素。虽然我们对前几章的回顾并不全面，但它应该会为你继续阅读本书的其余部分打下良好的基础。如果你对前两章的任何内容有困难，现在回头花更多的时间复习前面的内容。理解神经网络架构的基础和各种专门层的使用是很重要的，正如我们在本章中所涉及的(CNN和RNN)。请确保您了解CNN的基本知识，如何有效地使用它来选择特征，以及在使用汇集或子采样时的权衡。还要理解RNN的概念，以及如何和何时使用LSTM块来预测或检测时间事件。卷积层和LSTM块现在是深度学习的基本组件，我们将在未来构建的几个网络中使用它们。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在下一章，我们开始为这本书构建我们的样本游戏，并介绍GANs，或生成性对抗网络。我们将探索gan以及它们如何被用来生成游戏内容。</h1>

                

            

            

                

<p>For this chapter and the last, we took a deep dive into the core elements of deep learning and neural networks. While our review in the last couple chapters was not extensive, it should give you a good base for continuing through the rest of the book. If you had troubles with any of the material in the first two chapters, turn back now and spend more time reviewing the previous material. It is important that you understand the basics of neural network architecture and the use of various specialized layers, as we covered in this chapter (CNN and RNN). Be sure you understand the basics of CNN and how to use it effectively in picking features and what the trade—offs are when using pooling or sub sampling. Also understand the concept of RNN and how and when to use LSTM blocks for predicting or detecting temporal events. Convolutional layers and LSTM blocks are now fundamental components of deep learning, and we will use them in several networks we build going forward.  </p>

<p>In the next chapter, we start to build out our sample game for this book and introduce GANs, or generative adversarial networks. We will explore GANs and how they can be used to generate game content.</p>





            



            

        

    </body>



</html></body></html>