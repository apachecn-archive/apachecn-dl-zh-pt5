<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>GAN for Games</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">游戏用甘</h1>

                

            

            

                

<p class="mce-root">到目前为止，在我们的深度学习探索中，我们已经使用一种叫做<strong>监督训练</strong>的技术训练了我们所有的网络。当你花时间去识别和标记你的数据时，这种训练技术很有效。我们之前的所有示例练习都使用了监督训练，因为这是最简单的教学形式。然而，监督学习往往是最麻烦和乏味的方法，这主要是因为它需要在训练之前进行一些数据标记或识别。已经有人尝试在游戏和模拟中使用这种形式的训练进行机器学习或深度学习，但他们被证明是不成功的。</p>

<p class="mce-root">这就是为什么在本书的大部分时间里，我们将着眼于其他形式的训练，从一种被称为<strong>生成对抗网络</strong> ( <strong>甘</strong>)的无监督训练形式开始。基本上，GANs可以通过双人游戏来训练自己。这使得它们成为我们学习的理想下一步，也是真正开始为游戏生成内容的完美方式。</p>

<p>在这一章中，我们将探索gan及其在开发游戏内容中的应用。在这个过程中，我们将学习更多深度学习技术的基础知识。在本章中，我们将涵盖以下内容:</p>

<ul>

<li>GANs简介</li>

<li>用Keras编码GAN</li>

<li>瓦瑟斯坦·甘</li>

<li>GAN用于创建纹理</li>

<li>用GAN创作音乐</li>

<li>练习</li>

</ul>

<p>众所周知，GANs很难训练和成功建造。因此，建议您慢慢阅读本章内容，如果需要的话，可以反复练习几次。我们学习的有效的GANs的技巧将会让你对训练网络和许多其他可用的选择有一个更好的整体理解。我们还需要涵盖许多关于训练网络的基本概念，所以请通读本章。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Introducing GANs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">GANs简介</h1>

                

            

            

                

<p>GANs的概念通常是用两人游戏的类比来介绍的。在这个游戏中，通常有一个艺术专家和一个艺术伪造者。艺术品伪造者或伪造者的目标是制造一个足够令人信服的赝品来愚弄艺术专家，从而赢得这场游戏。这是如何首先被描绘成神经网络的示例如下:</p>

<div><img src="img/5736d34f-71e3-4723-acce-24916b356499.png" style="width:41.25em;height:15.00em;"/><br/>

<br/>

GAN by Ian and others</div>

<p>在上图中，生成器取代了艺术伪造者的位置，即试图击败艺术专家的人，显示为鉴别者。生成器使用随机噪声作为源来生成图像，目标是图像足够有说服力以欺骗鉴别器。鉴别器在真实和伪造的图像上被训练，它所做的只是将图像分类为真实或伪造。然后，生成器被训练来制造一个足够令人信服的赝品，来愚弄鉴别者。虽然这个概念作为一种自我训练网络的方式看起来足够简单，但是在过去几年中，这种对抗技术的实施已经在许多领域中被证明是异常的。</p>

<p>gan是由蒙特利尔大学的Ian Goodfellow等人在2014年首先开发的。在短短的几年里，这项技术已经发展成为许多广泛而多样的应用，从生成图像和文本到制作静态图像动画，所有这些都是在很短的时间内完成的。以下是深度学习社区中一些令人印象深刻的GAN改进/实施的简短总结:</p>

<ul>

<li>深度卷积gan(<strong xmlns:epub="http://www.idpf.org/2007/ops">DC gan</strong>):这些是对我们刚刚讨论的标准架构的第一次重大改进。我们将在本章的下一节中探讨这种GAN的第一种形式。</li>

<li><strong>对抗性自动编码器GAN </strong>:这种自动编码器的变种使用对抗性GAN技术来隔离数据的属性。它在确定数据中的潜在关系方面有着有趣的应用，比如能够区分一组手写数字的风格和内容。</li>

<li><strong>辅助分类器GAN </strong>:这是另一个增强的GAN，与条件或条件GAN相关。它已经被证明可以合成更高分辨率的图像，当然值得在游戏中进一步探索。</li>

<li>CycleGAN :这是一个令人印象深刻的变化，它允许风格从一个图像转换到另一个图像。有大量的例子表明，这种形式的甘被用来设计一幅画，就像梵高画的那样，来交换名人的面孔。如果这一章激起了你对GANs的兴趣，你想探索这种形式，看看这篇文章:<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://hardikbansal.github.io/CycleGANBlog/">https://hardikbansal.github.io/CycleGANBlog/</a>。</li>

<li>条件GANS :这是一种半监督学习的形式。这意味着训练数据被标记，但是带有元数据或属性。因此，你可以标记书写风格(草书或印刷体)，而不是将MNIST数据集中的手写数字标记为9。然后，这种新形式的条件化甘不仅可以学习数字，还可以学习它们是草书还是印刷体。这种形式的GAN已经显示出一些有趣的应用，当我们谈到游戏中的具体应用时，我们将进一步探索这种应用。</li>

<li><strong> DiscoGAN </strong>:这是另一种形式的GAN展示有趣的结果，从交换名人发型到性别。这个GAN提取特征或域，并允许您将它们转移到其他图像或数据空间。这种GAN在游戏中有许多应用，当然值得感兴趣的读者进一步探索。</li>

<li><strong> DualGAN </strong>:使用双GANs训练两个生成器对抗两个鉴别器，以便将图像或数据传输到其他样式。这将是一种非常有用的重新设计多种资源的方式，并且可以很好地为游戏生成不同形式的艺术内容。</li>

<li><strong>最小平方GAN </strong> ( <strong> LSGAN </strong>):这使用了一种不同的计算损耗的形式，并且已经被证明比DCGAN更有效。</li>

<li><strong> pix2pixGAN </strong>:这是对条件GANs的扩展，允许它从一个图像到另一个图像传输或生成多个特征。这允许对象草图的图像返回同一对象的实际3D渲染图像，反之亦然。虽然这是一个非常强大的GAN，但它仍然是非常研究驱动的，可能还没有准备好用于游戏。也许你只需要等六个月或一年。</li>

<li><strong> InfoGANs </strong>:这些类型的GANs，到目前为止，被广泛用于探索关于训练数据的特征或信息。例如，它们可用于识别MNIST数据集中某个数字的旋转。此外，它们还经常被用作识别条件性GAN训练属性的一种方式。</li>

<li><strong>堆叠或SGAN </strong>:这是GAN的一种形式，它将自身分解成多层，每一层都是一个发生器和鉴别器，相互竞争。这使得整体GAN更容易训练，但也需要你了解每个阶段或层的一些细节。如果您刚刚开始，这不是适合您的GAN，但是当您构建更复杂的网络时，请再次回顾这个模型。</li>

<li><strong> Wasserstein GANs </strong>:这是一个最先进的GAN，它也将在本章自己的部分得到关注。损耗的计算就是GAN这种形式的改进。</li>

<li><strong> WassGANs </strong>:这使用Wasserstein距离来确定损失，这极大地有助于模型收敛。</li>

</ul>

<p>在学习本章的过程中，我们将进一步探索特定GAN实施的实例。在这里，我们将看看如何用GAN生成游戏纹理和音乐。不过现在，让我们继续下一节，学习如何在Keras中编写GAN。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Coding a GAN in Keras</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">用Keras编码GAN</h1>

                

            

            

                

<p>当然，最好的学习方法是实践，所以让我们开始编写我们的第一个GAN。在本例中，我们将构建基本的DCGAN，然后根据我们的目的修改它。打开<kbd>Chapter_3_2.py</kbd>并遵循以下步骤:</p>

<p>这段代码最初是从https://github.com/eriklindernoren/Keras-GAN<a href="https://github.com/eriklindernoren/Keras-GAN"/>摘录的，这是喀拉斯地区GANs的最佳代表，这都要归功于Erik Linder-Norén。干得好，感谢你的辛勤工作，Erik。<br/> <br/>增加了一个香草甘的备选清单作为<kbd>Chapter_3_1.py</kbd>供您愉快地学习。</p>

<ol>

<li class="mce-root">我们从导入库开始:</li>

</ol>

<pre style="color: black;padding-left: 60px">from __future__ import print_function, division<br/>from keras.datasets import mnist<br/>from keras.layers import Input, Dense, <strong>Reshape</strong>, Flatten, Dropout<br/>from keras.layers import <strong>BatchNormalization</strong>, Activation, <strong>ZeroPadding2D</strong><br/>from keras.layers.advanced_activations import <strong>LeakyReLU</strong><br/>from keras.layers.convolutional import UpSampling2D, Conv2D<br/>from keras.models import Sequential, <strong>Model</strong><br/>from keras.optimizers import <strong>Adam</strong><br/>import matplotlib.pyplot as plt<br/>import sys<br/>import numpy as np</pre>

<ol start="2">

<li>在前面的代码中引入了几个突出显示的新类型:<kbd>Reshape</kbd>、<kbd>BatchNormalization</kbd>、<kbd>ZeroPadding2D</kbd>、<kbd>LeakyReLU</kbd>、<kbd>Model</kbd>和<kbd>Adam</kbd>。接下来，我们将更详细地探讨每一种类型。</li>

<li class="mce-root">我们之前的大多数例子都使用基本脚本。我们现在正处于这样一个阶段，我们希望构建我们自己的类型(类)供以后进一步使用。这意味着我们现在开始像这样定义我们的类:</li>

</ol>

<pre style="color: black;padding-left: 60px">class DCGAN():</pre>

<ol start="4">

<li>因此，我们为深度卷积GAN的实现创建了一个名为<kbd>DCGAN</kbd>的新类(类型)。</li>

<li>接下来，我们通常会按照Python约定定义我们的<kbd>init</kbd>函数。然而，出于我们的目的，让我们先来看看<kbd>generator</kbd>函数:</li>

</ol>

<pre style="padding-left: 60px">def build_generator(self):<br/>  model = Sequential()<br/>  model.add(Dense(128 * 7 * 7, activation="relu", input_dim=self.latent_dim))<br/>  model.add(<strong>Reshape</strong>((7, 7, 128)))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(128, kernel_size=3, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(64, kernel_size=3, padding="same")) <br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(Conv2D(self.channels, kernel_size=3, padding="same"))<br/>  model.add(Activation("<strong>tanh</strong>"))<br/>  model.summary()<br/><br/>  noise = Input(shape=(self.latent_dim,))<br/>  img = model(noise)<br/>  return <strong>Model</strong>(noise, img)</pre>

<p class="mce-root"><kbd>build_generator</kbd>函数构建艺术伪造者模型，这意味着它获取噪声样本集，并试图将其转换为鉴别器会相信是真实的图像。在这种形式中，它使用卷积的原理使其更有效，只是在这种情况下，它生成噪声的特征图，然后将其变成真实的图像。从本质上讲，生成器正在做与识别图像相反的事情，而是试图基于特征图生成图像。<br/>在前面的代码块中，请注意输入是如何从噪声的<kbd>128, 7x7</kbd>特征图开始，然后使用<kbd>Reshape</kbd>层将其转换为我们想要创建的正确图像布局。然后，它对特征图进行上采样(与池化或下采样相反)至2x大小(14 x 14)，训练另一层卷积，然后进行更多的上采样(2x至28 x 28)，直到生成正确的图像大小(MNIST为28x28)。我们还看到了一种叫做<kbd>BatchNormalization</kbd>的新图层类型的使用，稍后我们将对此进行更详细的介绍。</p>

<p class="mce-root">接下来，我们将像这样构建<kbd>build_discriminator</kbd>函数:</p>

<ol start="6">

<li>The <kbd>build_generator</kbd> function builds the art-forger model, which means it takes that sample set of noise and tries to convert it into an image the discriminator will believe is real. In this form, it uses the principle of convolution to make it more efficient, except, in this case, it generates a feature map of noise that it then turns into a real image. Essentially, the generator is doing the opposite of recognizing an image, but instead trying to generate an image based on feature maps. <br/>

In the preceding block of code, note how the input starts with <kbd>128, 7x7</kbd> feature maps of noise then uses a <kbd>Reshape</kbd> layer to turn it into the proper image layout we want to create. It then up-samples (the reverse of pooling or down-sampling) the feature map into 2x size (14 x 14), training another layer of convolution followed by more up-sampling (2x to 28 x 28) until the correct image size (28x28 for the MNIST) is generated. We also see the use of a new layer type called <kbd>BatchNormalization</kbd>, which we will cover in more detail shortly.</li>

<li class="mce-root">Next, we will build the <kbd>build_discriminator</kbd> function like so:</li>

</ol>

<pre style="color: black;padding-left: 60px">def build_discriminator(self):<br/>  model = Sequential()<br/>  model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding="same"))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))<br/>  model.add(<strong>ZeroPadding2D</strong>(padding=((0,1),(0,1))))<br/>  model.add(B<strong>atchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Flatten())<br/>  model.add(Dense(1, activation='sigmoid'))<br/>  model.summary()<br/><br/>  img = Input(shape=self.img_shape)<br/>  validity = model(img)<br/>  return <strong>Model</strong>(img, validity)</pre>

<p class="mce-root">这一次，鉴别器正在测试图像输入并确定它们是否是假的。它使用卷积来识别特征，但在本例中，它使用<kbd>ZeroPadding2D</kbd>在图像周围放置一个零缓冲区，以帮助识别。这一层的相反形式是<kbd>Cropping2D</kbd>，它裁剪一个图像。请注意该模型如何不使用下采样或卷积池。我们将在接下来的章节中探索其他新的特殊图层<kbd>LeakyReLU</kbd>和<kbd>BatchNormalization</kbd>。请注意，我们在卷积中没有使用任何池层。这样做是为了通过分数步长卷积增加空间维度。看看我们是如何在卷积层中使用一个奇怪的内核和步长的。</p>

<p class="mce-root">我们现在将返回并像这样定义<kbd>init</kbd>函数:</p>

<ol start="8">

<li>这个初始化代码为我们的输入图像设置大小(28 x 28 x 1，一个通道用于灰度)。然后它设置了一个<kbd>Adam</kbd>优化器，我们将在关于优化器的另一节中回顾它。在这之后，它构建了<kbd>discriminator</kbd>和<kbd>generator</kbd>。然后，它将两个模型或子网络(<kbd>generator</kbd>和<kbd>discriminator</kbd>)组合在一起。这允许网络协同工作，并优化整个网络的培训。同样，这是一个我们将在优化器中更仔细研究的概念。</li>

<li class="mce-root">在我们深入之前，花点时间运行这个例子。这个示例可能需要很长时间来运行，所以在它启动后返回到书中并保持运行。</li>

</ol>

<pre style="color: black;padding-left: 60px">def __init__(self):<br/>  self.img_rows = 28<br/>  self.img_cols = 28<br/>  self.channels = 1<br/>  self.img_shape = (self.img_rows, self.img_cols, self.channels)<br/>  self.latent_dim = 100<br/>  optimizer = <strong>Adam</strong>(0.0002, 0.5)<br/><br/>  self.discriminator = self.build_discriminator()<br/>  self.discriminator.compile(loss='binary_crossentropy',    <br/>  optimizer=optimizer, metrics=['accuracy'])<br/><br/>  self.generator = self.build_generator() <br/>  z = Input(shape=(self.latent_dim,))<br/>  img = self.generator(z)<br/>  <br/>  self.discriminator.trainable = False<br/>  valid = self.discriminator(img)<br/>  <br/>  self.combined = Model(z, valid)<br/>  self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)</pre>

<ol start="10">

<li>随着示例的运行，您将能够看到生成的输出被放置到一个名为<kbd>images</kbd>的文件夹中，该文件夹与您正在运行的Python文件位于同一个文件夹中。继续观察，每隔50个时期就会保存一幅新图像，如下图所示:</li>

<li><img src="img/dfa6688e-16d8-4d0f-b177-6f2ea058dd59.png" style="width:20.58em;height:15.50em;"/></li>

</ol>

<ol start="12">

<li class="CDPAlignLeft CDPAlign">GAN产生的输出示例</li>

</ol>

<p class="CDPAlignCenter CDPAlign">前面显示了大约3900个时期后的结果。当你开始训练时，需要一段时间才能达到这么好的效果。</p>

<p>这包括了建立模型的基础知识，除了培训中的所有工作，我们将在下一节中介绍。</p>

<p>训练GAN</p>

<p>训练一个GAN需要更多的关注细节和理解更先进的优化技术。我们将详细介绍该功能的每个部分，以便理解培训的复杂性。让我们打开<kbd>Chapter_3_1.py</kbd>，查看<kbd>train</kbd>函数，并遵循以下步骤:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training a GAN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在<kbd>train</kbd>功能开始时，您会看到以下代码:</h1>

                

            

            

                

<p>数据首先从MNIST训练集中加载，然后重新调整到<kbd>-1</kbd>到<kbd>1</kbd>的范围。我们这样做是为了更好地将数据集中在0附近，并适应我们的激活功能<kbd>tanh</kbd>。如果你回到发电机功能，你会看到底部激活是<kbd>tanh</kbd>。</p>

<ol>

<li class="mce-root">接下来，我们构建一个<kbd>for</kbd>循环来遍历这些时期，如下所示:</li>

</ol>

<pre style="color: black;padding-left: 60px">def train(self, epochs, batch_size=128, save_interval=50):  <br/>  (X_train, _), (_, _) = mnist.load_data()<br/>  X_train = X_train / 127.5 - 1.<br/>  X_train = np.expand_dims(X_train, axis=3)<br/><br/>  valid = np.ones((batch_size, 1))<br/>  fake = np.zeros((batch_size, 1))</pre>

<ol start="2">

<li>然后，我们随机选择一半的<em>真实</em>训练图像，使用以下代码:</li>

<li class="mce-root">之后，我们对<kbd>noise</kbd>进行采样，生成一组伪造的图像，代码如下:</li>

</ol>

<pre style="color: black;padding-left: 60px">for epoch in range(epochs):</pre>

<ol start="4">

<li>现在，一半的图像是真实的，另一半是我们的<kbd>generator</kbd>伪造的。</li>

</ol>

<pre class="mce-root" style="padding-left: 60px">idx = np.random.randint(0, X_train.shape[0], batch_size)<br/>imgs = X_train[idx]</pre>

<ol start="5">

<li>接下来，对图像训练<kbd>discriminator</kbd>,对错误预测的伪像和正确识别的真实图像产生损失，如图所示:</li>

</ol>

<pre style="padding-left: 60px">noise = np.random.normal(0, 1, (batch_size, self.latent_dim))<br/>gen_imgs = self.generator.predict(noise)</pre>

<ol start="6">

<li>请记住，这个代码块是跨集合或批处理运行的。这就是为什么我们使用<kbd>numpy np.add</kbd>功能来添加<kbd>d_loss_real</kbd>和<kbd>d_loss_fake</kbd>。<kbd>numpy</kbd>是一个我们经常用来处理数据集或张量数据的库。</li>

<li>最后，我们使用以下代码训练生成器:</li>

</ol>

<pre style="padding-left: 60px">d_loss_real = self.discriminator.train_on_batch(imgs, valid)<br/>d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)<br/>d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)</pre>

<ol start="8">

<li>注意<kbd>g_loss</kbd>是如何基于训练组合模型来计算的。您可能还记得，组合模型从真实和虚假图像中获取输入，并通过整个模型反向传播训练。这允许我们将<kbd>generator</kbd>和<kbd>discriminator</kbd>作为一个组合模型一起训练。接下来展示了一个例子，但请注意，图像大小与我们的略有不同:</li>

<li class="mce-root">现在我们对架构有了更好的理解，我们需要回过头来理解一些关于新的层类型和组合模型优化的细节。在下一节中，我们将探讨如何优化GAN等联合模型。</li>

</ol>

<pre style="color: black;padding-left: 60px">g_loss = self.combined.train_on_batch(noise, valid)<br/><br/>print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))<br/><br/>if epoch % save_interval == 0:<br/>  self.save_imgs(epoch)</pre>

<ol start="10">

<li>优化者</li>

</ol>

<div><img src="img/c85f204f-7196-444a-be1c-53b285b79cd8.png"/><br/>

<br/>

Layer architecture diagram of DCGAN</div>

<p>一个<strong>优化器</strong>实际上只不过是通过网络训练误差反向传播的另一种方式。正如我们在<a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">第一章</a> <em>游戏的深度学习</em>中所学的，我们用于反向传播的基本算法是梯度下降和更高级的<strong>随机梯度下降</strong> ( <strong> SGD </strong>)。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Optimizers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">SGD通过在每次训练迭代中随机选取批次顺序来改变梯度的评估。虽然SGD在大多数情况下都工作得很好，但它在GAN中的表现并不好，这是由于一个被称为<strong>消失</strong> / <strong>爆炸梯度</strong>的问题，当试图训练多个但组合的网络时会发生这种问题。记住，我们直接将生成器的结果输入到鉴别器中。相反，我们期待更先进的优化。下图显示了典型最佳优化器的性能:</h1>

                

            

            

                

<p>An <strong>optimizer</strong> is really nothing more than another way to train the backpropagation of error through a network. As we learned back in <a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">Chapter 1</a><em>, Deep Learning for Games</em>, the base algorithm we use for backpropagation is the gradient descent and the more advanced <strong>stochastic gradient descent</strong> (<strong>SGD</strong>). </p>

<p>图中的所有方法都源于SGD，但是你可以清楚地看到这个例子中的赢家是<strong>亚当</strong>。也有不是这样的情况，但是目前最喜欢的优化器是Adam。正如你可能已经注意到的，这是我们以前广泛使用的东西，并且你可能会在将来继续使用它。但是，让我们更详细地看看每个优化器，如下所示:</p>

<div><img src="img/ce2b1eaf-c1e9-4018-ac13-b8208344a68e.png" style="width:38.83em;height:34.58em;"/><br/>

<br/>

Performance comparison of various optimizers</div>

<p class="mce-root">这是我们研究的第一批模型之一，它通常是我们训练的基准。</p>

<p><strong> SGD与内斯特罗夫</strong>:SGD经常面临的问题是我们在早期的一个培训示例中看到的网络损耗的抖动效应。请记住，在训练期间，我们的网络损耗会在两个值之间波动，几乎就像是一个球在山坡上上下运动一样。本质上，这正是正在发生的事情，但是我们可以通过引入一个我们称之为<strong>动量</strong>的术语来纠正它。下图显示了动量对训练的影响示例:</p>

<ul>

<li>所以，现在，我们不再让球盲目地滚来滚去，而是控制它的速度。我们推它一把，以克服一些恼人的颠簸或摇晃，更有效地到达最低点。</li>

<li class="CDPAlignLeft CDPAlign">您可能还记得，在学习反向传播的数学时，我们控制SGD中的梯度来训练网络，以最小化误差或损失。通过引入动量，我们试图通过逼近值来更有效地控制梯度。<strong>内斯特罗夫技术</strong>，或者可以简称为<strong>动量</strong>，使用加速动量项来进一步优化梯度。</li>

</ul>

<div><img src="img/9ab302ca-8dd7-4e12-b02e-da2befc259a0.png" style="width:34.67em;height:10.33em;"/><br/>

<br/>

SGD with and without momentum</div>

<p style="padding-left: 90px"><strong> AdaGrad </strong>:这种方法基于更新的频率来优化单个训练参数，这使得它非常适合处理较小的数据集。另一个主要的好处是，它允许你不必调整学习速度。然而，这种方法的一个大缺点是平方梯度导致学习速率变得如此之小，以至于网络停止学习。</p>

<p style="padding-left: 90px"><strong> AdaDelta </strong>:这种方法是AdaGrad的扩展，处理平方梯度和消失学习率。它通过将学习率窗口固定到特定的最小值来实现这一点。</p>

<ul>

<li><strong> RMSProp </strong>:由深度学习之父杰夫·辛顿(Geoff Hinton)开发，这是一种管理阿达格拉德学习率下降问题的技术。如图所示，对于所示示例，它与AdaDelta相当。</li>

<li><strong>自适应力矩估计</strong> ( <strong> Adam </strong>):这是另一种尝试使用动量的更可控版本来控制梯度的技术。它通常被描述为Momentum plus RMSProp，因为它结合了两种技术的优点。</li>

<li><strong> AdaMax </strong>:这个方法没有显示在性能图上，但是值得一提。它是Adam的扩展，概括了应用于动量更新的每次迭代。</li>

<li><strong>那达慕</strong>:这是另一种不在图上的方法；它是内斯特罗夫加速动量和亚当的结合。香草亚当只是用了一个没有被加速的动量项。</li>

<li>AMSGrad :这是Adam的一个变体，在Adam无法收敛或抖动时效果最佳。这是由算法未能适应学习速率引起的，并且通过取最大值而不是先前平方梯度的平均值来解决。差别很微妙，倾向于更小的数据集。请记住这个选项，作为未来可能的工具。</li>

<li>这就完成了我们对优化器的简短概述；请务必参考本章末尾的练习，以了解进一步探索它们的方法。在下一节中，我们将构建自己的GAN，它可以生成在游戏中使用的纹理。</li>

<li>瓦瑟斯坦·甘</li>

</ul>

<p>现在你可以肯定地意识到，GANs有着广泛而多样的应用，其中一些非常适用于游戏。一个这样的应用是产生纹理或纹理变化。我们经常希望纹理有细微的变化，让我们的游戏世界看起来更有说服力。这是可以用<strong>着色器</strong>完成的，但出于性能原因，通常最好创建<strong>静态资产</strong>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Wasserstein GAN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">因此，在本节中，我们将构建一个GAN项目，它允许我们生成纹理或高度图。你也可以使用我们之前简单提到的任何其他酷gan来扩展这个概念。我们将使用Erik Linder-Norén的Wasserstein GAN的默认实现，并根据我们的目的对其进行转换。</h1>

                

            

            

                

<p>As you can most certainly appreciate by now, GANs have wide and varied applications, several of which apply very well to games. One such application is the generation of textures or texture variations. We often want slight variations in textures to give our game worlds a more convincing look. This is and can be done with <strong>shaders</strong>, but for performance reasons, it is often best to create <strong>static assets</strong>. </p>

<p>当你第一次处理深度学习问题时，你将面临的一个主要障碍是将数据塑造成你需要的形式。在原始示例中，Erik使用了MNIST数据集，但是我们将转换该示例以使用CIFAR100数据集。CIFAR100数据集是一组按类型分类的彩色图像，如下所示:</p>

<p class="mce-root">不过现在，让我们打开<kbd>Chapter_3_wgan.py</kbd>并遵循以下步骤:</p>

<p>打开Python文件并查看代码。大部分代码看起来与我们已经看过的DCGAN一样。但是，我们希望了解一些关键差异，如下所示:</p>

<div><img src="img/87786072-1218-45e0-868c-b71599064740.png"/><br/>

<br/>

CIFAR 100 dataset</div>

<p>Wasserstein GAN使用距离函数来确定每次训练迭代的成本或损失。与此同时，这种形式的GAN使用多个批评者而不是单个鉴别者来确定成本或损失。一起训练多个批评家提高了性能，并且处理了我们经常看到的困扰GANs的消失梯度问题。不同形式的GAN训练的例子如下:</p>

<ol>

<li>跨GAN实施的培训绩效(<a href="https://arxiv.org/pdf/1701.07875.pdf">https://arxiv.org/pdf/1701.07875.pdf</a>)</li>

</ol>

<pre style="padding-left: 60px">def train(self, epochs, batch_size=128, sample_interval=50):<br/>  (X_train, _), (_, _) = mnist.load_data()<br/><br/>  X_train = (X_train.astype(np.float32) - 127.5) / 127.5<br/>  X_train = np.expand_dims(X_train, axis=3)<br/><br/>  valid = -np.ones((batch_size, 1))<br/>  fake = np.ones((batch_size, 1))<br/><br/>  for epoch in range(epochs):<br/>    <strong>for _ in range(self.n_critic):</strong><br/>      idx = np.random.randint(0, X_train.shape[0], batch_size)<br/>      imgs = X_train[idx]<br/>      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))<br/>      gen_imgs = self.generator.predict(noise)<br/><br/>      d_loss_real = self.critic.train_on_batch(imgs, valid)<br/>      d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)<br/>      d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)<br/>      <br/>      for l in self.critic.layers:<br/>        weights = l.get_weights()<br/>        weights = [np.clip(w, -self.clip_value, self.clip_value) for <br/>        w in weights]<br/>        l.set_weights(weights)<br/><br/>    g_loss = self.combined.train_on_batch(noise, valid)<br/>    print ("%d [D loss: %f] [G loss: %f]" % (epoch, 1 - d_loss[0], 1 <br/>    - g_loss[0]))\<br/><br/>    if epoch % sample_interval == 0:<br/>      self.sample_images(epoch)</pre>

<ol start="2">

<li>WGAN通过确定移动成本的距离函数而不是误差值的差异来管理成本，从而克服了梯度问题。线性成本函数可以简单到一个角色为了正确拼写一个单词而需要移动的次数。例如，单词<em> SOPT </em>的成本为2，因为<em> T </em>角色需要移动两个位置才能正确拼写<em>停止</em>。单词<em> OTPS </em>正确拼写<em>停止</em>的距离代价为<em> 3 (S) + 1 (T) = 4 </em>。</li>

</ol>

<div><img src="img/b1e15842-e119-44ed-9f30-a52a0aab0e32.png" style="width:31.00em;height:21.33em;"/></div>

<p>Wasserstein距离函数实质上决定了将一种概率分布转换为另一种概率分布的成本。你可以想象，理解这一点的数学可能相当复杂，所以我们将把它留给更感兴趣的读者。</p>

<ol start="3">

<li>运行示例。此示例可能需要很长时间才能运行，请耐心等待。此外，该示例在某些GPU硬件上的训练也有问题。如果你发现是这种情况，就禁用GPU的使用。</li>

<li>当示例运行时，从Python文件所在的文件夹中打开<kbd>images</kbd>文件夹，并观察生成的训练图像。</li>

<li>只要您认为有必要，就运行这个示例，以便了解它是如何工作的。即使在高级硬件上，该示例也可能需要几个小时。完成后，继续下一节，我们将看到如何修改这个示例来生成纹理。</li>

<li>用GAN生成纹理</li>

</ol>

<p>高级深度学习书籍中很少涉及的一件事是对输入网络的数据进行整形的细节。除了塑造数据，还需要改变网络的内部结构来容纳新数据。这个例子的最终版本是<kbd>Chapter_3_3.py</kbd>，但是对于这个练习，从<kbd>Chapter_3_wgan.py</kbd>文件开始，并遵循以下步骤:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Generating textures with a GAN </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们首先将训练数据集从MNIST更改为CIFAR，交换导入，如下所示:</h1>

                

            

            

                

<p>One of the things so rarely covered in advanced deep learning books is the specifics of shaping data to input into a network. Along with shaping data is the need to alter the internals of a network to accommodate the new data. The final version of this example is <kbd>Chapter_3_3.py</kbd>, but for this exercise, start with the <kbd>Chapter_3_wgan.py</kbd> file and follow these steps:</p>

<ol>

<li class="mce-root">We will start by changing the training set of data from MNIST to CIFAR by swapping out the imports like so:</li>

</ol>

<pre style="color: black;padding-left: 60px">from keras.datasets import mnist  #remove or leave<br/>from keras.datasets import cifar100  #add</pre>

<p class="mce-root">在课程开始时，我们将图像尺寸参数从28 x 28灰度级更改为32 x 32彩色，如下所示:</p>

<p class="mce-root">现在，向下移动到<kbd>train</kbd>功能，并修改代码如下:</p>

<ol start="2">

<li>这段代码从CIFAR100数据集中加载图像，并按标签对它们进行排序。标签存储在<kbd>y</kbd>变量中，代码遍历所有下载的图像，并将它们隔离到一个特定的集合中。在这种情况下，我们使用标签<kbd>33</kbd>，它对应于森林图像。CIFAR100中有100个类别，我们选择了一个包含500张图片的类别。随意尝试从其他类别生成其他纹理。<br/>代码的其余部分相当简单，除了<kbd>np.reshape</kbd>调用，我们通过三个通道将数据重新整形为500个图像<kbd>32x32</kbd>像素的列表。您可能还想注意到，我们不需要像以前那样将轴扩展到三个。这是因为我们的图像已经缩放到三个通道。</li>

</ol>

<pre style="color: black;padding-left: 60px">class WGAN():<br/>  def __init__(self):<br/>    self.img_rows = <strong>32</strong><br/>    self.img_cols = <strong>32</strong><br/>    self.channels = <strong>3</strong></pre>

<ol start="3">

<li>我们现在需要回到生成器和评论家模型，稍微修改一下代码。首先，我们将像这样更改生成器:</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>#(X_train, _), (_, _) = mnist.load_data() or delete me</strong><br/>(X_train, y), (_, _) = cifar100.load_data(label_mode='fine')<br/>Z_train = []<br/>cnt = 0<br/>for i in range(0,len(y)):<br/>  if y[i] == 33:  #forest images<br/>  cnt = cnt + 1 <br/>  z = X_train[i]<br/>  Z_train.append(z)<br/>#X_train = (X_train.astype(np.float32) - 127.5) / 127.5 or delete me<br/>#X_train = np.expand_dims(X_train, axis=3)<br/><strong>Z_train = np.reshape(Z_train, [500, 32, 32, 3])</strong><br/>Z_train = (Z_train.astype(np.float32) - 127.5) / 127.5<br/><br/>#X_train = (X_train.astype(np.float32) - 127.5) / 127.5<br/>#X_train = np.expand_dims(X_train, axis=3)</pre>

<ol start="4">

<li>粗体代码表示变化。我们为这个模型所做的就是将<kbd>7x7</kbd>原始特征图转换成<kbd>8x8</kbd>。回想一下，原始完整图像的大小是<kbd>28x28</kbd>。我们的卷积从一个<kbd>7x7</kbd>特征图开始，加倍两次，等于<kbd>28x28</kbd>。由于我们的新图像大小是<kbd>32x32</kbd>，我们需要转换我们的网络，以<kbd>8x8</kbd>特征地图开始，其两倍等于<kbd>32x32</kbd>，与CIFAR100图像的大小相同。幸运的是，我们可以让批评家模型保持原样。</li>

</ol>

<ol start="5">

<li class="mce-root">接下来，我们添加一个新函数来保存原始CIFAR图像的样本，如下所示:</li>

</ol>

<pre style="color: black;padding-left: 60px">def build_generator(self):<br/>  model = Sequential()<br/>  model.add(Dense(128 * <strong>8 * 8</strong>, activation="relu", input_dim=self.latent_dim))<br/>  model.add(Reshape((<strong>8, 8</strong>, 128)))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(128, kernel_size=4, padding="same"))<br/>  model.add(BatchNormalization(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(64, kernel_size=4, padding="same"))<br/>  model.add(BatchNormalization(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(Conv2D(self.channels, kernel_size=4, padding="same"))<br/>  model.add(Activation("tanh"))<br/>  model.summary()<br/>  noise = Input(shape=(self.latent_dim,))<br/>  img = model(noise)<br/>  return Model(noise, img)</pre>

<ol start="6">

<li><kbd>save_images</kbd>函数输出原始图像的采样，由<kbd>train</kbd>函数中的以下代码调用:</li>

<li>新代码以粗体显示，只输出原始代码的一个示例，如下所示:</li>

</ol>

<pre style="padding-left: 60px">def save_images(self, imgs, epoch):<br/>  r, c = 5, 5 <br/>  gen_imgs = 0.5 * imgs + 1<br/>  fig, axs = plt.subplots(r, c)<br/>  cnt = 0<br/>  for i in range(r):<br/>    for j in range(c):<br/>      axs[i,j].imshow(gen_imgs[cnt, :,:,0],cmap='gray')<br/>      axs[i,j].axis('off')<br/>      cnt += 1<br/><br/>  fig.savefig("images/cifar_%d.png" % epoch)<br/>  plt.close()</pre>

<ol start="8">

<li><img src="img/ecd433e1-66ac-49a0-817c-0eb87d7531ae.png" style="width:37.25em;height:28.42em;"/></li>

</ol>

<pre style="padding-left: 60px">idx = np.random.randint(0, Z_train.shape[0], batch_size)<br/>imgs = Z_train[idx] <br/><strong>if epoch % sample_interval == 0:</strong><br/><strong>  self.save_images(imgs, epoch)</strong></pre>

<ol start="9">

<li class="CDPAlignLeft CDPAlign">原始图像示例</li>

</ol>

<p class="CDPAlignCenter CDPAlign">运行示例并观察<kbd>images</kbd>文件夹中再次标记为<kbd>cifar</kbd>的输出，显示训练的结果。同样，这个示例可能需要一些时间来运行，所以请继续阅读下一节。</p>

<p>随着示例的运行，您可以观察GAN如何训练以匹配图像。这里的好处是，你可以使用各种技术轻松地生成各种纹理。你可以在Unity或其他游戏引擎中使用这些作为纹理或高度图。在我们结束这一节之前，让我们进入一些标准化和其他参数。</p>

<ol start="10">

<li>批量标准化</li>

</ol>

<p><strong>批量归一化</strong>，顾名思义，将层中的权重分布归一化到某个平均值0附近。这允许网络使用更高级的学习，同时仍然避免消失或爆炸梯度问题。这是由于权重被归一化，这允许更少的移动或训练抖动，正如我们之前看到的。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Batch normalization</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">通过标准化层中的权重，我们允许网络使用更高的学习速率，从而训练得更快。同样，我们可以避免或减少使用<kbd>DropOut</kbd>的需要。您将会看到，我们使用此处显示的标准术语来标准化这些层:</h1>

                

            

            

                

<p>回想一下我们对优化器的讨论，动量控制着我们希望减少训练梯度的快慢。在这种情况下，动量是指归一化分布的平均值或中心的变化量。</p>

<p>在下一节中，我们来看另一个叫做LeakyReLU的特殊图层。</p>

<pre class="mce-root">model.add(BatchNormalization(momentum=0.8))</pre>

<p>渗漏和其他渗漏</p>

<p><strong> LeakyReLU </strong>增加了一个激活层，允许负值有一个小的斜率，而不是像标准ReLU激活函数那样只有0。标准的ReLU通过只允许正激活的神经元触发来鼓励网络中的稀疏性。然而，这也造成了一种死神经元状态，即部分网络基本上死亡或变得不可追踪。为了解决这个问题，我们引入了一种称为LeakyReLU的泄漏形式的ReLU激活。此处显示了该激活如何工作的示例:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Leaky and other ReLUs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">上图所示为<strong>参数ReLU </strong>，类似于Leaky，但它允许网络自己训练参数。这使得网络可以自行调整，但需要更长的训练时间。</h1>

                

            

            

                

<p>您可以使用的其他ReLU变体总结如下:</p>

<div><img src="img/0377b7b9-e422-4d78-9c9e-c8dc7c37d9b0.png" style="width:28.75em;height:15.67em;"/><br/>

<br/>

Example of a leaky and parametric ReLU</div>

<p><strong>指数线性</strong> ( <strong> ELU，SELU </strong>):这些形式的ReLU激活如下图所示:</p>

<p>ELU和SELU</p>

<ul>

<li><strong>Concatenated ReLU</strong>(<strong>CReLU</strong>):这将常规形式和泄漏形式连接在一起，提供一个产生两个输出值的新函数。对于正值，生成<em> [0，x]，</em>，对于负值，返回<em> [x，0] </em>。关于这一层要注意的一点是输出加倍，因为每个神经元产生两个值。</li>

</ul>

<div><img src="img/cea2fa2e-b196-457a-9603-d4cf88bcff72.png" style="width:35.00em;height:18.08em;"/></div>

<p><strong>ReLU-6</strong>:6的值是任意的，但是允许网络训练稀疏的神经元。稀疏性是有价值的，因为它鼓励网络学习或建立更强的权重或联系。人类大脑已经被证明在稀疏状态下运行，一次只有几个激活的神经元。你会经常听到这样的神话:我们一次最多只使用了大脑的10%。这很有可能是真的，但是其原因更多的是数学上的，而不是我们能够使用整个大脑。我们确实使用了整个大脑，只是不是同时使用。稀疏性所鼓励的更强的个体权重允许网络做出更好/更强的决策。较轻的权重也有助于减少数据的过度拟合或记忆。这种情况经常发生在拥有数千个神经元的深层网络中。</p>

<ul>

<li>正则化是我们经常使用的另一种技术，用来修剪或减少不需要的权重，创建稀疏网络。在接下来的章节中，我们将有一些机会研究正则化和稀疏性。</li>

<li>在下一节中，我们将使用我们所学的知识来构建一个可以生成游戏音乐的工作音乐GAN。</li>

</ul>

<p class="mce-root">创作音乐的人</p>

<p class="mce-root">在这一章的最后一个大例子中，我们将看看用GANs为游戏创作音乐。音乐生成并不特别困难，但它确实让我们看到了使用LSTM层来识别音乐序列和模式的GAN的整个变体。然后，它试图将音乐从随机的噪音中还原成一系列还过得去的音符和旋律。当你听那些生成的音符并意识到曲调来自计算机大脑时，这个样本就变得飘渺了。</p>

<p class="mce-root">这个样本来源于GitHub，【https://github.com/megis7/musegen】T2，由Michalis Megisoglou开发。我们查看这些代码示例的原因是为了让我们可以看到其他人所产生的最好的东西，并从中学习。在某些情况下，这些样本接近原始样本，而另一些则不那么接近。我们确实需要调整一些东西。Michalis还制作了一个很好的GitHub自述文件，内容是他为实现<strong xmlns:epub="http://www.idpf.org/2007/ops"> museGAN </strong>，与GAN的音乐生成而构建的代码。如果您对进一步构建这个示例感兴趣，也一定要查看GitHub站点。使用各种库可以实现一些museGAN其中之一就是TensorFlow。</p>

<p class="mce-root">为了让这个例子更容易理解，我们在这个例子中使用了Keras。如果你对使用TensorFlow很认真，那么一定要看看TensorFlow版本的museGAN。</p>

<p class="mce-root">这个例子分别训练鉴别器和生成器，这意味着它需要先训练鉴别器。对于我们的第一次运行，我们将使用作者以前生成的模型运行这个例子，但是我们仍然需要一些设置；让我们遵循以下步骤:</p>

<p class="mceNonEditable">我们首先需要安装几个依赖项。以管理员身份打开一个Anaconda或Python窗口，并运行以下命令:</p>

<p>Regularization is another technique we will often use to trim or reduce unneeded or weights and create sparse networks. We will have a few opportunities to look at regularization and sparsity later in the coming chapters.</p>

<p>In the next section, we use what we have learned to build a working music GAN that can generate game music.</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>A GAN for creating music</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title"><kbd xmlns:epub="http://www.idpf.org/2007/ops">Music21</kbd>是一个用于加载MIDI文件的Python库。MIDI 是一种音乐交换格式，你可能已经猜到了，它是用来描述音乐/音符的。最初的模型是在描述巴赫音乐的300首合唱曲的MIDI文件集合上训练的。您可以通过导航到<kbd xmlns:epub="http://www.idpf.org/2007/ops">musegen</kbd>文件夹并运行脚本来定位项目。</h1>

                

            

            

                

<p>导航到项目文件夹并执行脚本，该脚本运行先前训练的模型，如下所示:</p>

<p>这将加载先前保存的模型<strong> </strong>并使用这些模型来训练发电机并产生音乐。当然，你可以在以后需要时在你选择的其他MIDI文件上训练这个GAN。有很多免费的MIDI文件来源，从古典音乐到电视主题音乐、游戏和现代流行音乐。在这个例子中，我们使用作者的原始模型，但是可能性是无限的。</p>

<p>加载音乐文件和训练可能需要很长时间，训练通常就是这样。所以，借此机会看看代码。打开项目文件夹中的<kbd>musegen.py</kbd>文件。看一下第39行左右，如下:</p>

<p>这段代码从一个<kbd>hdf5</kbd>或分层数据文件中加载先前训练的模型。前面的代码设置了许多变量，这些变量定义了我们将用来生成新注释的词汇表的注释。</p>

<ol>

<li class="mce-root">找到位于同一项目文件夹中的<kbd>notegenerator.py</kbd>文件。看看模型代码的创建，如下所示:</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>pip install music21</strong><br/><strong>pip install h5py</strong></pre>

<p class="mce-root">请注意我们是如何从使用<kbd xmlns:epub="http://www.idpf.org/2007/ops">Conv2D</kbd>层转变为<kbd xmlns:epub="http://www.idpf.org/2007/ops">LSTM</kbd>层的，因为我们已经从图像识别转变为序列或音符模式识别。我们也从使用更简单的层发展到复杂的时间分布架构。此外，作者使用了一个被称为<strong xmlns:epub="http://www.idpf.org/2007/ops">变分自动编码</strong>的概念来确定音符在序列中的分布。这个网络是我们到目前为止看到的最复杂的网络，这里发生了很多事情。除了看看代码是如何流动的之外，不要太在意这个例子。我们将在第四章 <em xmlns:epub="http://www.idpf.org/2007/ops">“构建深度学习游戏聊天机器人</em> <a xmlns:epub="http://www.idpf.org/2007/ops" href="http://Chapter_4">”中详细了解更多这类高级时间分布式网络。</a></p>

<p class="mce-root">让样本运行并生成一些音乐样本到<kbd>samples/note-generator</kbd>文件夹中。当我们遇到更复杂的问题时，我们的训练时间会从几个小时到几天，甚至更复杂的问题。你可能很容易生成一个网络，但你没有足够的计算能力在合理的时间内训练它。</p>

<ol start="2">

<li>打开文件夹，双击其中一个样本文件，聆听生成的MIDI文件。记住，这音乐只是由电脑大脑产生的。</li>

<li class="mce-root">这个例子中有很多代码我们没有涉及到。因此，请务必返回并浏览<kbd>musegen.py</kbd>文件，以便更好地理解用于构建网络生成器的流程和层类型。在下一节中，我们将探讨如何训练这种GAN。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>cd musegen</strong><br/><strong>python musegen.py </strong>or<strong> python3 musegen.py</strong></pre>

<ol start="4">

<li>音乐甘训练</li>

<li class="mce-root">在我们开始训练这个网络之前，我们先来看看作者在GitHub原始资料中描述的整体架构:</li>

</ol>

<pre style="color: black;padding-left: 60px">print('loading networks...')<br/>dir_path = os.path.dirname(os.path.realpath(__file__))<br/>generator = loadModelAndWeights(os.path.join(dir_path, note_generator_dir, 'model.json'),<br/>                               os.path.join(dir_path, note_generator_dir, 'weights-{:02d}.hdf5'.format(generator_epoch)))</pre>

<ol start="6">

<li>这些网络几乎是相同的，直到你仔细观察，发现LSTM层的细微差别。请注意，一组是如何使用另一组两倍的单位的。</li>

<li class="mce-root">Locate the <kbd>notegenerator.py</kbd> file located in the same project folder. Take a look at the creation of the model code, as follows:</li>

</ol>

<pre style="color: black;padding-left: 60px">x_p = Input(shape=(sequence_length, pitch_dim,), name='pitches_input')<br/>h = LSTM(256, return_sequences=True, name='h_lstm_p_1')(x_p)<br/>h = LSTM(512, return_sequences=True, name='h_lstm_p_2')(h)<br/>h = LSTM(256, return_sequences=True, name='h_lstm_p_3')(h)<br/><br/># VAE for pitches<br/>z_mean_p = TimeDistributed(Dense(latent_dim_p, kernel_initializer='uniform'))(h)<br/>z_log_var_p = TimeDistributed(Dense(latent_dim_p, kernel_initializer='uniform'))(h)<br/><br/>z_p = <strong>Lambda</strong>(sampling)([z_mean_p, z_log_var_p])<br/>z_p = <strong>TimeDistributed</strong>(Dense(pitch_dim, kernel_initializer='uniform', activation='softmax'))(z_p)<br/><br/>x_d = Input(shape=(sequence_length, duration_dim, ), name='durations_input')<br/>h = LSTM(128, return_sequences=True)(x_d)<br/>h = LSTM(256, return_sequences=True)(h)<br/>h = LSTM(128, return_sequences=True)(h)<br/><br/># VAE for durations<br/>z_mean_d = TimeDistributed(Dense(latent_dim_d, kernel_initializer='uniform'))(h)<br/>z_log_var_d = TimeDistributed(Dense(latent_dim_d, kernel_initializer='uniform'))(h)<br/><br/>z_d = Lambda(sampling)([z_mean_d, z_log_var_d])<br/>z_d = TimeDistributed(Dense(duration_dim, kernel_initializer='uniform', activation='softmax'))(z_d)<br/>conc = Concatenate(axis=-1)([z_p, z_d])<br/>latent = TimeDistributed(Dense(pitch_dim + duration_dim, kernel_initializer='uniform'))(conc)<br/>latent = LSTM(256, return_sequences=False)(latent)<br/><br/>o_p = Dense(pitch_dim, activation='softmax', name='pitches_output', kernel_initializer='uniform')(latent)<br/>o_d = Dense(duration_dim, activation='softmax', name='durations_output', kernel_initializer='uniform')(latent)</pre>

<ol start="8">

<li>Note how we have changed from using <kbd>Conv2D</kbd> layers to <kbd>LSTM</kbd> layers, since we have gone from image recognition to sequence or note pattern recognition. We have also gone from using more straightforward layers to a complex time-distributed architecture. Also, the author used a concept known as <strong>variational auto encoding</strong> in order to determine the distribution of notes in a sequence. This network is the most complex we have looked at so far, and there is a lot going on here. Don't fret too much about this example, except to see how the code flows. We will take a closer look at more of these type of advanced time- distributed networks in <a href="a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml" target="_blank"/><a href="a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml" target="_blank">Chapter 4</a><em>, Building a Deep Learning Gaming Chatbot</em><a href="http://Chapter_4">.</a></li>

<li>我们可以通过在Python或Anaconda提示符下运行以下命令来生成音乐模型:</li>

</ol>

<ol start="10">

<li>这个脚本加载样本数据，并生成我们稍后在创建原创音乐时在<kbd>musegen.py</kbd>文件中使用的模型。打开<kbd>note-generator.py</kbd>文件，主要部分如下所示:</li>

</ol>

<p>对原始代码进行了修改，使其更加兼容Windows和跨平台。还是那句话，这当然不是对作者优秀作品的批评。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training the music GAN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">这段代码使用Music21库从音乐语料库中读取MIDI音符和其他音乐形式，您可以使用它们进行自己的测试。这个训练数据集是生成其他音乐源的一种极好的方式，它由以下内容组成:<a href="http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html">http://web . MIT . edu/music 21/doc/module reference/module corpus . html</a>。<a href="http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html"/></h1>

                

            

            

                

<p>您可以通过修改<kbd>config.py</kbd>文件中的内容或添加额外的配置选项来进一步修改该示例，如下所示:</p>

<div><img src="img/e630248f-63d9-43d6-9d64-616a450543f6.png"/><br/>

<br/>

Overview of museGAN network architecture</div>

<p>前一个样本对于探索音乐的产生是很棒的。下一节将介绍一个更实用且可能有用的例子。</p>

<p class="mce-root">通过替代GAN生成音乐</p>

<p class="mce-root">另一个音乐生成的例子也包含在<kbd>Chapter_3</kbd>源文件夹中，名为<strong>古典钢琴作曲家</strong>，源位于<a href="https://github.com/Skuldur/Classical-Piano-Composer">https://github.com/Skuldur/Classical-Piano-Composer</a>，由sigur skúLi开发。这个例子使用了一整套最终幻想MIDI文件作为音乐生成的灵感来源，是一个很好的生成你自己的音乐的实用例子。</p>

<p>为了运行这个示例，您需要首先从<kbd>Classical-Piano-Composer</kbd>项目文件夹中使用以下命令运行<kbd>lstm.py</kbd>:</p>

<pre><strong>python note-generator.py</strong> <br/>or <br/><strong>python3 note-generator.py</strong></pre>

<p>这个示例可能需要很长时间来训练，所以请确保打开文件并通读它的内容。</p>

<p>模型定型后，您可以通过运行以下命令来运行生成器:</p>

<pre>def loadChorales():<br/>    notes = []<br/>    iterator = getChoralesIterator()<br/><br/>    # load notes of chorales<br/>    for chorale in iterator[1:maxChorales]: # iterator is 1-based <br/>        transpose_to_C_A(chorale.parts[0])<br/>        notes = notes + parseToFlatArray(chorale.parts[0])<br/>        notes.append((['end'], 0.0)) # mark the end of the piece<br/>    <br/>    return notes</pre>

<p>这个脚本加载训练好的模型并生成音乐。这是通过将MIDI音符编码成音符序列或音符组的网络输入来实现的。我们在这里做的是将音乐文件分解成短序列，或者音乐快照。您可以通过调整代码文件中的<kbd>sequences_length</kbd>属性来控制这些序列的长度。</p>

<p>You can further modify this example by modifying the contents or adding additional configuration options in the <kbd>config.py</kbd> file as shown:</p>

<pre># latent dimension of VAE (used in pitch-generator)<br/>latent_dim = 512<br/><br/># latent dimensions for pitches and durations (used in note-generator)<br/>latent_dim_p = 512<br/>latent_dim_d = 256<br/><br/># directory for saving the note embedding network model --- not used anymore<br/>note_embedding_dir = "models/note-embedding"<br/><br/># directory for saving the generator network model<br/>pitch_generator_dir = 'models/pitch-generator'<br/><br/># directory for saving the note generator network model<br/>note_generator_dir = 'models/note-generator'<br/><br/># directory for saving generated music samples<br/>output_dir = 'samples'</pre>

<p>The previous sample is great for exploring the generation of music. A more practical and potentially useful example will be introduced in the next section.</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Generating music via an alternative GAN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第二个例子的优点是能够下载你自己的MIDI文件，并把它们放在适当的输入文件夹中进行训练。有趣的是，两个项目都使用相似的三层LSTM结构，但在其他执行形式上却有很大不同。</h1>

                

            

            

                

<p>如果你想了解更多关于游戏，尤其是Unity的音频或音乐开发，请查阅迈克尔·兰哈姆的《用Unity 5.x 开发游戏音频》一书。这本书可以向你展示更多在游戏中使用音频和音乐的技巧。</p>

<p>这两个音乐样本都需要一些时间来训练，然后生成音乐，但这无疑是值得的努力来运行这两个例子，并了解它们是如何工作的。GANs创新了我们对训练神经网络的思考方式，以及它们能够产生何种类型的输出。因此，他们在为游戏生成内容方面肯定有一席之地。</p>

<pre><strong>python lstm.py</strong> <br/>or<br/><strong>python3 lstm.py</strong></pre>

<p>练习</p>

<p>请花些时间通过以下练习来巩固您的知识:</p>

<pre><strong>python predict.py</strong><br/>or<br/><strong>python3 predict.py</strong></pre>

<p>您会使用哪种类型的甘来转移图像上的样式？</p>

<p class="mce-root">你会用哪种甘来分离或提取花柱？</p>

<p class="mce-root">修改Wasserstein GAN示例中使用的批评家数量，并查看其对训练的影响。</p>

<p>修改第一个GAN，DCGAN，使用你在本章中学到的任何技术来提高训练性能。你是如何提高训练成绩的？</p>

<p>修改BatchNormalization动量参数，看看对训练有什么影响。</p>

<p>通过将激活从LeakyReLU更改为另一种高级激活形式来修改一些示例。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">修改Wasserstein GAN示例以使用您自己的纹理。本章的下载代码示例中提供了一个示例数据加载器。</h1>

                

            

            

                

<p>从<a href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a>下载一个其他参考gan，并修改它以使用您自己的数据集。</p>

<ol>

<li>改变第一代音乐甘使用不同的语料库。</li>

<li>用自己的MIDI文件训练第二代甘音乐的例子。</li>

<li>(奖金)哪种音乐甘产生更好的音乐？这是你所期望的吗？</li>

<li>Modify the first GAN, the DCGAN, to improve training performance using any technique you learned in this chapter. How did you increase training performance?</li>

<li>Modify the BatchNormalization momentum parameter and see what effect it has on training.</li>

<li>你当然不需要完成所有这些练习，但是可以试着做几个。马上将这些知识付诸实践可以大大提高你对材料的理解。毕竟，熟能生巧。</li>

<li>摘要</li>

<li>在这一章中，我们研究了生成性对抗网络(GANs ),作为一种构建DNNs的方法，它可以基于从其他内容中复制或提取特征来生成独特的内容。这也使我们能够探索无监督的训练，这是一种不需要预先数据分类或标记的训练方法。在前一章中，我们使用了监督训练。我们从目前给DL社区留下深刻印象的GANs的许多变体开始。然后，我们在Keras中编码了一个深度卷积GAN，接着是最先进的Wasserstein GAN。从那里，我们看了如何使用样本图像生成游戏纹理或高度图。我们看了两个可以从采样音乐生成原始MIDI音乐的音乐生成GANs，从而结束了这一章。</li>

<li>对于最后一个样本，我们研究了严重依赖RNNs (LSTM)的GANs的音乐生成。当我们考虑如何为游戏构建DL聊天机器人时，我们将继续探索RNNs。</li>

<li>Use your own MIDI files to train the second music generation GAN example.  </li>

<li>(BONUS) Which music GAN generated better music? Is it what you expected?</li>

</ol>

<p class="mce-root"/>

<p class="mce-root"/>

<p>You certainly don't have to work through all these exercises, but give a few a try. Putting this knowledge to practice right away can substantially improve your understanding of the material. Practice does make perfect, after all.</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Summary </h1>

                

            

            

                

<p>In this chapter, we looked at generative adversarial networks, or GANs, as a way to build DNNs that can generate unique content based on copying or extracting features from other content. This also allowed us to explore unsupervised training, a method of training that requires no previous data classification or labeling. In the previous chapter, we used supervised training. We started with looking at the many variations of GANs currently making an impression in the DL community. Then we coded up a deep convolutional GAN in Keras, followed by the state-of-the-art Wasserstein GAN. From there, we looked at how to generate game textures or height maps using sample images. We finished the chapter off by looking at two music-generating GANs that can generate original MIDI music from sampled music.</p>

<p>For the final sample, we looked at music generation with GANs that relied heavily on RNNs (LSTM). We will continue our exploration of RNNs when we look at how to build DL chatbots for games.</p>





            



            

        

    </body>



</html></body></html>