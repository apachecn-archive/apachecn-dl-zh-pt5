<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building Multi-Agent Environments</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">构建多代理环境</h1>

                

            

            

                

<p class="mce-root">有了我们的单代理经验，我们可以进入更复杂但同样有趣的在多代理环境中工作的世界，训练多个代理以合作或竞争的方式在同一环境中工作。这也为训练具有对抗性自我游戏、合作性自我游戏、竞争性自我游戏等等的代理人提供了几个新的机会。可能性在这里变得无穷无尽，这可能就是AI真正的圣杯。</p>

<p>在这一章中，我们将涉及多代理培训环境的几个方面，主要部分的主题将在这里突出显示:</p>

<ul>

<li>对抗与合作的自我游戏</li>

<li>竞技自我游戏</li>

<li>多脑游戏</li>

<li>用内在奖励增添个性</li>

<li>个性的外在奖励</li>

</ul>

<p>本章假设您已经学习了前三章，并完成了每一章中的一些练习。在下一节中，我们将开始讲述各种自我游戏场景。</p>

<p>最好从ML-Agents库的一个新克隆开始这一章。我们这样做是为了清理我们的环境，并确保不会无意中保存错误的配置。如果你需要这方面的帮助，请查阅前面的章节。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Adversarial and cooperative self-play</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">对抗与合作的自我游戏</h1>

                

            

            

                

<p>当然，术语<em>自我游戏</em>对许多人来说可能意味着许多事情，但在这种情况下，我们指的是大脑通过操纵多个代理与自己竞争(对抗)或合作。在ML-agent的情况下，这可能意味着让一个大脑在同一个环境中操纵多个agent。在ML-Agents中有一个很好的例子，所以打开Unity并按照下一个练习来为多代理培训准备这个场景:</p>

<ol>

<li>从Assets | ML-Agents | Examples | Soccer | Scenes文件夹中打开SoccerTwos场景。默认情况下，场景设置为在玩家模式下运行，但是我们需要将其转换回学习模式。</li>

<li>选择并禁用所有SoccerFieldTwos(1)至SoccerFieldTwos(7)区域。我们还不会用那些。</li>

<li>选择并展开剩余的活动SoccerFieldTwos对象。这将显示有四个代理的游戏区域，两个标记为RedStriker和BlueStriker，两个标记为RedGoalie和BlueGoalie。</li>

<li>检查代理，并根据需要设置每个人的大脑进行击球学习或射门学习，如下所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/bad9504b-6031-4dc7-89a1-6a0fdeccacb9.png" style="width:44.25em;height:55.75em;"/></p>

<p>在代理上设置学习大脑</p>

<ol start="5">

<li>在这个环境中，我们有四个由大脑控制的智能体，它们既相互合作又相互竞争。老实说，这个例子很精彩，非常好地展示了合作和竞争自我游戏的整个概念。如果您仍然对一些概念感到困惑，请考虑这张图表，它显示了这些概念是如何组合在一起的:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/7d798d2c-a983-47a0-9e7f-123947814e6d.png"/></p>

<p>苏格拉底的大脑结构</p>

<ol start="6">

<li>正如我们所见，我们有两个大脑控制四个代理人:两个前锋和两个守门员。前锋的工作是向守门员射门得分，当然，守门员的工作是阻挡进球。</li>

<li>选择学院并将足球学院|大脑|控制设置为启用两个大脑，如图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/bd97dcc4-28e4-4857-9f62-ae7795aeb202.png"/></p>

<p>在学院中设置大脑控制</p>

<ol start="8">

<li>此外，请注意足球学院组件底部的前锋、守门员奖励和惩罚设置。同样重要的是要注意<kbd>reward</kbd>对每个大脑的作用方式。以下是对该样本进行数学描述的<kbd>reward</kbd>函数:</li>

</ol>

<p class="CDPAlignCenter CDPAlign">【T8<br/><img class="fm-editor-equation" src="img/e5e35e1a-b4c1-4fed-9ffd-7a88b020fdeb.png" style="font-size: 1em;width:11.75em;height:1.42em;"/><br/><img class="fm-editor-equation" src="img/d2aec05a-effa-4726-9bd9-3543994fc124.png" style="font-size: 1em;width:10.58em;height:1.58em;"/><br/><img class="fm-editor-equation" src="img/bb21059d-f367-4c9f-9c51-386c18dbd594.png" style="font-size: 1em;width:11.58em;height:1.58em;"/></p>

<ol start="9">

<li>这意味着，当一个进球得分时，四个代理中的每一个都根据其位置和球队获得奖励。因此，如果红色进球，红色前锋将获得<kbd>+1</kbd>奖励，蓝色前锋获得<kbd>-0.1</kbd>奖励，红色守门员获得<kbd>+0.1</kbd>奖励，可怜的蓝色守门员获得<kbd>-1</kbd>奖励。现在，你可能认为这会导致重叠，但是请记住，每个代理对一个状态或一个观察的看法是不同的。因此，奖励将应用于该状态或观察的策略。本质上，代理基于其当前的环境视图进行学习，该环境视图将基于哪个代理发送该观察而改变。</li>

<li>完成编辑后，保存场景和项目。</li>

</ol>

<p>这为使用两个大脑和四个代理的多代理训练设置了场景，使用竞争和合作的自我游戏。在下一节中，我们将完成外部配置并开始训练场景。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training self-play environments</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练自我游戏环境</h1>

                

            

            

                

<p>训练这些类型的自我游戏环境不仅为增强的训练可能性，而且为有趣的游戏环境开辟了进一步的可能性。在某些方面，这些类型的训练环境看起来同样有趣，我们将在本章的结尾看到。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>不过，现在我们要跳回来，继续设置我们在下一个练习中训练SoccerTwos多代理环境所需的配置:</p>

<ol>

<li>打开<kbd>ML-Agents/ml-agents/config/trainer_config.yaml</kbd>文件，检查<kbd>StrikerLearning</kbd>和<kbd>GoalieLearning</kbd>配置部分，如图所示:</li>

</ol>

<pre style="padding-left: 60px">StrikerLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    <strong>batch_size: 128</strong><br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/>GoalieLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    <strong>batch_size: 320</strong><br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false</pre>

<ol start="2">

<li>显而易见的想法是，大脑应该有相似的配置，你可以这样开始，是的。然而，注意，即使在这个例子中，<kbd>batch_size</kbd>参数也是为每个大脑不同地设置的。</li>

<li>打开Python/Anaconda窗口，切换到您的ML-Agents虚拟环境，然后从<kbd>ML-Agents/ml-agents</kbd>文件夹启动以下命令:</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer --train</strong></pre>

<ol start="4">

<li>出现提示时，请按Play，您应该会看到以下培训课程正在运行:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/d39dccad-9782-46dd-b092-b65d37365fab.png"/></p>

<p>在训练模式下运行的SoccerTwos场景</p>

<ol start="5">

<li>正如已经说过的，这可以是一个非常有趣的观看样本，而且它的训练速度惊人的快。</li>

</ol>

<ol start="6">

<li>经过一些训练后，打开Python/Anaconda控制台，注意你现在是如何获得两个大脑的统计数据的，StrikerLearning和GoalieLearning，如下面的截图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/38567196-c47a-438e-b8de-42f02e49ade0.png"/></p>

<p>控制台输出显示两个大脑的统计数据</p>

<ol start="7">

<li>注意罢工学习和进球学习是如何互相回报相反的回报的。这意味着，为了训练这些代理人，他们必须将两个代理人的平均奖励平衡为0。随着代理的训练，您会注意到他们的奖励开始收敛到0，这是本例中的最佳奖励。</li>

<li>让示例运行到完成。你很容易迷失在这些环境中，所以你可能甚至没有注意到时间的流逝。</li>

</ol>

<p>这个例子展示了我们如何通过自我游戏来利用多智能体训练的力量，教会两个大脑如何同时竞争和合作。在下一节中，我们将看到多个代理在自我游戏中相互竞争。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Adversarial self-play</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">对抗性自我游戏</h1>

                

            

            

                

<p>在前面的例子中，我们看到了一个合作和竞争自我游戏的例子，其中多个主体几乎是共生的。虽然这是一个很好的例子，但它仍然通过奖励功能将一个大脑的功能与另一个大脑的功能联系起来，因此我们观察到代理人处于几乎与奖励相反的情景中。相反，我们现在想看看一个环境，它可以用多个智能体通过对抗性的自我游戏来训练大脑。当然，ML-Agents有这样一个环境，称为Banana，它由几个随机漫游场景并收集香蕉的代理组成。代理人也有一个激光指示器，这允许他们在被击中时使对手瘫痪几秒钟。这是我们将在下一个练习中看到的场景:</p>

<ol>

<li>从“资产| ML-代理|示例| BananaCollectors |场景”文件夹中打开香蕉场景。</li>

<li>选择并禁用附加训练区域RLArea(1)至RLArea(3)。</li>

<li>在RLArea中选择五个代理(代理、代理(1)、代理(2)、代理(3)、代理(4))。</li>

<li>把香蕉代理|大脑从BananaPlayer换成BananaLearning。</li>

<li>选择学院，并将香蕉学院|大脑|控制属性设置为启用。</li>

<li>在编辑器中选择Banana代理组件(脚本)，并在您选择的代码编辑器中打开它。如果向下滚动到底部，可以看到如下所示的<kbd>OnCollisionEnter</kbd>方法:</li>

</ol>

<pre style="padding-left: 60px">void OnCollisionEnter(Collision collision)<br/>{<br/>  if (collision.gameObject.CompareTag("banana"))<br/>  {<br/>    Satiate();<br/>    collision.gameObject.GetComponent&lt;BananaLogic&gt;().OnEaten();<br/>    <strong>AddReward(1f);</strong><br/>    bananas += 1;<br/>    if (contribute)<br/>    {<br/>      myAcademy.totalScore += 1;<br/>    }<br/>  }<br/> if (collision.gameObject.CompareTag("badBanana"))<br/> {<br/>   Poison();<br/>   collision.gameObject.GetComponent&lt;BananaLogic&gt;().OnEaten();<br/><br/>   <strong>AddReward(-1f);</strong><br/>   if (contribute)<br/>   {<br/>     myAcademy.totalScore -= 1;<br/>   }<br/>  }<br/>}</pre>

<ol start="7">

<li>阅读前面的代码，我们可以将我们的<kbd>reward</kbd>函数总结如下:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/069ca155-ee9c-4ff0-a01a-2cff13ed6d22.png" style="font-size: 1em;width:12.17em;height:1.33em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/27024412-a985-427d-a522-a5586088562c.png" style="width:11.92em;height:1.25em;"/></p>

<p style="padding-left: 60px">这仅仅意味着代理人只收到吃香蕉的奖励。有趣的是，用激光或被禁用来禁用对手是没有奖励的。</p>

<ol start="8">

<li>保存场景和项目。</li>

<li>打开一个准备好的Python/Anaconda控制台，使用以下命令开始训练:</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=banana --train</strong></pre>

<ol start="10">

<li>当出现提示时，在编辑器中按Play，并观察动作的展开，如下一个屏幕截图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/e5e1b177-d51f-40c2-abed-6be4a9549669.png" style="width:42.50em;height:30.25em;"/></p>

<p>香蕉收集代理人正在做他们的工作</p>

<ol start="11">

<li>让场景运行多久都行。</li>

</ol>

<p>这个场景是一个很好的例子，说明代理人如何学习使用不返回奖励的第二游戏机制，但像激光一样，仍然被用来阻止敌对的收集者并获得更多的香蕉，同时只因为只吃香蕉而获得奖励。这个例子展示了RL的真正力量，以及它如何被用来寻找解决问题的次要策略。虽然这是一个非常有趣的方面，在游戏中观看也很有趣，但请考虑一下这一点的更大含义。RL已经被证明可以使用<strong>对抗性自我游戏</strong>来优化从网络到推荐系统的一切，看看这种学习方法在不久的将来能够实现什么将会很有趣。</p>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Multi-brain play</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">多脑游戏</h1>

                

            

            

                

<p>ML-Agents套件的一个真正伟大之处是能够快速添加由多个大脑驱动的多个代理。这反过来让我们有能力构建更复杂的游戏环境或场景，与有趣的代理人/人工智能一起玩或对抗。让我们看看将我们的足球示例转换成让代理都使用单独的大脑是多么容易:</p>

<ol>

<li>打开编辑器，进入我们之前看到的SoccerTwos场景。</li>

<li>在Assets | ML-Agents | Examples | Soccer | Brains中找到示例的<kbd>Brains</kbd>文件夹。</li>

<li>单击窗口右上角的创建菜单，从上下文菜单中选择ML-Agents | Learning Brain:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/5529ce20-f7be-4dcf-910c-89567b1b8c23.png" style="width:31.00em;height:24.75em;"/></p>

<p>创造一个新的学习大脑</p>

<ol start="4">

<li>将新大脑命名为<kbd>RedStrikerLearning</kbd>。在同一个文件夹中再创建三个名为<kbd>RedGoalieLearning</kbd>、<kbd>BlueGoalieLearning</kbd>和<kbd>BlueStrikerLearning</kbd>的新大脑。</li>

</ol>

<ol start="5">

<li>选择RedStrikerLearning。然后选择并拖动StrikerLearning brain，并将其放入“复制brain参数from slot:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/895d4ef3-1d95-41ae-90d8-6b2bd63ae8c5.png"/></p>

<p>从另一个大脑复制大脑参数</p>

<ol start="6">

<li>对BlueStrikerLearning执行此操作，从StrikerLearning复制参数。然后对RedGoalieLearning和BlueGoalieLearning做同样的事情，从GoalieLearning复制参数。</li>

<li>在“层次”窗口中选择RedAgent，并将代理Soccer | Brain设置为RedStrikerLearning。对其他每个代理都这样做，将颜色与位置匹配。blue goalie<strong>-&gt;</strong>blue goalie learning。</li>

<li>选择学院并从足球学院|大脑列表中删除所有当前大脑。然后使用“添加新大脑”按钮将我们刚刚创建的所有新大脑添加回列表中，并将其设置为控制:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/becf9e28-691c-454c-b52d-cbac98e3881e.png" style="width:32.50em;height:45.50em;"/></p>

<p>向学院添加新的大脑</p>

<ol start="9">

<li>保存场景和项目。现在，我们刚刚把在自我游戏模式中使用两个并行大脑的例子换成了团队中的个体代理。</li>

<li>打开为培训设置的Python/Anaconda窗口，并使用它启动以下内容:</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_mb --train</strong></pre>

<ol start="11">

<li>让培训继续进行，并注意代理开始时的表现如何与之前一样好。也看一下控制台输出。你会看到它现在报告了四个代理人，但代理人仍然有点共生，因为红色的前锋与蓝色的守门员相对。然而，他们现在训练得慢得多，部分原因是每个大脑现在只能看到一半的观察结果。请记住，我们之前让两个攻击代理人都向一个大脑提供信息，正如我们所了解的，这种额外的状态输入可以大大加快训练。</li>

</ol>

<p>在这一点上，我们有四个代理人用四个独立的大脑在玩足球游戏。当然，由于代理人仍然通过共享一个奖励功能来进行共生训练，我们不能真正地将他们描述为个体。除此之外，正如我们所知，在团队中打球的个人通常会受到他们自己内部或内在奖励系统的影响。在下一节中，我们将看看内在奖励的应用如何使最后一个练习变得更有趣。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Adding individuality with intrinsic rewards</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">用内在奖励增添个性</h1>

                

            

            

                

<p>正如我们在<a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">第9章</a>、<em>奖励和强化学习</em>中了解到的，内在奖励系统和代理动机的概念目前在ML-Agents中仅仅作为<strong>好奇心学习</strong>来实现。将内在奖励或动机与RL相结合的整个领域广泛应用于游戏和人际应用，如<strong>仆人代理</strong>。</p>

<p>在下一个练习中，我们将为我们的几个代理添加内在奖励，并看看这对游戏有什么影响。打开上一练习中的场景，并按照以下步骤操作:</p>

<ol>

<li>在文本编辑器中打开<kbd>ML-Agents/ml-agents/config/trainer_config.yaml</kbd>文件。我们从未给我们的代理添加任何专门的配置，但我们现在要纠正这一点，并添加一些额外的配置。</li>

</ol>

<ol start="2">

<li>将以下四种新的大脑配置添加到文件中:</li>

</ol>

<pre style="padding-left: 60px">BlueStrikerLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 128<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><strong><br/>BlueGoalieLearning:</strong><br/><strong>    use_curiosity: true</strong><br/><strong>    summary_freq: 1000</strong><br/><strong>    curiosity_strength: 0.01</strong><br/><strong>    curiosity_enc_size: 256</strong><br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 320<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256 <br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/><strong>RedStrikerLearning:</strong><br/>    <strong>use_curiosity: true</strong><br/><strong>    summary_freq: 1000</strong><br/><strong>    curiosity_strength: 0.01</strong><br/><strong>    curiosity_enc_size: 256</strong><br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 128<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256 <br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/>RedGoalieLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 320<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false</pre>

<ol start="3">

<li>请注意我们是如何在<kbd>BlueGoalieLearning</kbd>和<kbd>RedStrikerLearning</kbd>大脑上启用<kbd>use_curiosity: true</kbd>的。您可以从文件中已经存在的原始<kbd>GoalieLearning</kbd>和<kbd>StrikerLearning</kbd>大脑配置中复制并粘贴大部分内容；注意细节就好。</li>

<li>完成编辑后保存文件。</li>

<li>打开Python/Anaconda控制台，使用以下命令开始训练:</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_icl --train</strong></pre>

<ol start="6">

<li>让代理训练一段时间，您会注意到，虽然他们看起来更像个人，但他们的训练能力仍然很差，而我们在训练中看到的任何改进都可能会引起一些代理的好奇。</li>

</ol>

<p>这种通过内在奖励或动机为代理添加个性的能力肯定会像DRL为游戏和其他潜在应用所做的那样成熟，并有望提供其他可能不完全专注于学习的内在奖励模块。然而，内在奖励确实可以鼓励个性，所以在下一节中，我们在修改后的例子中引入外在奖励。</p>

<p>迁移学习的另一个很好的应用是，在代理人接受了一般任务的培训后，能够添加内在的奖励模块。</p>

<p class="mce-root"/>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Extrinsic rewards for individuality</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">个性的外在奖励</h1>

                

            

            

                

<p>我们已经在几个章节中广泛讨论了外部或外在奖励，以及如何使用技巧来优化和鼓励代理人获得这些奖励。现在，要改变一个代理人的行为，似乎最简单的方法就是改变它的外在奖励或者本质上的奖励功能。然而，这可能很容易出现困难，并且这经常会使培训表现变差，这就是我们在上一节中为几个代理添加<strong>课程学习</strong> ( <strong> CL </strong>)时所看到的情况。当然，即使我们让训练变得更糟糕，我们现在也有一些技巧，如<strong>迁移学习</strong> ( <strong> TL </strong>)，也称为<strong>模仿学习</strong>(<strong>IL</strong>)；<strong>好奇心</strong>；还有CL，帮助我们改正错误。</p>

<p>在下一个练习中，我们将通过增加额外的外部奖励来为我们的代理增加更多的个性。打开我们刚刚做的上一个练习示例，然后继续:</p>

<ol>

<li>从菜单中，选择窗口|资产存储。这将带您到Unity资产商店，这是一个很好的助手资产资源。虽然这些资产中的大部分都是付费的，但老实说，与可比的开发人员工具相比，价格是最低的，并且有几个免费的非常优秀的资产，您可以开始使用它们来增强您的培训环境。资产商店是Unity最好和最糟糕的事情之一，所以如果你购买资产，一定要阅读评论和论坛帖子。任何好的资产通常都会有自己的论坛，如果它是以开发人员为中心的，艺术资产就更少了。</li>

<li>在搜索栏中，输入<kbd>toony tiny people</kbd>并按下<em> Enter </em>键或点击搜索按钮。这将显示搜索结果。</li>

</ol>

<p>我们要感谢<strong>多边形铁匠</strong>的支持，允许我们将他们的Toony Tiny People演示资产与书的源代码一起发布。此外，他们收集的角色资产做得非常好，使用简单。如果你决定要制作一个完整的游戏或者增强版的试玩，这个价格对于一些大的资产包来说也是一个很好的起点。</p>

<ol start="3">

<li>选择多边形铁匠名为Toony Tiny People Demo的结果并选中它。它将如下图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/b1f3ba55-d605-4c21-828c-7cdf29d5f0a4.png" style="width:43.58em;height:33.50em;"/></p>

<p>来自多边形铁匠的图尼小人演示资产</p>

<ol start="4">

<li>单击红色的Download按钮，在资产下载完成后，该按钮将变为Import，如前面的屏幕截图所示。单击“导入”按钮导入资产。当导入对话框提示您时，确保所有内容都已选中，然后单击导入。</li>

</ol>

<p>这些类型的低多边形或卡通资产是完美的，使一个简单的游戏或模拟更具娱乐性和趣味性。这可能看起来不多，但你可以花很多时间看这些训练模拟人跑步，如果他们看起来有吸引力，那会有帮助。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<ol start="5">

<li>选择并展开层次结构中的所有代理对象。这包括RedStriker、BlueStriker、RedGoalie和BlueGoalie。</li>

<li>在项目窗口中打开资产| tooytinpeople | TT _演示|预设文件夹。</li>

<li>从前面的文件夹中选择并拖动TT _演示_女性预设，并将其放入“层次”窗口中的RedStriker代理对象。选择代理下方的立方体对象，并在检查器中禁用它。根据以下列表，对其他代理继续执行此操作:<ul>

<li>TT _德莫_女-&gt;红人</li>

<li>TT _德莫_马累_A -&gt;蓝魔</li>

<li>TT _ demo _警察-&gt; BlueGoalie</li>

<li>TT _ demo _僵尸-&gt; RedGoalie</li>

</ul>

</li>

</ol>

<p style="padding-left: 60px">这张截图进一步证明了这一点:</p>

<div><img src="img/13987114-bc54-4b7a-9fc1-4b3716e8b4ec.png"/></div>

<p>设置新的代理主体</p>

<ol start="8">

<li>确保还将新代理模型的变换位置和方向重置为<kbd>[0,0,0]</kbd>，如以下屏幕截图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/362c8dbb-4f85-4fc3-bbde-1b2f2d0de4f9.png"/></p>

<p>重置拖动的预设的方向和位置</p>

<ol start="9">

<li>保存场景和项目。</li>

</ol>

<p>此时，您可以在训练中运行场景，并观看新的代理模型四处移动，但没有多大意义。代理人仍然会采取相同的行动，所以我们接下来需要做的是根据一些任意的个性设置额外的外部奖励，我们将在下一节中定义。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Creating uniqueness with customized reward functions </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">利用定制的奖励功能创造独特性</h1>

                

            

            

                

<p>我们设法通过增加内在奖励使我们的代理变得独特，虽然结果可能不如我们希望的那样独特。这意味着我们现在想看看修改代理的外在奖励，希望使他们的行为更独特，并最终使游戏更具娱乐性。</p>

<p>我们开始这样做的最好方法是看看我们之前描述的<kbd>SoccerTwos</kbd>奖励函数；这里列出了这些，以供参考:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8360374e-f459-4fef-98dd-3b11264f0636.png" style="width:8.92em;height:1.42em;"/><br/><img class="fm-editor-equation" src="img/e943d6d7-592f-48e8-8bf4-1ec5798f5f5b.png" style="width:11.00em;height:1.33em;"/><br/><img class="fm-editor-equation" src="img/594438d5-58dc-40fe-b993-cdf625b439ae.png" style="width:9.42em;height:1.42em;"/><br/>T9】</p>

<p>我们现在要做的是根据当前角色对奖励函数进行一些个性化的修改。我们将通过简单地根据字符类型修改链接函数来实现这一点，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/03fb39c9-5222-466c-a90a-b30d4e4c340c.png" style="width:15.00em;height:1.25em;"/>或<img class="fm-editor-equation" src="img/7be32220-52f5-4067-a9a1-d9731f2d59dc.png" style="width:16.67em;height:1.25em;"/> <br/> <img class="fm-editor-equation" src="img/a2547f6b-3e83-4ce3-abb7-a9405f5ef26b.png" style="width:16.33em;height:1.42em;"/>或<img class="fm-editor-equation" src="img/33de502b-245d-4659-9715-41af693970b9.png" style="width:17.08em;height:1.33em;"/> <br/> <img class="fm-editor-equation" src="img/fd164fb9-7793-492c-a371-3bfac8a77c08.png" style="width:12.50em;height:1.25em;"/>或<img class="fm-editor-equation" src="img/b540589a-8861-490a-8476-253b634e98b7.png" style="width:14.17em;height:1.25em;"/> <br/> <img class="fm-editor-equation" src="img/dd76d52b-c033-4888-856b-f5e6384f4440.png" style="width:15.67em;height:1.33em;"/>或<img class="fm-editor-equation" src="img/ef696a36-6623-4c61-bb89-57805927aac0.png" style="width:16.33em;height:1.25em;"/></p>

<p>我们在这里用这些奖励函数所做的只是通过一些个性修改来修改奖励值。对于女孩，我们给她1.25倍奖励的奖金，反映她可能是兴奋的。男孩不太兴奋，所以我们把他的奖励修改了0.95倍，稍微减少了一些。警察，总是冷静和控制，保持不变，没有奖励修改。最后，我们引入一个通配符，半死不活的僵尸。为了将它描述为半死，我们也将它的所有奖励减半。</p>

<p>当然，根据你的游戏机制，你可以以任何你喜欢的方式修改这些功能，但是重要的是要注意你所应用的个性修改的效果可能会阻碍训练。当我们开始训练这个例子时，一定要记住这一点。</p>

<p class="mce-root">一个女孩、一个男孩、一个僵尸和一个警察进入足球场。</p>

<p>既然我们已经理解了新的奖励函数，我们想在我们的例子中添加一点，是时候打开Unity并对它们进行编码了。这个例子需要对C#文件稍加修改，但是代码非常简单，任何有C语言经验的程序员都应该很容易理解。</p>

<p>打开Unity，进入我们在上一个示例中修改的场景，然后进行下一个练习:</p>

<ol>

<li>在“层次”窗口中找到RedStriker代理并选择它。</li>

</ol>

<ol start="2">

<li>从检查器中，单击代理足球组件旁边的齿轮图标，并从上下文菜单中选择编辑脚本。这将在您的编辑器中打开脚本和解决方案。</li>

<li>在文件顶部当前<kbd>enum AgentRole</kbd>之后添加一个名为<kbd>PersonRole</kbd>的新<kbd>enum</kbd>，如代码所示:</li>

</ol>

<pre style="padding-left: 60px">public enum AgentRole<br/>{<br/>  striker,goalie<br/>} <em>//after this line</em><br/><strong>public enum PersonRole</strong><br/><strong>{</strong><br/><strong>  girl, boy, police, zombie</strong><br/><strong>}</strong></pre>

<ol start="4">

<li>这创造了一个新的角色，本质上是我们想要应用到每个大脑的个性。</li>

<li>向该类添加另一个新变量，如下所示:</li>

</ol>

<pre style="padding-left: 60px">public AgentRole agentRole; <em>//after this line</em><br/><strong>public PersonRole playerRole;</strong></pre>

<ol start="6">

<li>这就给代理添加了新的<kbd>PersonRole</kbd>。现在，我们还想通过向<kbd>InitializeAgent</kbd>方法添加一行来将新类型添加到设置中，如下所示:</li>

</ol>

<pre style="padding-left: 60px">public override void InitializeAgent()<br/>{<br/>  base.InitializeAgent();<br/>  agentRenderer = GetComponent&lt;Renderer&gt;();<br/>  rayPer = GetComponent&lt;RayPerception&gt;();<br/>  academy = FindObjectOfType&lt;SoccerAcademy&gt;();<br/>  PlayerState playerState = new PlayerState();<br/>  playerState.agentRB = GetComponent&lt;Rigidbody&gt;();<br/>  agentRB = GetComponent&lt;Rigidbody&gt;();<br/>  agentRB.maxAngularVelocity = 500;<br/>  playerState.startingPos = transform.position;<br/>  playerState.agentScript = this;<br/>  area.playerStates.Add(playerState);<br/>  playerIndex = area.playerStates.IndexOf(playerState);<br/>  playerState.playerIndex = playerIndex;<br/>  <strong>playerState.personRole = personRole;  <em>//add this line</em></strong><br/>}</pre>

<ol start="7">

<li>现在，您应该会在该行中看到一个错误。这是因为我们还需要将新的<kbd>personRole</kbd>属性添加到<kbd>PlayerState</kbd>中。打开<kbd>PlayerState</kbd>类并添加属性，如下所示:</li>

</ol>

<pre style="padding-left: 60px">[System.Serializable]<br/>public class PlayerState<br/>{<br/>  public int playerIndex; <br/>  public Rigidbody agentRB; <br/>  public Vector3 startingPos; <br/>  public AgentSoccer agentScript; <br/>  public float ballPosReward;<br/>  public string position;<br/>  <strong>public AgentSoccer.PersonRole personRole { get; set; }  <em>//add me</em></strong><br/>}</pre>

<ol start="8">

<li>你现在应该在<kbd>SoccerFieldArea.cs</kbd>文件中。滚动到<kbd>RewardOrPunishPlayer</kbd>方法并修改它，如下所示:</li>

</ol>

<pre style="padding-left: 60px">public void RewardOrPunishPlayer(PlayerState ps, float striker, float goalie)<br/>{<br/>  if (ps.agentScript.agentRole == AgentSoccer.AgentRole.striker)<br/>  { <br/>    <strong>RewardOrPunishPerson(ps, striker);  <em>//new line</em></strong><br/>  }<br/>  if (ps.agentScript.agentRole == AgentSoccer.AgentRole.goalie)<br/>  { <br/>    <strong>RewardOrPunishPerson(ps, striker); <em>//new line</em></strong><br/>  }<br/>  ps.agentScript.Done(); //all agents need to be reset<br/>}</pre>

<ol start="9">

<li>我们在这里做的是注入另一个奖励函数，<kbd>RewardOrPunishPerson</kbd>，以增加我们外在的个性奖励。接下来，添加一个新的<kbd>RewardOrPunishPerson</kbd>方法，如图所示:</li>

</ol>

<pre style="padding-left: 60px">private void RewardOrPunishPerson(PlayerState ps, float reward)<br/>{<br/>  switch (ps.personRole)<br/>  {<br/>    case AgentSoccer.PersonRole.boy:<br/>      ps.agentScript.AddReward(reward * .95f);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.girl:<br/>      ps.agentScript.AddReward(reward*1.25f);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.police:<br/>      ps.agentScript.AddReward(reward);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.zombie:<br/>      ps.agentScript.AddReward(reward * .5f);<br/>      break;<br/>  }<br/>}</pre>

<ol start="10">

<li>这段代码做的正是我们之前定制的奖励函数所做的。完成编辑后，保存所有文件并返回Unity编辑器。如果有任何错误或编译器警告，它们将显示在控制台中。如果您需要返回并修复任何(红色)错误问题，请这样做。</li>

</ol>

<p>正如你所看到的，用很少的代码，我们就能添加我们外在的个性奖励。当然，您可以用多种方式增强这个系统，甚至使它更加通用和参数驱动。在下一部分中，我们希望将所有这些放在一起，并对我们的代理进行单独培训。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Configuring the agents' personalities</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">配置代理的个性</h1>

                

            

            

                

<p>设置好所有代码后，我们现在可以继续回到编辑器中，设置代理来匹配我们想要应用到它们的个性。再次打开编辑器，按照下一个练习将个性应用到代理并开始培训:</p>

<ol>

<li>在层次结构中选择RedStriker，并将我们刚刚创建的Agent Soccer | Person角色参数设置为Girl，如图所示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/54e3b787-d936-453c-8103-5aa2c5fd599d.png"/></p>

<p>设置每个代理的个性</p>

<ol start="2">

<li>用与我们之前分配的模型相匹配的相关个性更新所有代理:BlueStriker-&gt; Boy、BlueGoalie -&gt; Police和RedGoalie -&gt; Zombie，如前面的截图所示。</li>

<li>保存场景和项目。</li>

<li>现在，在这一点上，如果你想让它更详细，你可能想回过头来更新每个代理大脑名称，以反映他们的个性，如GirlStrikerLearning或PoliceGoalieLearning，你可以省略团队颜色。确保将新的大脑配置设置添加到您的<kbd>trainer_config.yaml</kbd>文件中。</li>

<li>打开Python/Anaconda培训控制台，使用以下命令开始培训:</li>

</ol>

<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_peeps --train</strong></pre>

<ol start="6">

<li>现在，这看起来非常有趣，正如你在下面的截图中看到的:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/c9e87060-970f-4b74-91d6-0add4c847e12.png"/></p>

<p>观看个人踢足球</p>

<ol start="7">

<li>请注意，我们如何保持团队颜色立方体处于活动状态，以便显示每个代理属于哪个团队。</li>

</ol>

<ol start="8">

<li>让代理训练几千次迭代，然后打开控制台；请注意这些代理现在看起来不那么共生了。在我们的例子中，它们仍然是成对的，因为我们只对奖励进行了简单的线性变换。当然，你可以应用更复杂的非线性函数，这些函数描述你的代理人的其他动机或个性。</li>

<li>最后，让我们打开TensorBoard，看看我们的多智能体训练的更好的对比。打开另一个Python/Anaconda控制台，打开您当前所在的<kbd>ML-Agents/ml-agents</kbd>文件夹，并运行以下命令:</li>

</ol>

<pre style="padding-left: 60px"><strong>tensorboard --logdir=summaries</strong></pre>

<ol start="10">

<li>使用浏览器打开TensorBoard界面并检查结果。确保禁用任何额外的结果，只关注我们当前训练中的四个大脑。我们要关注的三个主要图在此图中合并在一起显示:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/702bce1d-2467-4a1a-b68e-dbc70a022b67.png" style="width:36.83em;height:28.00em;"/></p>

<p>显示四个大脑训练结果的张量图</p>

<p>从TensorBoard的结果可以看出，代理人的训练不是很好。当然，我们可以通过增加额外的培训领域和提供更多的观察来加强这一点，以便培训策略。然而，如果你看一下<strong>政策损失</strong>图，结果显示代理商的竞争导致最小的政策变化，这在培训的早期是一件坏事。如果有什么不同的话，僵尸代理似乎是从这些结果中学习最好的代理。</p>

<p>当然，还有很多其他方法可以修改你的外在奖励函数，以鼓励多主体训练场景中的一些行为方面。这些技术有些效果很好，有些则不太好。我们仍然处于开发这项技术的早期阶段，最佳实践仍然需要出现。</p>

<p>在下一节中，我们期待你能继续做进一步的练习，以巩固你对本章所有内容的了解。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">练习</h1>

                

            

            

                

<p>和往常一样，为了你自己的享受和学习，你可以尝试至少一两个下面的练习:</p>

<ol>

<li>打开BananaCollectors示例香蕉场景，并在训练模式下运行它。</li>

<li>修改BananaCollectors | Banana场景，使其使用五个独立的学习大脑，然后在训练模式下运行。</li>

<li>修改上次SoccerTwos练习中的奖励函数，使用指数或对数函数。</li>

<li>修改上次SoccerTwos练习中的奖励函数，以使用非逆相关和非线性函数。这样，对每个人来说，改变积极和消极奖励的方法是不同的。</li>

<li>用不同的角色和个性修改SoccerTwos场景。建立新的奖励函数模型，然后培训代理。</li>

<li>修改BananaCollectors示例香蕉场景，使用与SoccerTwos示例相同的个性和自定义奖励函数。</li>

<li>以BananaCollectors为例做练习3。</li>

<li>以BananaCollectors为例做练习4。</li>

<li>以BananaCollectors为例做练习5。</li>

<li>使用当前示例之一作为模板构建一个新的多代理环境，或者创建您自己的环境。这最后一个练习很可能会变成你自己的游戏。</li>

</ol>

<p class="mce-root"/>

<p class="mce-root"/>

<p>你可能已经注意到，随着我们阅读这本书，练习变得更加耗时和困难。为了你自己的利益，请至少完成几个练习。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在这一章中，我们探索了多智能体训练环境的可能性。我们首先研究了如何利用自我游戏建立环境，在自我游戏中，一个大脑可以控制多个大脑，这些大脑既相互竞争又相互合作。然后，我们研究了如何使用ML-Agents好奇心学习系统，以好奇心的形式增加内在奖励的个性。接下来，我们研究了外在奖励如何被用来塑造一个代理人的个性和影响培训。为此，我们添加了一个免费的风格资产，然后通过奖励函数链应用自定义的外部奖励。最后，我们训练了环境，并被男孩代理人扎实地击败僵尸的结果所娱乐；如果您观看培训直到结束，您将会看到这一点。</p>

<p>在下一章，我们将看看DRL的另一个新颖的应用程序，用于调试和测试已经构建好的游戏。</p>





            



            

        

    </body>



</html></body></html>