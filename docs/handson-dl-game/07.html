<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Introducing DRL</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">介绍DRL</h1>

                

            

            

                

<p class="mce-root"><strong>深度强化学习</strong> ( <strong> DRL </strong>)目前正在席卷全球，被视为机器学习技术的“it”，即达到某种形式的通用人工智能的it目标。也许这是因为DRL接近了人工智能或者我们所认为的一般智能的尖端。这也可能是你读这本书的主要原因之一。幸运的是，这一章，以及本书其余部分的大部分，都深深地聚焦于<strong>强化学习</strong> ( <strong> RL </strong>)及其许多变体。在这一章中，我们开始学习RL的基础知识，以及它如何适应<strong>深度学习</strong> ( <strong> DL </strong>)。我们将探索<strong> OpenAI Gym </strong>环境，一个很棒的RL游乐场，并看看如何用一些简单的DRL技术来使用它。</p>

<p>请记住，这是一本动手操作的书，所以我们将把技术理论保持在最低限度，相反，我们将探索大量的工作示例。一些读者可能会因为没有理论背景而感到迷茫，觉得有必要自己去探索RL更理论的一面。<br/> <br/>对于其他不熟悉RL理论背景的读者，我们会涉及几个核心概念，但这是删节版，所以建议你在准备好的时候从其他来源寻求理论知识。</p>

<p>在这一章，我们将开始学习DRL，这个话题将贯穿许多章节。我们将从基础开始，然后探索一些适用于DL的工作示例。以下是我们将在本章中涉及的内容:</p>

<ul>

<li>强化学习</li>

<li>Q-学习模型</li>

<li>经营露天健身房</li>

<li>第一个具有深Q网络的DRL</li>

<li>RL实验</li>

</ul>

<p class="mce-root"/>

<p class="mce-root"/>

<p>对于喜欢跳来跳去看书的人:是的，从这一章开始看这本书是可以的。但是，为了完成一些练习，您可能需要回到前面的章节。我们还假设您的Python环境配置了TensorFlow和Keras，但是如果您不确定，请查看项目文件夹中的<kbd>requirements.txt</kbd>文件。</p>

<p>本书中的所有项目都是用Visual Studio 2017 (Python)构建的，它是本书中示例的推荐编辑器。如果将VS 2017与Python配合使用，可以通过打开章节解决方案文件轻松管理样本。当然，还有很多其他优秀的Python编辑器和工具，所以使用你觉得舒服的。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reinforcement learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">强化学习</h1>

                

            

            

                

<p>与其他机器学习方法相比，RL目前在进步方面领先。请注意使用的是<em>方法</em>一词，而不是<em>技术</em>。RL是一种方法或算法，它应用了我们可以与神经网络一起使用的原理，而神经网络是一种可以应用于多种方法的机器学习技术。之前，我们研究了与DL相结合的其他方法，但是我们更关注实际的实现。然而，RL引入了一种新的方法，要求我们在了解如何应用它之前，了解更多的内部和外部工作方式。</p>

<p>RL是由加拿大人理查德·萨顿推广的，他现在是阿尔伯塔大学的教授。萨顿还在谷歌的DeepMind协助开发RL，并经常被视为RL之父。</p>

<p>任何机器学习系统的核心都是对训练的需求。通常，人工智能代理/大脑什么都不知道，然后我们通过一些自动化过程向它提供数据，让它学习。正如我们已经看到的，最常见的方式是所谓的<strong>监督训练</strong>。这是我们第一次标记我们的训练数据。我们还研究了<strong>无监督训练</strong>，其中我们的<strong>生成对抗网络</strong> ( <strong> GANs </strong>)通过相互竞争来训练。然而，这两个系统都没有复制我们在<strong>生物学</strong>中看到的学习或训练类型，这通常被称为<strong>奖励</strong>或RL:这种学习类型让你教你的狗吠叫以获得奖励，取纸，并利用户外进行自然呼唤，这种学习类型让代理探索其自身的环境并为自己学习。这与一般人工智能预期使用的学习类型没有什么不同；毕竟，RL很可能类似于我们使用的系统，或者我们是这样认为的。</p>

<p class="mce-root"/>

<p>大卫·西尔弗是萨顿教授以前的学生，现在是DeepMind的负责人，他有一个关于RL理论背景的优秀视频系列。前五个视频相当有趣，推荐观看，但后面的内容变得相当深入，可能不适合每个人。以下是视频的链接:<a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">https://www.youtube.com/watch?v=2pWv7GOvuf0</a></p>

<p>RL定义了自己的训练类型，并以相同的名称进行调用。下图显示了这种基于奖励的培训形式:</p>

<div><img src="img/9b176d0d-9b93-423d-9d0c-cb1998759379.png" style="width:40.08em;height:20.17em;"/><br/>

<br/>

Reinforcement learning </div>

<p class="mce-root">该图显示了环境中的代理。该代理读取环境的状态，然后决定并执行一个动作。这个行为可能会，也可能不会，给予奖励，而这个奖励可能是好的，也可能是坏的。在每次行动和可能的奖励之后，代理再次收集环境的状态。该过程重复进行，直到代理达到终止或结束状态。也就是说，直到它达到目标；也许它死了，或者只是累了。注意上图中的一些微妙之处是很重要的。首先，代理人并不总是收到奖励，这意味着奖励可能会被推迟，直到某个未来目标达成。这与我们之前探索的其他学习形式截然不同，后者为我们的培训网络提供了即时反馈。奖励可以是好的也可以是坏的，用这种方式消极地训练代理人通常同样有效，但对人类来说就没那么有效了。</p>

<p>现在，正如你可能预料的那样，对于任何强大的学习模型，数学可能会非常复杂，并且肯定会让新手望而生畏。除了在下一节描述RL的一些基础之外，我们不会过多地讨论理论细节。</p>

<p>多臂强盗</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The multi-armed bandit</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们之前看到的图表描述了完整的RL问题，我们将在本书的大部分内容中使用。然而，我们经常教授这个问题的一个更简单的单步变体，称为<strong>多臂强盗</strong>。武装匪徒指的是拉斯维加斯的老虎机，没有比这更邪恶的了。我们使用这些更简单的场景，以单步或单态问题的形式解释RL的基础知识。</h1>

                

            

            

                

<p>在多臂强盗的例子中，想象一个虚构的多臂拉斯维加斯吃角子老虎机，它根据拉动哪只手臂来奖励不同的奖励，但是每只手臂的奖励总是相同的。在这种情况下，代理的目标是每次都找出正确的手臂。我们可以用如下所示的等式对此进行进一步建模:</p>

<p><img class="fm-editor-equation" src="img/cfabb87d-9a06-4b96-8661-72ef51f3bb0a.png" style="color: #333333;font-size: 1em;width:1.92em;height:1.08em;"/> =值的向量(1，2，3，4)</p>

<div><img class="fm-editor-equation" src="img/cce0e6d0-2ea0-4ea0-b7e2-6e13a93eb5fb.png" style="width:12.00em;height:1.25em;"/></div>

<div><br/>

Consider the following equation:</div>

<ul>

<li><img class="fm-editor-equation" src="img/77c8a845-a682-4c89-b796-9055f0bd6b1f.png" style="color: #333333;font-size: 1em;width:0.67em;height:0.75em;"/> =动作</li>

<li><img class="fm-editor-equation" src="img/f7eea114-134d-4145-b713-94d4fb0ba75e.png" style="color: #333333;font-size: 1em;width:0.75em;height:0.67em;"/> =阿尔法=学习率</li>

<li><img class="fm-editor-equation" src="img/728ee055-bd4e-4791-be8b-c26fb766e47a.png" style="color: #333333;font-size: 1em;width:0.58em;height:0.92em;"/> =奖励</li>

<li>这个等式计算代理采取的每个动作的值(<em> V </em>)，这是一个向量。然后，它将这些值反馈给自己，从奖励中减去并乘以学习率。该计算值可用于确定拉动哪只手臂，但首先代理需要至少拉动每只手臂一次。让我们快速地用代码对此建模，这样作为游戏/模拟程序员，我们可以看到这是如何工作的。打开<kbd>Chapter_5_1.py</kbd>代码并遵循以下步骤:</li>

</ul>

<p>本练习的代码如下:</p>

<ol>

<li class="mce-root">The code for this exercise is as follows:</li>

</ol>

<pre style="color: black;padding-left: 60px">alpha = .9<br/>arms = [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]<br/>v = [0,0,0,0]<br/><br/>for i in range(10):<br/>    for a in range(len(arms)):<br/>        print('pulling arm '+ arms[a][0])<br/>        v[a] = v[a] + alpha * (arms[a][1]-v[a])<br/><br/>print(v)</pre>

<p class="mce-root"/>

<p class="mce-root">这段代码创建所需的设置变量、<kbd>arms</kbd> ( <kbd>gold</kbd>、<kbd>silver</kbd>和<kbd>bronze</kbd>)，以及值向量<kbd>v</kbd>(全零)。然后，代码循环通过多次迭代(<kbd>10</kbd>)，其中每个臂被拉动，并且值<kbd>v</kbd>基于等式被计算和更新。注意奖励值换成了拉臂值，也就是术语<kbd>arms[a][1]</kbd>。</p>

<ol start="2">

<li>运行该示例，您将看到生成的输出显示了每个动作的值，在本例中是手臂拉动。</li>

<li>正如我们所看到的，通过一个简单的等式，我们能够对多臂土匪问题进行建模，并得出一个解决方案，该方案将允许代理始终如一地拉动正确的臂。这为RL奠定了基础，在下一节中，我们将进行下一步，看看<strong>上下文强盗</strong>。</li>

</ol>

<p>语境强盗</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Contextual bandits</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们现在可以把单个多武器匪徒的问题提升为多个多武器匪徒的问题，每个匪徒都有自己的一套武器。现在，我们的问题将上下文或状态引入了等式。随着每一个强盗定义它自己的环境/状态，现在我们在质量和行动方面评估我们的等式。我们修改后的方程如下所示:</h1>

                

            

            

                

<p>考虑以下等式:</p>

<div><img class="fm-editor-equation" src="img/c461e128-42f5-4362-9277-a839f0c1589d.png" style="width:19.67em;height:1.58em;"/></div>

<p><img class="fm-editor-equation" src="img/259fdfe3-f4e2-4625-8dc9-c073c9051569.png" style="color: #333333;font-size: 1em;width:2.92em;height:1.25em;"/> =数值表/矩阵</p>

<ul>

<li>[1,2,3,4</li>

</ul>

<p>2,3,4,5</p>

<p>4,2,1,4]</p>

<p><img src="img/276ce14a-f047-46d2-afcb-e45b7a3caa43.png" style="width:0.75em;height:1.00em;"/> =状态</p>

<ul>

<li><img src="img/dea129da-958b-4492-8437-591af763c9b2.png" style="width:0.83em;height:0.92em;"/> =动作</li>

<li><img src="img/ca094865-f756-450b-b7c2-9ebc383af35f.png" style="width:1.00em;height:0.92em;"/> = alpha =学习率</li>

<li><img src="img/3083521e-5c5f-4684-bc1c-2453b810c4c3.png" style="width:0.67em;height:1.00em;"/> =奖励</li>

<li><img src="img/3083521e-5c5f-4684-bc1c-2453b810c4c3.png" style="width:0.67em;height:1.00em;"/> = reward</li>

</ul>

<p class="mce-root">让我们打开<kbd>Chapter_5_2.py</kbd>并观察以下步骤:</p>

<p>打开代码，如下所示，并遵循前面示例中所做的更改:</p>

<ol>

<li class="mce-root">这个代码设置了许多多臂强盗，每个强盗都有自己的一套武器。然后，它会进行多次迭代，但这一次，当它循环时，它还会循环每个bandit。在每一次循环中，它随机选取一个手臂来拉动并评估质量。</li>

</ol>

<pre style="color: black;padding-left: 60px">import random<br/><br/>alpha = .9<br/>bandits = [[['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]]<br/>q = [[0,0,0,0],<br/>     [0,0,0,0],<br/>     [0,0,0,0],<br/>     [0,0,0,0]]<br/><br/>for i in range(10): <br/>    for b in range(len(bandits)):<br/>        arm = random.randint(0,3)<br/>        print('pulling arm {0} on bandit {1}'.format(arm,b))<br/>        q[b][arm] = q[b][arm] + alpha * (bandits[b][arm][1]-q[b][arm])<br/><br/>print(q)</pre>

<ol start="2">

<li>运行示例并查看<kbd>q</kbd>的输出。请注意，即使在选择了随机手臂后，该等式仍然一致地选择了黄金手臂，即奖励最高的手臂来拉。</li>

<li>请随意使用这个示例，并从练习中寻找更多的灵感。当我们讨论Q-Learning时，我们将扩展RL问题的复杂性。然而，在我们进入这一部分之前，我们将快速转移话题，看看如何建立OpenAI健身房，以便进行更多的RL实验。</li>

</ol>

<p>开放健身房的RL</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>RL with the OpenAI Gym</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">RL已经变得如此流行，以至于现在出现了一场只构建工具来帮助构建RL算法的竞赛。目前这一领域的两个主要竞争对手是<strong>开放健身房</strong>和<strong> Unity </strong>。Unity很快成为了RL赛车，我们将在后面深入探讨。现在，我们将戴上训练轮，运行OpenAI Gym，进一步探索RL的基础。</h1>

                

            

            

                

<p>我们需要安装OpenAI Gym toolkit才能继续，根据您的操作系统，安装可能会有很大差异。因此，我们在这里将着重于Windows安装说明，因为其他操作系统用户可能会有较少的困难。按照以下步骤在Windows上安装OpenAI Gym:</p>

<p>安装一个C++编译器；如果你安装了Visual Studio 2017，你可能已经有推荐的了。你可以在这里找到其他支持的编译器:<a href="https://wiki.python.org/moin/WindowsCompilers">https://wiki.python.org/moin/WindowsCompilers</a>。</p>

<ol>

<li>确保安装了Anaconda，打开Anaconda命令提示符并运行以下命令:</li>

<li class="mce-root">就我们的目的而言，在短期内，我们不需要安装任何其他的健身房模块。Gym有大量的示例环境，Atari游戏和MuJoCo(机器人模拟器)是最有趣的工作环境。我们将在本章的后面看一下Atari游戏模块。</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>conda create -n gym</strong><br/><strong>conda activate gym</strong><br/><strong>conda install python=3.5  # reverts Python, for use with TensorFlow later</strong><br/><strong>pip install tensorflow</strong><br/><strong>pip install keras<br/></strong><strong>pip install gym</strong></pre>

<ol start="3"/>

<ol start="3">

<li>For our purposes, in the short term, we don't need to install any other Gym modules. Gym has plenty of example environments, Atari games and MuJoCo (robotics simulator) being some of the most fun to work with. We will take a look at the Atari games module later in this chapter.</li>

</ol>

<p class="mce-root"/>

<p class="mce-root">这将为您的系统安装健身房环境。我们需要的大部分东西只需很少的设置就能工作。如果你决定用Gym做更多的事情，那么你可能会想安装其他模块；有好几个。在下一节中，我们将在学习Q-Learning时测试这个新环境。</p>

<p>Q-学习模型</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>A Q-Learning model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">RL与几个数学和动态编程概念纠缠在一起，可以填满一本教科书，事实上也有几个。然而，就我们的目的而言，我们只需要理解关键概念，以便构建我们的DRL代理。因此，我们将选择不要让数学负担太重，但是有几个关键概念你需要理解才能成功。如果你在<a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">第一章</a>、<em>游戏的深度学习</em>中涉及了数学，这一节将是轻而易举的。对于那些没有，只是慢慢来，但你不能错过这一次。</h1>

                

            

            

                

<p>为了理解Q-Learning模型，它是RL的一种形式，我们需要回到基础。在下一部分，我们将讨论<strong>马尔可夫决策过程</strong>和<strong>贝尔曼</strong> e <strong>方程</strong>的重要性。</p>

<p>马尔可夫决策过程和贝尔曼方程</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Markov decision process and the Bellman equation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">RL的核心是<strong>马尔可夫决策过程</strong> ( <strong> MDP </strong>)。MDP通常被描述为离散时间随机控制过程。更简单地说，这只是意味着它是一个控制程序，通过时间步长来确定行动的概率，前提是每个行动都会导致奖励。这一过程已经用于机器人、无人机、网络以及RL的大多数自动化控制。下图显示了我们描绘这一过程的经典方式:</h1>

                

            

            

                

<p>其中使用以下变量将MDP表示为元组或向量<img class="fm-editor-equation" src="img/2f668c37-4a3c-4fb3-bea6-89ee1da65240.png" style="width:7.17em;height:1.33em;"/>:</p>

<div><img src="img/434ef2cd-a9d0-4c70-9cd8-e0cddd217734.png" style="width:34.25em;height:25.25em;"/><br/>

<br/>

The Markov decision process</div>

<p><img class="fm-editor-equation" src="img/db442408-6913-47ba-8e98-f0dc1ac4d37e.png" style="width:0.83em;height:1.17em;"/> -作为状态的有限集合，</p>

<ul>

<li><img class="fm-editor-equation" src="img/fd0b1ca8-560b-4a27-baa6-603d0ce3229d.png" style="width:0.92em;height:1.08em;"/> -作为一组有限的动作，</li>

<li><img class="fm-editor-equation" src="img/02393b19-30c2-43a5-b8d6-8e27fee233c9.png" style="width:1.42em;height:1.17em;"/> -在时间<img class="fm-editor-equation" src="img/2e0c7122-44b2-490a-9541-7005d2b217bd.png" style="width:0.50em;height:1.08em;"/>处于状态<img class="fm-editor-equation" src="img/612e1b85-51ac-4943-bf9f-d3043356022d.png" style="width:0.75em;height:1.08em;"/>的动作<img class="fm-editor-equation" src="img/bf5cc02b-c6f0-43ba-a6d4-bfd93a7b3364.png" style="width:0.92em;height:1.08em;"/>导致在时间<img class="fm-editor-equation" src="img/ed7e16b3-966f-43cd-8e6b-2cbccf8ab57a.png" style="width:2.83em;height:1.25em;"/>处于状态<img class="fm-editor-equation" src="img/db1fa5b6-7a59-4f45-904c-928e1e782c1f.png" style="width:2.17em;height:1.25em;"/>的概率，</li>

<li><img class="fm-editor-equation" src="img/a964622c-26ef-4841-865b-40ccc8197365.png" style="width:1.00em;height:1.17em;"/> -是即时奖励</li>

<li><img class="fm-editor-equation" src="img/aaa15109-a497-4e52-a731-a66acb40c59d.png" style="width:0.83em;height:1.25em;"/> - gamma是一个折扣系数，我们应用它来降低重要性或为未来奖励提供重要性</li>

<li>这个图表的工作原理是把你自己想象成一个处于其中一种状态的代理人。然后你根据概率决定行动，总是采取随机行动。当你移动到下一个状态时，动作给你一个奖励，你根据奖励更新概率。同样，大卫·西尔弗在他的讲座中很好地阐述了这一点。</li>

</ul>

<p>The diagram works by picturing yourself as an agent in one of the states. You then determine actions based on the probability, always taking a random action. As you move to the next state, the action gives you a reward and you update the probability based on the reward. Again, David Silver covers this piece very well in his lectures.</p>

<p class="mce-root">现在，前面的过程起作用了，但另一个变化出现了，它提供了更好的未来奖励评估，这是通过引入<strong>贝尔曼方程</strong>和政策/价值迭代的概念来完成的。之前我们有一个值<img class="fm-editor-equation" src="img/d62b7e72-e9f9-470c-aafa-95bf8254b13f.png" style="width:0.92em;height:1.08em;"/>，现在我们有一个策略(<img class="fm-editor-equation" src="img/8aedee8a-b54e-4b76-96ec-ebd39c12acc0.png" style="width:0.92em;height:0.92em;"/>)用于名为<img class="fm-editor-equation" src="img/6ae870a0-2615-43ca-a224-1b08d456a58f.png" style="width:1.83em;height:1.25em;"/>的值，这产生了一个新的等式，如下所示:</p>

<p><img class="fm-editor-equation" src="img/16cb44c4-34b7-4914-b933-ce07125fabf0.png" style="width:25.25em;height:3.75em;"/></p>

<p class="CDPAlignCenter CDPAlign">除了说记住质量迭代的概念之外，我们不会对这个等式做更多的介绍。在下一节中，我们将看到如何将这个等式还原为每个动作的质量指标，并将其用于Q学习。</p>

<p>q学习</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Q-learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">随着质量迭代方法的引入，推导出称为<strong> Q学习</strong>或<strong>质量学习</strong>的有限状态方法。q对给定的有限状态问题使用质量迭代技术来确定代理的最佳行动过程。我们在上一节中看到的等式现在可以表示如下:</h1>

                

            

            

                

<p><img class="fm-editor-equation" src="img/649eaf34-9453-4328-9183-5f8437af651f.png" style="width:30.92em;height:1.50em;"/></p>

<p class="CDPAlignCenter CDPAlign">考虑以下等式:</p>

<p><img class="fm-editor-equation" src="img/74182c8a-795c-46a2-bd88-8679475b3ddd.png" style="font-size: 1em;color: #333333;width:2.33em;height:1.00em;"/>当前状态</p>

<ul>

<li><img class="fm-editor-equation" src="img/a02d73e5-10d7-4937-b5bf-3b0d34f2807c.png" style="font-size: 1em;color: #333333;width:2.58em;height:1.00em;"/>当前动作</li>

<li><img class="fm-editor-equation" src="img/8fe87e76-4098-487c-8033-e01ad9688d4b.png" style="font-size: 1em;color: #333333;width:2.17em;height:0.83em;"/>下一个动作</li>

<li><img class="fm-editor-equation" src="img/1d73936e-4c41-408e-8249-949594662494.png" style="font-size: 1em;color: #333333;width:2.17em;height:0.92em;"/>当前奖励</li>

<li><img class="fm-editor-equation" src="img/6035c5ee-320c-4ab2-865c-5294f55549c2.png" style="font-size: 1em;color: #333333;width:2.42em;height:0.83em;"/>学习率(α)</li>

<li><img class="fm-editor-equation" src="img/57f56e14-a30f-4531-9da1-e5991077f44b.png" style="font-size: 1em;color: #333333;width:2.00em;height:1.00em;"/>奖励折扣系数(伽玛)</li>

<li>现在，当代理在其环境中漫游时，Q值以迭代方式更新。没有什么比一个例子更能说明这些概念了。打开<kbd>Chapter_5_3.py</kbd>并遵循以下步骤:</li>

</ul>

<p>我们从各种导入开始，并按照下面的代码设置它们:</p>

<ol>

<li class="mce-root">这些导入只是加载了我们在这个例子中需要的基本库。记住，您需要安装<kbd>Gym</kbd>来运行这个示例。</li>

</ol>

<pre style="color: black;padding-left: 60px">from collections import deque<br/>import numpy as np<br/>import os<br/>clear = lambda: os.system('cls') #linux/mac use 'clear'<br/>import time<br/>import gym<br/>from gym import wrappers, logger</pre>

<ol start="2">

<li>接下来，我们建立一个新的环境；在这个例子中，我们使用基本的<kbd>FrozenLake-v0</kbd>样本，一个测试Q-learning的完美例子:</li>

<li class="mce-root">然后，我们设置人工智能环境(<kbd>env</kbd>)和许多其他参数:</li>

</ol>

<pre style="color: black;padding-left: 60px">environment = 'FrozenLake-v0'<br/>env = gym.make(environment)</pre>

<ol start="4">

<li>在代码的这一部分，我们设置了一些变量，稍后我们会讲到。对于这个示例，我们使用一个包装工具来监控环境，这对于确定任何潜在的培训问题非常有用。另外要注意的是<kbd>q_table</kbd>数组的设置，由环境<kbd>observation_space</kbd>(状态)和<kbd>action_space</kbd>(动作)定义；空格定义数组而不仅仅是向量。在这个特殊的例子中，<kbd>action_space</kbd>是一个向量，但它也可以是一个多维数组或张量。</li>

</ol>

<pre style="color: black;padding-left: 60px">outdir = os.path.join('monitor','q-learning-{0}'.format(environment))<br/>env = wrappers.Monitor(env, directory=outdir, force=True)<br/>env.seed(0)<br/>env.is_slippery = False<br/>q_table = np.zeros([env.observation_space.n, env.action_space.n])<br/><br/>#parameters<br/>wins = 0<br/>episodes = 40000<br/>delay = 1<br/><br/>epsilon = .8<br/>epsilon_min = .1<br/>epsilon_decay = .001<br/>gamma = .9<br/>learning_rate = .1</pre>

<ol start="5">

<li>跳过函数的下一部分，跳到最后一部分，在这里进行训练迭代，如下面的代码所示:</li>

</ol>

<ol start="6">

<li class="mce-root">前面的大部分代码相对简单，应该很容易理解。看看<kbd>env</kbd>(环境)是如何使用<kbd>act</kbd>函数生成的<kbd>action</kbd>；这用于单步执行或执行对代理的操作。<kbd>step</kbd>函数的输出是<kbd>next_state</kbd>、<kbd>reward</kbd>和<kbd>done</kbd>，我们通过使用<kbd>learn</kbd>函数来确定最优Q策略。</li>

</ol>

<pre style="color: black;padding-left: 60px">for episode in range(episodes): <br/>    state = env.reset()<br/>    done = False<br/>    while not done:<br/>        <strong>action = act(env.action_space,state)</strong><br/>        next_state, reward, done, _ = env.step(action)<br/>        clear()<br/>        env.render()<br/>        <strong>learn(state, action, reward, next_state)</strong><br/>        if done:<br/>            if reward &gt; 0:<br/>                wins += 1<br/>            time.sleep(3*delay)<br/>        else:<br/>            time.sleep(delay)<br/><br/>print("Goals/Holes: %d/%d" % (wins, episodes - wins))<br/>env.close() </pre>

<ol start="7">

<li>在我们进入动作和学习功能之前，运行示例并观察代理如何训练。这可能需要一段时间来训练，所以请放心回到书上。</li>

<li>以下是OpenAI Gym FrozenLake环境运行我们的Q-learning模型的示例:</li>

</ol>

<p>随着示例的运行，您将看到一个显示环境的简单文本输出。<kbd>S</kbd>表示开始，<kbd>G</kbd>表示目标，<kbd>F</kbd>表示冷冻切片，<kbd>H</kbd>表示孔。智能体的目标是在环境中找到自己的路，而不会掉进洞里，并达到目标。特别注意代理如何移动并在环境中找到路。在下一节中，我们将解开<kbd>learn</kbd>和<kbd>act</kbd>功能，并理解探索的重要性。</p>

<div><img src="img/c2cbf3e7-ae6a-4e0c-a5c8-3961aa6119c6.png" style="width:20.42em;height:9.08em;"/><br/>

<br/>

FrozenLake Gym environment</div>

<p>q-学习和探索</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Q-learning and exploration</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们在Q-learning等策略迭代模型中面临的一个问题是探索与利用的问题。Q-model方程假设使用最高质量来确定一个动作，我们称之为利用(利用模型)。这样做的问题是，它经常会迫使代理人采用只寻求最佳短期利益的解决方案。相反，我们需要允许代理有一定的灵活性来探索环境并自己学习。我们通过在训练中引入溶解探索因素来做到这一点。让我们通过再次打开<kbd>Chapter_5_3.py</kbd>示例来看看这是怎么回事:</h1>

                

            

            

                

<p class="mce-root">向下滚动到<kbd>act</kbd>和<kbd>is_explore</kbd>功能，如图所示:</p>

<ol>

<li class="mce-root">Scroll down to the <kbd>act</kbd> and <kbd>is_explore</kbd> functions as shown:</li>

</ol>

<pre style="color: black;padding-left: 60px">def is_explore():<br/>    global epsilon, epsilon_decay, epsilon_min<br/>    epsilon = max(epsilon-epsilon_decay,epsilon_min)<br/>    if np.random.rand() &lt; epsilon:<br/>        return True<br/>    else:<br/>        return False<br/><br/>def act(action_space, state):<br/>    # 0 - left, 1 - Down, 2 - Right, 3 - Up<br/>    global q_table<br/>    if is_explore():<br/>        return action_space.sample()<br/>    else:<br/>        return np.argmax(q_table[state])</pre>

<p class="mce-root"/>

<p class="mce-root">注意在<kbd>act</kbd>函数中，它首先测试代理是否想或需要与<kbd>is_explore()</kbd>一起探索。在<kbd>is_explore</kbd>函数中，我们可以看到全局<kbd>epsilon</kbd>值随着<kbd>epsilon_decay</kbd>的每次迭代而衰减到全局最小值<kbd>epsilon_min</kbd>。当代理开始一集，他们的探索<kbd>epsilon</kbd>是高的，使他们更有可能探索。随着时间的推移，随着情节的进展，<kbd>epsilon</kbd>减小。我们的假设是，随着时间的推移，代理人需要探索的越来越少。勘探和开发之间的这种权衡是非常重要的，也是关于环境状态的大小需要理解的事情。我们将在本书中看到更多关于这种权衡的探讨。<br/>请注意，代理使用了探索功能，并且只选择了一个随机动作。</p>

<ol start="2">

<li>最后，我们得到了<kbd>learn</kbd>函数。该函数是计算<kbd>Q</kbd>值的地方，如下所示:</li>

<li class="mce-root">这里，等式被分解和简化，但这是计算代理在剥削时将使用的值的步骤。</li>

</ol>

<pre style="color: black;padding-left: 60px">def learn(state, action, reward, next_state):<br/>    # Q(s, a) += alpha * (reward + gamma * max_a' Q(s', a') - Q(s, a))<br/>    global q_table<br/>    q_value = gamma * np.amax(q_table[next_state])<br/>    q_value += reward<br/>    q_value -= q_table[state, action]<br/>    q_value *= learning_rate<br/>    q_value += q_table[state, action]<br/>    q_table[state, action] = q_value</pre>

<ol start="4">

<li>保持代理运行，直到它完成。我们刚刚完成了第一个完整的强化学习问题，尽管它有一个有限的状态。在下一节中，我们将大大扩展我们的视野，看看与强化学习相结合的深度学习。</li>

</ol>

<p>第一个深度Q学习的DRL</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>First DRL with Deep Q-learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">既然我们已经详细了解了强化学习过程，我们可以考虑调整我们的Q-learning模型来处理深度学习。正如你可能猜到的，这是我们努力的结果，也是RL真正力量闪耀的地方。正如我们在前面章节中了解到的，深度学习本质上是一个复杂的方程系统，可以通过非线性函数映射输入，以生成训练好的输出。</h1>

                

            

            

                

<p>Now that we understand the reinforcement learning process in detail, we can look to adapt our Q-learning model to work with deep learning. This, as you could likely guess, is the culmination of our efforts and where the true power of RL shines. As we learned through earlier chapters, deep learning is essentially a complex system of equations that can map inputs through a non-linear function to generate a trained output.</p>

<p class="mce-root">神经网络只是求解非线性方程的另一种更简单的方法。稍后我们将会看到如何使用DNN来求解其他方程，但是现在我们将集中于使用它来求解我们在上一节看到的Q-learning方程。</p>

<p>我们将使用OpenAI Gym toolkit中的<strong> CartPole </strong>训练环境。这种环境几乎是用来学习深度Q学习的标准。</p>

<p>打开<kbd>Chapter_5_4.py</kbd>,按照接下来的步骤，看看我们如何将求解器转换为使用深度学习:</p>

<p>像往常一样，我们查看导入和一些初始启动参数，如下所示:</p>

<ol>

<li class="mce-root">接下来，我们将创建一个包含DQN代理功能的类。<kbd>__init__</kbd>的功能如下:</li>

</ol>

<pre style="color: black;padding-left: 60px">import random<br/>import gym<br/>import numpy as np<br/>from collections import deque<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.optimizers import Adam<br/><br/>EPISODES = 1000</pre>

<ol start="2">

<li>大部分参数已经包含在内，但是请注意一个名为<kbd>memory</kbd>的新参数，它是一个保存最后2000步的<strong> deque </strong>集合。这允许我们以一种重放模式批量训练我们的神经网络。</li>

</ol>

<pre style="color: black;padding-left: 60px">class DQNAgent:<br/>    def __init__(self, state_size, action_size):<br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/>        self.memory = deque(maxlen=2000)<br/>        self.gamma = 0.95 # discount rate<br/>        self.epsilon = 1.0 # exploration rate<br/>        self.epsilon_min = 0.01<br/>        self.epsilon_decay = 0.995<br/>        self.learning_rate = 0.001<br/>        self.model = self._build_model()</pre>

<ol start="3">

<li>接下来，我们看看如何使用<kbd>_build_model</kbd>函数构建神经网络模型，如下所示:</li>

</ol>

<ol start="4">

<li>与我们已经看到的其他模型相比，这构建了一个相当简单的模型，三个<strong>密集</strong>层为每个动作输出一个值。这个网络的输入是国家。</li>

</ol>

<pre style="color: black;padding-left: 60px">def _build_model(self):<br/>    # Neural Net for Deep-Q learning Model<br/>    model = Sequential()<br/>    model.add(Dense(24, input_dim=self.state_size, activation='relu'))<br/>    model.add(Dense(24, activation='relu'))<br/>    model.add(Dense(self.action_size, activation='linear'))<br/>    model.compile(loss='mse',<br/>                      optimizer=Adam(lr=self.learning_rate))<br/>    return model</pre>

<ol start="5">

<li>跳到文件的底部，查看训练迭代循环，如下所示:</li>

<li class="mce-root">Jump down to the bottom of the file and look at the training iteration loop, shown as follows:</li>

</ol>

<pre style="color: black;padding-left: 60px">if __name__ == "__main__":<br/>    env = gym.make('CartPole-v1')<br/>    state_size = env.observation_space.shape[0]<br/>    action_size = env.action_space.n<br/>    agent = DQNAgent(state_size, action_size)<br/>    # agent.load("./save/cartpole-dqn.h5")<br/>    done = False<br/>    batch_size = 32<br/><br/>    for e in range(EPISODES):<br/>        state = env.reset()<br/>        state = np.reshape(state, [1, state_size]) <br/>        for time in range(500):<br/>            # env.render()<br/>            action = agent.act(state)<br/>            env.render()<br/>            next_state, reward, done, _ = env.step(action)<br/>            reward = reward if not done else -10<br/>            <strong>next_state = np.reshape(next_state, [1, state_size])</strong><br/><strong>            agent.remember(state, action, reward, next_state, done)</strong><br/>            state = next_state<br/>            if done:<br/>                print("episode: {}/{}, score: {}, e: {:.2}"<br/>                      .format(e, EPISODES, time, agent.epsilon))<br/>                break<br/>            if len(agent.memory) &gt; batch_size:<br/>                <strong>agent.replay(batch_size)</strong></pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mceNonEditable">在这个例子中，我们的训练发生在一个实时的<kbd>render</kbd>循环中。代码的重要部分被突出显示，显示了状态的整形并调用了<kbd>agent.remember</kbd>函数。最后的<kbd>agent.replay</kbd>功能是网络训练的地方。<kbd>remember</kbd>功能如下:</p>

<ol start="7">

<li>

<p>该功能仅存储重放训练的<kbd>state</kbd>、<kbd>action</kbd>、<kbd>reward</kbd>、<kbd>next_state</kbd>、<kbd>done</kbd>参数。向下滚动到<kbd>replay</kbd>功能，如下所示:</p>

</li>

</ol>

<pre style="color: black;padding-left: 60px">def remember(self, state, action, reward, next_state, done):<br/>    self.memory.append((state, action, reward, next_state, done))</pre>

<ol start="8">

<li><kbd>replay</kbd>功能是进行网络训练的地方。我们先定义一个<kbd>minibatch</kbd>，它是由<kbd>batch_size</kbd>分组的以往经验随机抽样定义的。然后，我们循环设置<kbd>reward</kbd>到<kbd>target</kbd>的批次，如果不是<kbd>done</kbd>则根据<kbd>next_state</kbd>上的模型预测计算新的目标。之后，我们使用<kbd>state</kbd>上的<kbd>model.predict</kbd>功能来确定最终目标。最后，我们使用<kbd>model.fit</kbd>函数将训练好的目标反向传播回网络。由于这一部分很重要，让我们重申一下。注意计算和设置变量<kbd>target</kbd>的行。这几行代码可能看起来很熟悉，因为它们符合我们之前看到的Q值方程。该<kbd>target</kbd>值是应该为当前动作预测的值。这是为当前动作反向传播的值，由返回的<kbd>reward</kbd>设置。</li>

</ol>

<pre style="color: black;padding-left: 60px">def replay(self, batch_size):<br/>    minibatch = random.sample(self.memory, batch_size)<br/>    for state, action, reward, next_state, done in minibatch:<br/>        target = reward<br/>        if not done:<br/>            target = (reward+self.gamma*<br/>                      np.amax(s<strong>elf.model.predict</strong>(next_state)[0]))<br/>            target_f = s<strong>elf.model.predict</strong>(state)<br/>            target_f[0][action] = target<br/>            self.model.fit(state, target_f, epochs=1, verbose=0)<br/>        if self.epsilon &gt; self.epsilon_min:<br/>            self.epsilon *= self.epsilon_decay</pre>

<ol start="9">

<li>运行示例并观察代理训练以平衡推车上的杆。下面显示了培训时的环境:</li>

</ol>

<ol start="10">

<li>钢管露天体育馆环境</li>

</ol>

<div><img src="img/48db2129-1bae-40e8-8dd2-1c61deb1dbc1.png" style="width:24.67em;height:16.83em;"/></div>

<p>示例环境使用典型的第一个环境CartPole，我们用它来学习构建我们的第一个DRL模型。在下一节中，我们将看看如何在其他场景和通过Keras-RL API提供的其他模型中使用DQNAgent。</p>

<p>RL实验</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>RL experiments</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">强化学习正在快速发展，我们刚刚看到的DQN模型已经很快被更先进的算法超越。RL算法有多种变化和进步，可能会占据几个章节，但大多数材料将被视为学术。因此，我们将转而查看Keras RL API提供的各种RL模型的一些更实用的示例。</h1>

                

            

            

                

<p>我们可以使用的第一个简单的例子是改变我们以前的例子，使其适用于新的<kbd>gym</kbd>环境。打开<kbd>Chapter_5_5.py</kbd>，进行下一个练习:</p>

<p>在以下代码中更改环境名称:</p>

<ol>

<li class="mce-root">在这种情况下，我们将使用<kbd>MountainCar</kbd>环境，如图所示:</li>

</ol>

<pre style="color: black;padding-left: 60px">if __name__ == "__main__":<br/>    env = gym.make('<strong>MountainCar-v0</strong>')</pre>

<ol start="2">

<li>山地车环境示例</li>

</ol>

<div><img src="img/e4ed1377-b31b-4cc7-adf3-e86c57db2247.png" style="width:23.58em;height:15.75em;"/></div>

<p>像平常一样运行代码，看看DQNAgent如何解决爬山问题。</p>

<ol start="3">

<li>您可以看到我们切换环境和在另一个环境中测试DQNAgent的速度有多快。在下一节中，我们将看到用Keras-RL API提供的各种RL算法训练Atari游戏。</li>

</ol>

<p>Keras RL</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Keras RL</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Keras提供了一个非常有用的RL API，它包装了几个变体，比如DQN、DDQN、SARSA等等。我们现在不会详细讨论这些不同的RL变体，但我们会在以后讨论更复杂的模型时讨论重要的部分。不过现在，我们将看看如何快速构建一个DRL模型来玩Atari游戏。打开<kbd>Chapter_5_6.py</kbd>并遵循以下步骤:</h1>

                

            

            

                

<p>我们首先需要用<kbd>pip</kbd>安装几个依赖项；打开命令shell或Anaconda窗口，输入以下命令:</p>

<ol>

<li class="mce-root">We first need to install several dependencies with <kbd>pip</kbd>; open a command shell or Anaconda window, and enter the following commands:</li>

</ol>

<pre style="color: black;padding-left: 60px"><strong>pip install Pillow</strong><br/><strong>pip install keras-rl</strong><br/><br/><strong>pip install gym[atari]</strong> # on Linux or Mac<br/><strong>pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</strong>  # on Windows thanks to Nikita Kniazev</pre>

<p class="mce-root">这将安装Keras RL API、<kbd>Pillow</kbd>、一个图像框架和用于<kbd>gym</kbd>的Atari环境。</p>

<ol start="2">

<li>像平常一样运行示例代码。这个示例确实接受脚本参数，但是我们不需要在这里使用它们。呈现的Atari分组环境的示例如下:</li>

<li class="mce-root">不幸的是，您不能在代理玩的时候看到游戏运行，因为所有的动作都发生在后台，但是让代理运行，直到它完成并保存模型。下面是我们运行示例的方式:</li>

</ol>

<div><img src="img/3f3e5a5c-4ad3-419f-8f5b-4a57934c63c3.png"/><br/>

<br/>Atari Breakout environment</div>

<p>您可以使用<kbd>--mode test</kbd>作为参数重新运行样本，让代理运行10集并查看结果。</p>

<ol>

<li>在示例运行时，浏览代码并特别注意模型，如下所示:</li>

<li class="mce-root">请注意我们的模型是如何使用<kbd>Convolution</kbd>和池的。这是因为该示例将游戏的每个屏幕/帧作为输入(状态)读取，并相应地做出响应。在这种情况下，模型国家是巨大的，这表明了DRL的真正力量。在这种情况下，我们仍然在训练一个状态模型，但是在以后的章节中，我们将着眼于训练一个策略，而不是一个模型。</li>

</ol>

<pre style="color: black;padding-left: 60px">model = Sequential()<br/>if K.image_dim_ordering() == 'tf':<br/>    # (width, height, channels)<br/>    model.add(Permute((2, 3, 1), input_shape=input_shape))<br/>elif K.image_dim_ordering() == 'th':<br/>    # (channels, width, height)<br/>    model.add(Permute((1, 2, 3), input_shape=input_shape))<br/>else:<br/>    raise RuntimeError('Unknown image_dim_ordering.')<br/>model.add(Convolution2D(32, (8, 8), strides=(4, 4)))<br/>model.add(Activation('relu'))<br/>model.add(Convolution2D(64, (4, 4), strides=(2, 2)))<br/>model.add(Activation('relu'))<br/>model.add(Convolution2D(64, (3, 3), strides=(1, 1)))<br/>model.add(Activation('relu'))<br/>model.add(Flatten())<br/>model.add(Dense(512))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())</pre>

<ol start="3">

<li>这是对RL的简单介绍，我们已经忽略了新人可能会忽略的几个细节。由于我们计划在RL上覆盖更多的章节，特别是<strong>近似策略优化</strong> ( <strong> PPO </strong> ) <strong> </strong>在<a href="1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml">第8章</a>、<em>理解PPO </em>中有更详细的介绍，所以不要太担心诸如策略和基于模型的RL之类的差异。</li>

</ol>

<p>在这个GitHub链接的TensorFlow中有一个同样的DQN的优秀例子:<a href="https://github.com/floodsung/DQN-Atari-Tensorflow" target="_blank">https://github.com/floodsung/DQN-Atari-Tensorflow</a>。该代码可能有点过时，但它是一个简单而优秀的示例，值得一看。</p>

<p>我们不会进一步查看代码，但读者肯定会被邀请。现在让我们做一些练习。</p>

<p>练习</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exercises</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">像往常一样，使用本节中的练习来更好地理解你所学的材料。尝试完成本节中至少两到三个练习:</h1>

                

            

            

                

<p>回到示例<kbd>Chapter_5_1.py</kbd>，改变<strong> alpha </strong> ( <kbd>learning_rate</kbd>)变量，看看这对计算值有什么影响。</p>

<ol>

<li>回到<kbd>Chapter_5_2.py</kbd>的例子，改变各种强盗的手臂位置。</li>

<li>改变示例<kbd>Chapter_5_2.py</kbd>的学习率，看看这对Q结果输出有什么影响。</li>

<li>Change the learning rate on the example <kbd>Chapter_5_2.py</kbd> and see what effect this has on the Q results output.</li>

</ol>

<p class="mce-root">改变<kbd>Chapter_5_3.py</kbd>示例中的gamma奖励折扣系数，看看这对代理培训有什么影响。</p>

<ol start="4">

<li>将<kbd>Chapter_5_3.py</kbd>中的探测ε更改为不同的值，并重新运行样品。看看改变各种探索参数对训练代理有什么影响。</li>

<li>改变<kbd>Chapter_5_4.py</kbd>示例中的各种参数(<strong>探索</strong>、<strong> alpha </strong>和<strong> gamma </strong>)，看看这对训练有什么影响。</li>

<li>改变<kbd>Chapter_5_4.py</kbd>例子中的内存大小，或者更大或者更小，看看这对训练有什么影响。</li>

<li>尝试在来自<kbd>Chapter_5_5.py</kbd>的DQNAgent示例中使用不同的健身房环境。你可以在谷歌上快速搜索一下，看看有没有其他可供选择的环境。</li>

<li><kbd>Chapter_5_6.py</kbd>示例目前使用一种叫做<kbd>LinearAnnealedPolicy</kbd>的表单探索策略；将策略更改为使用代码注释中提到的<kbd>BoltzmannQPolicy</kbd>策略。</li>

<li>请务必下载并运行来自https://github.com/keras-rl/keras-rl<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/keras-rl/keras-rl">的其他Keras-RL示例。同样，您可能需要安装其他健身房环境来让它们工作。</a></li>

<li>关于RL还有很多其他的例子、视频和其他材料可以学习。尽可能多的学习，因为这些材料广泛而复杂，不是你一夜之间就能学会的。</li>

</ol>

<p>摘要</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:027b0a9f-7d4f-44c6-963c-9f0ced4bbde0" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">RL是目前主导许多研究人员兴趣的机器学习技术。它对我们很有吸引力，因为它非常适合游戏和模拟。在这一章中，我们从多兵种背景强盗的基本入门问题开始，讨论了RL的一些基础。然后，我们快速查看了安装OpenAI Gym RL工具包。然后我们看了Q-learning以及如何在代码中实现它，并在OpenAI健身房环境中训练它。最后，我们研究了如何通过加载一些其他环境，包括Atari游戏模拟器，用Gym进行各种其他实验。</h1>

                

            

            

                

<p>在下一章中，我们将关注Unity目前正在开发的快速发展的尖端RL平台。</p>

<p>In the next chapter, we look at the quickly evolving a cutting-edge RL platform that Unity is currently developing.</p>





            



            

        

    </body>



</html></body></html>