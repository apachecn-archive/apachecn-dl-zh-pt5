<html><head/><body><title>Chapter 4. Building Realistic Images from Your Text</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch04"/>第四章。从文本中构建真实的图像</h1></div></div></div><p>对于许多现实生活中的复杂问题，单一的生成对抗网络可能不足以解决它。相反，最好将复杂的问题分解成多个更简单的子问题，并使用多个GANs分别处理每个子问题。最后，您可以将gan堆叠或耦合在一起以找到解决方案。</p><p>在这一章中，我们将首先学习堆叠多个生成网络的技术，以从文本信息生成逼真的图像。接下来，您将耦合两个生成网络，以自动发现不同领域之间的关系(鞋子和手袋之间的关系或男女演员之间的关系)。</p><p>我们将在本章中讨论以下主题:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">什么是StackGAN？它的概念和架构</li><li class="listitem" style="list-style-type: disc">利用TensorFlow从文本描述合成真实感图像</li><li class="listitem" style="list-style-type: disc">使用DiscoGAN发现跨域关系</li><li class="listitem" style="list-style-type: disc">使用PyTorch从边缘生成手提包图像</li><li class="listitem" style="list-style-type: disc">使用facescrub数据转换性别(男演员到女演员，反之亦然)</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec21"/>stack gan简介</h1></div></div></div><p>StackGAN <a id="id163" class="indexterm"/>的想法最初是由<em>张寒</em>、<em>徐涛</em>、<em>李洪生</em>、<em>张少婷</em>、<em>黄小蕾</em>、<em>王晓刚</em>、<em>迪米特里斯·梅塔克斯</em>、<em> arXiv: 1612.03242，2017 </em>在论文<em>正文中提出来的，以照片般逼真的图像合成</em></p><p>从文本合成照片级真实感图像是计算机视觉中一个具有挑战性的问题，有着巨大的实际应用价值。使用StackGAN可以将从文本生成图像的问题分解为两个可管理的子问题。在这种方法中，我们基于某些条件(例如文本描述和前一阶段的输出)堆叠生成网络的两个阶段，以实现从文本输入生成逼真图像的这一具有挑战性的任务。</p><p>在深入研究模型架构和实现细节之前，让我们定义一些概念和符号:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>木卫一</em>:这是<a id="id164" class="indexterm"/>原始图像</li><li class="listitem" style="list-style-type: disc"><em> t </em>:文字说明</li><li class="listitem" style="list-style-type: disc"><em> t </em>:文字嵌入</li><li class="listitem" style="list-style-type: disc"><em> (t) </em>:文字嵌入的意思</li><li class="listitem" style="list-style-type: disc"><em> ∑(t) </em>:文本嵌入的对角协方差矩阵</li><li class="listitem" style="list-style-type: disc"><em> pdata </em>:真实数据分布</li><li class="listitem" style="list-style-type: disc"><em> pz </em>:噪声的高斯分布</li><li class="listitem" style="list-style-type: disc"><em> z </em>:来自高斯分布的随机采样噪声</li></ul></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec46"/>条件增强</h2></div></div></div><p>正如我们从第2章、<em>的<a class="link" href="ch02.html" title="Chapter 2. Unsupervised Learning with GAN">中已经知道的，利用GAN </a></em>的无监督学习，在条件GAN中，生成器和<a id="id165" class="indexterm"/>鉴别器网络都接收额外的条件变量<em> c </em>以产生<em>G(z；c) </em>和<em>D(x；c) </em>。该公式有助于生成器根据变量<em> c </em>生成图像。给定少量的图像-文本对，条件增强产生更多的训练对，并且对于建模文本到图像的翻译是有用的，因为相同的句子通常映射到具有各种外观的对象。文本描述首先通过编码器编码转换为文本嵌入<em> t </em>，然后使用char-CNN-RNN模型进行非线性转换，以创建条件潜变量作为第一阶段生成器网络的输入。</p><p>由于用于文本<a id="id166" class="indexterm"/>嵌入的潜在空间通常是高维的，为了减轻具有有限数据量的潜在数据流形中的不连续问题，应用条件增强技术来产生从高斯分布<em> N( (t)，∑(t)) </em>采样的附加条件变量<em> c^ </em>。</p><div><div><div><div><h3 class="title">第一阶段</h3></div></div></div><p>在此阶段，GAN网络了解到以下信息:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">根据文本描述生成用于创建对象的粗略<a id="id167" class="indexterm"/>形状和基本颜色</li><li class="listitem" style="list-style-type: disc">根据从先验分布采样的随机噪声生成背景区域</li></ul></div><p>在此阶段生成的低分辨率粗糙图像可能看起来不真实，因为它们存在一些缺陷<a id="id168" class="indexterm"/>，如物体形状失真、缺少物体部分等。</p><p>第一级GAN训练鉴频器<em> D0 </em>(最大化损耗)和发生器<em> G0 </em>(最小化损耗)，交替如下式所示:</p><div><img src="img/B08086_04_01.jpg" alt="Stage-I"/></div></div><div><div><div><div><h3 class="title">第二阶段</h3></div></div></div><p>在这一阶段，<a id="id169" class="indexterm"/> GAN网络仅关注于绘制细节和纠正从第一阶段生成的低分辨率图像中的缺陷(例如缺少生动的对象部分、形状失真和一些从文本中省略的细节),以生成基于文本描述的高分辨率逼真图像。</p><p>第二级GAN交替训练鉴频器<em> D </em>(损耗最大化)和发生器<em> G </em>(损耗最小化)，以低分辨率<em>G</em><sub><em>0</em></sub><em>(z；c^0) </em>和高斯潜变量<em> c^ </em>:</p><div><img src="img/B08086_04_02.jpg" alt="Stage-II"/></div><div><div><h3 class="title"><a id="note05"/>注意</h3><p>随机噪声<em> z </em>在第二阶段被高斯条件变量<em> c^ </em>代替。此外，第二阶段中的<a id="id170" class="indexterm"/>条件增强具有不同的全连接层，以生成不同的文本嵌入均值和标准差。</p></div></div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec47"/>stack gan的架构细节</h2></div></div></div><p>如下图<a id="id171" class="indexterm"/>所示，对于第一阶段的生成网络<em>G</em>T3】0，首先将文本嵌入<em> t </em>馈入全连通层，生成<em> 0 </em>和<em> σ0 </em> ( <em> σ0 </em>为高斯分布<em> N( 0(t)的<em> ∑0】的对角线值)；∑0(t)) </em>然后从高斯分布中采样文本条件变量<em> c^0 </em>。</em></p><p>对于第一阶段的鉴别器网络<em> D0 </em>，嵌入<em> t </em>的文本首先用全连通层压缩到<em>和</em>维，然后空间复制到<em>Md</em>x<em>Md</em>x<em>Nd</em>张量。图像通过一系列下采样块，以挤进<em> Md </em> x <em> Md </em>空间维度，然后使用沿着通道维度的过滤图与文本张量连接。得到的张量经过1×1卷积层，以共同学习图像和文本的特征，并最终使用一个节点完全连接层输出决策得分。</p><p>第二阶段的生成器被设计为具有残差块和文本嵌入<em> t </em>的编码器-解码器网络，以生成<em> Ng </em>维文本条件向量<em> c^ </em>，其被空间复制到<em>MD</em>x<em>MD</em>x<em>nd</em>张量。生成的第一阶段结果<em> s0 </em>然后被送入几个下采样块(即编码器)，直到被压缩到空间大小<em> Mg </em> x <em> Mg </em>。沿着信道维度与文本特征连接的图像特征通过几个残差块，以学习跨图像和文本特征的多模态表示。最后，得到的张量经过一系列上采样层(即解码器)生成一幅<em> W </em> x <em> H </em>高分辨率图像。</p><p>第二级的鉴频器与第一级相似，只是增加了下采样模块，以适应该级的大图像尺寸。在鉴别器的训练过程中，正样本对是从真实图像及其相应的文本描述中构建的，而负<a id="id172" class="indexterm"/>样本由两组组成:一组具有嵌入了不匹配文本的真实图像，另一组具有嵌入了相应文本的合成图像:</p><div><img src="img/B08086_04_03.jpg" alt="Architecture details of StackGAN"/><div><p>图一。烟囱的建筑。</p><p>来源:<em> arXiv: 1612.03242，2017 </em></p></div></div><p>stage-I生成器首先根据给定的文本绘制对象的粗略形状和基本颜色，并根据随机噪声矢量绘制背景，从而绘制低分辨率图像。第二阶段生成器纠正缺陷，并将引人注目的细节添加到第一阶段的结果中，根据第一阶段的结果生成更真实的高分辨率图像。</p><p>上采样块包括最近邻上采样，后面是33个卷积，每个步长为1。除最后一次卷积外，每次卷积后都应用批量标准化和<code class="literal">ReLU</code>激活功能。剩余块再次由33个卷积组成，每个卷积步长为1，后面是批量归一化和<code class="literal">ReLU</code>激活函数。下采样块由步长为2的44个卷积组成，后面是批归一化和Leaky-ReLU，只是第一个卷积<a id="id173" class="indexterm"/>层中不存在批归一化。</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec48"/>使用TensorFlow从文本合成图像</h2></div></div></div><p>让我们实现代码<a id="id174" class="indexterm"/>从文本合成真实的图像，并产生令人兴奋的结果:</p><div><ol class="orderedlist arabic"><li class="listitem">首先克隆<code class="literal">git</code>库:<a class="ulink" href="https://github.com/Kuntal-G/StackGAN.git">https://github.com/Kuntal-G/StackGAN.git</a>并将目录更改为<code class="literal">StackGAN</code>:<div><pre class="programlisting"> <strong>git clone https://github.com/Kuntal-G/StackGAN.git</strong> <strong>cd StackGAN</strong> </pre></div><div><div><h3 class="title"><a id="note06"/>注意</h3> <p>目前代码与TensorFlow的旧版本(0.11)兼容，所以需要TensorFlow版本低于1.0才能成功运行这段代码。您可以使用<code class="literal">sudo pip install tensorflow==0.12.0</code>修改您的TensorFlow版本。</p> <p>还要确保系统中安装了torch <a id="id175" class="indexterm"/>。更多信息可以在这里找到:<a class="ulink" href="http://torch.ch/docs/getting-started.html">http://torch.ch/docs/getting-started.html</a>。</p> </div> </div></li><li class="listitem">然后使用<code class="literal">pip</code>命令安装以下软件包:<div> <pre class="programlisting"> <strong>sudo pip install prettytensor progressbar python-dateutil easydict pandas torchfile requests</strong> </pre> </div></li><li class="listitem">接下来使用以下命令从<a class="ulink" href="https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view">https://drive . Google . com/file/d/0B3y _ msrwzaxlt 1 bzdvdycdy 5 tee/view</a>下载<a id="id176" class="indexterm"/>预处理后的char-CNN-RNN文本嵌入birds模型:<div> <pre class="programlisting"> <strong>python google-drive-download.py 0B3y_msrWZaXLT1BZdVdycDY5TEE Data/ birds.zip</strong> </pre> </div></li><li class="listitem">现在使用<code class="literal">unzip</code>命令提取下载的文件:<div> <pre class="programlisting"> <strong>unzip Data/birds.zip</strong> </pre> </div></li><li class="listitem">接下来从加州理工大学UCSD下载并提取鸟类图像数据:<div> <pre class="programlisting"> <strong>wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/ CUB_200_2011.tgz -O Data/birds/CUB_200_2011.tgz</strong> <strong>tar -xzf CUB_200_2011.tgz</strong> </pre> </div></li><li class="listitem">现在我们将对图像进行预处理，分成训练集和测试集，并将图像保存为pickle格式:<div> <pre class="programlisting"> <strong>python misc/preprocess_birds.py</strong> </pre> </div> <div> <img src="img/B08086_04_04.jpg" alt="Synthesizing images from text with TensorFlow"/> </div></li><li class="listitem">现在我们将<a id="id177" class="indexterm"/>从:<a class="ulink" href="https://drive.google.com/file/d/0B3y_msrWZaXLNUNKa3BaRjAyTzQ/view">https://drive . Google . com/file/d/0B3y _ msrwzaxlnunka 3 barjaytzq/view</a>下载预先训练好的char-CNN-RNN文本嵌入模型，并使用:<div> <pre class="programlisting"> <strong>python google-drive-download.py 0B3y_msrWZaXLNUNKa3BaRjAyTzQ models/ birds_model_164000.ckpt</strong> </pre> </div>将其保存到<code class="literal">models/</code>目录</li><li class="listitem">也可以从<a class="ulink" href="https://drive.google.com/file/d/0B0ywwgffWnLLU0F3UHA3NzFTNEE/view">https://drive . Google . com/file/d/0 b 0 ywgffwnllu 0 f 3 uha 3 nzftnee/view</a>下载鸟类的char-CNN-RNN文本编码器，保存在<code class="literal">models/text_encoder</code>目录下:<div> <pre class="programlisting"> <strong>python google-drive-download.py 0B0ywwgffWnLLU0F3UHA3NzFTNEE models/text_encoder/  lm_sje_nc4_cub_hybrid_gru18_a1_c512_0.00070_1_10_trainvalids.txt_iter30000.t7</strong> </pre> </div></li><li class="listitem">Next, we will add some sentences to the <code class="literal">example_captions.txt</code> file to generate some exciting images of birds:<p><code class="literal">A white bird with a black crown and red beak</code></p><p><code class="literal">this bird has red breast and yellow belly</code></p></li><li class="listitem">最后，我们将执行<code class="literal">demo</code>目录下的<code class="literal">birds_demo.sh</code>文件，从<code class="literal">example_captions.txt</code>文件给出的文本描述中生成逼真的鸟类图像:<div> <pre class="programlisting"> <strong>sh demo/birds_demo.sh</strong> </pre> </div> <div> <img src="img/B08086_04_05.jpg" alt="Synthesizing images from text with TensorFlow"/> </div></li><li class="listitem">现在<a id="id178" class="indexterm"/>生成的图像将保存在<code class="literal">Data/birds/example_captions/</code>目录下，如下截图所示:<div> <img src="img/B08086_04_06.jpg" alt="Synthesizing images from text with TensorFlow"/> </div></li></ol></div><p>瞧，你现在已经从文本描述中生成了令人印象深刻的鸟类图像。用你自己的<a id="id179" class="indexterm"/>句子描述鸟类，并用描述直观地验证结果。</p></div></div></div></div></div>
<title>Discovering cross-domain relationships with DiscoGAN</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec22"/>使用DiscoGAN发现跨域关系</h1></div></div></div><p>跨领域关系通常对人类来说是自然的<a id="id180" class="indexterm"/>，他们可以在没有监督的情况下轻松识别来自不同领域的数据之间的关系(例如，识别英语句子与其西班牙语翻译句子之间的<a id="id181" class="indexterm"/>关系，或者选择适合服装风格的鞋子)，但自动学习这种关系非常具有挑战性，需要大量说明这种关系的基础事实配对信息。</p><p><strong>发现生成对抗网络</strong>(<strong>disco gan</strong>)<em>arXiv:1703.05192，2017 </em>发现两个视觉域之间的关系，并通过在没有任何配对信息的情况下给定一个域的图像，生成另一个域的新图像，成功地将样式从一个域转移到另一个域。DiscoGAN寻求将两个GAN耦合在一起，将每个域映射到其对应的域。DiscoGAN背后的关键思想是确保域1中的所有图像都可以由域2中的图像表示，并使用重建损失来衡量原始图像在两次转换(即从域1到域2再回到域1)后的重建情况。</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec49"/>disco gan的体系结构和模型制定</h2></div></div></div><p>在深入模型公式和与DiscoGAN相关的各种<code class="literal">loss</code>功能之前，让我们首先<a id="id182" class="indexterm"/>定义一些<a id="id183" class="indexterm"/>相关术语和概念:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>G</em><sub><em>AB</em></sub>:将A域的输入图像<em> x </em> <sub> <em> A </em> </sub>翻译成B域的图像<em>x</em><sub><em>AB</em></sub>的<code class="literal">generator</code>函数</li><li class="listitem" style="list-style-type: disc"><em> G </em> <sub/></li><li class="listitem" style="list-style-type: disc"><em> G </em></li><li class="listitem" style="list-style-type: disc"><em>G</em><sub>T5】BA</sub>(<em>x</em><sub><em>B</em></sub>):这是域B中所有<em> x </em> <sub> <em> B </em> </sub>的所有可能结果值的完整集合，应该包含在域A中</li><li class="listitem" style="list-style-type: disc"><em> D </em> <sub> <em> A </em> </sub>:域A中的<code class="literal">discriminator</code>函数</li><li class="listitem" style="list-style-type: disc"><em> D </em> <sub> <em> B </em> </sub>:域B <div> <img src="img/B08086_04_07.jpg" alt="The architecture and model formulation of DiscoGAN"/> <div> <p>中的<code class="literal">discriminator</code>函数图-2:两个耦合GAN模型的DiscoGAN架构</p> <p>来源:<em> arXiv- 1703.05192，2017 </em> </p> </div> </div></li></ul></div><p>DiscoGAN的生成器<a id="id185" class="indexterm"/>模块由一对编码器-解码器组成，用于执行背对背图像转换。A生成器<em>G</em><sub><em>AB</em></sub>首先将来自域A的输入图像<em> x </em> <sub> <em> A </em> </sub>翻译成来自域b的图像<em>x</em><sub><em>AB</em></sub>，然后将生成的图像翻译回域A图像<em>x</em><sub><em>ABA<em>最后，发生器的平移输出图像<em>x</em><sub><em>AB</em></sub>被馈入鉴别器，并通过将其与域B的真实图像进行比较而得到分数:</em></em></sub></p><div><img src="img/B08086_04_08.jpg" alt="The architecture and model formulation of DiscoGAN"/></div><p>发电机<em> GAB </em>接收<a id="id187" class="indexterm"/>两种类型的损耗，如图所示(等式-5):</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em> L </em></li><li class="listitem" style="list-style-type: disc"><em> L </em></li></ul></div><p>而鉴频器<em> D </em> <sub> <em> B </em> </sub>接收标准GAN鉴频器损耗，如图所示(等式-6):</p><div><img src="img/B08086_04_09.jpg" alt="The architecture and model formulation of DiscoGAN"/></div><p>两个耦合的gan被同时训练，并且两个gan学习从一个域到另一个域的映射，以及使用两个重建损失从两个域重建输入图像的反向映射:<em>L</em><sub><em>CONSTA</em></sub>和<em>L</em><sub><em>CONSTB</em></sub>。</p><p>参数在两个gan的生成器<em>G</em><sub><em>AB</em></sub>和<em>G</em><sub><em>BA</em></sub>和生成的图像<em>x</em><sub><em>BA</em></sub>和<em>x</em><sub><em>AB<em>之间共享</em></em></sub></p><div><img src="img/B08086_04_10.jpg" alt="The architecture and model formulation of DiscoGAN"/></div><p>发电机总损耗<em> L </em> <sub> <em> G </em> </sub>是耦合模型的两个GAN损耗和每个部分模型的重构<a id="id189" class="indexterm"/>损耗之和，如图所示(等式-7)。并且总鉴别器损耗，<em> L </em> <sub> <em> D </em> </sub>，是分别在域A和域B中鉴别真实和<a id="id190" class="indexterm"/>伪图像的两个鉴别器损耗<em>L</em><sub><em>DA</em></sub>和<em>L</em><sub><em>DB</em></sub>之和(等式- 8)。为了实现一一对应的双射映射，DiscoGAN模型受到两个<em>L</em><sub><em>GAN</em></sub>损失和两个<em>L</em><sub><em>CONST</em></sub>重建损失的约束。</p><p>内射映射意味着<strong> A </strong>的每个成员在<strong> B </strong>中都有自己唯一的匹配成员，满射映射意味着每个<strong> B </strong>至少有一个匹配<strong> A </strong>。</p><p>双射映射意味着内射和满射都在一起，并且集合成员之间存在完美的一对一对应:</p><div><img src="img/B08086_04_11.jpg" alt="The architecture and model formulation of DiscoGAN"/></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec50"/>disco gan的实现</h2></div></div></div><p>现在让我们深入研究<a id="id191" class="indexterm"/>代码，以理解这个概念(损耗和测量标准)以及DiscoGAN的架构。</p><p>发生器接收大小为64x64x3的输入图像，并通过编码器-解码器对进行馈送。发生器的编码器部分由五个带4x4滤波器的卷积层组成，每个卷积层后面都有批量归一化和泄漏ReLU。解码器部分由5个带4x4滤波器的反卷积层组成，后面是批量归一化和<code class="literal">ReLU</code>激活函数，输出大小为64x64x3的目标域图像。以下是生成器代码片段:</p><div><pre class="programlisting">class Generator(nn.Module):
 
            self.main = nn.Sequential(
            # Encoder
                nn.Conv2d(3, 64, 4, 2, 1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 2),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 4),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 8),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),
                nn.BatchNorm2d(100),
                nn.LeakyReLU(0.2, inplace=True),
             
             # Decoder
                nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),
                nn.BatchNorm2d(64 * 8),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 4),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 2),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(True),
                nn.ConvTranspose2d(64,3, 4, 2, 1, bias=False),
                nn.Sigmoid()
                
                . . . 
                
            )</pre></div><p>鉴别器类似于发生器的编码器部分，由五个带4x4滤波器的卷积层<a id="id192" class="indexterm"/>组成，每个卷积层后面都有一个批处理归一化和<code class="literal">LeakyReLU</code>激活功能。最后，我们在最后的卷积层(<code class="literal">conv-5</code>)上应用<code class="literal">sigmoid</code>函数，生成一个[0，1]之间的标量概率分数，以判断真假数据。以下是鉴别器代码片段:</p><div><pre class="programlisting">class Discriminator(nn.Module):

        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)
        self.relu1 = nn.LeakyReLU(0.2, inplace=True)

        self.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(64 * 2)
        self.relu2 = nn.LeakyReLU(0.2, inplace=True)

        self.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(64 * 4)
        self.relu3 = nn.LeakyReLU(0.2, inplace=True)

        self.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)
        self.bn4 = nn.BatchNorm2d(64 * 8)
        self.relu4 = nn.LeakyReLU(0.2, inplace=True)

        self.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)
        
        . . . .
        
   return torch.sigmoid( conv5 ), [relu2, relu3, relu4]</pre></div><p>然后，我们使用均方误差和二进制交叉熵度量来定义生成器和重建的损失标准:</p><div><pre class="programlisting">recon_criterion = nn.MSELoss()
gan_criterion = nn.BCELoss()


optim_gen = optim.Adam( gen_params, lr=args.learning_rate, betas=(0.5,0.999), weight_decay=0.00001)
optim_dis = optim.Adam( dis_params, lr=args.learning_rate, betas=(0.5,0.999), weight_decay=0.00001)</pre></div><p>现在我们开始<a id="id193" class="indexterm"/>生成从一个域到另一个域的图像，并计算重建损失，以了解原始图像在两次平移(<code class="literal">ABA</code>或<code class="literal">BAB</code>)后的重建情况:</p><div><pre class="programlisting">AB = generator_B(A)
BA = generator_A(B)

ABA = generator_A(AB)
BAB = generator_B(BA)

# Reconstruction Loss
recon_loss_A = recon_criterion( ABA, A )
recon_loss_B = recon_criterion( BAB, B )</pre></div><p>接下来，我们计算每个域的发电机损耗和鉴频器损耗:</p><div><pre class="programlisting"># Real/Fake GAN Loss (A)
A_dis_real, A_feats_real = discriminator_A( A )
A_dis_fake, A_feats_fake = discriminator_A( BA )

dis_loss_A, gen_loss_A = get_gan_loss( A_dis_real, A_dis_fake, gan_criterion, cuda )
fm_loss_A = get_fm_loss(A_feats_real, A_feats_fake, feat_criterion)

# Real/Fake GAN Loss (B)
B_dis_real, B_feats_real = discriminator_B( B )
B_dis_fake, B_feats_fake = discriminator_B( AB )

dis_loss_B, gen_loss_B = get_gan_loss( B_dis_real, B_dis_fake, gan_criterion, cuda )
fm_loss_B = get_fm_loss( B_feats_real, B_feats_fake, feat_criterion )

gen_loss_A_total = (gen_loss_B*0.1 + fm_loss_B*0.9) * (1.-rate) + recon_loss_A * rate
gen_loss_B_total = (gen_loss_A*0.1 + fm_loss_A*0.9) * (1.-rate) + recon_loss_B * rate</pre></div><p>最后，我们<a id="id194" class="indexterm"/>通过对来自两个跨域(<code class="literal">A</code>和<code class="literal">B</code>)的损耗求和来计算<code class="literal">discogan</code>模型的总损耗:</p><div><pre class="programlisting">if args.model_arch == 'discogan':
    gen_loss = gen_loss_A_total + gen_loss_B_total
    dis_loss = dis_loss_A + dis_loss_B</pre></div></div></div></div></div>
<title>Generating handbags from edges with PyTorch</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec23"/>用PyTorch从边缘生成手袋</h1></div></div></div><p>在这个例子中，我们将使用来自Berkley的<code class="literal">pix2pix</code>数据集从相应的边生成<a id="id195" class="indexterm"/>真实的手提包图像。在执行以下步骤之前，请确保您的机器上安装了py torch(<a class="ulink" href="http://pytorch.org/">http://pytorch.org/</a>)和OpenCV(<a class="ulink" href="http://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html">http://docs . OpenCV . org/trunk/D7/d9f/tutorial _ Linux _ install . html</a>):</p><div><ol class="orderedlist arabic"><li class="listitem">首先克隆<a id="id197" class="indexterm"/>的<code class="literal">git</code>库，并将目录改为<code class="literal">DiscoGAN</code> : <div> <pre class="programlisting"> <strong>git clone https://github.com/SKTBrain/DiscoGAN.git cd DiscoGAN</strong> </pre> </div></li><li class="listitem">接下来使用以下命令下载<code class="literal">edges2handbags</code>数据集:<div> <pre class="programlisting"> <strong>python ./datasets/download.py edges2handbags</strong> </pre> </div></li><li class="listitem">然后使用下载的数据集在两个域之间应用图像平移:边缘和手提包:<div> <pre class="programlisting">python ./discogan/image_translation.py --task_name='edges2handbags'</pre> </div> <div> <img src="img/B08086_04_12.jpg" alt="Generating handbags from edges with PyTorch"/> </div></li><li class="listitem">现在，图像将在每个历元每1000次迭代(根据<code class="literal">image_save_interval</code>参数)后保存在<code class="literal">results</code>目录下，并带有之前在图像转换步骤中使用的相应任务名称:<div> <img src="img/B08086_04_13.jpg" alt="Generating handbags from edges with PyTorch"/> </div></li></ol></div><p>以下是从域A到域B生成的<a id="id199" class="indexterm"/>跨域图像的<a id="id198" class="indexterm"/>样本输出:</p><div><img src="img/B08086_04_14.jpg" alt="Generating handbags from edges with PyTorch"/><div><p>图3:左边是跨域(A -&gt; B -&gt; A)生成的图像(边-&gt;手提包-&gt;边)，而右边是跨域(B -&gt; A -&gt; B)生成的图像(手提包-&gt;边-&gt;手提包)</p></div></div></div></div></div>
<title>Gender transformation using PyTorch</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec24"/>使用PyTorch进行性别转换</h1></div></div></div><p>在这个例子中，我们将使用来自<code class="literal">facescrub</code>数据集中的名人面部图像来转换<a id="id200" class="indexterm"/>男演员到女演员的性别，反之亦然。就像前面的例子一样，在执行以下步骤之前，请确保您的计算机上安装了PyTorch和OpenCV:</p><div><ol class="orderedlist arabic"><li class="listitem">首先克隆<code class="literal">git</code>存储库，并将目录更改为<code class="literal">DiscoGAN</code>(如果您执行了前面从边生成手提包的示例，可以跳过这一步):<div> <pre class="programlisting">git clone <strong>https://github.com/SKTBrain/DiscoGAN.git</strong> <strong>cd DiscoGAN</strong> </pre> </div></li><li class="listitem">接下来使用以下命令下载<code class="literal">facescrub</code>数据集:<div> <pre class="programlisting"> <strong>python ./datasets/download.py facescrub</strong> </pre> </div></li><li class="listitem">然后使用<a id="id201" class="indexterm"/>下载的数据集:<div> <pre class="programlisting"> <strong>python ./discogan/image_translation.py --task_name= facescrub</strong> </pre> </div> <div> <img src="img/B08086_04_15.jpg" alt="Gender transformation using PyTorch"/> </div>在男性和女性两个域之间应用图像平移</li><li class="listitem">现在，图像将在每个历元每1000次迭代(根据<code class="literal">image_save_interval</code>参数)后保存在<code class="literal">results</code>目录下，具有各自的任务名称(<code class="literal">facescrub</code>)和历元间隔:<div> <img src="img/B08086_04_16.jpg" alt="Gender transformation using PyTorch"/> </div></li></ol></div><p>以下是从域A(男性)到域B(女性)生成的跨域图像的输出示例:</p><div><img src="img/B08086_04_17.jpg" alt="Gender transformation using PyTorch"/><div><p>图4:左边是跨域(A -&gt; B -&gt; A)生成的图像(男-&gt;女-&gt;男)，右边是跨域(B -&gt; A -&gt; B)生成的图像(女-&gt;男-&gt;女)</p></div></div></div></div></div>
<title>DiscoGAN versus CycleGAN</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title">DiscoGAN对CycleGAN</h1></div></div></div><p>DiscoGAN(之前讨论过)和CycleGAN(在<a class="link" href="ch02.html" title="Chapter 2. Unsupervised Learning with GAN">第2章</a>、<em>使用GAN的无监督学习</em>中讨论过)的主要目标都是通过<a id="id202" class="indexterm"/>找到给定图像的源域X和目标域Y之间的映射，而无需配对信息，从而引入一种解决图像到图像转换问题的新方法。</p><p>从架构角度来看，两个模型都由两个gan组成，将一个域映射到其对应的域，并将它们的损耗作为传统发电机损耗(通常在gan中看到)和重建损耗/循环一致性损耗的函数。</p><p>这两个模型之间没有很大的不同，除了DiscoGAN使用两个重建损失(在两次平移X-&gt;Y-&gt;X之后原始图像重建得有多好的度量)，而CycleGAN使用两个平移器F和G的单周期一致性损失(F将图像从域X平移到域Y，G执行相反的操作)来确保两个平衡(<em> F(G(b)) = b和G(F(a)) = a </em>，给定<em> a </em>，<em/></p></div></div></div>
<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec26"/>总结</h1></div></div></div><p>到目前为止，您已经学习了通过使用StackGAN和DiscoGAN将多个GAN模型结合在一起来解决复杂现实问题的方法(例如从文本合成图像和发现跨域关系)。在下一章中，您将学习使用预训练模型和特征转移在深度学习中处理小数据集的重要技术，以及如何在分布式系统上大规模运行您的深度模型。</p></div></div></div></body></html>