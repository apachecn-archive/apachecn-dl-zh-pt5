<html><head/><body><title>Chapter 5. Using Various Generative Models to Generate Images</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05"/>第五章。使用各种生成模型生成图像</h1></div></div></div><p>深度学习闪耀着大数据和更深层次模型的光芒。它有数百万个参数，可能需要数周时间来训练。一些现实生活场景可能没有足够的数据、硬件或资源来训练更大的网络以达到期望的精度。有没有替代方法，或者我们是否需要从头开始重新发明训练轮？</p><p>在这一章中，我们将首先通过真实数据集(<code class="literal">MNIST</code>、<code class="literal">cars vs cats vs dogs vs flower</code>、<code class="literal">LFW</code>)的动手示例，来看看在基于现代深度学习的应用中被称为<strong>迁移学习</strong>的强大而广泛使用的训练方法。此外，您将使用Apache Spark和BigDL在大型分布式集群上构建基于深度学习的网络<a id="id203" class="indexterm"/>。然后，您将结合迁移学习和GAN来生成具有面部数据集的高分辨率逼真图像。最后，你还将了解如何在甘之外的图像上创造艺术幻觉。</p><p>我们将在本章中讨论以下主题:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">什么是迁移学习？—它的优点和应用</li><li class="listitem" style="list-style-type: disc">使用Keras通过预训练的VGG模型对<code class="literal">cars vs dog vs flower</code>进行分类</li><li class="listitem" style="list-style-type: disc">使用Apache Spark深度学习管道在大型分布式集群上培训和部署深度网络</li><li class="listitem" style="list-style-type: disc">使用BigDL通过特征提取和微调识别手写数字</li><li class="listitem" style="list-style-type: disc">使用预训练模型和SRGAN的高分辨率图像生成</li><li class="listitem" style="list-style-type: disc">用DeepDream生成艺术幻觉图像和用VAE生成图像</li></ul></div><p>从头开始构建深度学习模型需要复杂的资源，而且非常耗时。并且<a id="id204" class="indexterm"/>因此你不总是想要从零开始构建这样的深度模型来解决你手头的问题。您将重用为类似问题构建的现有模型来满足您的用例，而不是重新发明同一个轮子。</p><p>假设你想造一辆自动驾驶汽车。你可以花费数年时间从头开始构建一个像样的图像识别算法，或者你可以简单地从庞大的ImageNet数据集获取谷歌构建的预训练初始模型。预先训练的模型可能达不到您的应用所需的精度水平，但它节省了重新发明轮子所需的巨大努力。通过一些微调和技巧，你的准确度肯定会提高。</p><div><div><div><div><h1 class="title"><a id="ch05lvl1sec27"/>迁移学习简介</h1></div></div></div><p>预训练模型没有针对处理用户特定数据集进行优化，但是它们对于手头与训练模型任务有相似性的任务非常有用。</p><p>例如，一个流行的模型，InceptionV3，针对1000个类别的图像分类进行了优化，但我们的领域可能是对一些狗品种进行分类。深度学习中使用的一种众所周知的技术被称为迁移学习，这种技术将现有的已训练模型用于类似的任务。</p><p>这就是为什么迁移学习在深度学习实践者中获得了很大的流行，并在最近几年成为许多现实生活用例中的首选技术。它是关于在相关领域之间传递知识(或特性)的。</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>迁移学习的目的</h2></div></div></div><p>假设你已经<a id="id206" class="indexterm"/>训练了一个深度神经网络来区分新鲜芒果和腐烂芒果。在训练期间，网络将需要数以千计的腐烂和新鲜的芒果图像和数小时的训练来学习知识，例如如果任何水果腐烂，液体将从其中流出，它将产生难闻的气味。现在有了这种训练经验，网络可以用于不同的任务/用例，以使用在芒果图像的训练期间学到的腐烂特征的知识来区分腐烂的苹果和新鲜的苹果。</p><p>迁移学习的一般方法是训练一个基础网络，然后将其前n层复制到目标网络的前n层。目标网络的其余层随机初始化，并针对目标用例进行训练。</p><p>在深度学习工作流中使用迁移学习的主要场景如下:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>较小的数据集</strong>:当你有一个较小的数据集时，从头开始构建深度学习模型不会很好。迁移学习提供了将预先训练好的<a id="id207" class="indexterm"/>模型应用于新数据类别的方法。比方说，与用更小的数据集从头构建的深度学习模型相比，从100万张ImageNet数据构建的预训练模型将收敛到一个像样的解决方案(在只对一小部分可用的更小的训练数据进行训练后，例如CIFAR-10)。</li><li class="listitem" style="list-style-type: disc"><strong>资源更少</strong>:深度学习过程(比如卷积)需要大量的<a id="id208" class="indexterm"/>资源和时间。深度学习过程非常适合在基于高级GPU的机器上运行。但是使用预训练的模型，您可以使用您的笔记本电脑在不到一分钟的时间内轻松训练完整的训练集(假设有50，000张图像),而无需GPU，因为大多数情况下，模型会在最终层进行修改，只需简单更新分类器或回归器。</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>使用预训练模型的各种方法</h2></div></div></div><p>我们将讨论如何以不同方式使用预训练模型:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>使用预训练的架构</strong>:我们<a id="id209" class="indexterm"/>只能<a id="id210" class="indexterm"/>使用该架构，并将我们自己的随机权重初始化到我们的新数据集，而不是转移已训练模型的权重。</li><li class="listitem" style="list-style-type: disc"><strong>特征提取器</strong>:预先训练的模型可以用作特征提取机制，只需简单地<a id="id211" class="indexterm"/>移除网络的输出层(给出n个类别中每一个类别的概率)，然后冻结网络的所有先前层，作为新数据集的固定特征提取器。</li><li class="listitem" style="list-style-type: disc"><strong>部分冻结网络</strong>:有时我们可能会部分训练我们的新模型(也就是说，保持网络初始层的权重冻结，同时仅重新训练较高层)，而不是仅替换最后一层并从所有<a id="id212" class="indexterm"/>先前层中提取特征。冷冻层数的选择可以被认为是另一个超参数。<div> <img src="img/B08086_05_01.png.jpg" alt="Various approaches of using pre-trained models"/> <div> <p>图-1:用预先训练好的模型进行迁移学习</p> </div> </div></li></ul></div><p>主要取决于数据大小和数据集相似性，您可能必须决定如何进行迁移学习。下表讨论了这些情况:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>高数据相似性</p>
</th><th style="text-align: left" valign="bottom">
<p>低数据相似性</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><strong>数据量小</strong></p>
</td><td style="text-align: left" valign="top">
<p>在小数据大小但高数据相似性的情况下，我们将仅修改预训练模型的输出层，并将其用作特征提取器。</p>
</td><td style="text-align: left" valign="top">
<p>当数据大小和数据相似性都较低时，我们可以冻结预训练网络的初始<em> k </em>层，并且仅再次训练剩余的<em> (n-k) </em>层。这将有助于顶层根据新数据集进行定制，并且小数据量也将通过冻结网络的初始<em> k </em>层得到补偿。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>数据量大</strong></p>
</td><td style="text-align: left" valign="top">
<p>在这种情况下，我们可以使用预训练模型的架构和初始权重。</p>
</td><td style="text-align: left" valign="top">
<p>虽然我们有一个大型数据集，但与用于训练预训练模型的数据相比，这些数据有很大的不同，因此在这种情况下使用这些数据是无效的。相反，最好从头开始训练深层网络。</p>
</td></tr></tbody></table></div><p>在图像识别<a id="id213" class="indexterm"/>的情况下，转移学习利用预训练的卷积层来提取关于新输入图像的特征，这意味着只有原始模型的一小部分(主要是密集层)被重新训练。网络的其余部分仍处于冻结状态。这样，通过将原始图像仅通过网络的冻结部分一次，然后不再通过网络的该部分，节省了大量时间和资源。</p></div><div><div><div><div><h2 class="title">使用Keras对汽车、猫、狗和花进行分类</h2></div></div></div><p>让我们实施<a id="id214" class="indexterm"/>迁移学习和微调的概念，使用由150幅训练图像和50幅验证图像组成的定制数据集，为汽车、猫、狗和<a id="id215" class="indexterm"/>花的每个类别识别可定制的对象类别。</p><p>请注意，该数据集是通过从<em>卡格狗对猫</em>(<a class="ulink" href="https://www.kaggle.com/c/dogs-vs-cats">https://www.kaggle.com/c/dogs-vs-cats</a>)、史丹福汽车(<a class="ulink" href="http://ai.stanford.edu/~jkrause/cars/car_dataset.html">http://ai.stanford.edu/~jkrause/cars/car_dataset.html</a>)和<code class="literal">Oxford flower</code> <a id="id216" class="indexterm"/>数据集(<a class="ulink" href="http://www.robots.ox.ac.uk/~vgg/data/flowers">http://www.robots.ox.ac.uk/~vgg/data/flowers</a>)中获取图像来准备的。</p><div><img src="img/B08086_05_02.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/><div><p>图2:汽车vs猫vs狗vs花数据集结构</p></div></div><p>我们需要使用<code class="literal">preprocessing</code>函数执行一些预处理，并通过<code class="literal">rotation</code>、<code class="literal">shift</code>、<code class="literal">shear</code>、<code class="literal">zoom</code>和<code class="literal">flip</code>参数应用各种数据扩充变换:</p><div><pre class="programlisting">train_datagen = ImageDataGenerator(
	preprocessing_function=preprocess_input,
	rotation_range=30,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True
)
test_datagen = ImageDataGenerator(
	preprocessing_function=preprocess_input,
	rotation_range=30,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True
)
train_generator = train_datagen.flow_from_directory(
	args.train_dir,
	target_size=(IM_WIDTH, IM_HEIGHT),
	batch_size=batch_size,
)
validation_generator = test_datagen.flow_from_directory(
	args.val_dir,
	target_size=(IM_WIDTH, IM_HEIGHT),
	batch_size=batch_size,
)</pre></div><p>接下来，我们需要从<code class="literal">keras.applications</code>模块加载InceptionV3模型。标志<code class="literal">include_top=False</code>用于省去最后一个完全连接层的权重:</p><div><pre class="programlisting">base_model = InceptionV3(weights='imagenet', include_top=False)</pre></div><p>通过添加大小为1024的全连接<code class="literal">Dense</code>层，初始化新的最后一层<a id="id217" class="indexterm"/>，然后在输出上使用<code class="literal">softmax</code>函数来压缩<code class="literal">[0,1]</code>之间的值:</p><div><pre class="programlisting">
def addNewLastLayer(base_model, nb_classes):
	x = base_model.output
	x = GlobalAveragePooling2D()(x)
	x = Dense(FC_SIZE, activation='relu')(x)
	predictions = Dense(nb_classes, activation='softmax')(x)
	model = Model(input=base_model.input, output=predictions)
	return model
</pre></div><p>一旦网络的最后一层稳定下来(迁移学习)，我们就可以继续训练更多的层(微调)。</p><p>使用实用程序方法冻结所有层并编译模型:</p><div><pre class="programlisting">
def setupTransferLearn(model, base_model):
	for layer in base_model.layers:
	  layer.trainable = False
model.compile(optimizer='rmsprop',
			  loss='categorical_crossentropy',
              metrics=['accuracy'])
</pre></div><p>这是另一个实用方法，用于冻结InceptionV3架构中顶部两个先启块的底部，并重新训练剩余的顶部:</p><div><pre class="programlisting">
def setupFineTune(model):
   for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:
      layer.trainable = False
   for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:
	  layer.trainable = True
   model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),
loss='categorical_crossentropy')
</pre></div><p>现在我们已经准备好<a id="id218" class="indexterm"/>使用<code class="literal">fit_generator</code>方法进行训练，并最终保存我们的模型:</p><div><pre class="programlisting">
history = model.fit_generator(
	train_generator,
	samples_per_epoch=nb_train_samples,
	nb_epoch=nb_epoch,
	validation_data=validation_generator,
	nb_val_samples=nb_val_samples,
	class_weight='auto')
model.save(args.output_model_file)
</pre></div><p>运行以下命令进行训练和微调:</p><div><pre class="programlisting">python training-fine-tune.py --train_dir &lt;path to training images&gt; --val_dir &lt;path to validation images&gt;</pre></div><div><img src="img/B08086_05_03.png.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></div><p>即使数据集规模如此之小，通过利用预训练模型和迁移学习的能力，我们也能够在验证集上实现98.5%的准确率:</p><div><img src="img/B08086_05_04.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></div><p>瞧，我们现在可以<a id="id219" class="indexterm"/>使用保存的模型预测带有测试数据的图像(从本地文件系统或从URL ):</p><div><pre class="programlisting">python predict.py --image_url https://goo.gl/DCbuq8 --model inceptionv3-ft.model</pre></div><div><img src="img/B08086_05_05.png.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></div></div></div></div></div></div>
<title>Large scale deep learning with Apache Spark</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec28"/>使用Apache Spark进行大规模深度学习</h1></div></div></div><p>深度学习是一个<a id="id220" class="indexterm"/>资源饥渴和计算密集型的过程，你可以通过更多的数据和更大的网络获得更好的结果，但它的速度<a id="id221" class="indexterm"/>也会受到数据集大小的影响。在实践中，深度学习需要试验不同的训练参数值，称为超参数调整，其中你必须在大型数据集上迭代或多次运行你的深度网络，速度确实很重要。解决这个问题的一些常见方法是使用更快的硬件(通常是GPU)、优化的代码(使用合适的生产就绪框架)，以及在分布式集群上横向扩展以实现某种形式的并行性。</p><p>数据并行性是一个概念，它将大型数据集分割成多个数据块，然后通过运行在分布式集群的独立节点上的神经网络来处理这些数据块。</p><p>Apache Spark是一个<a id="id222" class="indexterm"/>快速、通用、容错的框架，通过在内存中处理rdd或数据帧而不是将数据保存到硬盘上，对大型分布式数据集进行交互式和迭代计算。It <a id="id223" class="indexterm"/>支持各种各样的数据源和存储层。它提供了统一的数据访问来组合不同的数据格式、流数据，并使用高级的可组合操作符来定义复杂的操作。</p><p>今天，Spark是大数据处理的超级大国，让每个人都可以访问大数据。但Spark或其核心模块本身无法在集群上训练或运行深层网络。在接下来的几节中，我们将使用优化的库在Apache Spark集群上开发深度学习应用程序。</p><div><div><h3 class="title"><a id="note07"/>注</h3><p>出于编码的目的，我们<a id="id224" class="indexterm"/>将不讨论分布式Spark集群设置，而是使用Apache Spark独立模式。更多关于火花簇模式的信息可以在:<a class="ulink" href="https://spark.apache.org/docs/latest/cluster-overview.html">https://spark.apache.org/docs/latest/cluster-overview.html</a>找到。</p></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>使用Spark深度学习运行预训练模型</h2></div></div></div><p>深度学习<a id="id225" class="indexterm"/> pipelines是一个开源库，它利用Apache Spark cluster的力量，轻松地将可扩展的深度学习<a id="id226" class="indexterm"/>集成到机器学习工作流中。它构建在Apache Spark的ML管道之上，用于培训，并使用Spark数据帧和SQL来部署模型。它通过将预先训练好的模型集成为Spark ML管道中的transformer，提供了以分布式方式运行迁移学习的高级API。</p><p>深度学习管道通过特征器的概念使迁移学习变得更容易。特征化器(或图像操作情况下的<code class="literal">DeepImageFeaturizer</code>)自动移除预训练神经网络模型的最后一层，并使用所有先前层的输出作为特定于新问题域的分类算法(例如，逻辑回归)的特征。</p><p>让我们实现<a id="id227" class="indexterm"/>深度学习管道，用Spark集群上的预训练模型预测<code class="literal">flower</code>数据集的样本图像(<a class="ulink" href="http://download.tensorflow.org/example_images/flower_photos.tgz">http://download . tensor flow . org/example _ images/flower _ photos . tgz</a>):</p><div><ol class="orderedlist arabic"><li class="listitem">首先使用深度学习管道包启动PySpark:<div><pre class="programlisting">pyspark --master local[*] --packages databricks:spark-deep-learning:0.1.0-spark2.1-s_2.11</pre></div><div><div><h3 class="title"><a id="tip02"/>提示</h3> <p>注意:如果在使用深度学习启动py spark时出现错误<strong>没有名为sparkdl </strong>的模块，请<a id="id228" class="indexterm"/>查看GitHub页面以获得解决方法:</p><p><a class="ulink" href="https://github.com/databricks/spark-deep-learning/issues/18">https://github.com/databricks/spark-deep-learning/issues/18</a></p></div></div></li><li class="listitem">首先读取<a id="id229" class="indexterm"/>图片，随机分成<code class="literal">train</code>、<code class="literal">test</code>组。<div>T5</div></li><li class="listitem">然后使用<code class="literal">InceptionV3</code>模型<div> <pre class="programlisting">featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3") lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label") p = Pipeline(stages=[featurizer, lr])</pre> </div>创建一个带有<code class="literal">DeepImageFeaturizer</code>的管道</li><li class="listitem">现在用现有的预训练模型拟合<a id="id230" class="indexterm"/>图像，其中<code class="literal">train_images_df</code>是图像和标签的数据集:<div> <pre class="programlisting">p_model = p.fit(train_df)   </pre> </div></li><li class="listitem">最后，我们来评估一下准确度:<div> <pre class="programlisting">tested_df = p_model.transform(test_df) evaluator = MulticlassClassificationEvaluator(metricName="accuracy") print("Test set accuracy = " + str(evaluator.evaluate(tested_df.select("prediction", "label"))))</pre> </div> <div> <img src="img/B08086_05_06.png.jpg" alt="Running pre-trained models using Spark deep learning"/> </div></li></ol></div><p>除了<code class="literal">DeepImageFeaturizer</code>之外，我们还可以利用预先存在的模型来进行预测，而不需要<a id="id231" class="indexterm"/>使用<code class="literal">DeepImagePredictor</code>进行任何重新训练或微调:</p><div><pre class="programlisting">sample_img_dir=&lt;path to your image&gt;

image_df = readImages(sample_img_dir)

predictor = DeepImagePredictor(inputCol="image", outputCol="predicted_labels", modelName="InceptionV3", decodePredictions=True, topK=10)
predictions_df = predictor.transform(image_df)

predictions_df.select("filePath", "predicted_labels").show(10,False)</pre></div><p>输入图像及其<a id="id232" class="indexterm"/>前五个预测如下所示:</p><div><img src="img/B08086_05_07.png.jpg" alt="Running pre-trained models using Spark deep learning"/></div><p>除了使用内置的预训练模型，深度学习管道还允许用户在火花预测管道中插入Keras模型或张量流图。这实际上将运行在单节点机器上的任何单节点深度模型变成了可以在大量数据上以分布式方式训练和部署的模型。</p><p>首先，我们将加载<a id="id234" class="indexterm"/> Keras内置的InceptionV3模型，并将其保存在文件中:</p><div><pre class="programlisting">model = InceptionV3(weights="imagenet")
model.save('model-full.h5')</pre></div><p>在预测阶段，我们将简单地加载模型并通过它传递图像以获得所需的预测:</p><div><pre class="programlisting">def loadAndPreprocessKerasInceptionV3(uri):
    # this is a typical way to load and prep images in keras
    image = img_to_array(load_img(uri, target_size=(299, 299)))
    image = np.expand_dims(image, axis=0)
    return preprocess_input(image)

transformer = KerasImageFileTransformer(inputCol="uri", 
  outputCol="predictions",
                                        modelFile="model-full.h5",
                                    imageLoader=loadAndPreprocessKerasInceptionV3,
                                        outputMode="vector")
dirpath=&lt;path to mix-img&gt;

files = [os.path.abspath(os.path.join(dirpath, f)) for f in os.listdir(dirpath) if f.endswith('.jpg')]
uri_df = sqlContext.createDataFrame(files, StringType()).toDF("uri")

final_df = transformer.transform(uri_df)
final_df.select("uri", "predictions").show()</pre></div><p>深度学习<a id="id235" class="indexterm"/>管道是在分布式Spark集群上进行迁移学习的一种非常快速的方式<a id="id236" class="indexterm"/>。但是你一定注意到了，featurizer只允许我们改变预训练模型的最后一层。但在某些情况下，你可能需要修改预训练网络的多个层才能获得想要的结果，深度学习管道不能提供这种完整的功能。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>使用BigDL进行大规模手写数字识别</h2></div></div></div><p>BigDL是一个开源的分布式高性能深度学习库，可以直接运行在Apache Spark集群之上。它的高性能是通过在每个Spark任务中结合<strong>英特尔数学内核库</strong> ( <strong> MKL </strong>)以及多线程编程实现的。BigDL提供Keras <a id="id238" class="indexterm"/>风格(顺序和函数)的高级API来构建<a id="id239" class="indexterm"/>深度学习应用，并向外扩展以执行大规模分析。使用BigDL的主要目的是:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">大规模运行深度学习模型，分析驻留在Spark或Hadoop集群(比如Hive、HDFS或HBase)中的海量数据</li><li class="listitem" style="list-style-type: disc">将深度学习功能(训练和预测)添加到您的大数据工作流<div> <img src="img/B08086_05_08.png.jpg" alt="Handwritten digit recognition at a large scale using BigDL"/> <div> <p>图Spark集群上的BigDL执行</p> </div> </div></li></ul></div><p>如图所示，BigDL驱动程序首先在集群的Spark主节点中启动。然后在<a id="id240" class="indexterm"/> <strong>集群管理器</strong>和驱动程序的帮助下，Spark任务被分配到工作节点上的Spark执行器上。BigDL与MKL互动，加快了这些任务的执行速度。</p><p>让我们实现一个大规模的深度神经网络，用<code class="literal">mnist</code>数据集识别手写数字。首先，我们将准备训练和验证样本:</p><div><pre class="programlisting">mnist_path = "datasets/mnist"
(train_data, test_data) = get_mnist(sc, mnist_path)
print train_data.count()
print test_data.count()</pre></div><p>然后我们将创建LeNet架构，它由两组卷积层、激活层和池层组成，接着是全连接层、激活层、另一个全连接层，最后是一个<code class="literal">SoftMax</code>分类器。LeNet很小，但功能强大，足以提供有趣的结果:</p><div><pre class="programlisting">def build_model(class_num):
    model = Sequential()
    model.add(Reshape([1, 28, 28]))
    model.add(SpatialConvolution(1, 6, 5, 5).set_name('conv1'))
    model.add(Tanh())
    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))
    model.add(Tanh())
    model.add(SpatialConvolution(6, 12, 5, 5).set_name('conv2'))
    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))
    model.add(Reshape([12 * 4 * 4]))
    model.add(Linear(12 * 4 * 4, 100).set_name('fc1'))
    model.add(Tanh())
    model.add(Linear(100, class_num).set_name('score'))
    model.add(LogSoftMax())
    return model
lenet_model = build_model(10)</pre></div><p>现在我们将配置一个<code class="literal">Optimizer</code>，并设置验证逻辑:</p><div><pre class="programlisting">optimizer = Optimizer(
    model=lenet_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=SGD(learningrate=0.4, learningrate_decay=0.0002),
    end_trigger=MaxEpoch(20),
    batch_size=2048)

optimizer.set_validation(
    batch_size=2048,
    val_rdd=test_data,
    trigger=EveryEpoch(),
    val_method=[Top1Accuracy()]
)

trained_model = optimizer.optimize()</pre></div><p>然后，我们将获取一些测试样本，并通过检查预测标签和实际标签来进行预测:</p><div><pre class="programlisting">predictions = trained_model.predict(test_data)</pre></div><p>最后，我们将使用<code class="literal">spark-submit</code>命令在Spark集群中训练LeNet模型。基于您的Apache Spark版本下载BigDL发行版(<a class="ulink" href="https://bigdl-project.github.io/master/#release-download/">https://bigdl-project.github.io/master/#release-download/</a>)，然后<a id="id241" class="indexterm"/>执行代码提供的文件(<code class="literal">run.sh</code>)在Spark集群中提交作业:</p><div><pre class="programlisting">SPARK_HOME= &lt;path to Spark&gt;
BigDL_HOME= &lt;path to BigDL&gt;
PYTHON_API_ZIP_PATH=${BigDL_HOME}/bigdl-python-&lt;version&gt;.zip
BigDL_JAR_PATH=${BigDL_HOME}/bigdl-SPARK-&lt;version&gt;.jar
export PYTHONPATH=${PYTHON_API_ZIP_PATH}:${BigDL_HOME}/conf/spark-bigdl.conf:$PYTHONPATH

${SPARK_HOME}/bin/spark-submit \
      --master &lt;local or spark master url&gt;\
      --driver-cores 5  \
      --driver-memory 5g  \
      --total-executor-cores 16  \
      --executor-cores 8  \
      --executor-memory 10g \
      --py-files ${PYTHON_API_ZIP_PATH},${BigDL_HOME}/BigDL-MNIST.py\
      --properties-file ${BigDL_HOME}/conf/spark-bigdl.conf \
      --jars ${BigDL_JAR_PATH} \
      --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \
      --conf spark.executor.extraClassPath=bigdl-SPARK&lt;version&gt;.jar \
      ${BigDL_HOME}/BigDL-MNIST.py</pre></div><p>关于<code class="literal">spark-submit</code>的更多信息<a id="id242" class="indexterm"/>可以在:<a class="ulink" href="https://spark.apache.org/docs/latest/submitting-applications.html">https://spark . Apache . org/docs/latest/submitting-applications . html</a>找到。</p><p>提交作业后，您可以在<strong> Spark Master </strong>应用页面上跟踪进度，如下所示:</p><div><img src="img/B08086_05_09.png.jpg" alt="Handwritten digit recognition at a large scale using BigDL"/><div><p>图4:运行在Apache Spark集群上的LeNet5模型的BigDL作业</p></div></div><p>作业成功完成后，您可以搜索Spark workers的日志来验证您的模型的准确性，如下所示:</p><div><pre class="programlisting">INFO  DistriOptimizer$:536 - Top1Accuracy is Accuracy(correct: 9568, count: 10000, accuracy: 0.9568)</pre></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec56"/>使用SRGAN生成高分辨率图像</h2></div></div></div><p><strong>超分辨率生成网络</strong> ( <strong> SRGAN) </strong>擅长从其低分辨率<a id="id243" class="indexterm"/>对应物生成高分辨率图像。在训练阶段，通过将高斯滤波器应用于高分辨率图像<a id="id244" class="indexterm"/>，然后进行下采样操作，将高分辨率图像转换为低分辨率图像。</p><p>在深入研究网络架构之前，让我们先定义一些符号:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>I</em><sup><em>LR</em></sup>:尺寸宽度(<em> W </em> ) x高度(<em> H </em> ) x颜色通道(<em> C </em>)的低分辨率图像</li><li class="listitem" style="list-style-type: disc"><em>I</em><sup>T5】HR</sup>:高分辨率图像，尺寸<em> rW × rH × C </em></li><li class="listitem" style="list-style-type: disc"><em>I</em><sup><em>SR</em></sup>:尺寸为<em> rW × rH × C </em>的超分辨率图像</li><li class="listitem" style="list-style-type: disc"><em> r </em>:下采样因子</li><li class="listitem" style="list-style-type: disc"><em>G</em><sub><em>θG</em></sub>:发电机网络</li><li class="listitem" style="list-style-type: disc"><em>D</em><sub><em>θD</em></sub>:鉴频器网络</li></ul></div><p>为了实现从其对应的低分辨率对应物估计高分辨率输入图像的目标，生成器网络被训练为前馈卷积神经网络<em>G</em><sub><em>θG</em></sub>，其由<em> θG </em>参数化，其中<em> θG </em>由深度网络的L层的权重(<em> W1:L </em>和偏差(<em> b1:L </em>表示，并且通过优化超分辨率来获得对于具有高分辨率的训练图像<img src="img/B08086_05_23.jpg" alt="High resolution image generation using SRGAN"/>，<em>n = 1</em>；N连同其对应的低分辨率<img src="img/B08086_05_24.jpg" alt="High resolution image generation using SRGAN"/>，<em> n=1 </em>，N，我们可以求解<em> θG </em>，如下:</p><div><img src="img/B08086_05_10.jpg" alt="High resolution image generation using SRGAN"/></div><div><div><h3 class="title"><a id="note08"/>注意</h3><p>感知损失<a id="id245" class="indexterm"/>函数<em> lSR </em>的公式对于发电机网络的性能至关重要。一般来说，感知损失通常基于<strong>均方误差</strong> ( <strong> MSE </strong>)来建模，但是为了避免具有过度平滑纹理的不令人满意的解决方案，基于预训练的19层VGG网络的ReLU激活层来制定新的内容损失。</p></div></div><p>感知损失是几个<code class="literal">loss</code>函数的加权组合，映射超分辨率图像的重要特征如下:</p><div><img src="img/B08086_05_11.jpg" alt="High resolution image generation using SRGAN"/></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>内容损失</strong>:基于VGG的<a id="id246" class="indexterm"/>内容损失定义为重构图像<em>G</em><em><sub>θG</sub></em>(<em>I</em><sup><em>LR</em></sup>)和对应的高分辨率图像<em> I </em> <em> HR </em>的特征表示之间的欧氏距离。这里<img src="img/B08086_05_25.jpg" alt="High resolution image generation using SRGAN"/> <sub> <em> i，j </em> </sub>表示在VGG19网络内第I个最大池层之前通过第j个卷积(激活后)获得的特征图。以及<em> Wi，j </em>，<em> Hi，j </em>描述<em> VGG </em>网络内各自特征地图的尺寸:<div> <img src="img/B08086_05_12.jpg" alt="High resolution image generation using SRGAN"/> </div></li><li class="listitem" style="list-style-type: disc"><strong>对抗性损失</strong>:生成性损失<a id="id247" class="indexterm"/>基于鉴别器的概率<em>D</em><sub><em>θD</em></sub><em>(G</em><sub><em>θG</em></sub><em>(I</em><sub><em>LR</em></sub><em>)</em>在所有训练图像上，鼓励网络偏向于驻留在自然图像流形上的解，以欺骗鉴别器</li></ul></div><p>类似于对抗网络的概念，SRGAN方法背后的一般思想是训练生成器<em> G </em>，其目标是欺骗鉴别器<em> D </em>，该鉴别器被训练来区分超分辨率图像和真实图像:</p><div><img src="img/B08086_05_14.jpg" alt="High resolution image generation using SRGAN"/></div><p>基于这种方法，生成器学习创建与真实图像高度相似的解决方案，因此很难通过鉴别器<em> D </em>进行分类。这鼓励了存在于自然图像的子空间——流形——中的感觉上更好的解决方案。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec57"/>SRGAN的架构</h2></div></div></div><p>如下图所示，发电机网络<strong> G </strong>由布局相同的<strong>B</strong>T39】剩余块组成。每个<a id="id248" class="indexterm"/>块有两个卷积层，带有3×3的小内核和64个特征图<a id="id249" class="indexterm"/>，后面是批量标准化层【32】和parameter CRI Lu【28】作为<code class="literal">activation</code>函数。输入图像的分辨率通过两个经过训练的子像素卷积层来提高。</p><p>鉴别器网络使用泄漏ReLU激活(其中<em> α </em> = 0.2)，由八个卷积层组成，具有数量不断增加的3×3滤波器内核，从64个内核增加到512个内核，增加了2倍。每当特征的数量增加一倍时，就使用步长卷积来降低图像分辨率。</p><p>得到的512个特征图经过两个密集层，随后是最终的sigmoid激活层，以获得生成的图像样本的分类概率:</p><div><img src="img/B08086_05_15.png.jpg" alt="Architecture of the SRGAN"/><div><p>图5:生成器和鉴别器网络的架构，对应于每个卷积层的内核大小(k)、特征图数量(n)和步距(s)。</p><p>来源:<em> arXiv，1609.04802，2017 </em></p></div></div><p>现在是时候使用TensorFlow深入研究代码，并使用LFW面部数据集生成高分辨率图像了。</p><p>生成器网络首先被构建为具有3×3核和64个特征图的单个去卷积层，随后是ReLU作为<code class="literal">activation</code>函数。然后有五个剩余块，每个块具有两个卷积层，然后是批量归一化和ReLU。最后，输入图像的分辨率通过两个经过训练的像素混合层得到提高:</p><div><pre class="programlisting">def generator(self, x, is_training, reuse):
    with tf.variable_scope('generator', reuse=reuse):
      with tf.variable_scope('deconv1'):
          x = deconv_layer(
               x, [3, 3, 64, 3], [self.batch_size, 24, 24, 64], 1)
          x = tf.nn.relu(x)
          shortcut = x
# 5 Residual block with identical layout of deconvolution layers having batch norm and relu as activation function.
    for i in range(5):
       mid = x
       with tf.variable_scope('block{}a'.format(i+1)):
         x = deconv_layer(x, [3, 3, 64, 64], [self.batch_size, 24, 
24, 64], 1)
         x = batch_normalize(x, is_training)
         x = tf.nn.relu(x)

# 2 deconvolution layers having pixel-suffle and relu as activation function. 
     with tf.variable_scope('deconv3'):
         x = deconv_layer(x, [3, 3, 256, 64], [self.batch_size, 24, 
24, 256], 1)
         x = pixel_shuffle_layer(x, 2, 64) # n_split = 256 / 2 ** 2
         x = tf.nn.relu(x)
     with tf.variable_scope('deconv4'):
         x = deconv_layer(x, [3, 3, 64, 64], [self.batch_size, 48, 
48, 64], 1)
         x = pixel_shuffle_layer(x, 2, 16)
         x = tf.nn.relu(x)

     . . . . . . . . [code omitted for clarity]

return x	</pre></div><p><code class="literal">deconvolution layer</code>函数由一个TensorFlow <code class="literal">conv2d_transpose</code>方法定义，Xavier初始化如下:</p><div><pre class="programlisting">def deconv_layer(x, filter_shape, output_shape, stride, trainable=True):
    filter_ = tf.get_variable(
        name='weight',
        shape=filter_shape,
        dtype=tf.float32,
        initializer=tf.contrib.layers.xavier_initializer(),
        trainable=trainable)
    return tf.nn.conv2d_transpose(
        value=x,
        filter=filter_,
        output_shape=output_shape,
        strides=[1, stride, stride, 1])</pre></div><p>鉴别器网络<a id="id251" class="indexterm"/>由八个卷积层组成，具有3×3个滤波器核，从64个核增加到512个核。得到的512个特征图被展平，并经过两个密集的完全连接的层，之后是最终的softmax层，以获得生成的图像样本的分类概率:</p><div><pre class="programlisting">def discriminator(self, x, is_training, reuse):
        with tf.variable_scope('discriminator', reuse=reuse):
            with tf.variable_scope('conv1'):
                x = conv_layer(x, [3, 3, 3, 64], 1)
                x = lrelu(x)
            with tf.variable_scope('conv2'):
                x = conv_layer(x, [3, 3, 64, 64], 2)
                x = lrelu(x)
                x = batch_normalize(x, is_training)

         .  .   .   .   .   . [code omitted for clarity]

            x = flatten_layer(x)
            with tf.variable_scope('fc'):
                x = full_connection_layer(x, 1024)
                x = lrelu(x)
            with tf.variable_scope('softmax'):
                x = full_connection_layer(x, 1)
                
return x</pre></div><p>网络使用LeakyReLU(其中<em> α </em> = 0.2)作为卷积层的<code class="literal">activation</code>函数:</p><div><pre class="programlisting">def lrelu(x, trainbable=None):
    alpha = 0.2
    return tf.maximum(alpha * x, x)</pre></div><p><code class="literal">convolution layer</code>函数使用张量流<code class="literal">conv2d</code>方法定义，Xavier初始化如下:</p><div><pre class="programlisting">def conv_layer(x, filter_shape, stride, trainable=True):
    filter_ = tf.get_variable(
        name='weight', 
        shape=filter_shape,
        dtype=tf.float32, 
        initializer=tf.contrib.layers.xavier_initializer(),
        trainable=trainable)
    return tf.nn.conv2d(
        input=x,
        filter=filter_,
        strides=[1, stride, stride, 1],
        padding='SAME')</pre></div><p>请注意，代码<a id="id252" class="indexterm"/>实现对鉴别器使用最小二乘<code class="literal">loss</code>函数(以避免消失梯度问题)，而不是SRGAN ( <em> arXiv，1609.04802，2017 </em>)的原始论文中提出的sigmoid交叉熵<code class="literal">loss</code>函数:</p><div><pre class="programlisting">def inference_adversarial_loss(real_output, fake_output):
alpha = 1e-5
g_loss = tf.reduce_mean(
tf.nn.l2_loss(fake_output - tf.ones_like(fake_output)))
d_loss_real = tf.reduce_mean(
tf.nn.l2_loss(real_output - tf.ones_like(true_output)))
d_loss_fake = tf.reduce_mean(
tf.nn.l2_loss(fake_output + tf.zeros_like(fake_output)))
d_loss = d_loss_real + d_loss_fake
return (g_loss * alpha, d_loss * alpha)

generator_loss, discriminator_loss = (
inference_adversarial_loss(true_output, fake_output))</pre></div><p>更多<a id="id253" class="indexterm"/>关于<strong>最小二乘甘</strong>的信息请访问:<a class="ulink" href="https://arxiv.org/abs/1611.04076">https://arxiv.org/abs/1611.04076</a></p><p>运行<a id="id254" class="indexterm"/> SRGAN的<code class="literal">code</code>目录结构如下所示:</p><div><img src="img/B08086_05_16.png.jpg" alt="Architecture of the SRGAN"/></div><p>首先，让我们下载一个<code class="literal">LFW facial</code>数据集，进行一些预处理(正面人脸检测，将数据集分割为训练和测试)并将数据集存储在<code class="literal">data/</code>目录下:</p><div><pre class="programlisting">
<strong>python download-preprocess-lfw.py</strong>
</pre></div><div><img src="img/B08086_05_17.png.jpg" alt="Architecture of the SRGAN"/></div><p>接下来从下面的链接下载预先训练好的<a id="id255" class="indexterm"/> VGG19模型，提取出来，保存在<code class="literal">backup/</code>目录下:</p><p><a class="ulink" href="https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k">https://drive.google.com/open?id = 0 b-s60k 7 b 0v 9 vcxnfszdjz 0 LCC 0k</a></p><p>接下来执行<code class="literal">trainSrgan.py</code>文件，使用VGG19模型启动SRGAN操作:</p><div><pre class="programlisting">
<strong>python trainSrgan.py</strong>
</pre></div><p>一旦训练开始，生成器网络将开始在<code class="literal">result/</code>目录中生成超分辨率图像。来自<code class="literal">result/</code>目录的一些样本图像如下所示:</p><div><img src="img/B08086_05_18.png.jpg" alt="Architecture of the SRGAN"/></div></div></div></div></div>
<title>Generating artistic hallucinated images using DeepDream</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec29"/>使用DeepDream生成艺术幻觉图像</h1></div></div></div><p>DeepDream算法是一种经过<a id="id256" class="indexterm"/>修改的神经网络，能够通过在训练数据的方向上改变图像来产生令人印象深刻的超现实主义、梦幻般的幻觉<a id="id257" class="indexterm"/>外观。它使用反向传播来改变图像，而不是通过网络改变权重。</p><p>概括地说，该算法可以概括为以下步骤:</p><div><ol class="orderedlist arabic"><li class="listitem">选择网络的一个图层和一个你觉得有意思的滤镜。</li><li class="listitem">然后计算图像到该层的激活。</li><li class="listitem">将滤波器的激活反向传播回输入图像。</li><li class="listitem">将梯度与学习率相乘，并将它们添加到输入图像中。</li><li class="listitem">重复步骤2到4，直到您对结果满意为止。</li></ol></div><p>对输出迭代地应用算法<a id="id258" class="indexterm"/>，并在每次迭代后应用一些缩放，这有助于网络通过探索它所知道的一组事物来生成源源不断的新印象。</p><p>让我们深入代码来<a id="id259" class="indexterm"/>生成一个迷幻梦幻的图像。我们将对Keras中可用的预训练VGG16模型的各个层应用以下设置。请注意，除了使用预训练，我们还可以将此设置应用于您选择的新神经网络架构:</p><div><pre class="programlisting">settings_preset = {
    'dreamy': {
        'features': {
            'block5_conv1': 0.05,
            'block5_conv2': 0.02
        },
        'continuity': 0.1,
        'dream_l2': 0.02,
        'jitter': 0
    }
}

settings = settings_preset['dreamy']</pre></div><p>该实用程序功能主要是将图像加载、调整大小和格式化为适当的张量格式:</p><div><pre class="programlisting">def preprocess_image(image_path):
    img = load_img(image_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg16.preprocess_input(img)
    return img</pre></div><p>然后，我们计算连续性损失，以使图像具有局部一致性，并避免看起来像论文中讨论的总变化损失的变体的混乱模糊(<a class="ulink" href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf">http://www . robots . ox . AC . uk/~ ve daldi/assets/pubs/mahendran 15 understanding . pdf</a>):</p><div><pre class="programlisting">def continuity_loss(x):
    assert K.ndim(x) == 4
    a = K.square(x[:, :img_height-1, :img_width-1, :] -
                 x[:, 1:, :img_width-1, :])
    b = K.square(x[:, :img_height-1, :img_width-1, :] -
                 x[:, :img_height-1, 1:, :])

# (a+b) is the squared spatial gradient, 1.25 is a hyperparameter # that should be &gt;1.0 as discussed in the aforementioned paper
    return K.sum(K.pow(a+b, 1.25))</pre></div><p>接下来，我们将在VGG16模型上加载预训练重量:</p><div><pre class="programlisting">model = vgg16.VGG16(input_tensor=dream, weights='imagenet', include_top=False)</pre></div><p>之后，我们将把一个层的特征的<code class="literal">l2</code>范数加到<code class="literal">loss</code>上，然后把连续性损失加到<a id="id260" class="indexterm"/>上，给出图像局部一致性<a id="id261" class="indexterm"/>，随后再把<code class="literal">l2</code>范数加到损失上，以防止像素取非常高的值，然后计算梦相对于损失的梯度:</p><div><pre class="programlisting">loss += settings['continuity'] * continuity_loss(dream) / np.prod(img_size)
loss += settings['dream_l2'] * K.sum(K.square(dream)) / np.prod(img_size)
grads = K.gradients(loss, dream)</pre></div><p>最后，我们将向输入图像添加一个<code class="literal">random_jitter</code>，并对生成图像的像素运行<code class="literal">L-BFGS</code>优化器，以最小化损失:</p><div><pre class="programlisting">random_jitter = (settings['jitter']*2) * (np.random.random(img_size)-0.5)
x += random_jitter

# run L-BFGS 
x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                              fprime=evaluator.grads, maxfun=7)</pre></div><p>最后，我们将对梦进行解码，并将其保存在输出图像文件中:</p><div><pre class="programlisting">    x = x.reshape(img_size)
    x -= random_jitter
    img = deprocess_image(np.copy(x))
    fn = result_prefix + '_at_iteration_%d.png' % i
    imsave(fn, img)</pre></div><p>五次迭代后生成的梦幻输出如下所示:</p><div><img src="img/B08086_05_19.png.jpg" alt="Generating artistic hallucinated images using DeepDream"/><div><p>图-6:左图为原始输入图像，右图为deep dream创建的梦幻艺术图像</p></div></div></div></div></div>
<title>Generating handwritten digits with VAE using TensorFlow</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec30"/>使用张量流通过VAE生成手写数字</h1></div></div></div><p><strong>变分自动编码器</strong> ( <strong> VAE </strong>)很好地将无监督学习与变分贝叶斯方法综合成一个<a id="id262" class="indexterm"/>圆滑的包。它通过将输入、隐藏表示和重构输出视为定向图形模型中的概率<a id="id263" class="indexterm"/>随机变量来应用基本自动编码器方法的概率开启。</p><p>从贝叶斯的角度来看，编码器成为一个变分推理网络，它将观察到的输入映射到潜在空间上的后验分布，而解码器成为一个生成网络，它将任意潜在坐标映射回原始数据空间上的分布。</p><p>VAE是在编码网络上添加一个约束，生成大致遵循<a id="id264" class="indexterm"/>单位高斯分布的潜在向量(该约束将VAE与标准自动编码器分开)，然后通过将潜在向量<a id="id265" class="indexterm"/>传递通过解码器网络来重建图像:</p><p> </p><div><img src="img/B08086_05_22.jpg" alt="Generating handwritten digits with VAE using TensorFlow"/></div><p>
</p></div></div></div>
<title>A real world analogy of VAE</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec31"/>VAE的真实世界类比</h1></div></div></div><p>假设我们想要<a id="id266" class="indexterm"/>生成数据(一种动物),一个好的方法是在实际生成数据之前，先决定我们想要生成什么样的数据。所以，我们必须想象一些关于表现动物的标准，比如它应该有四条腿并且能够游泳。一旦我们有了这些标准，我们就可以通过从动物王国取样来产生动物。我们的想象标准类似于潜在变量。首先确定潜在变量有助于很好地描述数据，否则就像盲目地生成数据。</p><p>VAE的基本思想是用<em> p(z|x) </em>来推断<em> p(z) </em>。现在让我们用一些数学符号来展开:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em> x </em>:表示我们要生成的数据(即动物)</li><li class="listitem" style="list-style-type: disc"><em> z </em>:代表潜在变量(即我们的想象力)</li><li class="listitem" style="list-style-type: disc"><em> p(x) </em>:代表数据的分布(即动物界)</li><li class="listitem" style="list-style-type: disc"><em> p(z) </em>:代表潜变量(即想象力的来源——我们的大脑)的正态概率分布</li><li class="listitem" style="list-style-type: disc"><em> p(x|z) </em>:给定潜在变量(即将想象变成现实动物)产生数据的概率分布</li></ul></div><p>根据类似的例子，我们希望将我们的想象力限制在动物王国领域，因此我们不应该想象诸如根、叶子、钱、玻璃、GPU、冰箱、地毯之类的东西，因为这些东西不太可能与<a id="id267" class="indexterm"/>动物王国有任何共同点。</p><p>VAE的<code class="literal">loss</code>函数基本上是负对数似然，正则化如下:</p><div><img src="img/B08086_05_20.jpg" alt="A real world analogy of VAE"/></div><p>第一项是第I个数据点的预期负对数似然或重建损失，其中根据编码器在表示上的分布来计算预期。这一术语有助于解码器很好地重建数据，如果解码器做不到这一点，就会产生巨大的成本。第二项表示编码器分布<em> q(z|x) </em>和<em> p(z) </em>之间的Kullback-Leibler散度，并作为正则项，当编码器的输出表示<em> z </em>不同于正态分布时，该正则项向损失添加惩罚。</p><p>现在，让我们深入研究一下使用TensorFlow通过VAE从MNIST数据集生成手写数字的代码。首先让我们创建具有单个隐藏层的编码器网络<em> Q(z|X) </em>，该网络将把<em> X </em>作为输入，把<em> μ(X) </em>和<em>σ(X)</em>作为高斯分布的一部分:</p><div><pre class="programlisting">X = tf.placeholder(tf.float32, shape=[None, X_dim])
z = tf.placeholder(tf.float32, shape=[None, z_dim])

Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))
Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))

Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))
Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))
Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))
Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))


def Q(X):
    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)
    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu
    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma
    return z_mu, z_logvar</pre></div><p>现在我们创建解码器网络<em> P(X|z) </em>:</p><div><pre class="programlisting">P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))
P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))

P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))
P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))


def P(z):
    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)
    logits = tf.matmul(h, P_W2) + P_b2
    prob = tf.nn.sigmoid(logits)
    return prob, logits</pre></div><p>然后我们计算<a id="id268" class="indexterm"/>重建损耗和Kullback-Leibler发散损耗，并将它们相加得到VAE网络的总损耗:</p><div><pre class="programlisting">recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)
kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)
# VAE loss
vae_loss = tf.reduce_mean(recon_loss + kl_loss)</pre></div><p>然后用一个<code class="literal">AdamOptimizer</code>把损失降到最低:</p><div><pre class="programlisting">solver = tf.train.AdamOptimizer().minimize(vae_loss)</pre></div><p>运行文件(<code class="literal">VAE.py</code>或<code class="literal">VAE.ipynb</code>)启动对<code class="literal">MNIST</code>数据集的VAE操作，图像将在输出文件夹中生成。样本手写数字图像是在10，000次迭代之后生成的:</p><div><img src="img/B08086_05_21.png.jpg" alt="A real world analogy of VAE"/></div></div></div></div>
<title>A comparison of two generative models—GAN and VAE</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec32"/>甘与两种生成模式的比较</h1></div></div></div><p>虽然两者都是非常令人兴奋的生成模型方法，并且帮助研究人员进入无监督领域以及生成能力，但这两个模型在训练方式上有所不同。甘的理论来源于博弈论，目标是找到鉴别器网络和发电机网络之间的纳什均衡。而VAE基本上是一个植根于贝叶斯推理的概率图形模型，其目标是潜在建模，也就是说，它试图<a id="id270" class="indexterm"/>建模底层数据的概率分布，以便从该分布中采样新数据。</p><p>与GAN相比，VAE有一种明确的已知方法来评估模型的质量(如对数似然，通过重要性采样或下限估计)，但VAE的问题是，它在计算潜在损失时使用直接均方误差，而不是敌对网络，因此它们过度简化了目标任务，因为它们必然要在潜在空间中工作，因此与GAN相比，它们经常生成模糊的图像。</p></div></div></div>
<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
<!-- kobo-style -->

<script type="text/javascript" src="img/kobo.js"/>
<style type="text/css" id="kobostylehacks">div#book-inner p, div#book-inner div { font-size: 1.0em; } a { color: black; } a:link, a:visited, a:hover, a:active { color: blue; } div#book-inner * { margin-top: 0 !important; margin-bottom: 0 !important;}</style>

<div><div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec33"/>摘要</h1></div></div></div><p>迁移学习解决了有效处理小数据的问题，而无需从头开始重新发明训练轮。您已经学会了从预先训练的模型中提取和转移特性，并将其应用到您自己的问题领域中。此外，您已经掌握了使用Spark及其相关组件在大规模分布式系统上训练和运行更深层次的模型。然后，您利用SRGAN中的迁移学习功能生成了一幅逼真的高分辨率图像。此外，你已经掌握了其他生成模型方法的概念，如艺术图像生成的VAE和DeepDream。在最后一章中，我们将从训练深度模型或生成模型转移我们的焦点，并学习在生产中部署基于深度学习的应用程序的各种方法。</p></div></div></div></body></html>