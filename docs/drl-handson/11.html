<html><head/><body>
<html>
  <head>
    <title>Chapter 11. Asynchronous Advantage Actor-Critic</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>第十一章。异步优势行动者-批评家</h1></div></div></div><p class="calibre8">本章致力于扩展我们在前一章详细讨论的<strong class="calibre2">演员-评论家</strong> ( <strong class="calibre2"> A2C </strong>)方法。该扩展增加了真正的异步环境交互。全称是<strong class="calibre2">异步优势演员-评论家</strong>，通常缩写为A3C。这种方法是RL实践者最广泛使用的方法之一。我们将看一下向基本A2C方法添加异步行为的两种方法。</p></div></body></html>


<html>
  <head>
    <title>Chapter 11. Asynchronous Advantage Actor-Critic</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec42" class="calibre1"/>相关性和样本效率</h1></div></div></div><p class="calibre8">提高<strong class="calibre2">策略梯度</strong> ( <strong class="calibre2"> PG </strong>)系列方法稳定性的<a id="id414" class="calibre1"/>方法之一是并行使用多个环境。这背后的原因是我们在<a class="calibre1" title="Chapter 6. Deep Q-Networks" href="part0043_split_000.html#190861-ce551566b6304db290b61e4d70de52ee">第六章</a>、<em class="calibre11">深度Q网络</em>中讨论的基本问题，当我们谈到样本之间的相关性时，它打破了<strong class="calibre2">独立同分布</strong>(<strong class="calibre2">I . I . d .</strong>)假设，这对于<strong class="calibre2">随机梯度下降</strong> ( <strong class="calibre2"> SGD </strong>)优化来说是至关重要的。这种相关性的负面结果是梯度中非常高的方差，这意味着我们的训练批次包含非常相似的示例，所有这些示例都将我们的网络推向相同的方向。然而，从全球角度来看，这可能完全是错误的方向，因为所有这些例子都可能来自一个幸运或不幸的插曲。</p><p class="calibre8">利用我们的<strong class="calibre2">深度Q网络</strong> ( <strong class="calibre2"> DQN </strong>)，我们通过在重放缓冲区中存储大量先前状态并从该缓冲区中采样我们的训练批次来解决这个问题。如果缓冲区足够大，从中随机抽取的样本可以更好地代表各州的总体分布情况。不幸的是，这种解决方案不适用于PG方法，因为它们中的大多数都是基于策略的，这意味着我们必须对当前策略生成的样本进行训练，因此，<em class="calibre11">记住旧的转换</em>不再可能。您可以尝试这样做，但是生成的PG将用于生成样本的旧策略，而不是您想要更新的当前策略。</p><p class="calibre8">几年来，这个问题一直是研究人员关注的焦点，并且提出了解决这个问题的几种方法，但是这个问题仍然远远没有解决。最常用的解决方案是使用几个并行环境收集转换，所有这些环境都利用当前策略。这打破了单个情节中的相关性，因为我们现在在从不同环境中获得的几个情节上进行训练。与此同时，我们仍在使用我们目前的政策。这样做的一个很大的缺点是<strong class="calibre2">样本效率低</strong>，因为我们基本上丢掉了我们在一次训练后获得的所有经验。比较DQN和控卫的方法很简单。例如，对于DQN，如果我们对每个新帧使用1M的重放缓冲样本和32个样本的训练批次大小，则在从体验重放推送之前，每个单个过渡将大约使用32次。对于第7章、<em class="calibre11"> DQN扩展</em>中讨论的<a id="id416" class="calibre1"/>优先级重放缓冲区，这个<a id="id417" class="calibre1"/>数字可能会高得多，因为样本概率并不均匀。在PG的情况下，从环境中获得的每一个经验只能使用一次，因为我们的方法需要新的数据，所以PG方法的数据效率可能比基于值的非策略方法低一个数量级。</p><p class="calibre8">另一方面，我们的A2C代理在8M帧中收敛于<strong class="calibre2"> Pong </strong>，这仅仅是第六章<em class="calibre11">Deep Q-Networks</em><a class="calibre1" title="Chapter 7. DQN Extensions" href="part0048_split_000.html#1DOR02-ce551566b6304db290b61e4d70de52ee">第七章</a><em class="calibre11">DQN扩展</em>中基本DQN的1M帧的八倍多。所以，这向我们表明PG方法并非完全无用；它们只是不同，有自己的特点，您需要在选择方法时加以考虑。如果您的环境在代理交互方面比较便宜(环境速度快、内存占用少、允许并行化等等)，PG方法可能是更好的选择。另一方面，如果环境是昂贵的，并且获得大量的经验会减慢训练过程，基于价值的方法可能是更聪明的方法。</p></div></div></body></html>


<html>
  <head>
    <title>Adding an extra A to A2C</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec43" class="calibre1"/>给A2C多加一个A</h1></div></div></div><p class="calibre8">从实践的角度来看，与几个并行环境进行通信是很简单的，我们已经在前一章中完成了，但是没有明确地说明。在A2C代理中，我们<a id="id418" class="calibre1"/>将一组健身房环境传递给<code class="literal">ExperienceSource</code>类，该类将其切换到循环数据收集模式:每次我们请求从体验源进行转换时，该类都会使用我们数组中的下一个环境(当然，为每个环境保留状态)。这种简单的方法相当于与环境的并行通信，但是有一点不同:严格意义上的通信不是并行的，而是以串行方式执行的。然而，来自我们经验源的样本被打乱了。这个想法如下图所示:</p><div><img src="img/00237.jpeg" alt="Adding an extra A to A2C" class="calibre9"/><div><p class="calibre14">图1:来自多个并行环境的代理培训</p></div></div><p class="calibre10"> </p><p class="calibre8">这种方法工作得很好，并且<a id="id419" class="calibre1"/>帮助我们在A2C方法中获得了收敛，但是在计算资源利用方面它仍然不是完美的。即使是现在最普通的工作站也有几个CPU内核，可以用于计算，例如训练和环境交互。另一方面，当你有一个清晰的执行流时，并行编程比传统的范例更难。幸运的是，Python是一种非常富于表现力和灵活性的语言，拥有许多第三方库，这使得您可以毫不费力地进行并行编程。另一个好消息是PyTorch在它的<code class="literal">torch.multiprocessing</code>模块中支持并行编程。并行和分布式编程是一个非常广泛的话题，远远超出了本书的范围。在这里，我们将仅仅触及并行化这个大领域的表面，但是还有很多东西需要学习。</p><p class="calibre8">关于行动者-批评家并行化，存在两种方法:</p><div><ol class="orderedlist"><li class="listitem" value="1"><strong class="calibre2">数据并行</strong>:我们<a id="id420" class="calibre1"/>可以有几个进程，每个进程与一个或多个环境通信，并为我们提供转换(s，r，a，s’)。所有这些样本都集中在一个单一的训练过程中，该过程计算损失并执行SGD更新。然后，需要将更新的神经网络参数广播给所有其他进程，以便在未来的环境通信中使用。</li><li class="listitem" value="2"><strong class="calibre2">梯度并行</strong>:由于训练过程的目标是计算梯度以更新我们的网络，我们可以让几个过程计算它们自己的训练样本的梯度。然后，可以将这些梯度相加，在一个过程中执行SGD更新。当然，更新后的网络权重也需要传播给所有工人，以保持数据符合策略。</li></ol><div/></div><p class="calibre8">下图说明了这两种方法。</p><div><img src="img/00238.jpeg" alt="Adding an extra A to A2C" class="calibre9"/><div><p class="calibre14">图2:基于正在收集的分布式训练样本的第一种演员-评论家并行方法</p></div></div><p class="calibre10"> </p><div><img src="img/00239.jpeg" alt="Adding an extra A to A2C" class="calibre9"/><div><p class="calibre14">图3:并行化的第二种方法，为模型收集梯度</p></div></div><p class="calibre10"> </p><p class="calibre8">从图中可以看出这两种方法的区别并不明显，但是您需要注意计算成本。A3C优化中最繁重的操作是训练过程，该过程包括根据数据样本计算损失(正向传递)以及计算与该损失相关的梯度。SGD优化步骤是相当轻量级的，基本上只是将比例梯度添加到网络的权重中。通过将第二种方法中的损失和梯度计算从中央流程中移出，我们消除了主要的潜在瓶颈，并使整个流程的可扩展性显著提高。</p><p class="calibre8">在实践中，方法的选择主要取决于你的资源和你的目标。如果您有一个单一的优化问题和大量的分布式计算资源，例如分布在网络中几台机器上的几十个GPU，那么梯度并行是加速您训练的最佳方法。然而，在单个GPU的情况下，这两种方法将向您展示相似的性能，并且第一种方法通常更容易实现，因为您不需要弄乱低级别的渐变值。在这一章中，我们将在我们最喜欢的Pong游戏上实现这两种方法，看看这两种方法之间的区别，并看看PyTorch的多处理能力。</p></div></body></html>


<html>
  <head>
    <title>Multiprocessing in Python</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec44" class="calibre1"/>Python中的多重处理</h1></div></div></div><p class="calibre8">Python包含了<code class="literal">multiprocessing</code>(大多数时候简称为<code class="literal">mp</code>)模块来支持进程级并行和<a id="id421" class="calibre1"/>所需的通信原语。在我们的<a id="id422" class="calibre1"/>例子中，我们将使用这个模块中的两个主要类:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">mp.Queue</code>:并发多生产者、多消费者FIFO队列，对队列中的对象进行透明的序列化和反序列化</li><li class="listitem"><code class="literal">mp.Process</code>:在子进程中运行的一段代码和从父进程控制它的方法</li></ul></div><p class="calibre8">PyTorch在<code class="literal">multiprocessing</code>模块周围提供了自己的瘦包装，增加了对CUDA设备和共享内存上的张量和变量的正确处理。它提供了与标准库中的<code class="literal">multiprocessing</code>模块完全相同的功能，所以您需要做的就是使用<code class="literal">import torch.multiprocessing</code>而不是<code class="literal">import multiprocessing</code>。</p></div></body></html>


<html>
  <head>
    <title>A3C – data parallelism</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec45" class="calibre1"/>A3C–数据并行</h1></div></div></div><p class="calibre8">我们将检查的A3C并行化<a id="id423" class="calibre1"/>的第一个版本(图2中有概述)既有一个执行训练的主进程，也有几个与环境通信并收集训练经验的子进程。为了简单和高效，没有实现来自训练器过程的<strong class="calibre2">神经网络</strong> ( <strong class="calibre2"> NN </strong>)权重广播。不是显式地收集权重并将其发送给子进程，而是使用PyTorch内置功能在所有进程之间共享网络，允许我们通过调用NN创建上的<code class="literal">share_memory()</code>方法，在不同进程中使用相同的<code class="literal">nn.Module</code>实例及其所有权重。在幕后，这种方法对CUDA(因为GPU内存在所有主机进程之间共享)或CPU计算情况下的共享内存IPC没有任何开销。在这两种情况下，该方法都提高了性能，但我们的示例仅限于使用一个GPU卡进行训练和数据收集的单台机器。对于我们的Pong示例来说，这并没有太大的限制，但是如果您需要更大的可伸缩性，这个示例应该扩展为显式共享网络权重。</p><p class="calibre8">完整的代码在<code class="literal">Chapter11/01_a3c_data.py</code>文件中，它使用具有以下功能块的<code class="literal">Chapter11/lib/common.py</code>模块:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">class AtariA2C(nn.Module)</code>:这实现了演员-评论家NN模块</li><li class="listitem"><code class="literal">class RewardTracker</code>:处理全集未打折奖励，写入TensorBoard，检查<em class="calibre11">游戏已解决</em>条件</li><li class="listitem"><code class="literal">unpack_batch(batch, net, last_val_gamma)</code>:该功能将<em class="calibre11"> n </em>集步骤的一批转换(状态、奖励、动作、last_state)转换成适合训练的数据</li></ul></div><p class="calibre8">那些类和函数的代码我们在前面几章已经看到了，这里就不赘述了。现在让我们检查主要模块的代码，包括子流程和主训练循环的功能。</p><div><pre class="programlisting">#!/usr/bin/env python3
import gym
import ptan
import numpy as np
import argparse
import collections
from tensorboardX import SummaryWriter

import torch.nn.utils as nn_utils
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp

from lib import common</pre></div><p class="calibre8">开始时，我们导入所需的模块。除了我们正在导入<code class="literal">torch.multiprocessing</code>库之外，这里没有什么新的东西。</p><div><pre class="programlisting">GAMMA = 0.99
LEARNING_RATE = 0.001
ENTROPY_BETA = 0.01
BATCH_SIZE = 128

REWARD_STEPS = 4
CLIP_GRAD = 0.1

PROCESSES_COUNT = 4
NUM_ENVS = 15

ENV_NAME = "PongNoFrameskip-v4"
NAME = 'pong'
REWARD_BOUND = 18</pre></div><p class="calibre8">在超参数中，我们有<a id="id424" class="calibre1"/>两个新值:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">PROCESSES_COUNT</code>指定将为我们收集培训数据的子流程的数量。这个活动大部分是CPU受限的，因为这里最繁重的操作是Atari帧预处理，所以这个值被设置为等于我的机器上的CPU核心的数量。</li><li class="listitem"><code class="literal">NUM_ENVS</code>是每个子进程将用来收集数据的环境数量。这个数字乘以进程的数量就是我们将从中获取训练数据的并行环境的总数。<div> <pre class="programlisting">def make_env():     return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME))</pre> </div></li></ul></div><p class="calibre8">在我们使用子进程函数之前，我们需要环境构造函数和一个微小的包装器，我们将使用它将总剧集奖励发送到主训练进程中。</p><div><pre class="programlisting">
def data_func(net, device, train_queue):
TotalReward = collections.namedtuple('TotalReward', field_names='reward')
    envs = [make_env() for _ in range(NUM_ENVS)]
    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], 
      device=device, apply_softmax=True)
    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, 
      agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    for exp in exp_source:
        new_rewards = exp_source.pop_total_rewards()
        if new_rewards:
          train_queue.put(TotalReward
           (reward=np.mean(new_rewards)))
        train_queue.put(exp)</pre></div><p class="calibre8">前面的函数非常简单，但很特殊，因为它将在子进程中执行(我们将使用<code class="literal">mp.Process</code>类在主代码块中启动这些进程)。我们传递给它三个参数:我们的NN，用于执行计算的设备(<code class="literal">cpu</code>或<code class="literal">cuda</code>字符串)，以及我们将用于从子进程向主进程发送数据的队列，主进程将执行训练。队列用于多生产者和单消费者模式，并且<a id="id425" class="calibre1"/>可以包含两种不同类型的对象:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">TotalReward</code>:这是我们定义的一个前置对象，它只有一个字段奖励，是已完成剧集的总未折扣奖励的浮点值。</li><li class="listitem"><code class="literal">ptan.experience.ExperienceFirstLast</code>:这个对象包装了<code class="literal">REWARD_STEPS</code>子序列中的第一个状态、采取的行动、这个子序列的折扣奖励以及最后一个状态。这是我们的经验，我们将用于培训。</li></ul></div><p class="calibre8">这就是子过程，现在让我们检查主过程和训练循环的开始代码。</p><div><pre class="programlisting">if __name__ == "__main__":
    mp.set_start_method('spawn')
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, 
      action="store_true", help="Enable cuda")
    parser.add_argument("-n", "--name", required=True, 
      help="Name of the run")
    args = parser.parse_args()
    device = "cuda" if args.cuda else "cpu"
    writer = SummaryWriter(comment="-a3c-data_" + NAME + "_" + args.name)</pre></div><p class="calibre8">开始时，我们采取熟悉的步骤，除了一个对<code class="literal">mp.set_start_method</code>的调用，它指示<code class="literal">multiprocessing</code>模块我们想要使用的并行类型。Python中的本机多处理库支持几种启动子进程的方式，但是由于PyTorch多处理的限制，spawn是最好的选择。</p><div><pre class="programlisting">    env = make_env()
    net = common.AtariA2C(env.observation_space.shape, 
      env.action_space.n).to(device)
    net.share_memory()
    optimizer = optim.Adam(net.parameters(), 
      lr=LEARNING_RATE, eps=1e-3)</pre></div><p class="calibre8">之后，我们创建我们的神经网络，将其移动到CUDA设备，并要求它共享其权重。CUDA张量默认共享，但是对于CPU模式，需要调用<code class="literal">share_memory</code>。</p><div><pre class="programlisting">    train_queue = mp.Queue(maxsize=PROCESSES_COUNT)
    data_proc_list = []
    for _ in range(PROCESSES_COUNT):
        data_proc = mp.Process(target=data_func, 
          args=(net, device, train_queue))
        data_proc.start()
        data_proc_list.append(data_proc)</pre></div><p class="calibre8">然后，我们必须启动我们的子进程，但是在此之前，我们创建一个队列，它们将使用这个队列向我们传递数据。队列构造函数的参数指定最大队列容量。将一个新项目推到满队列的所有尝试都将被阻止，这对于我们保持我们的数据样本符合策略非常方便。在队列创建之后，我们使用<code class="literal">mp.Process</code>类启动所需数量的进程，并将它们保存在一个列表中，用于正确的<a id="id426" class="calibre1"/>关闭。在<code class="literal">mp.Process.start()</code>调用之后，我们的<code class="literal">data_func</code>函数将由子进程执行。</p><div><pre class="programlisting">    batch = []
    step_idx = 0

    try:
        with common.RewardTracker(writer, 
stop_reward=REWARD_BOUND) as tracker:
            with ptan.common.utils.TBMeanTracker(writer, 
batch_size=100) as tb_tracker:
                while True:
                    train_entry = train_queue.get()
                    if isinstance(train_entry, TotalReward):
                        if tracker.reward(train_entry.reward, 
step_idx):
                            break
                        continue</pre></div><p class="calibre8">在训练循环的开始，我们从队列中获取下一个条目，并处理可能的<code class="literal">TotalReward</code>对象，我们将这些对象传递给奖励跟踪器。</p><div><pre class="programlisting">                    step_idx += 1
                    batch.append(train_entry)
                    if len(batch) &lt; BATCH_SIZE:
                        continue</pre></div><p class="calibre8">因为我们在队列中只能有两种类型的对象(<code class="literal">TotalReward</code>和经历转换)，所以我们只需要检查一次从队列中获得的条目。在处理完<code class="literal">TotalReward</code>条目后，我们将经验对象放入到批量累积中，直到达到所需的批量大小。</p><div><pre class="programlisting">                    states_v, actions_t, vals_ref_v = \
                        common.unpack_batch(batch, net, last_val_gamma=GAMMA**REWARD_STEPS, device=device)
                    batch.clear()</pre></div><p class="calibre8">当我们获得所需数量的经验样本时，我们使用<code class="literal">unpack_bach</code>函数将它们转换成训练数据，并清除该批次。需要注意的一点是:由于我们的经验样本代表四步<a id="id427" class="calibre1"/>子序列(因为<code class="literal">REWARD_STEPS</code>是4)，我们需要为最后的<em class="calibre11"> V(s) </em>奖励项使用一个合适的折扣因子<em class="calibre11"> γ </em> <sup class="calibre27"> <em class="calibre29"> 4 </em> </sup>。训练循环的其余部分是标准的行动者-批评者损失计算，其执行方式与上一章完全相同:我们使用当前网络计算策略和价值估计的逻辑，并计算策略、价值和熵损失。</p><div><pre class="programlisting">                    optimizer.zero_grad()
                    logits_v, value_v = net(states_v)

                    loss_value_v = F.mse_loss(value_v.squeeze(-1), 
                      vals_ref_v)

                    log_prob_v = F.log_softmax(logits_v, dim=1)
                    adv_v = vals_ref_v - value_v.detach()
                    log_prob_actions_v = adv_v * 
                      log_prob_v[range(BATCH_SIZE), actions_t]
                    loss_policy_v = -log_prob_actions_v.mean()

                    prob_v = F.softmax(logits_v, dim=1)
                    entropy_loss_v = ENTROPY_BETA * (prob_v * 
                      log_prob_v).sum(dim=1).mean()

                    loss_v = entropy_loss_v + loss_value_v + 
                      loss_policy_v
                    loss_v.backward()
                    nn_utils.clip_grad_norm_(net.parameters(), 
                      CLIP_GRAD)
                    optimizer.step()</pre></div><p class="calibre8">最后一步，我们将计算出的张量传递给TensorBoard <code class="literal">tracker</code>类，它将执行我们想要监控的数据的平均和存储。</p><div><pre class="programlisting">                    tb_tracker.track("advantage", adv_v, step_idx)
                    tb_tracker.track("values", value_v, step_idx)
                    tb_tracker.track("batch_rewards", vals_ref_v, 
                      step_idx)
                    tb_tracker.track("loss_entropy", 
                      entropy_loss_v, step_idx)
                    tb_tracker.track("loss_policy", loss_policy_v, 
                      step_idx)
                    tb_tracker.track("loss_value", loss_value_v, 
                      step_idx)
                    tb_tracker.track("loss_total", loss_v, 
                      step_idx)
    finally:
        for p in data_proc_list:
            p.terminate()
            p.join()</pre></div><p class="calibre8">在最后一个<code class="literal">finally</code>块中，由于异常(<em class="calibre11"> Ctrl </em> + <em class="calibre11"> C </em>)或<em class="calibre11">游戏解决</em>条件而可以执行<a id="id428" class="calibre1"/>，我们终止子进程并等待它们。这是确保没有剩余流程所必需的。</p></div></body></html>


<html>
  <head>
    <title>A3C – data parallelism</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec46" class="calibre1"/>结果</h2></div></div></div><p class="calibre8">像往常一样启动示例，经过一段时间的延迟后，它应该开始写入绩效和平均奖励数据。在GTX 1080Ti和4核机器上，它显示的速度约为每秒1800帧，比上一章的600帧/秒有了很大提高。</p><div><pre class="programlisting">
rl_book_samples/Chapter11$ ./01_a3c_data.py --cuda -n final
44830: done 1 games, mean reward -21.000, speed 1618.10 f/s
44856: done 2 games, mean reward -21.000, speed 2053.09 f/s
45037: done 3 games, mean reward -21.000, speed 2036.78 f/s
45351: done 4 games, mean reward -21.000, speed 1894.14 f/s
45562: done 5 games, mean reward -21.000, speed 2204.78 f/s
45573: done 6 games, mean reward -21.000, speed 629.41 f/s
...</pre></div><p class="calibre8">在收敛动力学方面，新版本类似于具有并行环境的A2C，并在来自环境的7M-8M观测中解决Pong。然而，这800万帧的处理只需一个多小时，而不是等待三个小时。</p><div><img src="img/00240.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图Pong上A3C数据并行版本的收敛动态</p></div></div><p class="calibre10"> </p></div></div></body></html>


<html>
  <head>
    <title>A3C – gradients parallelism</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec46" class="calibre1"/>A3C–梯度平行度</h1></div></div></div><p class="calibre8">我们将<a id="id430" class="calibre1"/>考虑的并行A2C实施的下一种方法将有几个子进程，但不是将训练数据馈送到中央训练循环，而是它们将使用本地训练数据计算梯度，并将这些梯度发送到中央主进程。此过程负责将这些梯度组合在一起(基本上就是将它们相加)，并在共享网络上执行SGD更新。</p><p class="calibre8">这种差异可能看起来很小，但这种方法更具可扩展性，尤其是如果您有几个功能强大的节点，并且有多个GPU与网络相连。在这种情况下，数据并行模型中的中央过程很快成为瓶颈，因为损失计算和反向传播需要大量计算。梯度并行化允许将负载分散到几个GPU上，只在中心位置执行相对简单的梯度组合操作。</p><p class="calibre8">完整的示例在<code class="literal">Chapter11/02_a3c_grad.py</code>文件中，它使用了与我们之前的示例相同的<code class="literal">Chapter11/lib/common.py</code>文件。</p><div><pre class="programlisting">GAMMA = 0.99
LEARNING_RATE = 0.001
ENTROPY_BETA = 0.01

REWARD_STEPS = 4
CLIP_GRAD = 0.1

PROCESSES_COUNT = 4
NUM_ENVS = 15

GRAD_BATCH = 64
TRAIN_BATCH = 2
ENV_NAME = "PongNoFrameskip-v4"
NAME = 'pong'
REWARD_BOUND = 18</pre></div><p class="calibre8">像往常一样，我们正在定义超参数，除了用两个参数<code class="literal">GRAD_BATCH</code>和<code class="literal">TRAIN_BATCH</code>代替<code class="literal">BATCH_SIZE</code>之外，这些超参数与前面的例子基本相同。<code class="literal">GRAD_BATCH</code>的值定义了每个子进程用来计算损失和获取梯度值的批处理的大小。第二个参数<code class="literal">TRAIN_BATCH</code>指定在每次SGD迭代中有多少来自子流程的梯度批次将被合并。子进程产生的每个条目都具有与我们的网络参数<a id="id431" class="calibre1"/>相同的形状，我们将它们的<code class="literal">TRAIN_BATCH</code>值加在一起。因此，对于每个优化步骤，我们都使用<code class="literal">TRAIN_BATCH * GRAD_BATCH</code>训练样本。由于损失计算和反向传播是相当繁重的操作，我们使用大的<code class="literal">GRAD_BATCH</code>使它们更有效。由于这个大批量，我们应该保持<code class="literal">TRAIN_BATCH</code>相对较低，以保持我们的网络更新政策。</p><div><pre class="programlisting">def make_env():
    return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME))


def grads_func(proc_name, net, device, train_queue):
    envs = [make_env() for _ in range(NUM_ENVS)]

    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], 
      device=device, apply_softmax=True)
    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, 
      agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    batch = []
    frame_idx = 0
    writer = SummaryWriter(comment=proc_name)</pre></div><p class="calibre8">前面是由子进程执行的函数，这比我们的数据并行示例要复杂得多。作为补偿，主流程中的训练循环变得几乎微不足道。在创建子进程时，我们向函数传递几个参数:</p><div><ul class="itemizedlist"><li class="listitem">用于创建TensorBoard编写器的进程的名称。在这个例子中，每个子进程都写自己的TensorBoard数据集。</li><li class="listitem">共享神经网络。</li><li class="listitem">执行计算的装置(<code class="literal">cpu</code>或<code class="literal">cuda</code>字符串)。</li><li class="listitem">用于将计算出的梯度传送到中央进程的队列。</li></ul></div><p class="calibre8">我们的子流程函数看起来非常类似于数据并行版本中的主训练循环，这并不奇怪，因为我们的子流程的责任增加了。然而，我们没有要求优化器更新网络，而是收集梯度并将它们发送到队列中。剩下的代码几乎是一样的。</p><div><pre class="programlisting">    with common.RewardTracker(writer, stop_reward=REWARD_BOUND) as 
tracker:
        with ptan.common.utils.TBMeanTracker(writer, 
batch_size=100) as tb_tracker:
            for exp in exp_source:
                frame_idx += 1
                new_rewards = exp_source.pop_total_rewards()
                if new_rewards and tracker.reward(new_rewards[0], 
frame_idx):
                    break

                batch.append(exp)
                if len(batch) &lt; GRAD_BATCH:
                    continue</pre></div><p class="calibre8">到目前为止，我们已经<a id="id432" class="calibre1"/>收集了带有过渡的批次，并处理了剧集结尾的奖励。</p><div><pre class="programlisting">                states_v, actions_t, vals_ref_v = \
                    common.unpack_batch(batch, net, 
last_val_gamma=GAMMA**REWARD_STEPS, device=device)
                batch.clear()

                net.zero_grad()
                logits_v, value_v = net(states_v)
                loss_value_v = F.mse_loss(value_v.squeeze(-1), 
vals_ref_v)

                log_prob_v = F.log_softmax(logits_v, dim=1)
                adv_v = vals_ref_v - value_v.detach()
                log_prob_actions_v = adv_v * 
log_prob_v[range(GRAD_BATCH), actions_t]
                loss_policy_v = -log_prob_actions_v.mean()

                prob_v = F.softmax(logits_v, dim=1)
                entropy_loss_v = ENTROPY_BETA * (prob_v * 
log_prob_v).sum(dim=1).mean()

                loss_v = entropy_loss_v + loss_value_v + 
loss_policy_v
                loss_v.backward()</pre></div><p class="calibre8">在前面的部分中，我们根据训练数据计算组合损失，并执行损失的反向传播，这有效地将每个网络参数的梯度存储在<code class="literal">Tensor.grad</code>字段中。这可以在不打扰与其他<a id="id433" class="calibre1"/>工作者同步的情况下完成，因为我们的网络参数是共享的，但是梯度是由每个进程本地分配的。</p><div><pre class="programlisting">                tb_tracker.track("advantage", adv_v, frame_idx)
                tb_tracker.track("values", value_v, frame_idx)
                tb_tracker.track("batch_rewards", vals_ref_v, 
                  frame_idx)
                tb_tracker.track("loss_entropy", entropy_loss_v, 
                  frame_idx)
                tb_tracker.track("loss_policy", loss_policy_v, 
                  frame_idx)
                tb_tracker.track("loss_value", loss_value_v, 
                  frame_idx)
                tb_tracker.track("loss_total", loss_v, frame_idx)</pre></div><p class="calibre8">在前面的代码中，我们将把我们在训练期间要监控的中间值发送到TensorBoard。</p><div><pre class="programlisting">                nn_utils.clip_grad_norm(net.parameters(), 
                  CLIP_GRAD)
                grads = [param.grad.data.cpu().numpy() if 
                  param.grad is not None else None
                         for param in net.parameters()]
                train_queue.put(grads)</pre></div><p class="calibre8">在循环结束时，我们需要裁剪梯度，并将它们从网络参数中提取到一个单独的缓冲区中(以防止它们被循环的下一次迭代破坏)。</p><div><pre class="programlisting">    train_queue.put(None)</pre></div><p class="calibre8"><code class="literal">grads_func</code>的最后一行将<code class="literal">None</code>放入队列，表示该子进程已经到达<em class="calibre11">游戏已解决</em>状态，训练应该停止。</p><div><pre class="programlisting">if __name__ == "__main__":
    mp.set_start_method('spawn')
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, 
      action="store_true", help="Enable cuda")
    parser.add_argument("-n", "--name", required=True, 
      help="Name of the run")
    args = parser.parse_args()
    device = "cuda" if args.cuda else "cpu"

    env = make_env()
    net = common.AtariA2C(env.observation_space.shape, 
      env.action_space.n).to(device)
    net.share_memory()</pre></div><p class="calibre8">主要过程从创建网络和共享其权重开始。</p><div><pre class="programlisting">    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, 
      eps=1e-3)

    train_queue = mp.Queue(maxsize=PROCESSES_COUNT)
    data_proc_list = []
    for proc_idx in range(PROCESSES_COUNT):
        proc_name = "-a3c-grad_" + NAME + "_" + args.name + "#%d" 
          % proc_idx
        data_proc = mp.Process(target=grads_func, args=(proc_name, 
          net, device, train_queue))
        data_proc.start()
        data_proc_list.append(data_proc)</pre></div><p class="calibre8">然后，像以前一样，我们创建<a id="id434" class="calibre1"/>通信队列，并产生所需数量的子进程。</p><div><pre class="programlisting">    batch = []
    step_idx = 0
    grad_buffer = None

    try:
        while True:
            train_entry = train_queue.get()
            if train_entry is None:
                break</pre></div><p class="calibre8">A3C的数据并行版本之间的主要区别在于训练循环，这里要简单得多，因为子进程已经为我们完成了所有繁重的计算。在循环的开始，我们处理当一个过程已经达到要求的平均回报时停止训练的情况。在这种情况下，我们只是退出循环。</p><div><pre class="programlisting">            step_idx += 1

            if grad_buffer is None:
                grad_buffer = train_entry
            else:
                for tgt_grad, grad in zip(grad_buffer,train_entry):
                    tgt_grad += grad</pre></div><p class="calibre8">为了平均来自不同孩子的梯度，我们为获得的每个<code class="literal">TRAIN_BATCH</code>梯度调用优化器的<code class="literal">step()</code>函数。对于中间步骤，我们只是把对应的梯度加在一起。</p><div><pre class="programlisting">            if step_idx % TRAIN_BATCH == 0:
                for param, grad in zip(net.parameters(),grad_buffer):
                    grad_v = torch.FloatTensor(grad).to(device)
                    param.grad = grad_v

                nn_utils.clip_grad_norm_(net.parameters(),CLIP_GRAD)
                optimizer.step()
                grad_buffer = None</pre></div><p class="calibre8">当我们积累了足够多的梯度片段时，我们将梯度的总和转换成PyTorch <code class="literal">FloatTensor</code>并将其分配给网络参数的<code class="literal">grad</code>字段。之后，我们需要做的就是调用优化器的<code class="literal">step()</code>方法，使用累积的梯度来更新网络参数。</p><div><pre class="programlisting">    finally:
        for p in data_proc_list:
            p.terminate()
            p.join()</pre></div><p class="calibre8">在退出<a id="id435" class="calibre1"/>训练循环时，我们停止所有子进程以确保我们终止了它们，即使按下<em class="calibre11"> Ctrl </em> + <em class="calibre11"> C </em>来停止优化。这是为了防止僵尸进程占用GPU资源。</p></div></body></html>


<html>
  <head>
    <title>A3C – gradients parallelism</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec47" class="calibre1"/>结果</h2></div></div></div><p class="calibre8">这个例子可以像以前一样开始，过一会儿应该开始显示速度和意思奖励；但是，您需要注意，显示的信息对于每个子进程都是本地的，这意味着速度、完成的游戏数和帧数需要乘以进程的数量。我的基准测试显示，每个孩子的速度大约是每秒550-600帧，总共2200-2400帧/秒。</p><div><pre class="programlisting">rl_book_samples/Chapter11$ ./02_a3c_grad.py --cuda -n final
11278: done 1 games, mean reward -21.000, speed 520.23 f/s
11640: done 2 games, mean reward -21.000, speed 610.54 f/s
11773: done 3 games, mean reward -21.000, speed 485.09 f/s
11803: done 4 games, mean reward -21.000, speed 359.42 f/s
11765: done 1 games, mean reward -21.000, speed 519.08 f/s
11771: done 2 games, mean reward -21.000, speed 531.22 f/s
...</pre></div><p class="calibre8">收敛动态也和之前的版本很像。观测总次数约为8M-10M，<a id="id436" class="calibre1"/>需要一个半小时才能完成。</p><div><img src="img/00241.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图Pong上基于梯度的A3C并行化的收敛</p></div></div><p class="calibre10"> </p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec47" class="calibre1"/>总结</h1></div></div></div><p class="calibre8">在这一章中，我们讨论了PG方法从多个环境中收集训练数据的重要性，这是由于它们的政策性质。我们还实现了两种不同的A3C方法，以便并行化和稳定训练过程。当我们讨论黑盒方法时(RL 中的<a class="calibre1" title="Chapter 16. Black-Box Optimization in RL" href="part0114_split_000.html#3CN041-ce551566b6304db290b61e4d70de52ee">第16章</a>、<em class="calibre11">黑盒优化)，并行化将在本书中再次出现。在接下来的章节中，我们将会看到使用PG方法可以解决的实际问题，这将会结束本书的PG部分。</em></p></div></body></html>
</body></html>