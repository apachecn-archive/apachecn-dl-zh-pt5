<html><head/><body>
<html>
  <head>
    <title>Preface</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="pref02" class="calibre1"/>前言</h1></div></div></div><p class="calibre8">这本书的主题是强化学习——这是机器学习的一个子领域——专注于在复杂环境中学习最佳行为的一般性和挑战性问题。学习过程仅由奖励值和从环境中获得的观察来驱动。该模型非常通用，可以应用于许多实际情况，从玩游戏到优化复杂的制造过程。</p><p class="calibre8">由于灵活性和通用性，强化学习领域发展非常迅速，吸引了试图改进现有方法或创造新方法的研究人员以及对以最有效的方式解决问题感兴趣的实践者的大量关注。</p><p class="calibre8">这本书试图填补关于强化学习方法和途径的实用和结构化信息的明显缺乏。一方面，世界各地都有大量的研究活动，几乎每天都有新的研究论文发表，NIPS或ICLR等深度学习会议的很大一部分都致力于RL方法。有几个大型研究小组专注于机器人、医学、多智能体系统等领域的RL方法应用。关于最近研究的信息随处可得，但是过于专业和抽象，不认真努力是无法理解的。更糟糕的是RL应用的实际情况，因为如何从研究论文中以数学形式描述的抽象方法到解决实际问题的工作实现并不总是显而易见的。这使得对该领域感兴趣的人很难直观地理解论文和会议讨论背后的方法和思想。有一些关于各种RL方面的非常好的博客帖子用工作示例进行了说明，但是博客帖子的有限格式允许作者仅描述一两种方法，而没有构建完整的结构化图片并显示不同方法之间的相互关系。这本书是我试图解决这个问题。</p><p class="calibre8">这本书的另一个方面是它面向实践。每种方法都适用于各种环境，从非常简单到非常复杂。我试图让例子变得清晰易懂，这是PyTorch的表现力和强大功能所带来的。另一方面，示例的复杂性和需求面向RL爱好者，而无法访问非常大的计算资源，例如GPU集群或非常强大的工作站。我相信，这将使充满乐趣和令人兴奋的RL领域面向更广泛的受众，而不仅仅是研究小组或大型人工智能公司。然而，它仍然是<strong class="calibre2">深度</strong>强化学习，因此，强烈建议使用GPU。书中大约有一半的例子将从在GPU上运行它们中受益。除了RL中使用的传统中型环境示例，如Atari游戏或连续控制问题，本书包含三章(8、12和13)，其中包含更大的项目，说明RL方法如何应用于更复杂的环境和任务。这些例子仍然不是完整的真实项目(否则它们将占据单独的一本书)，而只是更大的问题，说明了RL范例如何应用于成熟基准之外的领域。</p><p class="calibre8">关于本书前三部分中的示例，另一件需要注意的事情是，我已经尝试让示例自包含，并且完整显示了源代码。有时这导致了代码片段的重复(例如，大多数方法中的训练循环都非常相似)，但是我相信给你直接跳到你想要学习的方法的自由比避免少量重复更重要。书中所有例子在Github上都有:<a class="calibre1" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On">https://Github . com/packt publishing/Deep-Reinforcement-Learning-Hands-On</a>，欢迎大家叉出来，实验，投稿。</p><div><div><div><div><div/>
</div></div></div></div></div></body></html>


<html>
  <head>
    <title>Preface</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_2"><a id="ch00lvl1sec06" class="calibre1"/>这本书是给谁的</h1></div></div></div><p class="calibre8">主要目标受众是对机器学习有所了解，但对强化学习领域有兴趣的人。读者应该熟悉Python以及深度学习和机器学习的基础知识。理解统计学和概率将是一个优势，但不是绝对必要的理解书中的大部分材料。</p></div></div></body></html>


<html>
  <head>
    <title>What this book covers</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">这本书涵盖了什么</h1></div></div></div><p class="calibre8"><a class="calibre1" title="Chapter 1. What is Reinforcement Learning?" href="part0012_split_000.html#BE6O2-ce551566b6304db290b61e4d70de52ee">第1章</a>，<em class="calibre11">什么是强化学习？</em>，包含对RL思想和主要形式模型的介绍。</p><p class="calibre8"><a class="calibre1" title="Chapter 2. OpenAI Gym" href="part0016_split_000.html#F8901-ce551566b6304db290b61e4d70de52ee">第2章</a>，<em class="calibre11"> OpenAI Gym </em>，使用开源的library gym向读者介绍了RL的实用方面。</p><p class="calibre8"><a class="calibre1" title="Chapter 3. Deep Learning with PyTorch" href="part0022_split_000.html#KVCC2-ce551566b6304db290b61e4d70de52ee">第3章</a>，<em class="calibre11">用PyTorch进行深度学习</em>，给出了PyTorch库的快速概览。</p><p class="calibre8"><a class="calibre1" title="Chapter 4. The Cross-Entropy Method" href="part0030_split_000.html#SJGS1-ce551566b6304db290b61e4d70de52ee">第四章</a>，<em class="calibre11">交叉熵方法</em>，给你介绍一种RL最简单的方法，给你RL方法和问题的感受。</p><p class="calibre8"><a class="calibre1" title="Chapter 5. Tabular Learning and the Bellman Equation" href="part0036_split_000.html#12AK82-ce551566b6304db290b61e4d70de52ee">第5章</a>、<em class="calibre11">表格学习和贝尔曼方程</em>，介绍了基于值的RL方法系列。</p><p class="calibre8"><a class="calibre1" title="Chapter 6. Deep Q-Networks" href="part0043_split_000.html#190861-ce551566b6304db290b61e4d70de52ee">第六章</a>、<em class="calibre11">深度Q-Networks </em>，描述了DQN，基本的基于值的方法的扩展，允许解决复杂的环境。</p><p class="calibre8"><a class="calibre1" title="Chapter 7. DQN Extensions" href="part0048_split_000.html#1DOR02-ce551566b6304db290b61e4d70de52ee">第七章</a>，<em class="calibre11"> DQN扩展</em>，详细介绍了现代扩展到DQN方法，以提高其在复杂环境中的稳定性和收敛性。</p><p class="calibre8"><a class="calibre1" title="Chapter 8. Stocks Trading Using RL" href="part0059_split_000.html#1O8H61-ce551566b6304db290b61e4d70de52ee">第八章</a>、<em class="calibre11">利用RL进行股票交易</em>，是第一个将DQN方法应用于股票交易的实际项目。</p><p class="calibre8"><a class="calibre1" title="Chapter 9. Policy Gradients – An Alternative" href="part0068_split_000.html#20R682-ce551566b6304db290b61e4d70de52ee">第9章</a>，<em class="calibre11">策略梯度——另一种选择</em>，介绍了基于策略学习的另一系列RL方法。</p><p class="calibre8"><a class="calibre1" title="Chapter 10. The Actor-Critic Method" href="part0074_split_000.html#26I9K1-ce551566b6304db290b61e4d70de52ee">第十章</a>、<em class="calibre11">演员-评论家法</em>，描述了RL中使用最广泛的一种方法。</p><p class="calibre8"><a class="calibre1" title="Chapter 11. Asynchronous Advantage Actor-Critic" href="part0081_split_000.html#2D7TI1-ce551566b6304db290b61e4d70de52ee">第十一章</a>，<em class="calibre11">异步优势Actor-Critic </em>，用并行环境通信扩展Actor-Critic，提高稳定性和收敛性。</p><p class="calibre8"><a class="calibre1" title="Chapter 12. Chatbots Training with RL" href="part0087_split_000.html#2IV0U1-ce551566b6304db290b61e4d70de52ee">第12章</a>，<em class="calibre11">用RL </em>训练聊天机器人，是第二个项目，展示了如何将RL方法应用于NLP问题。</p><p class="calibre8"><a class="calibre1" title="Chapter 13. Web Navigation" href="part0092_split_000.html#2NNJO1-ce551566b6304db290b61e4d70de52ee">第十三章</a>，<em class="calibre11">网页导航</em>，又是一个长项目，将RL应用到网页导航中，使用MiniWoB的任务集。</p><p class="calibre8"><a class="calibre1" title="Chapter 14. Continuous Action Space" href="part0099_split_000.html#2UD7M1-ce551566b6304db290b61e4d70de52ee">第14章</a>，<em class="calibre11">连续动作空间</em>，用连续动作空间和各种方法描述环境的细节。</p><p class="calibre8"><a class="calibre1" title="Chapter 15. Trust Regions – TRPO, PPO, and ACKTR" href="part0107_split_000.html#361C61-ce551566b6304db290b61e4d70de52ee">第15章</a>、<em class="calibre11">信任区域——TRPO、PPO和ACKTR </em>，是另一个关于描述“信任区域”方法集的连续动作空间的章节。</p><p class="calibre8"><a class="calibre1" title="Chapter 16. Black-Box Optimization in RL" href="part0114_split_000.html#3CN041-ce551566b6304db290b61e4d70de52ee">第16章</a>，<em class="calibre11">RL</em>中的黑盒优化，展示了另一组不使用显式渐变的方法。</p><p class="calibre8"><a class="calibre1" title="Chapter 17. Beyond Model-Free – Imagination" href="part0124_split_000.html#3M85O1-ce551566b6304db290b61e4d70de52ee">第17章</a>，<em class="calibre11">超越无模型想象</em>，介绍了基于模型的学习方法，使用了最近关于学习中想象的研究成果。</p><p class="calibre8"><a class="calibre1" title="Chapter 18. AlphaGo Zero" href="part0131_split_000.html#3STPM1-ce551566b6304db290b61e4d70de52ee">第十八章</a>、<em class="calibre11"> AlphaGo Zero </em>，描述了AlphaGo Zero方法应用于游戏Connect Four。</p></div></body></html>


<html>
  <head>
    <title>To get the most out of this book</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">为了充分利用这本书</h1></div></div></div><p class="calibre8">书中描述RL方法的所有章节都有相同的结构:一开始我们讨论了该方法的动机、理论基础及其背后的直觉。然后，我们跟随几个例子的方法适用于不同的环境与完整的源代码。所以，你可以用不同的方式使用这本书:</p><div><ol class="orderedlist"><li class="listitem" value="1">为了快速熟悉一些方法，你可以只阅读相关章节的介绍性部分。</li><li class="listitem" value="2">为了更深入地理解方法的实现方式，你可以阅读代码和周围的注释。</li><li class="listitem" value="3">为了更好地熟悉这个方法(我认为这是最好的学习方法)，你应该使用提供的源代码作为参考点，尝试重新实现这个方法并使它工作。</li></ol><div/></div><p class="calibre8">无论如何，希望这本书对你有用！</p></div></body></html>


<html>
  <head>
    <title>To get the most out of this book</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch00lvl2sec01" class="calibre1"/>下载示例代码文件</h2></div></div></div><p class="calibre8">你可以从你在<a class="calibre1" href="http://www.packtpub.com">http://www.packtpub.com</a>的账户下载本书的示例代码文件。如果你在其他地方购买了这本书，你可以访问http://www.packtpub.com/support<a class="calibre1" href="http://www.packtpub.com/support">网站</a>并注册，让文件直接通过电子邮件发送给你。</p><p class="calibre8">您可以按照以下步骤下载代码文件:</p><div><ol class="orderedlist"><li class="listitem" value="1">登录或在http://www.packtpub.com注册。</li><li class="listitem" value="2">选择<strong class="calibre2">支架</strong>标签。</li><li class="listitem" value="3">点击<strong class="calibre2">代码下载&amp;勘误表</strong>。</li><li class="listitem" value="4">在<strong class="calibre2">搜索</strong>框中输入图书名称，并按照屏幕上的说明进行操作。</li></ol><div/></div><p class="calibre8">下载文件后，请确保使用最新版本的解压缩或解压文件夹:</p><div><ul class="itemizedlist"><li class="listitem">WinRAR / 7-Zip for Windows</li><li class="listitem">适用于Mac的Zipeg / iZip / UnRarX</li><li class="listitem">用于Linux的7-Zip / PeaZip</li></ul></div><p class="calibre8">该书的代码包也托管在GitHub上，网址为<a class="calibre1" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On">https://GitHub . com/packt publishing/Deep-Reinforcement-Learning-Hands-On</a>。我们在<a class="calibre1" href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>也有丰富的书籍和视频目录中的其他代码包。看看他们！</p></div></div></body></html>


<html>
  <head>
    <title>To get the most out of this book</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch00lvl2sec02" class="calibre1"/>下载彩色图像</h2></div></div></div><p class="calibre8">我们还提供了一个PDF文件，其中有本书中使用的截图/图表的彩色图像。可以在这里下载:<a class="calibre1" href="https://www.packtpub.com/sites/default/files/downloads/DeepReinforcementLearningHandsOn_ColorImages.pdf">https://www . packtpub . com/sites/default/files/downloads/DeepReinforcementLearningHandsOn _ color images . pdf</a>。</p></div></div></body></html>


<html>
  <head>
    <title>To get the most out of this book</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch00lvl2sec03" class="calibre1"/>习惯用法</h2></div></div></div><p class="calibre8">本书通篇使用了许多文本约定。</p><p class="calibre8"><code class="literal">CodeInText</code>:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪URL、用户输入和Twitter句柄。比如说；"方法<code class="literal">get_observation()</code>应该将当前环境的观察结果返回给代理."</p><p class="calibre8">代码块设置如下:</p><div><pre class="programlisting">    def get_actions(self):
        return [0, 1]</pre></div><p class="calibre8">当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:</p><div><pre class="programlisting">
<strong class="calibre2">    def get_actions(self):</strong>
        return [0, 1]</pre></div><p class="calibre8">任何命令行输入或输出都按如下方式编写:</p><div><pre class="programlisting">
<strong class="calibre2">$ xvfb-run -s "-screen 0 640x480x24" python 04_cartpole_random_monitor.py</strong>
</pre></div><p class="calibre8"><strong class="calibre2">粗体</strong>:表示一个新的术语，一个重要的单词，或者你在屏幕上看到的单词，例如，在菜单或对话框中，也像这样出现在文本中。例如:“实际上，它是一些代码，实现了一些<strong class="calibre2">策略</strong>。”</p></div></div></body></html>


<html>
  <head>
    <title>Get in touch</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">取得联系</h1></div></div></div><p class="calibre8">我们随时欢迎读者的反馈。</p><p class="calibre8"><strong class="calibre2">总体反馈</strong>:发电子邮件<code class="literal">&lt;<a class="calibre1" href="mailto:feedback@packtpub.com">feedback@packtpub.com</a>&gt;</code>，在邮件主题中提及书名。如果您对本书的任何方面有疑问，请发邮件至<code class="literal">&lt;<a class="calibre1" href="mailto:questions@packtpub.com">questions@packtpub.com</a>&gt;</code>联系我们。</p><p class="calibre8"><strong class="calibre2">勘误表</strong>:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问<a class="calibre1" href="http://www.packtpub.com/submit-errata">http://www.packtpub.com/submit-errata</a>，选择您的图书，点击勘误表提交表格链接，输入详细信息。</p><p class="calibre8">盗版:如果您在互联网上遇到任何形式的我们作品的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过<code class="literal">&lt;<a class="calibre1" href="mailto:copyright@packtpub.com">copyright@packtpub.com</a>&gt;</code>联系我们，并提供材料链接。</p><p class="calibre8"><strong class="calibre2">如果你有兴趣成为一名作家</strong>:如果有一个你擅长的主题，并且你有兴趣写书或投稿，请访问<a class="calibre1" href="http://authors.packtpub.com">http://authors.packtpub.com</a>。</p></div></body></html>


<html>
  <head>
    <title>Get in touch</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch00lvl2sec04" class="calibre1"/>评论</h2></div></div></div><p class="calibre8">请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们Packt可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！</p><p class="calibre8">更多关于Packt的信息，请访问<a class="calibre1" href="http://packtpub.com">packtpub.com</a>。</p></div></div></body></html>
</body></html>