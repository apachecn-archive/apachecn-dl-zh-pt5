<html><head/><body>
<html>
  <head>
    <title>Chapter 2. OpenAI Gym</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>第二章。奥鹏健身馆</h1></div></div></div><p class="calibre8">说了这么多<strong class="calibre2"> RL </strong>的理论概念，我们开始做点实际的吧。在这一章中，我们将学习OpenAI Gym API的基础知识，并编写我们的第一个随机行为代理，以使我们熟悉所有的概念。</p></div></body></html>


<html>
  <head>
    <title>Chapter 2. OpenAI Gym</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec14" class="calibre1"/>代理人的解剖</h1></div></div></div><p class="calibre8">正如我们在前一章看到的，RL的世界观中有几个实体:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">代理人</strong>:起积极作用的<a id="id36" class="calibre1"/>人或事物。实际上，它是一些代码，实现了一些<strong class="calibre2">策略</strong>。基本上，根据我们的观察，这个政策必须决定在每个时间步需要什么行动。</li><li class="listitem"><strong class="calibre2">环境</strong>:世界的某个<a id="id37" class="calibre1"/>模型，它在代理的外部，有责任给我们提供观察，给我们奖励。它会根据我们的动作改变状态。</li></ul></div><p class="calibre8">让我们在一个简单的情况下展示如何用Python实现这两者。我们将定义一个环境，不管代理的行为如何，在有限的步数内给代理随机奖励。这个场景不是很有用，但是可以让我们关注环境和代理类中的特定方法。先说环境:</p><div><pre class="programlisting">class Environment:
    def __init__(self):
        self.steps_left = 10</pre></div><p class="calibre8">在前面的代码中，我们允许环境初始化其内部状态。在我们的例子中，状态只是一个计数器，它限制了允许代理与环境交互的时间步数:</p><div><pre class="programlisting">    def get_observation(self):
        return [0.0, 0.0, 0.0]</pre></div><p class="calibre8"><code class="literal">get_observation()</code>方法应该将当前环境的观察结果返回给代理。它通常被实现为环境内部状态的某个函数。在我们的例子中，观察向量总是零，因为环境基本上没有内部状态:</p><div><pre class="programlisting">    def get_actions(self):
        return [0, 1]</pre></div><p class="calibre8"><code class="literal">get_actions()</code>方法允许<a id="id38" class="calibre1"/>代理查询它可以执行的动作集。通常情况下，代理可以执行的动作集不会随着时间的推移而改变，但有些动作在不同的状态下可能会变得不可能(例如，在TicTacToe游戏的任何位置，不是每个移动都是可能的)。在我们这个简单的例子中，代理只能执行两个动作，用整数0和1编码:</p><div><pre class="programlisting">    def is_done(self):
        return self.steps_left == 0</pre></div><p class="calibre8">前面的方法向代理发出剧集结束的信号。正如我们在<a class="calibre1" title="Chapter 1. What is Reinforcement Learning?" href="part0012_split_000.html#BE6O2-ce551566b6304db290b61e4d70de52ee">第一章</a>、<em class="calibre11">中看到的，什么是强化学习？</em>、一系列环境——代理交互被分成称为情节的一系列步骤。情节可以是有限的，就像在一场国际象棋比赛中一样，也可以是无限的，就像旅行者2号任务(这是一个40多年前发射的著名太空探测器，已经超越了我们的太阳系)。为了涵盖这两种情况，环境为我们提供了一种方法来检测一集何时结束，并且再也没有办法与之交流:</p><div><pre class="programlisting">    def action(self, action):
        if self.is_done():
            raise Exception("Game is over")
        self.steps_left -= 1
        return random.random()</pre></div><p class="calibre8">方法是环境功能的核心部分。它做两件事:处理代理的动作并返回这个动作的报酬。在我们的例子中，奖励是随机的，其行为被丢弃。此外，我们更新步数，并拒绝继续已经结束的剧集。</p><p class="calibre8">现在来看代理部分，它要简单得多，只包含两个方法:构造函数和在环境中执行一个步骤的方法:</p><div><pre class="programlisting">class Agent:
    def __init__(self):
        self.total_reward = 0.0</pre></div><p class="calibre8">在构造函数中，我们初始化了一个计数器，它将保存代理在剧集中累积的总奖励:</p><div><pre class="programlisting">    def step(self, env):
        current_obs = env.get_observation()
        actions = env.get_actions()
        reward = env.action(random.choice(actions))
        self.total_reward += reward</pre></div><p class="calibre8">step函数接受环境实例作为参数，并允许代理执行以下操作:</p><div><ul class="itemizedlist"><li class="listitem">观察环境</li><li class="listitem">根据观察结果决定要采取的行动</li><li class="listitem">将行动提交给环境</li><li class="listitem">获得当前步骤的奖励</li></ul></div><p class="calibre8">对于我们的例子，代理是迟钝的，并且<a id="id39" class="calibre1"/>忽略了在决策过程中获得的关于采取哪种行动的观察结果。相反，每个动作都是随机选择的。最后一部分是粘合代码，它创建两个类并运行一集:</p><div><pre class="programlisting">if __name__ == "__main__":
    env = Environment()
    agent = Agent()

    while not env.is_done():
        agent.step(env)

    print("Total reward got: %.4f" % agent.total_reward)</pre></div><p class="calibre8">您可以在本书的Git资源库中的<code class="literal">Chapter02/01_agent_anatomy.py</code>目录下的<a class="calibre1" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On">https://github . com/packt publishing/Deep-Reinforcement-Learning-Hands-On</a>中找到上述代码。它没有外部依赖性，应该可以与任何或多或少现代的Python版本一起工作。通过运行几次，您将获得代理收集的不同金额的奖励。</p><p class="calibre8">前面代码的简单性使我们能够说明来自RL模型的重要基本概念。环境可以是一个极其复杂的物理模型，代理可以很容易地是一个实现最新RL算法的大型神经网络，但基本模式保持不变:在每一步，代理从环境中获取一些观察结果，进行计算，并选择要发布的动作。这一行动的结果是一种奖励和新的观察。</p><p class="calibre8">你可能会想，如果模式是一样的，为什么我们需要从头开始写呢？也许它已经被某人实现了，并且可以被用作一个库？当然，这样的框架是存在的，但是在我们花一些时间讨论它们之前，让我们准备一下您的开发环境。</p></div></div></body></html>


<html>
  <head>
    <title>Hardware and software requirements</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec15" class="calibre1"/>硬件和软件要求</h1></div></div></div><p class="calibre8">本书中的例子是使用Python版本实现和测试的<a id="id40" class="calibre1"/>。我假设<a id="id41" class="calibre1"/>你已经熟悉这种语言和虚拟环境等常见概念，所以我不会详细介绍如何安装包以及如何以独立的方式完成。我们将在本书中使用的外部库是开源软件，包括以下内容:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2"> NumPy </strong>:这是一个用于科学计算和实现矩阵运算和常用函数的库<a id="id42" class="calibre1"/>。</li><li class="listitem"><strong class="calibre2"> OpenCV Python绑定</strong>:这是一个<a id="id43" class="calibre1"/>计算机视觉库，提供了很多图像处理的函数。</li><li class="listitem"><strong class="calibre2"> Gym </strong>:这是一个由OpenAI开发和维护的RL框架<a id="id44" class="calibre1"/>，具有各种可以通信的环境，以统一的方式。</li><li class="listitem"><strong class="calibre2"> PyTorch </strong>:这是一个灵活且<a id="id45" class="calibre1"/>富有表现力的<strong class="calibre2">深度学习</strong> ( <strong class="calibre2"> DL </strong>)库。下一章将给出一个简短的基本速成课程。</li><li class="listitem">https://github.com/Shmuma/ptan):这是作者创建的一个对Gym的<a id="id47" class="calibre1"/>开源扩展，支持现代深度RL方法和构建块。所有用到的类都会和源代码一起详细描述。</li></ul></div><p class="calibre8">这本书的很大一部分(第二、三和四部分)集中在过去几年发展起来的现代深层RL方法上。这个上下文中的“深度”一词意味着深度学习被大量使用，你可能意识到DL方法是计算密集型的。一个现代的GPU甚至可以比最快的多GPU系统快10到100倍。在实践中，这意味着在使用GPU的系统上花费一个小时训练的相同代码，即使在最快的CPU系统上也可能花费半天到一周的时间。这并不意味着您不能在没有GPU的情况下尝试本书中的示例，但这将需要更长的时间。要自己试验代码(这是学习任何东西最有用的方法)，最好是使用带有GPU的机器。这可以通过多种方式实现:</p><div><ul class="itemizedlist"><li class="listitem">购买适合CUDA的现代GPU</li><li class="listitem">使用云实例:亚马逊AWS和谷歌云都可以为你提供GPU驱动的实例</li></ul></div><p class="calibre8">关于如何设置系统的说明超出了本书的范围，但是网上有大量的手册。在OS方面，你应该使用Linux或macOS，因为PyTorch和Gym的大部分环境都不支持Windows(至少在编写本文时)。</p><p class="calibre8">为了给你我们将在整本书中使用的外部依赖的准确版本，这里是<code class="literal">pip freeze</code>命令的输出(它可能对本书中示例的潜在故障排除有用，因为开源软件和DL工具包发展非常快):</p><div><pre class="programlisting">
<strong class="calibre2">numpy==1.14.2</strong>
<strong class="calibre2">atari-py==0.1.1</strong>
<strong class="calibre2">gym==0.10.4</strong>
<strong class="calibre2">ptan==0.3</strong>
<strong class="calibre2">opencv-python==3.4.0.12</strong>
<strong class="calibre2">scipy==1.0.1</strong>
<strong class="calibre2">torch==0.4.0</strong>
<strong class="calibre2">torchvision==0.2.1</strong>
<strong class="calibre2">tensorboard-pytorch==0.7.1</strong>
<strong class="calibre2">tensorflow==1.7.0</strong>
<strong class="calibre2">tensorboard==1.7.0</strong>
</pre></div><p class="calibre8">书中所有的例子都是用PyTorch 0.4编写和测试的，可以用<code class="literal">pip install pytorch==0.4.0</code>命令安装。</p><p class="calibre8">现在，让我们来看看OpenAI Gym API的细节，它并不复杂，但为我们提供了大量的环境，从琐碎到具有挑战性的环境。</p></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec16" class="calibre1"/> OpenAI健身房API</h1></div></div></div><p class="calibre8">名为<code class="literal">Gym</code>的Python库由<a id="id48" class="calibre1"/>开发，并由open ai<a id="id49" class="calibre1"/>(<a class="calibre1" href="http://www.openai.com">www.openai.com</a>)维护。Gym的主要目标是使用统一的界面为RL实验提供丰富的环境集合。所以，库里面的中心类是一个环境也就不足为奇了，这个环境叫做<code class="literal">Env</code>。它公开了几个方法和字段，这些方法和字段提供了有关环境功能的必需信息。从高层次来看，每个环境都为您提供了这些信息和功能:</p><div><ul class="itemizedlist"><li class="listitem">允许在环境中执行的一组操作。Gym支持离散动作和连续动作，以及它们的组合。</li><li class="listitem">环境为代理提供的观察的形状和边界。</li><li class="listitem">一个叫做<code class="literal">step</code>的方法来执行一个动作，这个动作会返回当前的观察、奖励和剧集结束的指示。</li><li class="listitem">一种叫<code class="literal">reset</code>的方法，把环境恢复到初始状态，获得第一次观测。</li></ul></div><p class="calibre8">让我们详细讨论一下环境的那些组件。</p></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec13" class="calibre1"/>动作空间</h2></div></div></div><p class="calibre8">您可能还记得，代理可以执行的<a id="id50" class="calibre1"/>动作可以是离散的、连续的，或者是两者的组合。离散动作是代理可以做的一组固定的事情，例如，网格中的方向，如左、右、上或下。另一个例子是按钮，它可以被按下或释放。这两种状态是互斥的，因为离散动作空间的主要特征是动作空间中只有一个动作是可能的。</p><p class="calibre8">一个连续的动作有一个附加的值，例如，一个可以转到特定角度的方向盘，或者一个可以用不同力度踩下的油门踏板。对连续动作的描述包括动作可能具有的值的边界。在方向盘的情况下，它可以从720度到720度。对于油门踏板，通常是从0到1。</p><p class="calibre8">当然，我们并不局限于执行单个动作，环境可能有多个动作，例如同时按下多个按钮或方向盘并踩下两个踏板(刹车和油门)。为了支持这种情况，Gym定义了一个特殊的容器类，允许将几个动作空间嵌套到一个统一的动作中。</p></div></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec14" class="calibre1"/>观察空间</h2></div></div></div><p class="calibre8">除了奖励之外，观察是环境在每个时间戳上提供给代理的<a id="id51" class="calibre1"/>信息。观察结果可以简单到一串数字，也可以复杂到包含来自几个相机的彩色图像的几个多维张量。观察甚至可以是离散的，很像动作空间。这种离散观察空间的一个例子可以是灯泡，它可以有两种状态:开或关，作为一个布尔值给我们。</p><p class="calibre8">所以，你可以看到行动和观察之间的相似性，以及它们是如何在体操课上表现出来的。让我们看一个类图:</p><div><img src="img/00016.jpeg" alt="Observation space" class="calibre9"/><div><p class="calibre14">图Gym中<code class="literal">Space</code>类的层次结构</p></div></div><p class="calibre10"> </p><p class="calibre8">基本抽象类<code class="literal">Space</code>包括<a id="id52" class="calibre1"/>两个与我们相关的方法:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">sample()</code>:这将从空间中返回一个随机样本</li><li class="listitem"><code class="literal">contains(x)</code>:检查参数<code class="literal">x</code>是否属于空间的域</li></ul></div><p class="calibre8">这两个方法都是抽象的，并在子类<code class="literal">Space</code>class’中重新实现:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal"> Discrete</code>类表示一组互斥的项目，编号从0到n1。它唯一的字段<em class="calibre11"> n </em>是它描述的项目的计数。例如，<code class="literal">Discrete(n=4)</code>可用于四个方向的动作空间移动【左、右、上、下】。</li><li class="listitem"><code class="literal"> Box</code>类表示区间为[低，高]的有理数的n维张量。例如，在0.0和1.0之间有一个单一值的油门踏板可以用<code class="literal">Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)</code>来编码(<code class="literal">shape</code>参数被分配了一个长度为<code class="literal">1</code>的元组和一个单一值的<code class="literal">1</code>，这给了我们一个单一值的一维张量)。<code class="literal">dtype</code>参数指定了空间的值类型，这里我们将其指定为NumPy <code class="literal">32-bit float</code>。<code class="literal">Box</code>的另一个例子可能是Atari屏幕观察(我们稍后会看到很多Atari环境)，这是一个大小为210 × 160的RGB图像:<code class="literal">Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)</code>。在这种情况下，<code class="literal">shape</code>参数是一个由三个元素组成的元组:第一维是图像的高度，第二维是宽度，第三维等于<code class="literal">3</code>，它们分别对应于红、绿、蓝三个颜色平面。所以，总的来说，每个观测值是一个100，800字节的3D张量。</li><li class="listitem">这里我们要提到的<code class="literal">Space</code>的最后一个孩子是一个<code class="literal">Tuple</code>类，它允许我们将几个<code class="literal">Space</code>类实例组合在一起。这使我们能够创造我们想要的任何复杂的行动和观察空间。例如，假设我们想要为一辆汽车创建一个动作空间规范。这辆车有几个<a id="id53" class="calibre1"/>控制装置，可以在每个时间戳改变，包括方向盘角度、刹车踏板位置和油门踏板位置。这三个控件可以由单个<code class="literal">Box</code>实例中的三个浮点值指定。除了这些基本的控制，汽车还有额外的分立控制，如转向灯(可能是“关”、“右”、“左”)、喇叭(“开”或“关”)等。要将所有这些组合成一个动作空间规范类，我们可以创建<code class="literal">Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(n=3), Discrete(n=2)))</code>。这种灵活性很少使用，例如，在本书中我们将只看到<code class="literal">Box</code>和<code class="literal">Discrete</code>动作和观察空间，但是<code class="literal">Tuple</code>类在某些情况下可能是有用的。</li></ul></div><p class="calibre8">在Gym中还定义了其他的<code class="literal">Space</code>子类，但是前面的三个是我们将要处理的最有用的。所有子类都实现了<code class="literal">sample()</code>和<code class="literal">contains()</code>方法。<code class="literal">sample()</code>功能执行与<code class="literal">Space</code>类别和参数相对应的随机抽样。当我们需要选择随机动作时，这对于动作空间非常有用。<code class="literal">contains()</code>方法验证给定的参数符合<code class="literal">Space</code>参数，它在Gym的内部用于检查代理的行为是否正常。例如，<code class="literal">Discrete.sample()</code>从一个离散范围中返回一个随机元素，而<code class="literal"> Box.sample()</code>将是一个在给定范围内具有适当维数和值的随机张量。</p><p class="calibre8">每个环境都有两个类型为<code class="literal">Space</code>的成员，称为<code class="literal">action_space</code>和<code class="literal">observation_space</code>。这允许您创建通用代码，它可以在任何环境下工作。当然，处理屏幕的像素不同于处理离散观察值(就像前一种情况一样，您可能希望使用卷积层或计算机视觉工具箱中的其他方法来预处理图像)；所以，大多数时候，我们会针对特定的环境或一组环境优化我们的代码，但是Gym并不阻止你编写通用代码。</p></div></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec15" class="calibre1"/>环境</h2></div></div></div><p class="calibre8">环境<a id="id54" class="calibre1"/>在Gym中由<code class="literal">Env</code>类表示，该类有以下成员:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">action_space</code>:这是<code class="literal">Space</code>类的字段，为环境中允许的动作提供规范。</li><li class="listitem"><code class="literal">observation_space</code>:该字段具有相同的<code class="literal">Space</code>类，但指定了环境提供的观测值。</li><li class="listitem"><code class="literal">reset()</code>:将环境重置为初始状态，返回初始观察向量</li><li class="listitem"><code class="literal">step()</code>:该方法允许代理给出动作，并返回关于动作结果的信息:下一次观察、本地奖励和剧集结束标志。这个方法有点复杂，我们将在本节的后面详细讨论。</li></ul></div><p class="calibre8">在<code class="literal">Env</code>类中有额外的实用<a id="id55" class="calibre1"/>方法，比如<code class="literal">render()</code>，它允许你以一种人类友好的形式获得观察结果，但是我们不会使用它们。您可以在Gym的文档中找到完整的列表，但是现在让我们关注核心的<code class="literal">Env</code>方法:<code class="literal">reset()</code>和<code class="literal">step()</code>。</p><p class="calibre8">到目前为止，我们已经看到了我们的代码如何获得关于环境的动作和观察的信息，所以现在是时候熟悉动作本身了。与环境的通信通过<code class="literal">Env</code>类的两种方法来执行:<code class="literal">step</code>和<code class="literal">reset</code>。</p><p class="calibre8">由于重置简单得多，我们将从它开始。<code class="literal">reset()</code>方法没有参数，它指示一个环境重置到它的初始状态并获得初始观察。注意，在创建环境之后，您必须调用<code class="literal">reset()</code>。你可能还记得<a class="calibre1" title="Chapter 1. What is Reinforcement Learning?" href="part0012_split_000.html#BE6O2-ce551566b6304db290b61e4d70de52ee">第一章</a>，<em class="calibre11">什么是强化学习？</em>，代理与环境的通信可能会结束(就像“游戏结束”屏幕)。这样的会话称为<strong class="calibre2">集</strong>，一集结束后，代理需要重新开始。此方法返回的值是对环境的第一次观察。</p><p class="calibre8"><code class="literal">step()</code>方法是环境功能的核心，它在一个调用中完成几件事情，如下所示:</p><div><ol class="orderedlist"><li class="listitem" value="1">告诉环境我们将在下一步执行哪个操作</li><li class="listitem" value="2">在这一行动之后，从环境中获得新的观察</li><li class="listitem" value="3">获得代理通过此步骤获得的奖励</li><li class="listitem" value="4">得到插曲结束的指示</li></ol><div/></div><p class="calibre8">第一项(action)作为唯一的参数传递给这个方法，其余的由function返回。确切地说，它是一个由四个元素(观察、奖励、完成和extra_info)组成的元组(Python tuple，不是我们在上一节讨论的<code class="literal">Tuple</code>类)。它们有以下类型和含义:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">观测</strong>:这是一个有观测数据的NumPy向量或矩阵。</li><li class="listitem"><strong class="calibre2">奖励</strong>:这是奖励的浮点值。</li><li class="listitem"><strong class="calibre2"> done </strong>:这是一个布尔指示器，剧集结束时为<code class="literal">True</code>。</li><li class="listitem">extra_info :这可以是任何特定于环境的内容，带有关于环境的额外信息。通常的做法是在一般RL方法中忽略该值(不考虑特定环境的具体细节)。</li></ul></div><p class="calibre8">因此，您可能已经了解了代理代码中的环境用法:在一个循环中，调用<code class="literal">step()</code>方法并执行一个动作，直到该方法的<code class="literal">done</code>标志变为<code class="literal">True</code>。然后我们可以调用<code class="literal">reset()</code>重新开始。这里只缺少一部分:首先我们如何创建<code class="literal">Env</code>对象。</p></div></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch02lvl2sec16" class="calibre1"/>环境的创造</h2></div></div></div><p class="calibre8">每个环境都有一个唯一的<code class="literal">EnvironmentName-vN</code>形式的<a id="id57" class="calibre1"/>名称，其中<code class="literal">N</code>是用于区分同一环境的不同版本的编号(例如，当某个环境中的一些bug得到修复或者执行了一些其他重大更改时)。为了创建环境，<code class="literal">Gym</code>包为<code class="literal">make(env_name)</code>函数提供了字符串形式的环境名称的唯一参数。</p><p class="calibre8">在撰写本文时，Gym版本0.9.3包含了777个不同名称的环境。当然，所有这些都不是唯一的环境，因为该列表包括环境的所有版本。此外，相同的环境在设置和观察空间中可以有不同的变化。例如，Atari游戏Breakout有以下环境名称:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">突破-v0 </strong>、<strong class="calibre2">突破-v4 </strong>:球的初始位置和方向随机的原始突破</li><li class="listitem"><strong class="calibre2">突破极限-v0 </strong>，<strong class="calibre2">突破极限-v4 </strong>:以相同的球的初始位置和速度矢量突破</li><li class="listitem"><strong class="calibre2"> BreakoutNoFrameskip-v0 </strong>，<strong class="calibre2"> BreakoutNoFrameskip-v4 </strong>:向代理显示每一帧的Breakout</li><li class="listitem"><strong class="calibre2"> Breakout-ram-v0 </strong>，<strong class="calibre2"> Breakout-ram-v4 </strong>:观察完整Atari仿真内存(128字节)而非屏幕像素的Breakout。</li><li class="listitem">突破-斜坡确定性-v0，突破-斜坡确定性-v4</li><li class="listitem">突破-ramNoFrameskip-v0，突破-ramNoFrameskip-v4</li></ul></div><p class="calibre8">总的来说，有12个良好的旧突破的环境。如果你以前从未见过它，这里是它的游戏截图:</p><div><img src="img/00017.jpeg" alt="Creation of the environment" class="calibre9"/><div><p class="calibre14">图2:突围的玩法</p></div></div><p class="calibre10"> </p><p class="calibre8">即使在删除了这样的重复之后，Gym 0.9.3还是提供了一个令人印象深刻的116个独特环境的列表，这些环境可以分为几个组:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">经典控制问题</strong>:这些是玩具任务，在最优控制理论和RL论文中用作基准或演示。它们通常很简单，具有低维的观察和操作空间，但在实现算法时，它们作为快速检查是有用的。把它们想象成“RL的MNIST”(如果你没听说过MNIST，这是来自<em class="calibre11"> Yann LeCun </em>的手写数字识别数据集)。</li><li class="listitem">雅达利2600 :这些游戏来自20世纪70年代的经典游戏平台。有63个独特的游戏。</li><li class="listitem"><strong class="calibre2">算法</strong>:这些是旨在执行小计算任务的问题，比如复制观察到的序列或者数字相加。</li><li class="listitem"><strong class="calibre2">桌游</strong>:这些是围棋和Hex的游戏。</li><li class="listitem"><strong class="calibre2"> Box2D </strong>:这些是使用Box2D物理模拟器学习行走或汽车控制的环境。</li><li class="listitem">这是另一个物理模拟器，用于几个连续控制问题。</li><li class="listitem"><strong class="calibre2">参数调整</strong>:这是用于优化神经网络参数的RL。</li><li class="listitem">这些是简单的网格世界文本环境。</li><li class="listitem">PyGame :这是几个使用PyGame引擎实现的环境。</li><li class="listitem">这是在ViZdoom上实现的九个小游戏。</li></ul></div><p class="calibre8">完整的环境列表<a id="id59" class="calibre1"/>可以在<a class="calibre1" href="https://gym.openai.com/envs">https://gym.openai.com/envs</a>或者该项目的GitHub仓库的wiki页面上找到。OpenAI Universe中有一个更大的环境集，它提供了虚拟机的通用连接器，同时运行Flash和原生游戏、web浏览器和其他现实世界的应用程序。OpenAI Universe <a id="id60" class="calibre1"/>扩展了Gym API，但是遵循相同的设计原则和范例。你可以到https://github.com/openai/universe去看看。</p><p class="calibre8">理论化已经足够了，现在让我们看一个使用Gym环境的Python会话。</p></div></div></body></html>


<html>
  <head>
    <title>OpenAI Gym API</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_5">钢管舞训练</h2></div></div></div><div><pre class="programlisting">$ python
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58)
[GCC 7.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import gym
&gt;&gt;&gt; e = gym.make('CartPole-v0')
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.</pre></div><p class="calibre8">这里我们将导入<a id="id61" class="calibre1"/>包<code class="literal">Gym</code>并创建一个名为<code class="literal">CartPole</code>的环境。该环境来自“经典控制”组，其要点是用底部连接的操纵杆控制平台(见下图)。棘手的是，这根棍子往往会向右或向左落下，你需要通过每一步向右或向左移动平台来平衡它。我们看到的警告信息不是我们的错，而是Gym内部的一个小的不一致，不影响结果。</p><div><img src="img/00018.jpeg" alt="The CartPole session" class="calibre9"/><div><p class="calibre14">图3:横竿环境</p></div></div><p class="calibre10">对这个<a id="id62" class="calibre1"/>环境的观察是四个浮点数，包含关于操纵杆质心的<em class="calibre11"> x </em>坐标、其速度、其与平台的角度以及其角速度的信息。当然，通过应用一些数学和物理知识，当我们需要平衡操纵杆时，将这些数字转换成动作并不复杂，但我们的问题要棘手得多:在不知道观察到的数字的确切含义，只通过获得奖励的情况下，我们如何学习平衡这个系统<strong class="calibre2">？在这种环境下，每个时间步长的回报是1。插曲继续，直到棍子落下；所以为了获得更多的累积奖励，我们需要以避免棍子掉落的方式来平衡平台。</strong></p><p class="calibre8">这个问题看起来很难，但在短短的两章中，我们将编写算法，在几分钟内轻松解决这个问题，而不需要知道观察到的数字意味着什么。我们将只通过试错法和一点RL魔法来完成它。</p><p class="calibre8">不过，让我们继续玩我们的游戏:</p><p class="calibre8">这里我们重置环境并获得第一个观察结果(我们总是需要重置新创建的环境)。正如我刚才所说的，观察值是四个数字，所以让我们检查一下如何提前知道这一点:</p><div><pre class="programlisting">&gt;&gt;&gt; obs = e.reset()
&gt;&gt;&gt; obs
array([-0.04937814, -0.0266909 , -0.03681807, -0.00468688])</pre></div><p class="calibre8"><code class="literal">action_space</code>字段是<code class="literal">Discrete</code>类型的，所以我们的动作只是0或1，其中0表示将平台推到左边，1表示推到右边。观察空间是<code class="literal">Box(4,)</code>的，这意味着大小为4的向量<a id="id63" class="calibre1"/>，其值在<code class="literal">[−inf, inf]</code>区间内:</p><div><pre class="programlisting">&gt;&gt;&gt; e.action_space
Discrete(2)
&gt;&gt;&gt; e.observation_space
Box(4,)</pre></div><p class="calibre8">在这里，我们通过执行动作<code class="literal">0</code>将我们的平台推到左边，并得到四个元素的元组:</p><div><pre class="programlisting">&gt;&gt;&gt; e.step(0)
(array([-0.04991196, -0.22126602, -0.03691181,  0.27615592]), 1.0, False, {})</pre></div><p class="calibre8">一个新的观察是一个新的四个数字的向量</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">1.0</code>的奖励</li><li class="listitem"><code class="literal">done flag = False</code>，这意味着这一集还没有结束，我们或多或少还好</li><li class="listitem">关于环境的额外信息是一个空字典<div> <pre class="programlisting">&gt;&gt;&gt; e.action_space.sample() 0 &gt;&gt;&gt; e.action_space.sample() 1 &gt;&gt;&gt; e.observation_space.sample() array([  2.06581792e+00,   6.99371255e+37,   3.76012475e-02,         -5.19578481e+37]) &gt;&gt;&gt; e.observation_space.sample() array([4.6860966e-01, 1.4645028e+38, 8.6090848e-02, 3.0545910e+37],       dtype=float32)</pre> </div></li><li class="listitem">这里我们在<code class="literal">action_space</code>和<code class="literal">observation_space</code>上使用了<code class="literal">Space</code>类的<code class="literal">sample()</code>方法。这个方法从底层空间返回一个随机样本，在我们的<code class="literal">Discrete</code>动作空间中，这意味着一个0或1的随机数，对于观察空间，这是一个4个数字的随机向量。观察空间的随机样本可能看起来没有用，这是真的，但是当我们不确定如何执行一个动作时，可以使用来自动作空间的样本。这个特性对我们来说特别方便，因为我们还不知道任何RL方法，但仍然想在健身房环境中玩玩。现在我们已经知道了足够的知识来实现我们的第一个CartPole随机行为代理，所以让我们开始吧。</li></ul></div><p class="calibre8"><a id="ch02lvl1sec17" class="calibre1"/>随机挑杆剂</p></div></div></body></html>


<html>
  <head>
    <title>The random CartPole agent</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">尽管这个环境比我们在<em class="calibre11">代理的剖析</em>一节中的第一个例子要复杂得多，但是代理的代码要短得多。这就是可重用性、抽象和第三方库的力量！</h1></div></div></div><p class="calibre8">所以，下面是代码(你可以在<code class="literal">Chapter02/02_cartpole_random.py</code>里找到):</p><p class="calibre8">在这里，我们创建环境并初始化步骤计数器和奖励累加器。在最后一行，我们重置环境以获得第一个观察结果(我们不会使用它，因为我们的代理是随机的):</p><div><pre class="programlisting">import gym

if __name__ == "__main__":
    env = gym.make("CartPole-v0")
    total_reward = 0.0
    total_steps = 0
    obs = env.reset()</pre></div><p class="calibre8">在这个循环中，我们对一个随机动作进行采样，然后要求环境执行它，并向我们返回下一个观察结果(<code class="literal">obs</code>)、<code class="literal">reward</code>和<code class="literal">done</code>标志。如果这一集结束了，我们就停止循环，显示我们完成了多少步，累积了多少奖励。如果您开始这个示例，您将看到类似这样的内容(由于代理的随机性，不完全是这样):</p><div><pre class="programlisting">   while True:
        action = env.action_space.sample()
        obs, reward, done, _ = env.step(action)
        total_reward += reward
        total_steps += 1
        if done:
            break

   print("Episode done in %d steps, total reward %.2f" % (total_steps, total_reward))</pre></div><p class="calibre8">与交互式会话一样，警告与我们的代码无关，而是与Gym的内部有关。平均来说，我们的随机代理人在杆子倒下之前走了12-15步，这一集就结束了。Gym中的大多数环境都有一个“奖励边界”，即代理在连续100集“解决”环境期间应该获得的平均奖励。对于拉杆来说，这个界限是195，这意味着平均来说，代理人必须在195步或更长的时间内握杆。从这个角度来看，我们的随机代理的性能看起来很差。然而，不要太早失望，因为我们才刚刚开始，很快我们就会解决CartPole和其他许多有趣和具有挑战性的环境。</p><div><pre class="programlisting">rl_book_samples/Chapter02$ python 02_cartpole_random.py
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
Episode done in 12 steps, total reward 12.00</pre></div><p class="calibre8"><a id="ch02lvl1sec18" class="calibre1"/>健身房的额外功能——包装器和监控器</p></div></body></html>


<html>
  <head>
    <title>The extra Gym functionality – wrappers and monitors</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">到目前为止，我们讨论的内容涵盖了健身房核心API的三分之二，以及开始编写代理所需的基本功能。剩下的API你可以不用，但它会让你的生活更轻松，代码更干净。那么，让我们来快速浏览一下API的其余部分。</h1></div></div></div><p class="calibre8"><a id="ch02lvl2sec18" class="calibre1"/>糖纸</p></div></body></html>


<html>
  <head>
    <title>The extra Gym functionality – wrappers and monitors</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">通常，你会希望以某种通用的方式扩展环境的功能。例如，一个环境给了你一些观察结果，但是你想把它们累积在某个缓冲区中，并把最后的<em class="calibre11"> N </em>个观察结果提供给代理，这是动态计算机游戏的一个常见场景，当一个单独的帧不足以获得关于游戏状态的完整信息时。另一个例子是，当你希望能够裁剪或预处理图像的像素，使其更便于代理消化，或者如果你想以某种方式正常化奖励分数。有许多这样的情况具有相同的结构:您想要“包装”现有的环境，并添加一些额外的逻辑来做一些事情。Gym为这些情况提供了一个方便的框架，叫做<code class="literal">Wrapper</code>类。类结构如下图所示。</h2></div></div></div><p class="calibre8"><code class="literal">Wrapper</code>类继承了<code class="literal">Env</code>类。它的构造函数接受唯一的参数:要“包装”的<code class="literal">Env</code>类的实例为了增加额外的功能，你需要重新定义你想要扩展的方法，比如<code class="literal">step()</code>或者<code class="literal">reset()</code>。唯一的要求是调用超类的原始方法。</p><p class="calibre8">图Gym中<code class="literal">Wrapper</code>类的层次结构</p><div><img src="img/00019.jpeg" alt="Wrappers" class="calibre9"/><div><p class="calibre14">Figure 4: The hierarchy of <code class="literal">Wrapper</code> classes in Gym</p></div></div><p class="calibre10">为了处理更具体的<a id="id66" class="calibre1"/>需求，比如一个<code class="literal">Wrapper</code>类想要只处理来自环境的观察或者只处理动作，有一些<code class="literal">Wrapper</code>的子类允许只过滤特定部分的信息。</p><p class="calibre8">它们如下:</p><p class="calibre8"><code class="literal">ObservationWrapper</code>:您需要重新定义父的<code class="literal">observation(obs)</code>方法。<code class="literal">obs</code>参数是来自包装环境的观察，这个方法应该返回将提供给代理的观察。</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">RewardWrapper</code>:这公开了<code class="literal">reward(rew)</code>方法，该方法可以修改给予代理的奖励值。</li><li class="listitem"><code class="literal">ActionWrapper</code>:您需要覆盖<code class="literal">action(act)</code>方法，该方法可以将传递给包装环境的动作调整到代理。</li><li class="listitem">为了更实际一点，让我们想象这样一种情况，我们想要干预代理发送的动作流，并以10%的概率用一个随机动作替换当前动作。这看起来似乎是一件不明智的事情，但是这个简单的技巧是解决我在第1章、<em class="calibre11">什么是强化学习中简单提到的“探索/利用问题”的最实用和最有效的方法之一。</em>。通过发布随机动作，我们让我们的代理探索环境，并不时地偏离其策略的既定路线。这很容易做到，使用<code class="literal">ActionWrapper</code>类(完整示例，<code class="literal">Chapter02/03_random_action_wrapper.py</code>)。</li></ul></div><p class="calibre8">在这里，我们通过调用父类的<code class="literal">__init__</code>方法并保存epsilon(随机动作的概率)来初始化我们的包装器:</p><div><pre class="programlisting">import gym
import random
class RandomActionWrapper(gym.ActionWrapper):
    def __init__(self, env, epsilon=0.1):
        super(RandomActionWrapper, self).__init__(env)
        self.epsilon = epsilon</pre></div><p class="calibre8">这是一个我们需要从父类中覆盖的方法，以调整代理的动作。每次我们掷骰子，以ε的概率，我们从动作空间中随机抽取一个动作，并返回它，而不是代理发送给我们的动作。请注意，使用<code class="literal">action_space</code>和包装抽象，我们能够编写抽象代码，它将与健身房的任何环境一起工作。此外，我们在每次替换动作时都打印消息，只是为了验证我们的包装器在工作。当然，在生产代码中，这是不必要的:</p><div><pre class="programlisting">    def action(self, action):
        if random.random() &lt; self.epsilon:
            print("Random!")
            return self.env.action_space.sample()
        return action</pre></div><p class="calibre8">现在是时候应用我们的包装器了。我们将创建一个普通的CartPole环境，并将其传递给我们的包装构造函数。从现在开始，我们使用我们的包装器作为一个普通的<code class="literal">Env</code>实例，而不是原来的CartPole。由于<code class="literal">Wrapper</code>类继承了<code class="literal">Env</code>类并公开了相同的接口，所以我们可以按照我们想要的任何组合来嵌套包装器。这是一个强大、优雅、通用的解决方案:</p><div><pre class="programlisting">if __name__ == "__main__":
    env = RandomActionWrapper(gym.make("CartPole-v0"))</pre></div><p class="calibre8">下面是几乎相同的代码，除了每次我们发出相同的动作:0。我们的代理人很迟钝，总是做同样的事情。通过运行代码，您应该看到包装器确实在工作:</p><div><pre class="programlisting">    obs = env.reset()
    total_reward = 0.0

    while True:
        obs, reward, done, _ = env.step(0)
        total_reward += reward
        if done:
            break

    print("Reward got: %.2f" % total_reward)</pre></div><p class="calibre8">如果你愿意，你可以在包装器的创建上玩epsilon参数，并验证随机性提高了代理的平均得分。我们应该继续前进，看看藏在体育馆里的另一个有趣的宝石:<code class="literal">Monitor</code>。</p><div><pre class="programlisting">rl_book_samples/Chapter02$ python 03_random_actionwrapper.py
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
Random!
Random!
Random!
Random!
Reward got: 12.00</pre></div><p class="calibre8"><a id="ch02lvl2sec19" class="calibre1"/>监视器</p></div></div></body></html>


<html>
  <head>
    <title>The extra Gym functionality – wrappers and monitors</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">你应该知道的另一个类是<code class="literal">Monitor</code>。它的实现方式类似于<code class="literal">Wrapper</code>，可以将有关您的代理的表现的信息写入一个文件中，并可以选择录制您的代理在<a id="id68" class="calibre1"/>动作中的视频。前一段时间，可以将<code class="literal">Monitor</code>班的录制结果上传到<a class="calibre1" href="https://gym.openai.com">https://gym.openai.com</a>网站，并在与其他人的结果对比中看到你的代理人的位置(见以下截图)，但不幸的是，在2017年8月底，OpenAI决定关闭这一上传功能，并冻结所有结果。有几个活动来实现一个替代原始网站，但他们还没有准备好。我希望这种情况会很快得到解决，但在撰写本文时，不可能将您的结果与其他人的结果进行对比。</h2></div></div></div><p class="calibre8">为了让你对健身房的网页界面有个概念，这里是CartPole环境排行榜:</p><p class="calibre8">图5: OpenAI Gym web界面与CartPole提交</p><div><img src="img/00020.jpeg" alt="Monitor" class="calibre9"/><div><p class="calibre14">Figure 5: OpenAI Gym web interface with CartPole submissions</p></div></div><p class="calibre10">网络界面上的每个提交都有关于训练动态的细节。例如，以下是作者对Doom的一款迷你游戏的解决方案:</p><p class="calibre8">图DoomDefendLine环境中的提交动态</p><div><img src="img/00021.jpeg" alt="Monitor" class="calibre9"/><div><p class="calibre14">Figure 6: Submission dynamics on the DoomDefendLine environment</p></div></div><p class="calibre10">尽管如此，<code class="literal">Monitor</code>仍然是有用的，因为你可以看看你的代理在环境中的生活。所以，下面是我们如何将<code class="literal">Monitor</code>添加到我们的随机CartPole代理，这是唯一的区别(整个代码在<code class="literal">Chapter02/04_cartpole_random_monitor.py</code>):</p><p class="calibre8">我们<a id="id69" class="calibre1"/>传递给<code class="literal">Monitor</code>的第二个参数是它将结果写入的目录的名称。这个目录不应该存在，否则你的程序会因异常而失败(为了克服这个问题，你可以删除现有的目录或者将<code class="literal">force=True</code>参数传递给<code class="literal">Monitor</code>类的构造函数)。</p><div><pre class="programlisting">if __name__ == "__main__":
    env = gym.make("CartPole-v0")
    env = gym.wrappers.Monitor(env, "recording")</pre></div><p class="calibre8"><code class="literal">Monitor</code>类要求系统上有<code class="literal">FFmpeg</code>实用程序，用于将捕获的观察结果转换成输出视频文件。该实用程序必须可用，否则<code class="literal">Monitor</code>将引发异常。安装<code class="literal">FFmpeg</code>最简单的方法是使用系统的软件包管理器，它是特定于操作系统发行版的。</p><p class="calibre8">要开始这个例子，应该满足这三个额外的先决条件之一:</p><p class="calibre8">该代码应该在带有OpenGL扩展(GLX)的X11会话中运行</p><div><ul class="itemizedlist"><li class="listitem">代码应在<code class="literal">Xvfb</code>虚拟显示中启动</li><li class="listitem">您可以在ssh连接中使用X11转发</li><li class="listitem">造成这种情况的原因是视频录制，通过对环境绘制的窗口进行截图来完成。有些环境使用OpenGL来绘制其图片，因此需要使用OpenGL的图形模式。对于云中的虚拟机来说，这可能是一个问题，因为它实际上没有运行监视器和图形界面。为了克服这一点，有一个特殊的“虚拟”图形显示，称为<strong class="calibre2"> Xvfb </strong> ( <strong class="calibre2"> X11虚拟帧缓冲区</strong>)，它<a id="id70" class="calibre1"/>基本上在服务器上启动一个虚拟图形显示，并强制程序在其中进行绘制。这足以让<code class="literal">Monitor</code>愉快地创作出想要的视频。</li></ul></div><p class="calibre8">要在<code class="literal">Xvbf</code>环境中启动你的程序，你需要把它安装在你的机器上(通常需要安装<code class="literal">xvfb</code>包)并运行特殊的脚本<code class="literal">xvfb-run</code>:</p><p class="calibre8">正如您从前面的日志中看到的，视频已经成功编写，因此您可以通过播放它来查看代理的一个部分。</p><div><pre class="programlisting">$ xvfb-run -s "-screen 0 640x480x24" python 04_cartpole_random_monitor.py
[2017-09-22 12:22:23,446] Making new env: CartPole-v0
[2017-09-22 12:22:23,451] Creating monitor directory recording
[2017-09-22 12:22:23,570] Starting new video recorder writing to recording/openaigym.video.0.31179.video000000.mp4
Episode done in 14 steps, total reward 14.00
[2017-09-22 12:22:26,290] Finished writing results. You can upload them to the scoreboard via gym.upload('recording')</pre></div><p class="calibre8">记录代理动作的另一种方法是使用ssh X11转发，它使用ssh功能在X11客户机(Python代码，它想要显示一些图形信息)和X11服务器(软件，它知道如何显示这些信息，并且可以访问您的物理显示器)之间建立X11通信。在X11架构中，客户端和服务器是分离的，可以在不同的机器上工作。要使用这种方法，您需要以下内容:</p><p class="calibre8">运行在本地机器上的X11服务器。Linux自带X11服务器作为标准组件(所有桌面环境都使用X11)。在Windows机器上，你可以设置第三方X11实现，比如开源的<a id="id72" class="calibre1"/> VcXsrv(在<a class="calibre1" href="https://sourceforge.net/projects/vcxsrv/">https://sourceforge.net/projects/vcxsrv/</a>有售)。</p><div><ol class="orderedlist"><li class="listitem" value="1">能够通过ssh登录到您的远程机器，传递<code class="literal">–X</code>命令行选项:<code class="literal">ssh –X servername</code>。这将启用X11隧道，并允许在此会话中启动的所有进程使用本地显示器进行图形输出。</li><li class="listitem" value="2">然后，您可以启动一个使用<code class="literal">Monitor</code>类的程序，它将显示代理的动作，将图像捕获到一个视频文件中。</li></ol><div/></div><p class="calibre8"><a id="ch02lvl1sec19" class="calibre1"/>摘要</p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">恭喜你！你已经开始学习RL实用的一面了！在这一章中，我们安装了OpenAI Gym和大量的游戏环境，研究了它的基本API并创建了一个随机行为代理。您还了解了如何以模块化的方式扩展现有环境的功能，并熟悉了使用<code class="literal">Monitor</code>包装器记录代理活动的方法。</h1></div></div></div><p class="calibre8">在下一章，我们将使用PyTorch做一个快速的DL回顾，py torch是DL研究者最喜欢的库。敬请关注。</p><p class="calibre8">In the next chapter, we will do a quick DL recap using PyTorch, which is a favorite library among DL researchers. Stay tuned.</p></div></body></html>
</body></html>