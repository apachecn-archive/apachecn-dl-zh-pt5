<html><head/><body>
<html>
  <head>
    <title>Chapter 10. The Actor-Critic Method</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>第十章。演员-评论家方法</h1></div></div></div><p class="calibre8">在<a class="calibre1" title="Chapter 9. Policy Gradients – An Alternative" href="part0068_split_000.html#20R682-ce551566b6304db290b61e4d70de52ee">第9章</a>、<em class="calibre11">策略梯度——一种替代方案</em>中，我们开始研究一种常见的基于价值的方法家族的替代方案，称为基于策略的。特别是，我们重点研究了称为加强的方法及其修改，该方法使用折扣奖励来获得策略的梯度(这为我们提供了改进策略的方向)。这两种方法对于小CartPole问题都很有效，但是对于更复杂的Pong环境，收敛速度非常慢。</p><p class="calibre8">在这一章中，我们将讨论对传统<strong class="calibre2">策略梯度</strong> ( <strong class="calibre2"> PG </strong>)方法的另一个扩展，它神奇地提高了新方法的稳定性和收敛速度。尽管修改很小，但新方法有自己的名字，<strong class="calibre2">演员兼评论家</strong>，它是深度<strong class="calibre2">强化学习</strong> ( <strong class="calibre2"> RL </strong>)中最强大的方法之一。</p></div></body></html>


<html>
  <head>
    <title>Chapter 10. The Actor-Critic Method</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec35" class="calibre1"/>方差减少</h1></div></div></div><p class="calibre8">在<a id="id390" class="calibre1"/>上一章中，我们简要提到了提高PG方法稳定性的方法之一是减少梯度的方差。现在让我们试着理解为什么这很重要，以及减少方差意味着什么。在统计学中，方差是随机变量与该变量期望值的预期方差。</p><div><img src="img/00213.jpeg" alt="Variance reduction" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">方差告诉我们数值离平均值有多远。当方差很大时，随机变量的取值会大大偏离均值。在下图中，正态(高斯)分布具有相同的平均值<img src="img/00214.jpeg" alt="Variance reduction" class="calibre24"/>，但是具有不同的方差值。</p><div><img src="img/00215.jpeg" alt="Variance reduction" class="calibre9"/><div><p class="calibre14">图1:方差对高斯分布的影响</p></div></div><p class="calibre10"> </p><p class="calibre8">现在让我们回到PG。在前一章已经说过，这种方法的思想是增加好行为的概率，减少坏行为的机会。在数学符号中，我们的PG被写成<img src="img/00216.jpeg" alt="Variance reduction" class="calibre24"/>。比例因子<em class="calibre11"> Q(s，a) </em>指定了我们希望在特定状态下增加或减少多少采取行动的概率。在强化方法中，我们使用总报酬的贴现值作为梯度的标度。为了增加强化稳定性，我们从梯度量表中减去了平均奖励。</p><p class="calibre8">为了理解<a id="id391" class="calibre1"/>为什么会有帮助，让我们考虑一个优化步骤的非常简单的场景，在这个场景中，我们有三个具有不同总折扣回报的动作:Q <sub class="calibre25"> 1 </sub>、Q <sub class="calibre25"> 2 </sub>和Q <sub class="calibre25"> 3 </sub>。现在让我们考虑一下这些Qs相对价值的政策梯度。</p><p class="calibre8">作为第一个例子，让Q <sub class="calibre25"> 1 </sub>和Q <sub class="calibre25"> 2 </sub>都等于某个较小的正数，而Q <sub class="calibre25"> 3 </sub>是较大的负数。所以，第一步和第二步的行动会带来一些小回报，但第三步并不是很成功。由此产生的所有三个步骤的<strong class="calibre2">合并</strong>梯度将试图推动我们的政策远离第三步的行动，而略微倾向于第一和第二步采取的行动，这是完全合理的做法。</p><p class="calibre8">现在让我们想象<a id="id392" class="calibre1"/>我们的报酬总是正的，只是价值不同。这对应于对所有Q <sub class="calibre25"> 1 </sub>、Q <sub class="calibre25"> 2 </sub>和Q <sub class="calibre25"> 3 </sub>增加一些常数。在这种情况下，Q1和Q2成为大的正数，而Q <sub class="calibre25"> 3 </sub>将具有小的正值。不过，我们的政策更新会变得不同！接下来，我们将努力推动我们的政策朝着第一步和第二步的行动，并略微推动它朝着第三步的行动。因此，严格地说，我们不再试图避免第三步所采取的行动，尽管事实上相对回报是相同的。</p><p class="calibre8">我们的政策更新依赖于报酬中添加的常量，这可能会显著减慢我们的培训，因为我们可能需要更多的样本<em class="calibre11">来平均</em>PG的此类变化的影响。更糟糕的是，随着我们的总折现回报随着时间的推移而变化，随着代理学习如何表现得越来越好，我们的PG方差也可能发生变化。例如，在Atari Pong环境中，开始时的平均报酬为-21...20，所以所有的行动看起来几乎同样糟糕。</p><p class="calibre8">为了克服这个问题，在上一章中，我们从Q值中减去了平均总回报，并将其称为平均值<strong class="calibre2">基线</strong>。这个技巧让我们的PGs正常化，因为在平均奖励为-21的情况下，获得-20的奖励对代理来说看起来像是赢了，并且它将策略推向所采取的动作。</p></div></div></body></html>


<html>
  <head>
    <title>CartPole variance</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec36" class="calibre1"/> CartPole方差</h1></div></div></div><p class="calibre8">为了在实践中检验这个理论上的<a id="id393" class="calibre1"/>结论，让我们为基线版本和没有基线的版本绘制训练期间PG的方差图。完整的示例在<code class="literal">Chapter10/01_cartpole_pg.py</code>中，大部分代码与<a class="calibre1" title="Chapter 9. Policy Gradients – An Alternative" href="part0068_split_000.html#20R682-ce551566b6304db290b61e4d70de52ee">第9章</a>、<em class="calibre11">政策梯度中的代码相同——另一种选择</em>。该版本的不同之处如下:</p><div><ul class="itemizedlist"><li class="listitem">它现在接受命令行选项<code class="literal">--baseline</code>，该选项允许从奖励中减去平均值。默认情况下，不使用基线。</li><li class="listitem">在每个训练循环中，我们从保单损失中收集梯度，并使用该数据计算方差。</li></ul></div><p class="calibre8">要仅从策略损失中收集梯度，并从为探索添加的熵奖励中排除梯度，我们需要分两个阶段计算梯度。幸运的是，PyTorch允许这很容易地完成。下面，只包括训练循环的相关部分来说明这个想法。</p><div><pre class="programlisting">        optimizer.zero_grad()
        logits_v = net(states_v)
        log_prob_v = F.log_softmax(logits_v, dim=1)
        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]
        loss_policy_v = -log_prob_actions_v.mean()</pre></div><p class="calibre8">我们按照前面的<a id="id394" class="calibre1"/>计算保单损失，方法是根据采取行动的概率计算对数，然后乘以保单比例(如果我们不使用基线，则为总折扣奖励或总奖励减去基线)。</p><div><pre class="programlisting">        loss_policy_v.backward(retain_graph=True)</pre></div><p class="calibre8">下一步，我们要求PyTorch反向传播保单损失，计算梯度并将它们保存在我们模型的缓冲区中。正如我们之前执行的<code class="literal">optimizer.zero_grad()</code>，那些缓冲区将只包含来自保单损失的梯度。当我们调用<code class="literal">backward()</code>时，这里有一个棘手的问题是<code class="literal">retain_graph=True</code>选项。它指示PyTorch保持变量的图形结构。通常，这被<code class="literal">backward()</code>调用破坏，但是在我们的例子中，这不是我们想要的。一般来说，当我们需要在调用优化器之前多次反向传播丢失时，保留图形会很有用。这种情况并不常见，但有时会变得很方便。</p><div><pre class="programlisting">        grads = np.concatenate([p.grad.data.numpy().flatten()
                                for p in net.parameters()
                                if p.grad is not None])</pre></div><p class="calibre8">然后，我们从我们的模型中迭代所有参数(我们的模型的每个参数都是具有梯度的张量),并在展平的NumPy数组中提取它们的梯度场。这给了我们一个长数组，包含了模型变量的所有梯度。然而，我们的参数更新不仅要考虑政策梯度，还要考虑熵红利提供的梯度。为了实现这一点，我们计算熵损失并再次调用<code class="literal">backward()</code>。为了能够第二次这样做，我们需要通过<code class="literal">retain_graph=True</code>。</p><p class="calibre8">在第二次<code class="literal">backward()</code>调用时，PyTorch将反向传播我们的熵损失，并将梯度添加到内部梯度的缓冲区中。因此，我们现在需要做的只是让我们的优化器使用这些组合梯度来执行优化步骤。</p><div><pre class="programlisting">        prob_v = F.softmax(logits_v, dim=1)
        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()
        entropy_loss_v = -ENTROPY_BETA * entropy_v
        entropy_loss_v.backward()
        optimizer.step()</pre></div><p class="calibre8">后来，我们唯一需要做的就是把我们感兴趣的统计数据写入TensorBoard。</p><div><pre class="programlisting">        writer.add_scalar("grad_l2", np.sqrt(np.mean(np.square(grads))), step_idx)
        writer.add_scalar("grad_max", np.max(np.abs(grads)), step_idx)
        writer.add_scalar("grad_var", np.var(grads), step_idx)</pre></div><p class="calibre8">通过运行这个例子两次，一次使用<code class="literal">--baseline</code>命令行选项，一次不使用，我们得到了PG的方差图<a id="id395" class="calibre1"/>。以下是奖励动态的图表:</p><div><img src="img/00217.jpeg" alt="CartPole variance" class="calibre9"/><div><p class="calibre14">图2:有基线(橙色)和无基线(蓝色)版本的收敛动态</p></div></div><p class="calibre10"> </p><p class="calibre8">以下三个图表显示了渐变的幅度、最大值和方差:</p><div><img src="img/00218.jpeg" alt="CartPole variance" class="calibre9"/><div><p class="calibre14">图3:减去基线(橙色)和基本版本(蓝色)的梯度l2、最大值和方差</p></div></div><p class="calibre10"> </p><p class="calibre8">如您所见，有基线的版本的方差比没有基线的版本低两到三个数量级，这有助于系统更快地收敛。</p></div></body></html>


<html>
  <head>
    <title>Actor-critic</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">演员兼评论家</h1></div></div></div><p class="calibre8">减少<a id="id396" class="calibre1"/>方差的下一步是使我们的基线依赖于状态(直觉上，这是一个好主意，因为不同的状态可能有非常不同的基线)。事实上，为了决定某一特定行为在某个州的适宜性，我们使用了该行为的总报酬折现。然而，总奖励本身可以表示为状态的<em class="calibre11">值</em>加上动作的<em class="calibre11">优势</em>:<em class="calibre11">Q(s，a) = V(s) + A(s，a) </em>。我们在第7章、<em class="calibre11"> DQN扩展</em>中已经看到了这一点，当时我们讨论了DQN的修改，特别是决斗DQN。</p><p class="calibre8">那么，为什么不能用<em class="calibre11"> V(s) </em>作为基线呢？在这种情况下，我们的梯度的规模将正好是advantage <em class="calibre11"> A(s，a) </em>，显示这个采取的行动相对于平均状态的值如何更好。事实上，我们可以这样做，这是一个非常好的改进PG方法的想法。这里唯一的问题是:我们不知道从贴现总回报中减去<em class="calibre11"> V(s) </em>状态的值<em class="calibre11"> Q(s，a) </em>。为了解决这个问题，让我们使用<em class="calibre11">另一个神经网络</em>，它将对每个观测值近似为<em class="calibre11"> V(s) </em>。为了训练它，我们可以利用我们在DQN方法中使用的相同训练程序:我们将执行贝尔曼步骤，然后最小化均方误差，以改善<em class="calibre11"> V(s) </em>近似。</p><p class="calibre8">当我们知道任何状态的值时(或者至少有一些近似值)，我们可以使用它来计算PG并更新我们的策略网络，以增加具有良好优势值的行动的概率，并减少具有不良优势的行动的机会。策略网络(返回动作的概率分布)被称为<em class="calibre11">参与者</em>，因为它告诉我们该做什么。另一个网络叫做<em class="calibre11">评论家</em>，因为它让我们了解我们的行为有多好。下面是该架构的示意图。</p><div><img src="img/00219.jpeg" alt="Actor-critic" class="calibre9"/><div><p class="calibre14">图4:A2C架构</p></div></div><p class="calibre10"> </p><p class="calibre8">在实践中，政策和价值网络部分重叠，主要是出于效率和融合的考虑。在这种情况下，政策和价值被实现为网络的不同头部，从公共主体获取输出，并将其转换为概率分布和表示状态值的单个数字。这有助于两个网络共享低级功能(如Atari代理中的卷积滤波器)，但以不同的<a id="id397" class="calibre1"/>方式组合它们。该架构如下所示。</p><div><img src="img/00220.jpeg" alt="Actor-critic" class="calibre9"/><div><p class="calibre14">图5:具有共享网络主体的A2C架构</p></div></div><p class="calibre10"> </p><p class="calibre8">从培训的角度来看，我们完成了以下步骤:</p><div><ol class="orderedlist"><li class="listitem" value="1">用随机值初始化网络参数<img src="img/00221.jpeg" alt="Actor-critic" class="calibre24"/></li><li class="listitem" value="2">在环境中玩N步使用当前策略<img src="img/00222.jpeg" alt="Actor-critic" class="calibre24"/>，保存状态st，动作at，奖励rt</li><li class="listitem" value="3">如果剧集结束，R = 0，或者<img src="img/00223.jpeg" alt="Actor-critic" class="calibre24"/></li><li class="listitem" value="4">对于<img src="img/00224.jpeg" alt="Actor-critic" class="calibre24"/>(注意步骤是向后处理的):<div> <ul class="itemizedlist1"> <li class="listitem"> <img src="img/00225.jpeg" alt="Actor-critic" class="calibre24"/> </li> <li class="listitem">累加PG <img src="img/00226.jpeg" alt="Actor-critic" class="calibre24"/> </li> <li class="listitem">累加值渐变<img src="img/00227.jpeg" alt="Actor-critic" class="calibre24"/> </li> </ul> </div></li><li class="listitem" value="5">使用累积的梯度更新网络参数，在PG <img src="img/00228.jpeg" alt="Actor-critic" class="calibre24"/>方向和数值梯度<img src="img/00229.jpeg" alt="Actor-critic" class="calibre24"/>的相反方向移动</li><li class="listitem" value="6">从步骤2开始重复，直到达到收敛</li></ol><div/></div><p class="calibre8">前面的算法是一个大纲，类似于那些通常打印在研究论文。在实践中，需要考虑一些因素:</p><div><ul class="itemizedlist"><li class="listitem">熵奖励通常是用来提高探索的。它通常被写成熵值加上损失函数<img src="img/00230.jpeg" alt="Actor-critic" class="calibre24"/>的<a id="id398" class="calibre1"/>。当概率分布是均匀的时候，这个函数有一个最小值，所以通过把它加到损失函数中，我们正在推动我们的代理人不要太确定它的行为。</li><li class="listitem">梯度累积通常被实现为结合了所有三个部分的损失函数:策略损失、值损失和熵损失。你应该小心这些损失的迹象，作为PGs？正在向您展示政策改进的方向，但是价值和熵损失都应该最小化。</li><li class="listitem">为了提高稳定性，值得使用几个环境，同时为您提供观察(当我们有多个环境时，我们的训练批次将从它们的观察中创建)。我们将在下一章中探讨几种方法。</li></ul></div><p class="calibre8">前一种方法被称为演员-评论家，或有时优势演员-评论家，简称为A2C。几个环境并行运行的版本称为Advantage Asynchronous Actor-Critic，也称为A3C。A3C方法将是下一章的主题，但是现在让我们实现A2C。</p></div></body></html>


<html>
  <head>
    <title>A2C on Pong</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec38" class="calibre1"/>乒乓上的A2C</h1></div></div></div><p class="calibre8">在前一章中，我们<a id="id399" class="calibre1"/>看到了一个(不太成功的)用PG解决我们最喜欢的Pong环境的尝试。让我们用手头的演员-评论家方法再试一次。</p><div><pre class="programlisting">GAMMA = 0.99
LEARNING_RATE = 0.001
ENTROPY_BETA = 0.01
BATCH_SIZE = 128
NUM_ENVS = 50

REWARD_STEPS = 4
CLIP_GRAD = 0.1</pre></div><p class="calibre8">像往常一样，我们从定义超参数开始(省略了导入)。这些值是不可调的，因为我们将在本章的下一节进行调整。这里我们有一个新的值:<code class="literal">CLIP_GRAD</code>。这个超参数指定了梯度削波的阈值，这基本上防止了我们在优化阶段的梯度变得太大，并将我们的策略推得太远。使用PyTorch功能来实现裁剪，但是想法非常简单:如果梯度的L2范数大于这个超参数，那么梯度向量被裁剪到这个值。</p><p class="calibre8"><code class="literal">REWARD_STEPS </code>超参数决定了<a id="id400" class="calibre1"/>我们将采取多少步骤来估算每个行动的总折扣奖励。在PG中，我们使用了大约10个步骤，但在A2C，我们将使用我们的值近似值来获得进一步步骤的状态值，因此减少步骤的数量也没问题。</p><div><pre class="programlisting">class AtariA2C(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(AtariA2C, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        conv_out_size = self._get_conv_out(input_shape)
        self.policy = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

        self.value = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )</pre></div><p class="calibre8">我们的网络体系结构有一个共享的卷积体和两个头部:第一个头部返回带有我们行为的概率分布的策略，第二个头部返回一个单一的数字，它将近似状态的值。它可能看起来类似于我们在<a class="calibre1" title="Chapter 7. DQN Extensions" href="part0048_split_000.html#1DOR02-ce551566b6304db290b61e4d70de52ee">第7章</a>、<em class="calibre11"> DQN扩展</em>中的决斗DQN建筑，但是我们的训练程序是不同的。</p><div><pre class="programlisting">    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        fx = x.float() / 256
        conv_out = self.conv(fx).view(fx.size()[0], -1)
        return self.policy(conv_out), self.value(conv_out)</pre></div><p class="calibre8">通过网络的前向传递返回两个张量的元组:策略和值。现在我们有了一个大而重要的函数，它接受环境转换的批次并返回三个张量:状态批次、采取的动作批次和使用<a id="id401" class="calibre1"/>公式<img src="img/00231.jpeg" alt="A2C on Pong" class="calibre24"/>计算的Q值批次。这个Q值将用于两个地方:计算均方误差(MSE)损失以改进值的近似值，与DQN的方法相同，以及计算行动的优势。</p><div><pre class="programlisting">def unpack_batch(batch, net, device='cpu'):
    states = []
    actions = []
    rewards = []
    not_done_idx = []
    last_states = []
    for idx, exp in enumerate(batch):
        states.append(np.array(exp.state, copy=False))
        actions.append(int(exp.action))
        rewards.append(exp.reward)
        if exp.last_state is not None:
            not_done_idx.append(idx)
            last_states.append(np.array(exp.last_state, copy=False))</pre></div><p class="calibre8">在第一个循环中，我们只是遍历我们的一批转换，并将它们的字段复制到列表中。注意，奖励值已经包含了前面<code class="literal">REWARD_STEPS</code>的折扣奖励，因为我们使用了<code class="literal">ptan.ExperienceSourceFirstLast</code>类。我们还需要处理剧集结束的情况，并记住非结束剧集的批量条目的索引。</p><div><pre class="programlisting">    states_v = torch.FloatTensor(states).to(device)
    actions_t = torch.LongTensor(actions).to(device)</pre></div><p class="calibre8">在前面的代码中，我们将收集的状态和动作转换为PyTorch张量，并在需要时将其复制到GPU中。该函数的其余部分计算Q值，将末期发作考虑在内。</p><div><pre class="programlisting">    rewards_np = np.array(rewards, dtype=np.float32)
    if not_done_idx:
        last_states_v = torch.FloatTensor(last_states).to(device)
        last_vals_v = net(last_states_v)[1]
        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]
        rewards_np[not_done_idx] += GAMMA ** REWARD_STEPS * last_vals_np</pre></div><p class="calibre8">前面的代码用我们的转移链中的最后一个状态准备了<a id="id402" class="calibre1"/>变量，并查询我们的网络以获得<em class="calibre11"> V(s) </em>近似值。然后，将该近似值添加到折扣奖励中，并分几步乘以gamma指数。</p><div><pre class="programlisting">    ref_vals_v = torch.FloatTensor(rewards_np).to(device)
    return states_v, actions_t, ref_vals_v</pre></div><p class="calibre8">在函数的开始，我们将Q值打包成适当的形式并返回。</p><div><pre class="programlisting">if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    parser.add_argument("-n", "--name", required=True, help="Name of the run")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")

    make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make("PongNoFrameskip-v4"))
    envs = [make_env() for _ in range(NUM_ENVS)]
    writer = SummaryWriter(comment="-pong-a2c_" + args.name)</pre></div><p class="calibre8">训练循环的准备代码与往常一样，只是我们现在使用一系列环境来收集经验，而不是一个环境。</p><div><pre class="programlisting">    net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)
    print(net)

    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, device=device)
    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)</pre></div><p class="calibre8">这里一个非常重要的细节是将<code class="literal">eps</code>参数传递给优化器。如果你熟悉<strong class="calibre2"> Adam </strong>算法，你可能知道epsilon是一个加在分母上的小数字，以防止出现零除的情况。通常，该值被设置为某个较小的数字，如1e-8或1e-10，但是，在我们的例子中，这些值被证明太小了。对此我没有数学上严格的解释，但是使用默认值ε，这个方法根本不会收敛。很有可能，划分为1e-8的小值使得梯度太大，这对训练稳定性是致命的。</p><div><pre class="programlisting">    batch = []

    with common.RewardTracker(writer, stop_reward=18) as tracker:
        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:
            for step_idx, exp in enumerate(exp_source):
                batch.append(exp)

                # handle new rewards
                new_rewards = exp_source.pop_total_rewards()
                if new_rewards:
                    if tracker.reward(new_rewards[0], step_idx):
                        break

                if len(batch) &lt; BATCH_SIZE:
                    continue</pre></div><p class="calibre8">在训练循环中，我们使用两个包装器。第一个是您已经熟悉的:<code class="literal">common.RewardTracker</code>，它计算最近100集的平均奖励，并在这个平均奖励超过所需阈值时告诉我们。另一个包装器<code class="literal">TBMeanTracker</code>来自ptan库，负责将最后10步测量参数的平均值写入TensorBoard。当训练可能需要数百万步时，这很有帮助，所以我们不想将数百万个点写入TensorBoard，而是每10步写入平滑的值。下一个代码块负责我们的损失计算，这是A2C方法的核心。</p><div><pre class="programlisting">                states_v, actions_t, vals_ref_v = unpack_batch(batch, net, device=device)
                batch.clear()

                optimizer.zero_grad()
                logits_v, value_v = net(states_v)</pre></div><p class="calibre8">开始时，我们使用前面描述的函数解包我们的批处理，并要求我们的网络返回该批处理的策略和值。策略是以非规范化的形式返回的，所以要将其转换成概率分布，我们需要对其应用softmax。我们推迟这一步使用<code class="literal">log_softmax</code>，因为它在数值上更稳定。</p><div><pre class="programlisting">                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)</pre></div><p class="calibre8">价值损失部分几乎是微不足道的:我们只是计算我们的网络返回的价值和我们使用向前展开四步的贝尔曼方程执行的近似之间的MSE。</p><div><pre class="programlisting">                log_prob_v = F.log_softmax(logits_v, dim=1)
                adv_v = vals_ref_v - value_v.detach()
                log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]
                loss_policy_v = -log_prob_actions_v.mean()</pre></div><p class="calibre8">这里，我们计算保单<a id="id404" class="calibre1"/>损失以获得PG。前两步是获取我们策略的日志，计算行动的优势，即<em class="calibre11"> A(s，a) = Q(s，a) - V(s) </em>。对<code class="literal">value_v.detach()</code>的调用很重要，因为我们不想将PG传播到我们的值近似值中。然后，我们对所采取的行动的概率进行记录，并根据优势对其进行衡量。我们的PG损失值将等于该策略的比例对数的否定平均值，因为PG指导我们改进策略，但是损失值应该被最小化。</p><div><pre class="programlisting">                prob_v = F.softmax(logits_v, dim=1)
                entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()</pre></div><p class="calibre8">我们损失函数的最后一部分是熵损失，它等于我们策略的缩放熵，符号相反(熵计算为<img src="img/00232.jpeg" alt="A2C on Pong" class="calibre24"/>)。</p><div><pre class="programlisting">                loss_policy_v.backward(retain_graph=True)
                grads = np.concatenate([p.grad.data.cpu().numpy().flatten()
                                        for p in net.parameters()
                                        if p.grad is not None])</pre></div><p class="calibre8">在前面的代码中，我们计算并提取策略的梯度，这将用于跟踪最大梯度、其方差和L2范数。</p><div><pre class="programlisting">                loss_v = entropy_loss_v + loss_value_v
                loss_v.backward()
                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)
                optimizer.step()
                loss_v += loss_policy_v</pre></div><p class="calibre8">作为我们训练的最后一步，我们反向传播熵损失an、值损失和剪辑梯度，并要求我们的优化器更新网络。</p><div><pre class="programlisting">                tb_tracker.track("advantage", adv_v, step_idx)
                tb_tracker.track("values", value_v, step_idx)
                tb_tracker.track("batch_rewards", vals_ref_v, step_idx)
                tb_tracker.track("loss_entropy", entropy_loss_v, step_idx)
                tb_tracker.track("loss_policy", loss_policy_v, step_idx)
                tb_tracker.track("loss_value", loss_value_v, step_idx)
                tb_tracker.track("loss_total", loss_v, step_idx)
                tb_tracker.track("grad_l2", np.sqrt(np.mean(np.square(grads))), step_idx)
                tb_tracker.track("grad_max", np.max(np.abs(grads)),step_idx)
                tb_tracker.track("grad_var", np.var(grads), step_idx)</pre></div><p class="calibre8">在训练循环的最后，我们跟踪我们将在TensorBoard中监控的所有值。它们有很多，我们将在下一节讨论它们。</p></div></body></html>


<html>
  <head>
    <title>A2C on Pong results</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec39" class="calibre1"/> A2C谈乒乓成绩</h1></div></div></div><p class="calibre8">要开始<a id="id405" class="calibre1"/>训练，使用<code class="literal">--cuda</code>和-n选项运行<code class="literal">02_pong_a2c.py</code>(为TensorBoard提供运行的名称):</p><div><pre class="programlisting">rl_book_samples/Chapter10$ ./02_pong_a2c.py --cuda -n t2
AtariA2C (
  (conv): Sequential (
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU ()

    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU ()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU ()
  )
  (policy): Sequential (
    (0): Linear (3136 -&gt; 512)
    (1): ReLU ()
    (2): Linear (512 -&gt; 6)
  )
  (value): Sequential (
    (0): Linear (3136 -&gt; 512)
    (1): ReLU ()
    (2): Linear (512 -&gt; 1)
  )
)
37799: done 1 games, mean reward -21.000, speed 722.89 f/s
39065: done 2 games, mean reward -21.000, speed 749.92 f/s
39076: done 3 games, mean reward -21.000, speed 755.26 f/s
...
 </pre></div><p class="calibre8">作为一个警告:培训过程是漫长的。对于原始的超参数，需要超过8M的帧来求解，这在GPU上大约需要三个小时。在本章的下一节，我们将调整参数来提高收敛速度，但是，现在，它是三个<a id="id406" class="calibre1"/>小时。为了进一步改善这种情况，在下一章中，我们将研究分布式版本，它在一个单独的进程中执行环境，但首先让我们关注TensorBoard中的绘图。</p><p class="calibre8">首先，奖励动态看起来比上一章的例子好得多:</p><div><img src="img/00233.jpeg" alt="A2C on Pong results" class="calibre9"/><div><p class="calibre14">图6:A2C方法的收敛动态</p></div></div><p class="calibre10"> </p><p class="calibre8">第一个图batch_rewards显示了使用贝尔曼方程近似的Q值和Q近似中的总体正动态。接下来的两个情节是总的未折扣奖励和相同的奖励，但平均为最后100集。这表明，随着时间的推移，我们的培训流程或多或少在不断改进。</p><div><img src="img/00234.jpeg" alt="A2C on Pong results" class="calibre9"/><div><p class="calibre14">图7:培训期间的损失成分</p></div></div><p class="calibre10"> </p><p class="calibre8">接下来的四个图表与<a id="id407" class="calibre1"/>我们的损失相关，包括单个损失部分和总损失。在这里我们可以看到各种各样的东西。首先，我们的价值损失在持续减少，这表明我们的<em class="calibre11"> V(s) </em>近似值在训练过程中不断提高。我们可以分享的第二个观察结果是，我们的熵损失在增长，但在总损失中并不占主导地位。这基本上意味着，随着政策变得不那么一致，我们的代理人对其行动变得更加自信。这里要注意的最后一点是，保单损失在大多数时间都在减少，保单损失与总损失相关，这很好，因为我们首先对PG感兴趣。</p><div><img src="img/00235.jpeg" alt="A2C on Pong results" class="calibre9"/><div><p class="calibre14">图8:培训期间的优势和梯度指标</p></div></div><p class="calibre10">最后一组图显示了优势值和PG指标。优势是我们PGs的一个尺度，它等于<em class="calibre11"> Q(s，a) - V(s) </em>。我们预计它会在零附近振荡，图表符合我们的预期。梯度图表明我们的梯度既不太小也不太大。方差在训练开始时非常小(对于1.5M的帧)，但后来开始增长，这意味着我们的策略正在改变。</p><p class="calibre8"><a id="ch10lvl1sec40" class="calibre1"/>调谐超参数</p></div></body></html>


<html>
  <head>
    <title>Tuning hyperparameters</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">在前面的部分中，我们在三个小时的优化和9M帧中解决了<a id="id408" class="calibre1"/> Pong。现在是调整我们的超参数来加速收敛的好时机。这里的黄金法则是一次调整一个选项，并仔细做出结论，因为整个过程是随机的。</h1></div></div></div><p class="calibre8">在本节中，我们将从原始超参数开始，并执行以下实验:</p><p class="calibre8">提高学习率</p><div><ul class="itemizedlist"><li class="listitem">增加熵β</li><li class="listitem">改变我们用来收集经验的环境数量</li><li class="listitem">调整批量的大小</li><li class="listitem">严格地说，下面的<a id="id409" class="calibre1"/>实验不是适当的超参数调整，只是试图更好地理解A2C收敛动力学如何依赖于参数。为了找到最佳的一组参数，全网格搜索或随机采样值可以得到更好的结果，但需要更多的时间和资源。</li></ul></div><p class="calibre8"><a id="ch10lvl2sec42" class="calibre1"/>学习速度</p></div></body></html>


<html>
  <head>
    <title>Tuning hyperparameters</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">我们的起始<strong class="calibre2">学习率</strong> ( <strong class="calibre2"> LR </strong>)是<a id="id410" class="calibre1"/> 0.001，我们可以预期更大的学习率将导致更快的收敛。在我的测试中，这被证明是正确的，但只是在一定程度上:收敛速度提高到0.003，但对于更大的值，系统根本不收敛。</h2></div></div></div><p class="calibre8">性能结果如下:</p><p class="calibre8">LR = 0.002:480万帧，1.5小时</p><div><ul class="itemizedlist"><li class="listitem">LR=0.003: 3.6M帧，1小时</li><li class="listitem">LR=0.004:未收敛</li><li class="listitem">LR=0.005:未收敛</li><li class="listitem">回报动态和价值损失如下图所示。较大的LR值导致较低的值损失，这表明对策略和值头使用两个优化器(具有不同的学习速率)可能会导致更稳定的学习。</li></ul></div><p class="calibre8">图9:不同学习速率的实验(较快的收敛对应于较大的学习速率)</p><div><img src="img/00236.jpeg" alt="Learning rate" class="calibre9"/><div><p class="calibre14">Figure 9: Experiments with different learning rates (faster convergence corresponds to large learning rates)</p></div></div><p class="calibre10"><a id="ch10lvl2sec43" class="calibre1"/>熵β</p></div></div></body></html>


<html>
  <head>
    <title>Tuning hyperparameters</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">我尝试了两个熵值<a id="id411" class="calibre1"/>损失标度:0.02和0.03。第一个值提高了速度，但第二个值又使速度变得更差，所以最优值介于两者之间。结果如下:</h2></div></div></div><p class="calibre8">beta = 0.02:680万帧，2小时</p><div><ul class="itemizedlist"><li class="listitem">beta=0.03: 12M帧，4小时</li><li class="listitem"><a id="ch10lvl2sec44" class="calibre1"/>环境计数</li></ul></div></div></div></body></html>


<html>
  <head>
    <title>Tuning hyperparameters</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_3">不清楚什么样的<a id="id412" class="calibre1"/>环境效果最好，所以我尝试了几个小于或大于初始值50的计数。结果是矛盾的，但似乎环境越多，收敛越快:</h2></div></div></div><p class="calibre8">envs = 40:8.6兆帧，3小时</p><div><ul class="itemizedlist"><li class="listitem">envs = 30:620万帧，2小时(看起来像一颗幸运的种子)</li><li class="listitem">envs = 20:950万帧，3小时</li><li class="listitem">Envs=10:没有收敛</li><li class="listitem">Envs=60: 11.6M帧，4小时(看起来像个倒霉的种子)</li><li class="listitem">envs = 70:770万帧，2.5小时</li><li class="listitem"><a id="ch10lvl2sec45" class="calibre1"/>批量大小</li></ul></div></div></div></body></html>


<html>
  <head>
    <title>Tuning hyperparameters</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_4">关于批量大小的实验<a id="id413" class="calibre1"/>产生了一个意想不到的结果:较小的批量导致更快的收敛，但是对于非常小的批量，回报不会增加。从RL的角度来看，这是合乎逻辑的，因为对于较小的批次，我们执行更频繁的网络更新，并且我们需要更少的观察，但是这对于深度学习来说是反直觉的，因为较大的批次通常会带来更多的i.i.d .训练数据:</h2></div></div></div><p class="calibre8">批次= 64:490万帧，1.7小时</p><div><ul class="itemizedlist"><li class="listitem">批次= 32:380万帧，1.5小时</li><li class="listitem">批量=16，不收敛</li><li class="listitem"><a id="ch10lvl1sec41" class="calibre1"/>总结</li></ul></div></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">在本章中，我们看到了deep RL中使用最广泛的方法之一:A2C，它明智地将PG更新与状态近似值结合起来。通过分析基线对梯度的统计和收敛的影响，我们介绍了A2C背后的思想。然后，我们检查了基线概念的扩展:A2C，其中一个单独的网络负责人为我们提供当前状态的基线。</h1></div></div></div><p class="calibre8">在下一章中，我们将研究以分布式方式执行相同算法的方法。</p><p class="calibre8">In the next chapter, we will look at ways to perform the same algorithm in a distributed way.</p></div></body></html>
</body></html>