<html><head/><body>
<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>第七章。DQN扩展公司</h1></div></div></div><p class="calibre8">在上一章中，我们实现了DeepMind在2015年发布的深度Q-网络(DQN)模型。这篇论文对强化学习(RL)领域产生了重大影响，它表明，尽管普遍认为，在RL中使用非线性近似器是可能的。这个概念证明激发了人们对深度Q学习领域特别是深度RL的兴趣。</p><p class="calibre8">从那以后，人们提出了许多改进，以及对基本架构的调整，这些改进显著提高了DeepMind发明的基本DQN的收敛性、稳定性和采样效率。在这一章中，我们将更深入地研究其中的一些观点。非常方便的是，2017年10月，DeepMind发表了一篇名为<em class="calibre11">Rainbow:Combining Improvements in Deep Reinforcement Learning</em>(【1】<em class="calibre11">Hessel and others</em>，2017)的论文，该论文提出了DQN最重要的七项改进，其中一些是在2015年发明的，但有些是最近才发明的。在这篇论文中，Atari Games suite上的最新结果是通过将所有这七种方法结合在一起得到的。</p><p class="calibre8">本章将介绍所有这些方法。我们将分析他们背后的想法，以及他们是如何实现的，并与经典的DQN表演进行比较。最后，我们将检查所有方法的组合系统。</p><p class="calibre8">我们将熟悉的DQN扩展如下:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2"> N步DQN </strong>:如何通过简单地展开贝尔曼方程来提高收敛速度和稳定性，以及为什么它不是最终的解决方案</li><li class="listitem"><strong class="calibre2">双DQN </strong>:如何应对DQN高估价值观的行动</li><li class="listitem"><strong class="calibre2">噪声网络</strong>:如何通过在网络权重中增加噪声来提高探索效率</li><li class="listitem"><strong class="calibre2">优先重放缓冲区</strong>:为什么我们经验的统一采样不是最好的训练方式</li><li class="listitem"><strong class="calibre2">决斗DQN </strong>:如何通过使我们的网络架构更加紧密来提高收敛速度代表了我们正在解决的问题</li><li class="listitem"><strong class="calibre2">分类DQN </strong>:如何超越行动的单一期望值，使用完全分布</li></ul></div><p class="calibre8">首先，我们应该通过在我们的DQN代码中加入更高级的库来简化我们的实验。</p></div></body></html>


<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec09" class="calibre1"/>py torch代理网库</h1></div></div></div><p class="calibre8">在<a class="calibre1" title="Chapter 6. Deep Q-Networks" href="part0043_split_000.html#190861-ce551566b6304db290b61e4d70de52ee">第六章</a>、<em class="calibre11">深度Q网</em>中，我们<a id="id235" class="calibre1"/>从零开始实现了一个DQN，只用到了PyTorch、OpenAI Gym、<code class="literal">pytorch-tensorboard</code>。它满足了我们演示事物如何工作的需要，但现在我们要通过额外的调整来扩展基本的DQN。有些调整非常简单和琐碎，但有些需要对代码进行重大修改。为了能够只关注重要的部分，拥有一个尽可能小而简洁的DQN版本是非常有用的，最好是具有可重用的代码段。当你在试验一些发表在论文中的方法或你自己的想法时，这将非常有帮助。在这种情况下，您不需要一次又一次地重新实现相同的功能，与不可避免的错误作斗争。</p><p class="calibre8">考虑到这一点，不久前我开始为深度RL领域实现自己的工具包。我把它叫做PTAN，代表PyTorch Agent Net，因为它的灵感来自另一个叫做Agent Net的开源库<a id="id236" class="calibre1"/>(<a class="calibre1" href="https://github.com/yandexdataschool/AgentNet">https://github.com/yandexdataschool/AgentNet</a>)。我在PTAN尝试遵循的基本设计原则如下:</p><div><ul class="itemizedlist"><li class="listitem">尽可能简单和干净</li><li class="listitem">py torch-本土性</li><li class="listitem">包含小的、可重用的功能块</li><li class="listitem">扩展性和灵活性</li></ul></div><p class="calibre8">该库可在GitHub:<a class="calibre1" href="https://github.com/Shmuma/ptan">https://github.com/Shmuma/ptan</a>获得。所有后续示例都是使用PTAN 0.3版实现的，可以通过运行以下命令将其安装在您的虚拟环境中:</p><div><pre class="programlisting">
<strong class="calibre2">pip install ptan==0.3</strong>
</pre></div><p class="calibre8">让我们看看PTAN提供的基本构件。</p></div></div></body></html>


<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec11" class="calibre1"/>代理</h2></div></div></div><p class="calibre8">代理实体<a id="id238" class="calibre1"/>提供了一种统一的方式来连接来自环境的<strong class="calibre2">观察</strong>和我们想要执行的<strong class="calibre2">动作</strong>。到目前为止，我们只看到了一个简单的、无状态的DQN代理，它使用神经网络从当前观察中获取动作值，并对这些值表现出贪婪的行为。我们已经使用ε-贪婪行为来探索环境，但这并没有改变多少情况。</p><p class="calibre8">在RL领域，这可能更复杂。例如，我们的代理可以预测动作的概率分布，而不是预测动作的值。这样的代理被称为策略代理，我们将在本书的第三部分讨论这些方法。另一个要求可能是代理中的某种内存。例如，通常一次观察(或者甚至k次最后观察)不足以做出关于动作的决定，我们希望在代理中保留一些记忆来捕获必要的信息。RL的整个子域试图用部分可观察马尔可夫决策过程(POMDP)形式来解决这种复杂性。我们将在本书的最后一部分简要地谈到这个案例。</p><p class="calibre8">为了捕获所有这些变体并使代码具有灵活性，PTAN中的代理被实现为一个可扩展的类层次结构，其中<code class="literal">ptan.agent.BaseAgent</code>抽象类位于顶部。从高层来看，代理需要接受一批观察(以NumPy数组的形式)并返回代理想要采取的一批操作。批处理用于提高处理效率，因为在GPU中一次处理多个观察值通常比单独处理它们要快得多。抽象基类不定义输入和输出的类型，这使得它非常灵活，易于扩展。例如，在连续域中，我们的动作不再是离散动作的索引，而是浮动值。</p><p class="calibre8">对应于我们当前DQN需求的代理是<code class="literal">ptan.agent.DQNAgent</code>，它使用提供的PyTorch <code class="literal">nn.Module</code>将一批观察值转换成行动值。为了将网络的输出转换成要采取的实际动作，<code class="literal">DQNAgent</code>类需要在创建时传递第二个对象:动作选择器。</p><p class="calibre8">动作选择器的目的是将网络的输出(通常是一个数字向量)转换成某种动作。在离散动作空间的情况下，动作将是要采取的一个或几个动作索引。PTAN中有两个我们需要的动作选择器:<code class="literal">ptan.actions.ArgmaxActionSelector</code>和<code class="literal">ptan.actions.EpsilonGreedyActionSelector</code>。正如您可能从名称中猜到的，第一个(<code class="literal">ArgmaxActionSelector</code>)将argmax应用于所提供的值，这对应于对Q值的贪婪操作。</p><p class="calibre8">第二个动作选择器支持ε-贪婪行为，将ε作为参数，并以此概率采取随机动作而不是贪婪选择。要将所有这些结合在一起，为CartPole创建代理，使用ε-greedy操作选择，我们可以编写以下代码:</p><div><pre class="programlisting">import gym
import ptan
import numpy as np
import torch.nn as nn

env = gym.make("CartPole-v0")
net = nn.Sequential(nn.Linear(env.observation_space.shape[0], 256),nn.ReLU(),nn.Linear(256, env.action_space.n))

action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1)agent = ptan.agent.DQNAgent(net, action_selector)</pre></div><p class="calibre8">然后，我们可以将观察结果传递给代理，询问它要采取的行动。</p><div><pre class="programlisting">&gt;&gt;&gt; obs = np.array([env.reset()], dtype=np.float32)
&gt;&gt;&gt; agent(obs)
(array([0]), [None])</pre></div><p class="calibre8"><a id="id239" class="calibre1"/>结果元组中的第一项是要采取的一批动作，而第二个值与有状态代理相关，应该被忽略。在运行期间，我们可以在动作选择器中更改epsilon属性，以更改训练期间的随机动作概率。</p></div></div></div></body></html>


<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec12" class="calibre1"/>代理的经验</h2></div></div></div><p class="calibre8">第二个<a id="id240" class="calibre1"/>在PTAN重要的抽象是所谓的<em class="calibre11">经验来源</em>。在前一章的DQN例子中，我们使用了单步体验片段，它包括四件事:</p><div><ul class="itemizedlist"><li class="listitem">在某一时间步观察到的环境状态:<img src="img/00138.jpeg" alt="Agent's experience" class="calibre24"/></li><li class="listitem">代理已采取的操作:<img src="img/00139.jpeg" alt="Agent's experience" class="calibre24"/></li><li class="listitem">代理获得的奖励:<img src="img/00140.jpeg" alt="Agent's experience" class="calibre24"/></li><li class="listitem">下一个状态的观察:<img src="img/00141.jpeg" alt="Agent's experience" class="calibre24"/></li></ul></div><p class="calibre8">我们使用这些值(<img src="img/00142.jpeg" alt="Agent's experience" class="calibre24"/>)通过贝尔曼方程更新我们的Q近似值。然而，对于一般情况，我们可以对更长的经验链感兴趣，包括主体与环境交互的更多时间步骤。</p><p class="calibre8">贝尔曼的方程也可以展开到更长的经验链。</p><div><img src="img/00143.jpeg" alt="Agent's experience" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">本章讨论的提高DQN稳定性和收敛性的方法之一就是这样做的:将贝尔曼方程向前展开k步(当<em class="calibre11"> k </em>通常是2...5)我们显著提高了我们训练收敛的速度。</p><p class="calibre8">为了以一种通用的方式支持这种情况，在PTAN我们有<code class="literal">ptan.experience.ExperienceSourceFirstLast </code>类，它接受环境和代理，并向我们提供经验元组流:(<img src="img/00144.jpeg" alt="Agent's experience" class="calibre24"/>)，其中<img src="img/00145.jpeg" alt="Agent's experience" class="calibre24"/>。当<em class="calibre11"> k = 1 </em>时，<img src="img/00146.jpeg" alt="Agent's experience" class="calibre24"/>就是<img src="img/00147.jpeg" alt="Agent's experience" class="calibre24"/>。</p><p class="calibre8">这个类自动处理剧集结尾的情况，通过将最后一个元组条目设置为None来让我们了解它们。在这种情况下，会自动执行环境重置。类<code class="literal">ExperienceSourceFirstLast</code>公开迭代器接口，在每次<a id="id241" class="calibre1"/>迭代中生成有经验的元组。该类的示例如下:</p><div><pre class="programlisting">&gt;&gt; exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)
&gt;&gt; it = iter(exp_source)
&gt;&gt; next(it)
ExperienceFirstLast(state=array([ 0.03937284, -0.01242409,  0.03980117,  0.02457287]), action=0, reward=1.0, last_state=array([ 0.03912436, -0.20809355,  0.04029262,  0.32954308]))</pre></div></div></div></div></body></html>


<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec13" class="calibre1"/>经验缓冲区</h2></div></div></div><p class="calibre8">在DQN的例子中，一旦我们得到了经验，我们很少想要从中学习。我们通常将它存储在某个大的缓冲区中，并从中进行随机抽样，以获得用于训练的小批次。这个场景由<code class="literal">ptan.experience.ExperienceReplayBuffer </code>类支持，它与我们在前一章看到的实现非常相似。为了构造它，我们需要传递经验源和缓冲区的大小。通过调用<code class="literal">populate(n) method</code>，我们要求缓冲区从经验源中提取n个例子，并将它们存储在缓冲区中。<code class="literal">sample(batch_size)</code>方法从当前缓冲区内容中返回一个给定大小的随机样本。</p></div></div></div></body></html>


<html>
  <head>
    <title>Chapter 7. DQN Extensions</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch07lvl2sec14" class="calibre1"/>健身房环境包装纸</h2></div></div></div><p class="calibre8">为了避免<a id="id243" class="calibre1"/>一遍又一遍地实现(或复制粘贴)通用的Atari包装器，我把它们放在了<code class="literal">ptan.common.wrappers</code>模块中。它们与<a id="id244" class="calibre1"/> OpenAI基线项目:<a class="calibre1" href="https://github.com/openai/baselines">https://github.com/openai/baselines</a>中可用的包装器基本相同(有微小的特定于PyTorch的修改)。要将Atari环境总结成一行，调用<code class="literal">ptan.common.wrappers.wrap_dqn(env)</code>方法就足够了。基本就是这样！正如我之前说过的，PTAN不应该是最终的RL框架。它只是一个设计用来一起使用的<a id="id245" class="calibre1"/>实体的集合，但彼此不太依赖。</p></div></div></div></body></html>


<html>
  <head>
    <title>Basic DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec10" class="calibre1"/>基本DQN</h1></div></div></div><p class="calibre8">通过结合上述所有内容，我们可以用一种更短但仍然灵活的方式重新实现同一个DQN代理，这在以后我们开始修改和更改DQN的各个部分以使DQN更好时会变得很方便。</p><p class="calibre8">在基本DQN <a id="id246" class="calibre1"/>实施中，我们有三个模块:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">Chapter07/lib/dqn_model.py</code>:DQN神经网络，和我们在上一章看到的一样</li><li class="listitem"><code class="literal">Chapter07/lib/common.py</code>:本章示例中使用的通用函数，但是过于专业，不能移到PTAN</li><li class="listitem"><code class="literal">Chapter07/01_dqn_basic.py</code>:创建所有用过的片段和训练循环</li></ul></div><p class="calibre8">先说<code class="literal">lib/common.py</code>的内容。首先，我们有Pong环境的超参数，这在前一章已经介绍过了。超参数存储在dict中，键作为配置名，值作为参数的dict。这使得为更复杂的Atari游戏添加另一个配置集变得容易。</p><div><pre class="programlisting">HYPERPARAMS = {
    'pong': {
        'env_name':         "PongNoFrameskip-v4",
        'stop_reward':      18.0,
        'run_name':         'pong',
        'replay_size':      100000,
        'replay_initial':   10000,
        'target_net_sync':  1000,
        'epsilon_frames':   10**5,
        'epsilon_start':    1.0,
        'epsilon_final':    0.02,
        'learning_rate':    0.0001,
        'gamma':            0.99,
        'batch_size':       32
    },
}</pre></div><p class="calibre8">此外，common.py有一个函数，它获取一批转换并将其打包到NumPy数组集中。从<code class="literal">ExperienceSourceFirstLast</code>开始的每个转换都有一个类型为<code class="literal">namedtuple</code>的转换，包含以下字段:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">state</code>:从环境中观察。</li><li class="listitem"><code class="literal">action</code>:代理采取的整数动作。</li><li class="listitem"><code class="literal">rewards</code>:如果我们已经创造了属性为<code class="literal">steps_count=1</code>的<code class="literal">ExperienceSourceFirstLast</code>，那就是直接奖励。对于更大的步数，它包含该步数的折扣奖励总额。</li><li class="listitem"><code class="literal">last_state</code>:如果转换对应于环境中的最后一步，则该字段为<code class="literal">None</code>，否则包含经验链中的最后一次观察。</li></ul></div><p class="calibre8"><code class="literal">unpack_batch</code>的<a id="id247" class="calibre1"/>代码如下:</p><div><pre class="programlisting">def unpack_batch(batch):
    states, actions, rewards, dones, last_states = [], [], [], [], []
    for exp in batch:
        state = np.array(exp.state, copy=False)
        states.append(state)
        actions.append(exp.action)
        rewards.append(exp.reward)
        dones.append(exp.last_state is None)
        if exp.last_state is None:
            last_states.append(state)
       # the result will be masked anyway
        else:
            last_states.append(np.array(exp.last_state, copy=False))
    return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \
           np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)</pre></div><p class="calibre8">请注意我们如何处理批处理中的最终转换。为了避免这种情况的特殊处理，对于终端转换，我们将初始状态存储在<code class="literal">last_states</code>数组中。为了使我们对Bellman更新的计算正确，我们将在损失计算期间使用<code class="literal">dones</code>数组屏蔽这样的批量条目。另一个解决方案是只为非终结跃迁计算最后状态的值，但这会使我们的损失函数逻辑变得更加复杂。</p><p class="calibre8">损失函数和我们在上一章中的完全一样。我们计算从第一个状态采取的动作的值，然后使用贝尔曼方程计算相同动作的值。由此产生的损耗是这两个量之间的均方误差:</p><div><pre class="programlisting">def calc_loss_dqn(batch, net, tgt_net, gamma, device="cpu"):
    states, actions, rewards, dones, next_states = unpack_batch(batch)

    states_v = torch.tensor(states).to(device)
    next_states_v = torch.tensor(next_states).to(device)
    actions_v = torch.tensor(actions).to(device)
    rewards_v = torch.tensor(rewards).to(device)
    done_mask = torch.ByteTensor(dones).to(device)

    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    next_state_values = tgt_net(next_states_v).max(1)[0]
    next_state_values[done_mask] = 0.0

    expected_state_action_values = next_state_values.detach() * gamma + rewards_v
    return nn.MSELoss()(state_action_values, expected_state_action_values)</pre></div><p class="calibre8">此外，在<code class="literal">common.py,</code>中，我们有两个实用程序类来帮助我们简化训练循环:</p><div><pre class="programlisting">class EpsilonTracker:
    def __init__(self, epsilon_greedy_selector, params):
        self.epsilon_greedy_selector = epsilon_greedy_selector
        self.epsilon_start = params['epsilon_start']
        self.epsilon_final = params['epsilon_final']
        self.epsilon_frames = params['epsilon_frames']
        self.frame(0)

    def frame(self, frame):
        self.epsilon_greedy_selector.epsilon = \
            max(self.epsilon_final, self.epsilon_start - frame / self.epsilon_frames)</pre></div><p class="calibre8"><code class="literal">EpsilonTracker</code>类接受<a id="id248" class="calibre1"/>的实例<code class="literal">EpsilonGreedyActionSelector</code>和我们的超参数用于特定的配置。此外，在其唯一的方法<code class="literal">frame()</code>中，它根据标准的DQNε衰减时间表更新ε的值:在第一个<code class="literal">epsilon_frames</code>步骤中线性减小它，然后保持它恒定。</p><p class="calibre8">第二个类，<code class="literal">RewardTracker,</code>应该在每集结束时被告知总奖励，并跟踪上一集的平均奖励，报告TensorBoard和控制台中的当前值，最后，检查游戏是否已成功解决。它还测量每秒帧数的速度，了解这一点很有用，因为性能是训练的一个重要指标。</p><div><pre class="programlisting">class RewardTracker:
    def __init__(self, writer, stop_reward):
        self.writer = writer
        self.stop_reward = stop_reward

    def __enter__(self):
        self.ts = time.time()
        self.ts_frame = 0
        self.total_rewards = []
        return self

    def __exit__(self, *args):
        self.writer.close()</pre></div><p class="calibre8">实现该类是为了用作上下文管理器，在退出时自动关闭TensorBoard编写器。在<code class="literal">reward()</code>方法中执行<a id="id249" class="calibre1"/>主逻辑，每次一集结束时都会调用该方法。它与上一章的训练循环代码基本相同。</p><div><pre class="programlisting">    def reward(self, reward, frame, epsilon=None):
        self.total_rewards.append(reward)
        speed = (frame - self.ts_frame) / (time.time() - self.ts)
        self.ts_frame = frame
        self.ts = time.time()
        mean_reward = np.mean(self.total_rewards[-100:])
        epsilon_str = "" if epsilon is None else ", eps %.2f" % epsilon
        print("%d: done %d games, mean reward %.3f, speed %.2f f/s%s" % (
            frame, len(self.total_rewards), mean_reward, speed, epsilon_str
        ))
        sys.stdout.flush()
        if epsilon is not None:
            self.writer.add_scalar("epsilon", epsilon, frame)
        self.writer.add_scalar("speed", speed, frame)
        self.writer.add_scalar("reward_100", mean_reward, frame)
        self.writer.add_scalar("reward", reward, frame)
        if mean_reward &gt; self.stop_reward:
            print("Solved in %d frames!" % frame)
            return True
        return False</pre></div><p class="calibre8"><code class="literal">common.py</code>到此为止。它还有另一个功能，现在还不相关，将在后面的示例中使用。现在，让我们来看看<code class="literal">01_dqn_basic.py</code>，它只包含所需类的创建和训练循环。</p><div><pre class="programlisting">#!/usr/bin/env python3
import gym
import ptan
import argparse
import torch
import torch.optim as optim
from tensorboardX import SummaryWriter
from lib import dqn_model, common</pre></div><p class="calibre8">首先，我们导入所需的模块。</p><div><pre class="programlisting">if __name__ == "__main__":
    params = common.HYPERPARAMS['pong']
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")

    env = gym.make(params['env_name'])
    env = ptan.common.wrappers.wrap_dqn(env)</pre></div><p class="calibre8">然后，我们获得Pong游戏的<a id="id250" class="calibre1"/>超参数，解析CUDA的选项并创建我们的环境。接下来，我们使用PTAN的DQN包装器，它将一组通用的预处理应用到环境中。</p><div><pre class="programlisting">    writer = SummaryWriter(comment="-" + params['run_name'] + "-basic")
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    tgt_net = ptan.agent.TargetNet(net)</pre></div><p class="calibre8">然后，我们使用观察和动作的维度为TensorBoard和我们的DQN神经网络(NN)创建一个摘要编写器。<code class="literal">ptan.agent.TargetNet</code>类是一个非常简单的网络包装器，它允许我们创建神经网络权重的副本，并定期同步它们。</p><div><pre class="programlisting">    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])
    epsilon_tracker = common.EpsilonTracker(selector, params)
    agent = ptan.agent.DQNAgent(net, selector, device=device)</pre></div><p class="calibre8">这里我们创建了我们的代理，它需要一个网络来将观察值转换成动作值，还需要一个动作选择器来决定采取哪个动作。对于动作选择器，我们使用ε贪婪策略，ε根据超参数定义的时间表衰减。</p><div><pre class="programlisting">    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)
    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])</pre></div><p class="calibre8">下一个要定义的元素是我们的经验源，这是一个单步<code class="literal">ExperienceSourceFirstLast</code>和经验重放缓冲区，它将存储固定数量的过渡。</p><div><pre class="programlisting">    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])
    frame_idx = 0</pre></div><p class="calibre8">训练循环之前需要的最后一步是优化器和帧计数器。</p><div><pre class="programlisting">    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:
        while True:
            frame_idx += 1
            buffer.populate(1)
            epsilon_tracker.frame(frame_idx)</pre></div><p class="calibre8">在训练循环的开始，我们创建奖励跟踪器，它将报告每一集完成的平均奖励，增加帧计数器，并要求我们的经验重放缓冲器从经验源中提取一个<a id="id251" class="calibre1"/>过渡。对<code class="literal">buffer.populate(1)</code>的调用将在PTAN库内部启动以下动作链:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">ExperienceReplayBuffer</code>会要求经验源获得下一个过渡。</li><li class="listitem">经验源将当前观察反馈给代理以获得动作。</li><li class="listitem">代理将把神经网络应用到观察中来计算Q值，然后要求动作选择器选择要采取的动作。</li><li class="listitem">动作选择器(是一个ε贪婪选择器)将生成随机数来检查如何行动:贪婪还是随机。在这两种情况下，它将决定采取何种行动。</li><li class="listitem">行动将被返回到经验源，经验源将把行动反馈到环境中，以获得奖励和下一次观察。所有这些数据(当前观察、行动、奖励和下一次观察)都将返回到缓冲区。</li><li class="listitem">缓冲区将存储转换，推出旧的观察值以保持其长度不变。</li></ul></div><p class="calibre8">以上所有内容可能看起来很复杂，但是，基本上，它与我们之前完成的过程是相同的，只是以不同的方式包装。</p><div><pre class="programlisting">            new_rewards = exp_source.pop_total_rewards()
            if new_rewards:
                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):
                    break</pre></div><p class="calibre8">上面的训练循环向经验源请求已完成剧集的奖励列表(未打折的总奖励),并将其传递给奖励跟踪器以报告和检查训练是否已完成。因为我们之前只执行了一个步骤，所以可能只有一集或零集完成。如果奖励跟踪器返回True，则表明平均奖励已经达到分数界限，我们可以停止训练。</p><div><pre class="programlisting">            if len(buffer) &lt; params['replay_initial']:
                continue</pre></div><p class="calibre8">在这里，我们检查缓冲区的长度是否足以开始训练。否则，我们就等着收集更多的数据。</p><div><pre class="programlisting">            optimizer.zero_grad()
            batch = buffer.sample(params['batch_size'])
            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)
            loss_v.backward()
            optimizer.step()</pre></div><p class="calibre8">这部分执行标准的随机梯度下降(SGD)更新。我们对梯度进行调零，从体验回放缓冲区对迷你批次进行采样，并使用我们已经看到的函数计算损耗。</p><div><pre class="programlisting">            if frame_idx % params['target_net_sync'] == 0:
                tgt_net.sync()</pre></div><p class="calibre8">训练循环的最后一部分在我们的主模型(被训练)和目标网络之间执行定期同步，我们在贝尔曼更新中使用该目标网络来计算动作值。</p><p class="calibre8">好的，让我们训练模型并检查它的收敛性。</p><div><pre class="programlisting">rl_book_samples/Chapter07$ ./01_dqn_basic.py --cuda
865: done 1 games, mean reward -20.000, eps 0.99, speed 364.42 f/s
2147: done 2 games, mean reward -20.500, eps 0.98, speed 493.27 f/s
3061: done 3 games, mean reward -20.333, eps 0.97, speed 493.09 f/s
3974: done 4 games, mean reward -20.500, eps 0.96, speed 492.45 f/s
4810: done 5 games, mean reward -20.600, eps 0.95, speed 490.46 f/s
5836: done 6 games, mean reward -20.500, eps 0.94, speed 495.29 f/s
6942: done 7 games, mean reward -20.571, eps 0.93, speed 491.58 f/s
7953: done 8 games, mean reward -20.500, eps 0.92, speed 491.78 f/s
9109: done 9 games, mean reward -20.444, eps 0.91, speed 492.71 f/s
...</pre></div><p class="calibre8">输出中的每一行都写在下一集的结尾，显示当前帧计数器、已完成集的数量、最近100场游戏的平均奖励、epsilon和计算速度。在前10k帧期间，速度很快，因为我们不进行训练，等待我们的重放缓冲区被填充。对于基本的DQN版本，通常需要大约100万帧<a id="id253" class="calibre1"/>才能达到17的平均回报，所以请耐心等待。训练结束后，我们可以在TensorBoard中查看训练过程的动态，其中显示了epsilon、原始奖励值、平均奖励和速度的图表。</p><div><img src="img/00148.jpeg" alt="Basic DQN" class="calibre9"/><div><p class="calibre14">图1:一个基本DQN实现的收敛</p></div></div><p class="calibre10"> </p></div></body></html>


<html>
  <head>
    <title>N-step DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec11" class="calibre1"/> N步DQN</h1></div></div></div><p class="calibre8">我们将实施和评估的第一个改进<a id="id254" class="calibre1"/>是一个非常老的改进。它是由Richard Sutton在论文中首次提出的([2] Sutton，1988)。为了得到这个想法，让我们再一次看看Q-learning中使用的贝尔曼更新。</p><div><img src="img/00149.jpeg" alt="N-step DQN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">这个方程是递归的，这意味着我们可以用它本身来表示<img src="img/00150.jpeg" alt="N-step DQN" class="calibre24"/>，这给了我们这个结果:</p><div><img src="img/00151.jpeg" alt="N-step DQN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">值r <sub class="calibre25"> <em class="calibre29"> a，t+1 </em> </sub>表示发布动作<em class="calibre11"> a </em>后，t+1时刻的局部奖励。然而，如果我们假设我们在步骤<em class="calibre11"> t+1 </em>中的动作a是最优选择的，或者接近最优选择，我们可以省略<em class="calibre11">max</em><sub class="calibre25"><em class="calibre29">a</em></sub>and运算，并获得:</p><div><img src="img/00152.jpeg" alt="N-step DQN" class="calibre9"/></div><p class="calibre10">这个值可以一次又一次地展开任意次。正如您可能猜到的，通过用n步的更长转换序列替换一步转换采样，这种展开可以很容易地应用于我们的DQN更新。为了理解为什么这种展开会帮助我们加速训练，让我们考虑下面的例子。这里我们有一个四个状态的简单环境，s <sub class="calibre25"> 1 </sub>，s <sub class="calibre25"> 2 </sub>，s <sub class="calibre25"> 3 </sub>，s <sub class="calibre25"> 4 </sub>，以及在每个状态下唯一可用的动作，除了s <sub class="calibre25"> 4 </sub>，它是一个<a id="id255" class="calibre1"/>终止状态。</p><p class="calibre8">图2:一个简单环境的转换图</p><div><img src="img/00153.jpeg" alt="N-step DQN" class="calibre9"/><div><p class="calibre14">Figure 2: A transition diagram for a simple environment</p></div></div><p class="calibre10">那么，在单步执行的情况下会发生什么呢？我们总共有三个可能的更新(我们不使用<em class="calibre11"> max </em>，因为只有一个动作可用):</p><p class="calibre8">Q(s <sub class="calibre25"> 1 </sub>，a) ← r <sub class="calibre25"> 1 </sub> + γ Q(s <sub class="calibre25"> 2 </sub>，a)</p><div><ol class="orderedlist"><li class="listitem" value="1">Q(s <sub class="calibre25"> 2 </sub>，a) ← r <sub class="calibre25"> 2 </sub> + γ Q(s <sub class="calibre25"> 3 </sub>，a)</li><li class="listitem" value="2">Q(s <sub class="calibre25"> 3 </sub>，a) ← r <sub class="calibre25"> 3 </sub></li><li class="listitem" value="3">让我们想象一下，在培训开始时，我们按照这个顺序完成上面的更新。前两次更新将是无用的，因为我们当前的Q(s <sub class="calibre25"> 2 </sub>，a)和Q(s <sub class="calibre25"> 3 </sub>，a)是不正确的，并且包含初始随机数据。唯一有用的更新将是更新三，其在终止状态之前正确地将奖励r <sub class="calibre25"> 3 </sub>分配给状态s <sub class="calibre25"> 3 </sub>。现在让我们一遍又一遍地完成上面的更新。在第二次迭代时，正确的值将被分配给Q(s <sub class="calibre25"> 2 </sub>，a)，但是Q(s <sub class="calibre25"> 1 </sub>，a)的更新仍然会有噪声。只有在第三次迭代中，我们才能得到所有<em class="calibre11"> Q </em>的有效值。因此，即使在一步的情况下，也需要三步<em class="calibre11">将正确的值传播到所有状态</em>。</li></ol><div/></div><p class="calibre8">现在让我们考虑一个两步的情况。这种情况又有三个更新:</p><p class="calibre8">Q(s <sub class="calibre25"> 1 </sub>，a)←r<sub class="calibre25">1</sub>+γr<sub class="calibre25">2</sub>+γ<sup class="calibre27">2</sup>Q(s<sub class="calibre25">3</sub>，a)</p><div><ol class="orderedlist"><li class="listitem" value="1">Q(s <sub class="calibre25"> 2 </sub>，a) ← r <sub class="calibre25"> 2 </sub> + γr <sub class="calibre25"> 3 </sub></li><li class="listitem" value="2">Q(s <sub class="calibre25"> 3 </sub>，a) ← r <sub class="calibre25"> 3 </sub></li><li class="listitem" value="3">在这种情况下，在更新的第一次循环中，正确的值将被分配给Q(s <sub class="calibre25"> 2 </sub>，a)和Q(s <sub class="calibre25"> 3 </sub>，a)。在第二次迭代时，Q(s <sub class="calibre25"> 1 </sub>，a)的值也将被适当地更新。因此，多个步骤提高了值的传播速度，从而提高了收敛性。好吧，你可能会想，如果这真的有帮助，让我们展开贝尔曼方程，比方说，向前100步。会让我们的收敛速度加快100倍吗？很遗憾，答案是否定的。</li></ol><div/></div><p class="calibre8">尽管我们有所期待，但我们的DQN将根本不会趋同。为了理解为什么，让我们再次回到我们的展开过程，特别是我们放下了<em class="calibre11"> max </em> <sub class="calibre25"> <em class="calibre29"> a </em> </sub>的地方。这是正确的吗？严格地说，没有。我们已经省略了中间步骤的max操作，假设我们在经验收集期间的动作选择(或者我们的<em class="calibre11">策略</em>)是最优的。如果不是呢，比如说，在训练开始的时候，我们的特工随机行动的时候？在这种情况下，我们对Q(st，at)的计算值可能小于状态的最佳值(正如我们随机采取的一些步骤，但不是通过最大化Q值来遵循最有希望的路径)。我们展开贝尔曼方程的步骤越多，我们的更新就可能越不正确。</p><p class="calibre8">我们的大经验重放缓冲区将使情况变得更糟，因为它增加了从旧的坏策略(由Q的旧的坏近似决定)获得转换的机会。这会导致当前Q近似值的错误更新，因此很容易打断我们的训练进度。上述问题是RL方法的一个基本特征，在第四章、<em class="calibre11">交叉熵方法、</em>中我们谈到RL方法的分类时曾简要提及。有两大类:不符合策略的方法和符合策略的方法。</p><p class="calibre8">第一类非策略<a id="id256" class="calibre1"/>方法不依赖于“数据的新鲜度”。例如，简单的DQN是不符合策略的，这意味着我们可以使用几百万步前从环境中采样的非常旧的数据，并且这些数据对于学习仍然是有用的。这是因为我们只是更新了行动Q(s <sub class="calibre25"> t </sub>，a <sub class="calibre25"> t </sub>)的价值，加上最佳行动价值的贴现当前近似值。即使在的动作是随机抽样的，也没有关系，因为对于在st状态下的这个特定动作，我们的更新将是正确的。这就是为什么在非策略方法中，我们可以使用非常大的经验缓冲区来使我们的数据更接近于<strong class="calibre2">独立和同分布</strong> ( <strong class="calibre2"> i.i.d </strong>)。</p><p class="calibre8">另一方面，基于策略的方法严重依赖于根据我们正在更新的当前策略进行采样的训练数据。发生这种情况是因为政策上的方法试图间接地(如上面的n步DQN)或直接地(这本书的整个第三部分都致力于这种方法)改善当前的政策。</p><p class="calibre8">那么，哪一类方法比较好呢？嗯，看情况。非策略方法允许您在以前的大量历史数据上进行训练，甚至在人类演示上进行训练，但是，通常，它们收敛速度较慢。基于策略的方法通常更快，但需要来自环境的更多新数据，这可能成本高昂。想象一辆用政策方法训练的自动驾驶汽车。在系统知道墙和树是它应该避开的东西之前，它会让你付出很多撞车的代价。</p><p class="calibre8">你可能会有一个问题:如果这个“n步”将它变成一个政策上的方法，这将使我们的大经验缓冲区无用，为什么我们要谈论一个n步DQN？在实践中，这通常不是非黑即白的。如果n步DQN有助于加速DQNs，您仍然可以使用它，但您需要适度选择n。2或3的小值通常很好，因为我们在经验缓冲区中的轨迹与一步过渡没有太大不同。在这种情况下，收敛速度通常成比例地提高，但是n的大的<a id="id257" class="calibre1"/>值会破坏训练过程。因此，应该调整步骤的数量，但是加快收敛速度通常是值得的。</p><p class="calibre8"><a id="ch07lvl2sec15" class="calibre1"/>实施</p></div></body></html>


<html>
  <head>
    <title>N-step DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">由于<code class="literal">ExperienceSourceFirstLast</code>类已经<a id="id258" class="calibre1"/>支持多步贝尔曼展开，我们的DQN的n步版本非常简单。我们只需要对基本DQN进行两处修改，就可以将其变成n步版本:</h2></div></div></div><p class="calibre8">传递我们希望在<code class="literal">steps_count parameter</code>中的<code class="literal">ExperienceSourceFirstLast</code>创建上展开的步骤数。</p><div><ul class="itemizedlist"><li class="listitem">将正确的伽马值传递给<code class="literal">calc_loss_dqn</code>函数。这种修改确实很容易被忽略，但它可能对收敛有害。由于我们的更夫现在是n步，经验链中最后一个状态的折现系数将不再只是γ，而是γ <sup class="calibre27"> n </sup>。</li><li class="listitem">您可以在<code class="literal">Chapter07/02_dqn_n_steps.py</code>中找到完整的示例，但以下是修改后的行:</li></ul></div><p class="calibre8"><code class="literal">args.n</code>值是命令行参数中传递的步数，默认情况下使用2步。</p><div><pre class="programlisting">    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=args.n)</pre></div><p class="calibre8">下面是简单DQN(浅色线)和两步DQN(深色线)的回报和平均100点回报的图表。</p><div><pre class="programlisting">    loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,
                       gamma=params['gamma']**args.n, device=device)</pre></div><p class="calibre8">图3:与基本DQN相比，两步DQN的收敛</p><div><img src="img/00154.jpeg" alt="Implementation" class="calibre9"/><div><p class="calibre14">Figure 3: The convergence of a two-step DQN in comparison to a basic DQN</p></div></div><p class="calibre10">正如您在<a id="id259" class="calibre1"/>图中看到的，两步DQN比简单的DQN收敛快两倍多，这是一个很好的改进。那么，更大的<em class="calibre11"> n </em>呢？下图显示了两步(深色)和三步(浅色)DQN的对比:</p><p class="calibre8">图4:两步DQN和三步DQN的比较</p><div><img src="img/00155.jpeg" alt="Implementation" class="calibre9"/><div><p class="calibre14">Figure 4: A comparison of a two-step DQN and three-step DQN</p></div></div><p class="calibre10">因此，与两步疗法相比，并没有改善。优化过程是随机的，因此您的结果可能会略有不同。</p><p class="calibre8"><a id="ch07lvl1sec12" class="calibre1"/>双DQN</p></div></div></body></html>


<html>
  <head>
    <title>Double DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">下一个关于如何<a id="id260" class="calibre1"/>改进基本DQN的富有成效的想法来自DeepMind研究人员在一篇题为<em class="calibre11">使用双Q学习的深度强化学习</em>(van Hasselt，Guez和Silver，2015年)的论文中。在论文中，作者证明了基本DQN有高估Q值的趋势，这可能对训练表现有害，有时可能导致次优策略。造成这种情况的根本原因是贝尔曼方程中的max运算，但严格证明太复杂，这里就不写了。作为这个问题的解决方案，作者建议修改贝尔曼更新一点。</h1></div></div></div><p class="calibre8">在基本DQN中，我们的Q的目标值<a id="id261" class="calibre1"/>如下所示:</p><p class="calibre8">In the basic DQN, our target <a id="id261" class="calibre1"/>value for Q looked like this:</p><div><img src="img/00156.jpeg" alt="Double DQN" class="calibre9"/></div><p class="calibre10">Q( <sub class="calibre25"> t+1 </sub>，a)是使用我们的目标网络计算的Q值，所以我们每n步用训练好的网络更新。该论文的作者建议使用训练过的网络为下一个状态选择动作，但是从目标网络中取值<em class="calibre11"> Q </em>。因此，目标Q值的新表达式将如下所示:</p><p class="calibre8">Q(<sub class="calibre25">t+1</sub>, a) was Q-values calculated using our target network, so we update with the trained network every n steps. The authors of the paper proposed choosing actions for the next state using the trained network but taking values of <em class="calibre11">Q</em> from the target net. So, the new expression for target Q-values will look like this:</p><div><img src="img/00157.jpeg" alt="Double DQN" class="calibre9"/></div><p class="calibre10">作者证明了这个简单的调整完全修正了高估，他们称这个新架构为双DQN。</p><p class="calibre8"><a id="ch07lvl2sec16" class="calibre1"/>实现</p></div></body></html>


<html>
  <head>
    <title>Double DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">核心的<a id="id262" class="calibre1"/>实现非常简单。我们需要做的是稍微修改我们的损失函数。让我们更进一步，比较基本DQN和双DQN产生的动作值。为了做到这一点，我们存储了一组随机保留的状态，并定期计算评估集中每个状态的最佳动作的平均值。</h2></div></div></div><p class="calibre8">完整的示例在<code class="literal">Chapter07/03_dqn_double.py</code>中。我们先来看看损失函数。</p><p class="calibre8">额外的参数<code class="literal">double</code>开启和关闭计算要采取的行动的双DQN方式。</p><div><pre class="programlisting">def calc_loss(batch, net, tgt_net, gamma, device="cpu", double=True):
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)</pre></div><p class="calibre8">以上部分与之前相同。</p><div><pre class="programlisting">    states_v = torch.tensor(states).to(device)
    next_states_v = torch.tensor(next_states).to(device)
    actions_v = torch.tensor(actions).to(device)
    rewards_v = torch.tensor(rewards).to(device)
    done_mask = torch.ByteTensor(dones).to(device)</pre></div><p class="calibre8">这是与基本DQN损失函数的区别。如果启用了双DQN，我们将使用我们的主训练网络来计算在下一个状态中要采取的最佳动作，但是与该动作相对应的值来自目标网络。当然，这部分可以以更快的方式实现，通过将<code class="literal">next_states_v</code>和<code class="literal">states_v</code>结合起来，并且只调用<a id="id263" class="calibre1"/>一次我们的主网络，但是这将使代码不那么清晰。</p><div><pre class="programlisting">    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    if double:
        next_state_actions = net(next_states_v).max(1)[1]
        next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)
    else:
        next_state_values = tgt_net(next_states_v).max(1)[0]</pre></div><p class="calibre8">函数的其余部分是相同的:我们屏蔽完整的情节，并计算网络预测的Q值和近似Q值之间的均方误差(MSE)损失。我们考虑的最后一个函数计算我们坚持状态的值。</p><div><pre class="programlisting">    next_state_values[done_mask] = 0.0
    expected_state_action_values = next_state_values.detach() * gamma + rewards_v
    return nn.MSELoss()(state_action_values, expected_state_action_values)</pre></div><p class="calibre8">这里没有什么太复杂的:我们只是将我们的hold-out States数组分成相等的块，并将每个块传递给网络以获得动作值。从这些值中，我们选择具有最大值的动作，并计算这些值的平均值。由于我们的状态数组在整个训练过程中是固定的，并且这个数组足够大(在我们存储1000个状态的代码中)，我们可以比较这两个DQN变量中平均值的动态。</p><div><pre class="programlisting">def calc_values_of_states(states, net, device="cpu"):
    mean_vals = []
    for batch in np.array_split(states, 64):
        states_v = torch.tensor(batch).to(device)
        action_values_v = net(states_v)
        best_action_values_v = action_values_v.max(1)[0]
        mean_vals.append(best_action_values_v.mean().item())
    return np.mean(mean_vals)</pre></div><p class="calibre8"><code class="literal">03_dqn_double.py</code>文件的其余部分是我们模型的训练循环，大部分和以前一样。</p><p class="calibre8">该程序现在有一个额外的命令行选项来打开和关闭双DQN扩展，以便能够在训练期间比较动作值(注意，您需要显式提供<a id="id264" class="calibre1"/>选项来启用双DQN行为)。</p><div><pre class="programlisting">if __name__ == "__main__":
    params = common.HYPERPARAMS['pong']
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    parser.add_argument("--double", default=False, action="store_true", help="Enable double DQN")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")</pre></div><p class="calibre8">前面的代码与基本的DQN变体没有区别。在初始重放缓冲器填充后,<code class="literal">eval_states</code>变量将被填充我们的保持状态。</p><div><pre class="programlisting">    env = gym.make(params['env_name'])
    env = ptan.common.wrappers.wrap_dqn(env)

    writer = SummaryWriter(comment="-" + params['run_name'] + "-double=" + str(not args.no_double))
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)

    tgt_net = ptan.agent.TargetNet(net)
    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])
    epsilon_tracker = common.EpsilonTracker(selector, params)
    agent = ptan.agent.DQNAgent(net, selector, device=device)

    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)
    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])
    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])

    frame_idx = 0
    eval_states = None</pre></div><p class="calibre8">这部分也和之前一样。</p><div><pre class="programlisting">    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:
        while True:
            frame_idx += 1
            buffer.populate(1)
            epsilon_tracker.frame(frame_idx)

            new_rewards = exp_source.pop_total_rewards()
            if new_rewards:
                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):
                    break

            if len(buffer) &lt; params['replay_initial']:
                continue</pre></div><p class="calibre8">在这里，我们执行初始创建状态，以便在训练期间进行评估。常数<code class="literal">STATES_TO_EVALUATE</code>在程序的开始被定义，等于1000，这足够让<a id="id265" class="calibre1"/>有一组代表性的游戏状态。</p><div><pre class="programlisting">            if eval_states is None:
                eval_states = buffer.sample(STATES_TO_EVALUATE)
                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]
                eval_states = np.array(eval_states, copy=False)</pre></div><p class="calibre8">这部分也没有太大变化，除了我们传递给损失函数的标志，它启用或禁用双DQN。</p><div><pre class="programlisting">            optimizer.zero_grad()
            batch = buffer.sample(params['batch_size'])
            loss_v = calc_loss(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device, double=args.double)
            loss_v.backward()
            optimizer.step()

            if frame_idx % params['target_net_sync'] == 0:
                tgt_net.sync()</pre></div><p class="calibre8">最后，对于每100帧(在<code class="literal">EVAL_EVERY_FRAME</code>常量中定义)，我们计算我们状态的平均值，并将其写入TensorBoard。</p><div><pre class="programlisting">            if frame_idx % EVAL_EVERY_FRAME == 0:
                mean_val = calc_values_of_states(eval_states, net, device=device)
                writer.add_scalar("values_mean", mean_val, frame_idx)</pre></div><p class="calibre8"><a id="ch07lvl2sec17" class="calibre1"/>结果</p></div></div></body></html>


<html>
  <head>
    <title>Double DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">要训练双DQN，启用<a id="id266" class="calibre1"/>扩展，传递<code class="literal">--double</code>命令行参数:</h2></div></div></div><p class="calibre8">为了比较一个<a id="id267" class="calibre1"/>基本DQN的动作值，在没有<code class="literal">--double</code>选项的情况下再次训练它。训练需要一些时间，取决于你的计算能力。在GTX 1080Ti上，100万帧大约需要两个小时。奖励图如下所示，它揭示了尽管在训练开始时有相似的动态，双DQN已经更快地达到收敛。</p><div><pre class="programlisting">
<strong class="calibre2">rl_book_samples/Chapter07$ ./03_dqn_double.py --cuda --double</strong>
<strong class="calibre2">1041: done 1 games, mean reward -19.000, speed 272.36 f/s, eps 0.99</strong>
<strong class="calibre2">2056: done 2 games, mean reward -19.000, speed 396.04 f/s, eps 0.98</strong>
<strong class="calibre2">3098: done 3 games, mean reward -19.000, speed 462.68 f/s, eps 0.97</strong>
<strong class="calibre2">3918: done 4 games, mean reward -19.500, speed 569.58 f/s, eps 0.96</strong>
<strong class="calibre2">4819: done 5 games, mean reward -19.600, speed 563.84 f/s, eps 0.95</strong>
<strong class="calibre2">5697: done 6 games, mean reward -19.833, speed 565.74 f/s, eps 0.94</strong>
<strong class="calibre2">6596: done 7 games, mean reward -20.000, speed 563.71 f/s, eps 0.93</strong>
<strong class="calibre2">...</strong>
</pre></div><p class="calibre8">图5:双DQN(浅色)和基本DQN(深色)的对比</p><div><img src="img/00158.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 5: The comparison of double DQN (light) and basic DQN (dark)</p></div></div><p class="calibre10">同时，有价值的图表显示，经典的DQN在大多数时候高估了行动的价值。在训练结束时，经典的DQN甚至被要求减小该值以达到收敛。</p><p class="calibre8">图6:双DQN(浅色)和基本DQN(深色)中动作的平均值</p><div><img src="img/00159.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 6: The mean value of actions in double DQN (light) and basic DQN (dark)</p></div></div><p class="calibre10"><a id="ch07lvl1sec13" class="calibre1"/>噪音网络</p></div></div></body></html>


<html>
  <head>
    <title>Noisy networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">我们要检查的下一个改进解决了另一个RL问题:探索环境。这篇论文名为<em class="calibre11">探索的嘈杂网络</em>(<em class="calibre11">【4】Fortunato等人，2017 </em>)，有一个非常简单的想法，即在训练期间学习探索特性，而不是有一个单独的与探索相关的时间表。</h1></div></div></div><p class="calibre8">经典DQN通过选择具有特别定义的超参数ε的随机动作来实现探索，该超参数ε随着时间从1.0(完全随机动作)缓慢降低到0.1或0.02的某个小比率。这个过程适用于具有短插曲的简单环境，在游戏期间没有太多的非平稳性，但是即使在这样简单的情况下，也需要调整以使训练过程高效。</p><p class="calibre8">在上面提到的论文中，作者提出了一个非常简单的解决方案，尽管如此，它仍然工作得很好。他们向网络的全连接层的权重添加噪声，并在训练期间使用反向传播来调整该噪声的参数。当然，这种方法不应该与“网络决定在哪里探索更多”相混淆，后者是一种更复杂的方法，也有广泛的支持(例如，参见关于内在动机和基于计数的探索方法的文章[5]或[6])。</p><p class="calibre8">作者提出了两种添加噪声的方法，根据他们的实验，这两种方法都有效，但是具有不同的计算开销:</p><p class="calibre8"><strong class="calibre2">独立高斯噪声</strong>:对于全连接层中的每个<a id="id269" class="calibre1"/>权重，我们有一个从正态分布中抽取的随机值。噪声μ和σ的参数存储在层内，并使用反向传播进行训练，与我们训练标准线性层的权重的方式相同。这种“噪声层”的输出以与线性层相同的方式计算。</p><div><ol class="orderedlist"><li class="listitem" value="1"><strong class="calibre2">因式分解的高斯噪声</strong>:为了<a id="id270" class="calibre1"/>最小化要采样的随机值的数量，作者建议只保留两个随机向量，一个具有层的输入大小，另一个具有层的输出大小。然后，通过计算向量的外积来创建该层的随机矩阵。</li><li class="listitem" value="2"><a id="ch07lvl2sec18" class="calibre1"/>实施</li></ol><div/></div></div></body></html>


<html>
  <head>
    <title>Noisy networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">在PyTorch中，这两种<a id="id271" class="calibre1"/>方法都可以用非常简单的方式轻松实现。我们需要做的是创建我们自己的<code class="literal">nn.Linear</code>层，每次调用<code class="literal">forward()</code>时，用额外的随机值进行采样。我已经实现了两个噪声层，它们的实现在<code class="literal">Chapter07/lib/dqn_model.py</code>中，在<code class="literal">NoisyLinear</code>类中(对于独立的高斯噪声)和NoisyFactorizedLinear(对于分解的噪声变量)。</h2></div></div></div><p class="calibre8">在构造函数中，我们为σ创建一个矩阵(μ的值将存储在从<code class="literal">nn.Linear</code>继承的矩阵中)。为了使sigmas可训练，我们需要将张量包装在一个<code class="literal">nn.Parameter</code>中。<code class="literal">register_buffer</code>方法在网络中创建一个张量，该张量在反向传播期间不会更新，但将由<code class="literal">nn.Module</code>机器处理(例如，它将通过<code class="literal">cuda()</code>调用被复制到GPU)。为层的偏置创建一个额外的参数和缓冲。sigmas的初始值(0.017)取自本节开头引用的噪声网络文章。最后，我们将调用<code class="literal">reset_parameters() method</code>，它被<code class="literal">nn.Linear</code>覆盖，并且<a id="id272" class="calibre1"/>应该执行层的初始化。</p><div><pre class="programlisting">class NoisyLinear(nn.Linear):
    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):
        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)
        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))
        self.register_buffer("epsilon_weight", torch.zeros(out_features, in_features))
        if bias:
            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))
            self.register_buffer("epsilon_bias", torch.zeros(out_features))
        self.reset_parameters()</pre></div><p class="calibre8">在<code class="literal">reset_parameters method</code>中，我们根据文章中的建议执行<code class="literal">nn.Linear</code>权重和偏差的初始化。</p><div><pre class="programlisting">    def reset_parameters(self):
        std = math.sqrt(3 / self.in_features)
        self.weight.data.uniform_(-std, std)
        self.bias.data.uniform_(-std, std)</pre></div><p class="calibre8">在前向方法中，我们对权重和偏差缓冲器中的随机噪声进行采样，并以与<code class="literal">nn.Linear</code>相同的方式对输入数据进行线性变换。因式分解的高斯噪声以类似的方式工作，我在结果中没有发现太大的差异。所以，为了完整起见，我只把它的代码放在下面。如果你很好奇，你可以在文章[4]中找到<a id="id273" class="calibre1"/>的细节和方程式。</p><div><pre class="programlisting">    def forward(self, input):
        self.epsilon_weight.normal_()
        bias = self.bias
        if bias is not None:
            self.epsilon_bias.normal_()
            bias = bias + self.sigma_bias * self.epsilon_bias
        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight, bias)</pre></div><p class="calibre8">从实现的角度来说，就是这样。我们现在需要做的是，把经典的DQN变成一个无意识的变种，只是替换掉神经网络。线性层(这是我们的DQN网络中的最后两层)和非线性层(如果你愿意，也可以称为T1)。当然，你必须删除所有与ε贪婪策略相关的代码。为了在训练期间检查内部噪声水平，我们可以监控噪声层的信噪比(SNR)，即均方根(μ) /均方根(σ)之比，其中均方根为相应权重的均方根。在我们的例子中，SNR表示噪声层的静态成分比注入噪声大多少倍。</p><div><pre class="programlisting">class NoisyFactorizedLinear(nn.Linear):
    def __init__(self, in_features, out_features, sigma_zero=0.4, bias=True):
        super(NoisyFactorizedLinear, self).__init__(in_features, out_features, bias=bias)
        sigma_init = sigma_zero / math.sqrt(in_features)
        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))
        self.register_buffer("epsilon_input", torch.zeros(1, in_features))
        self.register_buffer("epsilon_output", torch.zeros(out_features, 1))
        if bias:
            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))

    def forward(self, input):
        self.epsison_input.normal_()
        self.epsilon_output.normal_()

        func = lambda x: torch.sign(x) * torch.sqrt(torch.abs(x))
        eps_in = func(self.epsilon_input)
        eps_out = func(self.epsilon_output)

        bias = self.bias
        if bias is not None:
            bias = bias + self.sigma_bias * eps_out.t()
        noise_v = torch.mul(eps_in, eps_out)
        return F.linear(input, self.weight + self.sigma_weight * noise_v, bias)</pre></div><p class="calibre8">我们的NoisyNet样本的训练代码在<code class="literal">Chapter07/04_dqn_noisy_net.py</code>。让我们看看与基本DQN版本不同的代码部分:</p><p class="calibre8"><a id="id274" class="calibre1"/>吵闹版DQN的开头和以前一样。区别在于网络的其余部分。</p><div><pre class="programlisting">class NoisyDQN(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(NoisyDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )</pre></div><p class="calibre8">创建的噪波图层具有与其线性对应图层相同的形状。我们将它们放入列表中，以便以后能够访问它们。</p><div><pre class="programlisting">        conv_out_size = self._get_conv_out(input_shape)
        self.noisy_layers = [
            model.NoisyLinear(conv_out_size, 512),
            model.NoisyLinear(512, n_actions)
        ]
        self.fc = nn.Sequential(
            self.noisy_layers[0],
            nn.ReLU(),
            self.noisy_layers[1]
        )</pre></div><p class="calibre8">获得卷积部分形状的函数和<code class="literal">forward()</code>与之前相同。我们在课堂上有一个额外的功能是计算噪声层的信噪比。</p><div><pre class="programlisting">    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        fx = x.float() / 256
        conv_out = self.conv(fx).view(fx.size()[0], -1)
        return self.fc(conv_out)</pre></div><p class="calibre8">训练循环也与之前完全相同，除了一个额外的部分:每500帧，我们从网络中查询噪声层的SNR值，并将其写入TensorBoard。</p><div><pre class="programlisting">    def noisy_layers_sigma_snr(self):
        return [
            ((layer.weight ** 2).mean().sqrt() / (layer.sigma_weight ** 2).mean().sqrt()).data.cpu().numpy()[0]
            for layer in self.noisy_layers
        ]</pre></div><p class="calibre8"><a id="ch07lvl2sec19" class="calibre1"/>结果</p><div><pre class="programlisting">            if frame_idx % 500 == 0:
                snr_vals = net.noisy_layers_sigma_snr()
                for layer_idx, sigma_l2 in enumerate(snr_vals):
                    writer.add_scalar("sigma_snr_layer_%d" % (layer_idx+1),
                                      sigma_l2, frame_idx)</pre></div></div></div></body></html>


<html>
  <head>
    <title>Noisy networks</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">训练后，TensorBoard图表<a id="id275" class="calibre1"/>显示了更好的训练动态。该模型能够在不到600k的帧中达到18的平均分数。</h2></div></div></div><p class="calibre8">图7:嘈杂网络的融合</p><div><img src="img/00160.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 7: Noisy networks convergence</p></div></div><p class="calibre10">与基本的DQN相比，这是一个重大的改进(暗线是嘈杂的DQN，亮线是基本的DQN)。在下图中，显示了前1M帧。</p><p class="calibre8">图8:噪声网络(亮)与基本DQN(暗)的对比</p><div><img src="img/00161.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">检查信噪比图表后，你可能会注意到两层都很快降低了噪声水平。第一层从1到几乎1/2.5的噪声比。第二层甚至更有趣，因为它的噪声水平从开始的1/3下降到1/16，但在250k帧之后，这大致与原始奖励攀升到接近20分的时间相同，最后一层的噪声水平开始增加回来，推动代理更多地探索环境。这很有意义，因为在达到高分水平后，代理基本上知道如何在一个好的水平上玩，但仍然需要“润色”其动作以进一步改善结果。</p></div></div><p class="calibre10">图9:训练期间的噪音水平变化</p><p class="calibre8">After checking the SNR chart, you <a id="id276" class="calibre1"/>may notice that both layers have decreased the noise level very quickly. The first layer went from 1 to almost 1/2.5 ratio of noise. The second layer is even more interesting, as its noise level decreased from 1/3 in the beginning to 1/16, but after 250k frames, which is roughly the same time as when raw rewards climbed close to the 20 score, the level of the noise in the last layer started to increase back, pushing the agent to explore the environment more. This makes a lot of sense, as after reaching high score levels, the agent basically knows how to play at a good level, but still needs to 'polish' its actions to improve the results even more.</p><div><img src="img/00162.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14"><a id="ch07lvl1sec14" class="calibre1"/>优先重放缓冲区</p></div></div><p class="calibre10">下一个非常<a id="id277" class="calibre1"/>有用的关于如何改进DQN培训的想法是在2015年的论文中提出的，<em class="calibre11">优先经验回放</em>(<em class="calibre11">【7】Schaul等人，2015 </em>)。这种方法试图通过根据训练损失对那些样本进行优先排序来提高重放缓冲器中的样本的效率。</p></div></div></body></html>


<html>
  <head>
    <title>Prioritized replay buffer</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">基本的DQN使用重播缓冲器来打破我们剧集中直接过渡之间的相关性。正如我们在<a class="calibre1" title="Chapter 6. Deep Q-Networks" href="part0043_split_000.html#190861-ce551566b6304db290b61e4d70de52ee">第6章</a>、<em class="calibre11">深度Q-Networks </em>中所讨论的，我们在这一集所经历的例子将是高度相关的，因为大部分时间环境是“平稳的”，不会随着我们的行动而发生太大变化。然而，SGD方法假设我们用于训练的数据具有i.i.d .属性。为了解决这个问题，经典的DQN方法使用一个大的过渡缓冲区，随机采样以获得下一个训练批次。</h1></div></div></div><p class="calibre8">该论文的作者对这种统一随机样本策略提出了质疑，并证明了通过为缓冲样本分配优先级，根据训练损失并按照这些优先级对缓冲区进行采样，我们可以显著提高DQN的收敛性和策略质量。这个方法可以看做是“在让你吃惊的数据上多训练”。这里棘手的一点是保持“不寻常”样本的训练和缓冲液其余部分的训练之间的平衡。如果我们只关注缓冲区的一个小的子集，我们可能会失去我们的i.i.d .属性，并简单地在这个子集上过度拟合。</p><p class="calibre8">从数学的角度来看，缓冲器中每个样本的优先级计算为<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/>，其中<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/>是缓冲器中第<em class="calibre11"> i </em>个样本的优先级，<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/>是表示我们对优先级重视程度的数字。如果<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/> <em class="calibre11"> = 0 </em>，我们的采样将像经典的DQN方法一样变得均匀。<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/>的值越大，优先级越高的样本承受的压力越大。因此，这是另一个需要调整的超参数，本文提出的<img src="img/00163.jpeg" alt="Prioritized replay buffer" class="calibre24"/>的初始值为0.6。</p><p class="calibre8">对于如何定义优先级，论文中提出了几种选择，最流行的是使优先级与贝尔曼更新中这个特定示例的损失成比例。添加到缓冲区的新样本需要被赋予最大优先级值，以确保它们很快被采样。</p><p class="calibre8">通过调整样本的优先级，我们在数据分布中引入了偏差(我们对一些转换的采样比其他转换频繁得多)，这需要得到补偿，以便SGD能够工作。为了得到这个结果，该研究的作者使用了样本权重，需要乘以个体样本损失。每个样本的权重值定义为<img src="img/00164.jpeg" alt="Prioritized replay buffer" class="calibre24"/>，其中<em class="calibre11"> β </em>是另一个超参数，应该在0和1之间。在<em class="calibre11"> β = 1 </em>的情况下，采样引入的偏差得到了完全补偿，但作者已经证明，从0和1之间的某个<em class="calibre11"> β </em>开始，并在训练期间慢慢增加到1，这对收敛是有好处的。</p><p class="calibre8"><a id="ch07lvl2sec20" class="calibre1"/>实施</p><p class="calibre8">为了实现这个方法，我们必须在代码中引入一些变化。首先，我们需要一个新的重放缓冲区，它将跟踪优先级，根据优先级对一批进行采样，计算权重，并让我们在损失已知后更新优先级。第二个变化是损失函数本身。现在，我们不仅需要合并每个样本的权重，还需要将丢失值传递回重放缓冲区，以调整采样转换的优先级。</p></div></body></html>


<html>
  <head>
    <title>Prioritized replay buffer</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">在示例文件<code class="literal">Chapter07/05_dqn_prio_replay.py</code>中，我们已经实现了所有这些更改。为了简单起见，新的优先级重放缓冲区类使用了与我们以前的重放缓冲区非常相似的存储方案。不幸的是，对优先级的新要求使得不可能在O(1)时间内实现对缓冲器大小的采样。如果我们使用简单的列表，每次我们采样一个新的批次，我们需要处理所有的优先级，这使得我们的采样O(N)到缓冲区大小。如果我们的缓冲区很小，比如100k个样本，这没什么大不了的，但对于现实生活中数百万次转换的大型缓冲区，这可能会成为一个问题。还有其他存储方案支持O(log N)时间内的高效采样，例如，使用段树数据结构。你可以在OpenAI Baselines项目中找到这样的实现，https://github.com/openai/baselines<a class="calibre1" href="https://github.com/openai/baselines"/>。</h2></div></div></div><p class="calibre8">让我们看看优先级重放缓冲区的例子。</p><p class="calibre8">开始时，我们为样本的优先级定义了<img src="img/00163.jpeg" alt="Implementation" class="calibre24"/>的值，并为<em class="calibre11"> β </em>改变时间表定义了参数。在最初的100k帧中，我们的beta将从0.4更改为1.0。</p><p class="calibre8">优先级重放缓冲区的类将样本存储在一个循环缓冲区(它允许我们保留固定数量的条目，而无需重新分配列表)和NumPy数组中，以保持优先级。我们还将迭代器存储到体验源对象中，以便从环境中提取样本。</p><div><pre class="programlisting">PRIO_REPLAY_ALPHA = 0.6
BETA_START = 0.4
BETA_FRAMES = 100000</pre></div><p class="calibre8"><code class="literal">populate()</code>方法<a id="id279" class="calibre1"/>需要从<code class="literal">ExperienceSource</code>对象中取出给定数量的转换，并将它们存储在缓冲区中。由于我们的过渡存储被实现为循环缓冲区，因此该缓冲区有两种不同的情况:</p><div><pre class="programlisting">class PrioReplayBuffer:
    def __init__(self, exp_source, buf_size, prob_alpha=0.6):
        self.exp_source_iter = iter(exp_source)
        self.prob_alpha = prob_alpha
        self.capacity = buf_size
        self.pos = 0
        self.buffer = []
        self.priorities = np.zeros((buf_size, ), dtype=np.float32)</pre></div><p class="calibre8">当我们的缓冲区没有达到最大容量时，我们只需要向缓冲区追加一个新的转换。</p><div><pre class="programlisting">    def __len__(self):
        return len(self.buffer)

    def populate(self, count):
        max_prio = self.priorities.max() if self.buffer else 1.0
        for _ in range(count):
            sample = next(self.exp_source_iter)
            if len(self.buffer) &lt; self.capacity:
                self.buffer.append(sample)
            else:
                self.buffer[self.pos] = sample
            self.priorities[self.pos] = max_prio
            self.pos = (self.pos + 1) % self.capacity</pre></div><p class="calibre8">如果缓冲区已经满了，我们需要覆盖由pos class字段跟踪的最旧的转换，并调整这个位置模块的缓冲区大小。<div> <pre class="programlisting">    def sample(self, batch_size, beta=0.4):         if len(self.buffer) == self.capacity:             prios = self.priorities         else:             prios = self.priorities[:self.pos]         probs = prios ** self.prob_alpha         probs /= probs.sum()</pre> </div></p><div><ol class="orderedlist"><li class="listitem" value="1">在示例方法中，我们需要使用我们的</li><li class="listitem" value="2">If the buffer is already full, we need to overwrite the oldest transition, which is tracked by the pos class field, and adjust this position module's buffer size.<div><pre class="programlisting">    def sample(self, batch_size, beta=0.4):
        if len(self.buffer) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.pos]
        probs = prios ** self.prob_alpha
        probs /= probs.sum()</pre></div></li></ol><div/></div><p class="calibre8">超参数。</p><div><img src="img/00163.jpeg" alt="Implementation" class="calibre9"/></div><p class="calibre10">然后，使用这些概率，我们对缓冲区进行采样以获得一批样本。</p><p class="calibre8">最后一步，我们计算批次中样本的权重，并返回三个对象:批次、指数和权重。需要批次样本的索引来更新抽样项目的优先级。</p><div><pre class="programlisting">        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[idx] for idx in indices]</pre></div><p class="calibre8">优先级重放缓冲区的最后一个功能允许我们为已处理的批处理更新新的优先级。调用者有责任使用该函数计算批次的损失。</p><div><pre class="programlisting">        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        return samples, indices, weights</pre></div><p class="calibre8">我们例子中的下一个自定义函数<a id="id280" class="calibre1"/>是损失计算。由于PyTorch中的<code class="literal">MSELoss</code>类不支持权重(这是可以理解的，因为MSE是回归问题中使用的损失，但样本的加权通常用于分类损失)，我们需要计算MSE并显式地将结果乘以权重。</p><div><pre class="programlisting">    def update_priorities(self, batch_indices, batch_priorities):
        for idx, prio in zip(batch_indices, batch_priorities):
            self.priorities[idx] = prio</pre></div><p class="calibre8">该函数的开始与之前完全相同，除了样本权重数组的额外参数，该参数需要转换为张量并放在GPU上。</p><p class="calibre8">在损失计算的最后一部分，我们实现了相同的MSE损失，但是显式地编写我们的表达式，而不是使用库。这使我们能够考虑样品的重量，并保留每个样品的单个损失值。这些值将被传递到优先级重放缓冲区以更新优先级。每次亏损都加上小值来处理亏损值为零的情况，会导致进场优先级为零。</p><div><pre class="programlisting">def calc_loss(batch, batch_weights, net, tgt_net, gamma, device="cpu"):
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)

    states_v = torch.tensor(states).to(device)
    next_states_v = torch.tensor(next_states).to(device)
    actions_v = torch.tensor(actions).to(device)
    rewards_v = torch.tensor(rewards).to(device)
    done_mask = torch.ByteTensor(dones).to(device)
    batch_weights_v = torch.tensor(batch_weights).to(device)

    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    next_state_values = tgt_net(next_states_v).max(1)[0]
    next_state_values[done_mask] = 0.0</pre></div><p class="calibre8">现在，是我们训练循环的时候了。</p><div><pre class="programlisting">    expected_state_action_values = next_state_values.detach() * gamma + rewards_v
    losses_v = batch_weights_v * (state_action_values - expected_state_action_values) ** 2
    return losses_v.mean(), losses_v + 1e-5</pre></div><p class="calibre8">初始化<a id="id281" class="calibre1"/>部分应该非常熟悉，因为我们已经创建了我们需要的所有东西，唯一的区别是使用了<code class="literal">PrioReplayBuffer</code>而不是简单的重放缓冲区。</p><p class="calibre8">在训练循环中，和以前一样，我们从经验源中抽取一个转换，并根据时间表更新epsilon。我们使用类似的调度来线性增加β超参数，用于优先级重放缓冲器权重的调整。</p><div><pre class="programlisting">if __name__ == "__main__":
    params = common.HYPERPARAMS['pong']
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")

    env = gym.make(params['env_name'])
    env = ptan.common.wrappers.wrap_dqn(env)

    writer = SummaryWriter(comment="-" + params['run_name'] + "-prio-replay")
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    tgt_net = ptan.agent.TargetNet(net)
    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])
    epsilon_tracker = common.EpsilonTracker(selector, params)
    agent = ptan.agent.DQNAgent(net, selector, device=device)

    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)
    buffer = PrioReplayBuffer(exp_source, params['replay_size'], PRIO_REPLAY_ALPHA)
    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])</pre></div><p class="calibre8">和以前一样，我们追踪已完成剧集的<a id="id282" class="calibre1"/>总奖励，现在显示beta随着训练时间的变化。</p><div><pre class="programlisting">    frame_idx = 0
    beta = BETA_START

    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:
        while True:
            frame_idx += 1
            buffer.populate(1)
            epsilon_tracker.frame(frame_idx)
            beta = min(1.0, BETA_START + frame_idx * (1.0 - BETA_START) / BETA_FRAMES)</pre></div><p class="calibre8">优化器的调用不同于基本的DQN版本。首先，我们来自缓冲区的样本现在返回的不是单个批次，而是三个值:批次、样本索引及其权重。我们将批次和权重传递给损失函数，其结果有两个:第一个是我们需要反向传播的累积损失值，第二个是批次中每个样本的单个损失值的张量。我们反向传播累积的损失，并要求我们的优先级重放缓冲器更新样本的优先级。</p><div><pre class="programlisting">            new_rewards = exp_source.pop_total_rewards()
            if new_rewards:
                writer.add_scalar("beta", beta, frame_idx)
                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):
                    break

            if len(buffer) &lt; params['replay_initial']:
                continue</pre></div><p class="calibre8"><a id="ch07lvl2sec21" class="calibre1"/>结果</p><div><pre class="programlisting">            optimizer.zero_grad()
            batch, batch_indices, batch_weights = buffer.sample(params['batch_size'], beta)
            loss_v, sample_prios_v = calc_loss(batch, batch_weights, net, tgt_net.target_model, params['gamma'], device=device)
            loss_v.backward()
            optimizer.step()
            buffer.update_priorities(batch_indices, sample_prios_v.data.cpu().numpy())</pre></div><p class="calibre8">这个例子可以照常<a id="id283" class="calibre1"/>训练。以下是与基本DQN相比的奖励动态。</p></div></div></body></html>


<html>
  <head>
    <title>Prioritized replay buffer</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">图10:优先重放缓冲区(上图)与基本DQN(下图)的比较</h2></div></div></div><p class="calibre8">This example can be <a id="id283" class="calibre1"/>trained as usual. The following are the reward dynamics in comparison to the basic DQN.</p><div><img src="img/00165.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">正如所料，在<a id="id284" class="calibre1"/>中，重放缓冲区样本的优先级显示了更好的收敛动态。</p></div></div><p class="calibre10"><a id="ch07lvl1sec15" class="calibre1"/>决斗DQN</p><p class="calibre8">对<a id="id285" class="calibre1"/>的这种改进是在2015年提出的，论文名为<em class="calibre11">深度强化学习的决斗网络架构</em>(<em class="calibre11">【8】王等，2015 </em>)。本文的核心观察点在于，我们的网络试图逼近的Q值<em class="calibre11"> Q(s，a) </em>可以分解为量:状态<em class="calibre11"> V(s) </em>的值和该状态下动作的优势<em class="calibre11"> A(s，a) </em>。我们以前见过数量<em class="calibre11"> V(s) </em>，因为它是第五章、<em class="calibre11">表格学习和贝尔曼方程</em>中的值迭代法的核心。它就等于从这种状态下可获得的折现期望回报。优势<em class="calibre11"> A(s，a) </em>被认为是从<em class="calibre11"> A(s) </em>到<em class="calibre11"> Q(s，a) </em>的桥梁，因为，根据定义:<em class="calibre11"> Q(s，a) = V(s) + A(s，a) </em>。换句话说，advantage <em class="calibre11"> A(s，a) </em>就是delta，表示国家的某个特定行为给我们带来了多少额外的奖励。优势可以是正面的，也可以是负面的，一般来说，优势的大小可以是任意的。例如，在某个临界点上，选择一个行动而不是另一个会让我们损失很多总回报。</p></div></div></body></html>


<html>
  <head>
    <title>Dueling DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">上述论文的贡献是在网络架构中明确分离了价值和优势，这带来了更好的训练稳定性、更快的收敛和在Atari基准上的更好结果。下图显示了与经典DQN网络的架构差异。经典的DQN网络(顶部)从卷积层提取要素，并使用完全连接的图层将它们转换为Q值向量，每个动作一个。另一方面，决斗DQN(下图)采用卷积特征，并使用两条独立的路径对其进行处理:一条路径负责<em class="calibre11"> V(s) </em>预测，这只是一个数字，另一条路径预测各个优势值，与经典情况下的Q值具有相同的维数。之后我们把<em class="calibre11"> V(s) </em>加到<em class="calibre11"> A(s，a) </em>的每一个<a id="id286" class="calibre1"/>值上，得到<em class="calibre11"> Q(s，a) </em>，正常使用和训练。</h1></div></div></div><p class="calibre8">图11:一个基本的DQN(上)和决斗架构(下)</p><p class="calibre8">架构的上述变化不足以确保网络如我们所愿地学习<em class="calibre11"> V(s) </em>和<em class="calibre11"> A(s，a) </em>。例如，没有什么可以阻止网络预测某个状态<em class="calibre11"> V(s) = 0 </em>，以及<em class="calibre11">A(s)=【1，2，3，4】</em>，这是完全错误的，因为预测的<em class="calibre11"> V(s) </em>不是该状态的期望值。我们还需要设置另一个约束条件:我们希望任何一个州的优势均值为零。在这种情况下，上述示例的正确预测将是<em class="calibre11"> V(s) = 2.5 </em>和<em class="calibre11"> A(s) = [-1.5，-0.5，0.5，1.5] </em>。</p><div><img src="img/00166.jpeg" alt="Dueling DQN" class="calibre9"/><div><p class="calibre14">这种约束可以通过多种方式实施，例如，通过损失函数，但在论文中，作者提出了一种非常优雅的解决方案，从网络中的Q表达式中减去优势的平均值，这有效地将优势的平均值拉至零:<img src="img/00167.jpeg" alt="Dueling DQN" class="calibre24"/>。这使得经典DQN中需要进行的更改非常简单:要将其转换为双DQN，您只需更改网络架构，而不影响实现的其他部分。</p></div></div><p class="calibre10"><a id="ch07lvl2sec22" class="calibre1"/>实施</p><p class="calibre8">完整的例子是<code class="literal">Chapter07/06_dqn_dueling.py</code>中的<a id="id287" class="calibre1"/>，所以这里我只展示网络类。</p><p class="calibre8">卷积层与之前完全相同。</p></div></body></html>


<html>
  <head>
    <title>Dueling DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">我们没有定义完全连接的层的单一路径，而是创建了两个不同的转换:一个用于优势，一个用于值预测。</h2></div></div></div><p class="calibre8">得益于PyTorch的表现力，<code class="literal">forward()</code>函数的变化也非常简单:我们计算一批样本的价值和优势，并将它们相加，减去<a id="id288" class="calibre1"/>优势的平均值，从而获得最终的Q值。</p><div><pre class="programlisting">class DuelingDQN(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(DuelingDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )</pre></div><p class="calibre8"><a id="ch07lvl2sec23" class="calibre1"/>结果</p><div><pre class="programlisting">        conv_out_size = self._get_conv_out(input_shape)
        self.fc_adv = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )
        self.fc_val = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )</pre></div><p class="calibre8">在<a id="id289" class="calibre1"/>训练了一个决斗的DQN之后，我们可以在我们的Pong基准上将其与经典的DQN收敛进行比较，如下所示。</p><div><pre class="programlisting">    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        fx = x.float() / 256
        conv_out = self.conv(fx).view(fx.size()[0], -1)
        val = self.fc_val(conv_out)
        adv = self.fc_adv(conv_out)
        return val + adv - adv.mean()</pre></div><p class="calibre8">图12:决斗建筑(浅色)与基本DQN(深色)的对比</p></div></div></body></html>


<html>
  <head>
    <title>Dueling DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec23" class="calibre1"/>Results</h2></div></div></div><p class="calibre8"><a id="ch07lvl1sec16" class="calibre1"/>绝对的DQN</p><div><img src="img/00168.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">在我们的<em class="calibre11"> DQN改进工具箱</em>中，最后一个也是<a id="id290" class="calibre1"/>最复杂的方法来自DeepMind在2017年6月发表的一篇名为<em class="calibre11">关于强化学习的分布式观点</em> ( <em class="calibre11"> [9] Bellemare、Dabney和Munos 2017 </em>)。</p></div></div><p class="calibre10">在论文中，作者质疑了Q学习的基础部分:Q值，并试图用更通用的Q值概率分布来代替它们。让我们试着理解这个想法。Q-learning和value iteration方法都是用简单的数字来表示行为或状态的值，并显示我们可以从状态或行为中获得多少总回报。但是，把未来所有可能的奖励都挤到一个数字里，实际可行吗？在复杂的环境中，未来可能是随机的，以不同的概率给我们不同的值。例如，想象一下你经常开车从家去上班的通勤场景。大多数时候，交通并不拥挤，你大约需要30分钟才能到达目的地。不完全是30分钟，但平均是30分钟。有时，会发生一些事情，比如道路维修或事故，由于交通堵塞，你要花三倍的时间去上班。您的通勤时间的概率可以表示为“通勤时间”随机变量的分布<a id="id291" class="calibre1"/>，如下图所示。</p></div></div></body></html>


<html>
  <head>
    <title>Categorical DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">图13:通勤时间的概率分布</h1></div></div></div><p class="calibre8">The last and the <a id="id290" class="calibre1"/>most complicated method in our <em class="calibre11">DQN improvements toolbox</em> is from the very recent paper published by DeepMind in June 2017 called <em class="calibre11">A Distributional Perspective on Reinforcement Learning</em> (<em class="calibre11">[9] Bellemare, Dabney and Munos 2017</em>).</p><p class="calibre8">现在想象一下，你有另一种方式去上班:坐火车。这要花一点时间，因为你需要从家到火车站，再从火车站到办公室，但它们要可靠得多。举例来说，火车通勤时间平均为40分钟，火车中断的可能性很小，这给旅程增加了20分钟的额外时间。火车通勤的分布如下图所示。</p><div><img src="img/00169.jpeg" alt="Categorical DQN" class="calibre9"/><div><p class="calibre14">图14:火车通勤时间的概率分布</p></div></div><p class="calibre10"> </p><p class="calibre8">想象一下，现在我们<a id="id292" class="calibre1"/>想要决定如何通勤。如果我们只知道汽车和火车的平均时间，汽车看起来更有吸引力，因为平均旅行时间为35.43分钟，比火车的40.54分钟要好。然而，如果我们看完整的分布，我们可能会决定坐火车去，因为即使在最坏的情况下，通勤时间也将是1小时，而不是1小时30分钟。切换到统计语言，汽车分布有更高的<strong class="calibre2">方差</strong>，所以，当你真的必须在最多60分钟内到达办公室时，火车更好。</p><div><img src="img/00170.jpeg" alt="Categorical DQN" class="calibre9"/><div><p class="calibre14">论文作者提出了完全相同的想法，<em class="calibre11">关于强化学习的分布式观点</em>【9】。当潜在价值可能具有复杂的潜在分布时，为什么我们要限制自己去预测一个行为的平均值呢？也许它会帮助我们直接使用发行版。</p></div></div><p class="calibre10">论文中给出的结果表明，事实上，这个想法可能是有帮助的，但代价是引入了更复杂的方法。我不打算在这里给出严格的数学定义，但总体思路是预测每个动作的价值分布，类似于上面汽车/火车例子中的分布。作为下一步，作者已经表明，贝尔曼方程可以推广到分布情况，它将具有形式<img src="img/00171.jpeg" alt="Categorical DQN" class="calibre24"/>，这非常类似于熟悉的贝尔曼方程，但现在<em class="calibre11"> Z(x，a) </em>，<em class="calibre11"> R(x，a) </em>是概率分布，而不是数字。</p><p class="calibre8">得到的分布可用于训练我们的网络，以更好地预测给定状态的每个动作的值分布，与Q-learning的方式完全相同。唯一的区别是损失函数，现在必须替换成适合于分布比较的函数。有几种替代方法可用，例如分类问题中使用的kull back-lei bler(KL)-散度(或交叉熵损失)或<a id="id293" class="calibre1"/> Wasserstein度量。在论文中，作者给出了Wasserstein度量的理论依据，但当他们试图在实践中应用它时，他们面临着局限性，因此，论文最后使用了KL-divergence。这篇论文是最近发表的，因此很有可能会对方法进行改进。</p><p class="calibre8"><a id="ch07lvl2sec24" class="calibre1"/>实施</p><p class="calibre8">如前所述，<a id="id294" class="calibre1"/>方法相当复杂，所以我花了一段时间来实现它并确保它正常工作。完整的代码在<code class="literal">Chapter07/07_dqn_distrib.py</code>中，它使用了<code class="literal">lib/common.py</code>中的一个函数来执行分布的投影，这个函数我们以前没有讨论过。在开始之前，我们需要说几句关于实现逻辑的话。</p><p class="calibre8">该方法的核心部分是我们正在逼近的概率分布。有很多方法来表示分布，但论文的作者选择了一个相当通用的<em class="calibre11">参数分布</em>，它基本上是有规律地放置在一个值范围内的固定数量的值。值的范围应涵盖可能的累积折扣奖励的范围。在论文中，作者用不同数量的原子做了实验，但最好的结果是在从<code class="literal">Vmin=-10</code>到<code class="literal">Vmax=10</code>的值范围内以<code class="literal">N_ATOMS=51</code>间隔分割范围获得的。</p></div></body></html>


<html>
  <head>
    <title>Categorical DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">对于每个原子(我们有51个)，我们的网络预测未来贴现值落入该原子范围的概率。该方法的核心部分是代码，它使用gamma执行下一个状态的最佳行为的分布收缩，将本地奖励添加到分布中，并将结果投射回我们的原始原子。下面的函数就是这样做的:</h2></div></div></div><p class="calibre8">开始时，我们分配保存投影结果的数组。该函数需要一批带有形状(<code class="literal">batch_size</code>、<code class="literal">n_atoms</code>)、奖励数组、已完成剧集的标志以及我们的超参数:<code class="literal">Vmin</code>、<code class="literal">Vmax</code>、<code class="literal">n_atoms</code>和<code class="literal">gamma</code>的分布。<code class="literal">delta_z</code>变量是我们取值范围内每个原子的宽度。</p><p class="calibre8">在前面的代码中，我们对原始分布中的每个原子进行迭代，并计算这个原子将被Bellman算子投影的位置，同时考虑我们的值界限。例如，索引为0的第一个原子对应于值<code class="literal">Vmin=-10</code>，但是对于奖励为+1的样本，将被投影为值-10 * 0.99 + 1 = -8.9。换句话说，它将向右移动(假设我们的gamma=0.99)。如果值超出了由<code class="literal">Vmin</code>和<code class="literal">Vmax</code>给定的值范围，我们会将其限制在边界内。</p><p class="calibre8">在下一行中，我们计算我们的样本所投射的<a id="id295" class="calibre1"/>原子数。当然，样品可以被投射到原子之间。在这种情况下，我们将在源原子的原始分布中，在它所在的两个原子之间传播值。这种扩散应该小心处理，因为我们的目标原子可以准确地落在某个原子的位置上。在这种情况下，我们只需要将源分布值加到目标原子上。</p><div><pre class="programlisting">def distr_projection(next_distr, rewards, dones, Vmin, Vmax, n_atoms, gamma):
    batch_size = len(rewards)
    proj_distr = np.zeros((batch_size, n_atoms), dtype=np.float32)
    delta_z = (Vmax - Vmin) / (n_atoms - 1)</pre></div><p class="calibre8">上面的代码处理投射的原子正好落在目标原子上的情况。否则，<code class="literal">b_j</code>就不是整数值和变量<code class="literal">l</code>和<code class="literal">u</code>(对应于投影点上下原子的索引)。</p><div><pre class="programlisting">    for atom in range(n_atoms):
        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards + (Vmin + atom * delta_z) * gamma))</pre></div><p class="calibre8">当投影点落在原子之间时，我们需要将源原子的概率分散到下面和上面的原子之间。这是通过上面代码中的两行实现的，当然，我们需要正确处理剧集的最后过渡。在这种情况下，我们的预测不应该考虑下一次分配，只会有1的概率对应于获得的奖励。然而，我们需要再次考虑我们的原子，如果奖励值落在原子之间，适当地分配这个概率。这种情况由下面的代码分支处理，它将设置了done标志的样本的结果分布置零，然后计算结果投影。</p><div><pre class="programlisting">        b_j = (tz_j - Vmin) / delta_z</pre></div><p class="calibre8">为了给你一个关于<a id="id296" class="calibre1"/>这个函数做什么的例子，让我们来看看这个函数处理的人工生成的分布。我用它们来调试功能，并确保它按预期工作。这些检查的代码在<code class="literal">Chapter07/adhoc/distr_test.py</code>中。</p><div><pre class="programlisting">        l = np.floor(b_j).astype(np.int64)
        u = np.ceil(b_j).astype(np.int64)
        eq_mask = u == l
        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]</pre></div><p class="calibre8">图15:应用于正态分布的概率分布转换示例</p><div><pre class="programlisting">        ne_mask = u != l
        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]
        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]</pre></div><p class="calibre8">When the projected point lands between atoms, we need to spread the probability of the source atom between atoms below and above. This is carried out by two lines in the above code and, of course, we need to properly handle the final transitions of episodes. In that case, our projection shouldn't take into account the next distribution and will just have a 1 probability corresponding to the reward obtained. However, we need, again, to take into account our atoms and properly distribute this probability if the reward value falls between the atoms. This case is handled by the code branch below, which zeroes resulting distribution for samples with the done flag set and then calculates the resulting projection.</p><div><pre class="programlisting">    if dones.any():
        proj_distr[dones] = 0.0
        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))
        b_j = (tz_j - Vmin) / delta_z
        l = np.floor(b_j).astype(np.int64)
        u = np.ceil(b_j).astype(np.int64)
        eq_mask = u == l
        eq_dones = dones.copy()
        eq_dones[dones] = eq_mask
        if eq_dones.any():
            proj_distr[eq_dones, l] = 1.0
        ne_mask = u != l
        ne_dones = dones.copy()
        ne_dones[dones] = ne_mask
        if ne_dones.any():
            proj_distr[ne_dones, l] = (u - b_j)[ne_mask]
            proj_distr[ne_dones, u] = (b_j - l)[ne_mask]
    return proj_distr</pre></div><p class="calibre8"><a id="id297" class="calibre1"/>下面的第一幅图对应于用<em class="calibre11"> gamma=0.9 </em>投影的正态分布，并且用<em class="calibre11"> reward=2 </em>向右移动。在我们用相同的数据传递done=True的情况下，结果会有所不同。在这种情况下，资源分配将被完全忽略，结果将只有奖励计划。</p><div><img src="img/00172.jpeg" alt="Implementation" class="calibre9"/><div><p class="calibre14">图16:剧集最后一步的分布图</p></div></div><p class="calibre10"> </p><p class="calibre8">现在让我们看看<code class="literal">Chapter07/07_dqn_distrib.py</code>中方法的主要<a id="id298" class="calibre1"/>源代码。</p><div><img src="img/00173.jpeg" alt="Implementation" class="calibre9"/><div><p class="calibre14">开始时，我们将matplotlib切换到“无头”模式，此时它不需要显示来绘图。代码具有特殊的调试模式标志，这些标志允许保存概率分布以简化调试和训练过程的可视化(默认情况下它们是禁用的)。</p></div></div><p class="calibre10">然后我们定义我们的常数，它包括值分布的范围<code class="literal">Vmax</code>、<code class="literal">Vmin</code>，原子的数量，以及每个原子的宽度。</p><p class="calibre8">接下来的两个常量定义了<a id="id299" class="calibre1"/>我们在保持的缓冲区中保留了多少状态来执行平均值计算，以及该平均值更新的频率。这对于评估训练进度很有用，因为随着我们的代理在游戏中变得越来越好，它的Q值也在增长。</p><div><pre class="programlisting">SAVE_STATES_IMG = False
SAVE_TRANSITIONS_IMG = False

if SAVE_STATES_IMG or SAVE_TRANSITIONS_IMG:
    import matplotlib as mpl
    mpl.use("Agg")
    import matplotlib.pylab as plt</pre></div><p class="calibre8">这两个标志支持保存分布图，这对于调试很有用，但是会大大降低训练过程的速度。如果每10k帧启用第一个标志，我们将前200个状态的所有动作的预测分布保存在保持缓冲器中。由此产生的图像显示了这些状态的分布是如何从一开始的均匀分布收敛到更真实、更像高斯分布的。第二个标志允许保存具有非零奖励或最终剧集的批次的投影分布，这对于发现分布投影代码中的错误非常有帮助，并且对于可视化该方法的内部非常有用。</p><div><pre class="programlisting">Vmax = 10
Vmin = -10
N_ATOMS = 51
DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)</pre></div><p class="calibre8">NN构造器的主要区别是网络的输出。现在，不是大小<code class="literal">n_actions</code>的张量；这是一个由<code class="literal">n_actions * n_atoms</code>个元素组成的矩阵，包含每个动作的概率分布。使用batch dimension，得到的输出有三个维度。我们还用原子的值注册了torch张量，以便以后使用。</p><div><pre class="programlisting">STATES_TO_EVALUATE = 1000
EVAL_EVERY_FRAME = 100</pre></div><p class="calibre8">与基线DQN相比，<code class="literal">forward()</code>功能基本相同，除了最终形状需要调整。然而，<code class="literal">forward()</code>对于我们的目的来说是不够的。除了原始分布，我们还需要一批状态的分布和Q值。为了避免多重神经网络转换，我们将定义函数<code class="literal">both()</code>，它返回原始分布和Q值。q值将用于行动决策。当然，使用分布意味着我们可以有不同的行动选择策略，但是关于Q值的贪婪策略使得该方法可以与标准的DQN版本相媲美。</p><div><pre class="programlisting">SAVE_STATES_IMG = False
SAVE_TRANSITIONS_IMG = False</pre></div><p class="calibre8">为了从分布中获得Q值，我们只需要计算归一化分布和原子值的加权和。结果将是分布的期望值。</p><div><pre class="programlisting">class DistributionalDQN(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(DistributionalDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        conv_out_size = self._get_conv_out(input_shape)
        self.fc = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions * N_ATOMS)
        )

        self.register_buffer("supports", torch.arange(Vmin, Vmax+DELTA_Z, DELTA_Z))
        self.softmax = nn.Softmax(dim=1)</pre></div><p class="calibre8">剩下的两个函数是简单的效用函数。第一个只计算Q值，而第二个将<code class="literal">softmax</code>应用于输出张量，保持张量的正确形状。</p><div><pre class="programlisting">    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        batch_size = x.size()[0]
        fx = x.float() / 256
        conv_out = self.conv(fx).view(batch_size, -1)
        fc_out = self.fc(conv_out)
        return fc_out.view(batch_size, -1, N_ATOMS)</pre></div><p class="calibre8"><a id="id301" class="calibre1"/>分类DQN(作者也称其为C51，根据使用的原子数量)的损失函数的开始方式与之前相同:我们解开批处理并将数组转换为张量。</p><div><pre class="programlisting">    def both(self, x):
        cat_out = self(x)
        probs = self.apply_softmax(cat_out)
        weights = probs * self.supports
        res = weights.sum(dim=2)
        return cat_out, res</pre></div><p class="calibre8">稍后，我们将需要下一个状态的概率分布和Q值，因此我们使用对网络的<code class="literal">both()</code>调用，获得在下一个状态采取的最佳行动，将<code class="literal">softmax</code>应用于分布，并将其转换为数组。</p><div><pre class="programlisting">    def qvals(self, x):
        return self.both(x)[1]

    def apply_softmax(self, t):
        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())</pre></div><p class="calibre8">然后，我们提取最佳行动的分布，并使用贝尔曼算子执行它们的投影。预测的结果将是我们希望我们的网络输出看起来像什么的目标分布。</p><div><pre class="programlisting">def calc_loss(batch, net, tgt_net, gamma, device="cpu", save_prefix=None):
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)
    batch_size = len(batch)

    states_v = torch.tensor(states).to(device)
    actions_v = torch.tensor(actions).to(device)
    next_states_v = torch.tensor(next_states).to(device)</pre></div><p class="calibre8">在函数的最后，我们需要计算网络的输出，并计算投影分布和所采取行动的网络输出之间的KL-divergence。KL-divergence显示了两个分布之间的差异，并被定义为<img src="img/00174.jpeg" alt="Implementation" class="calibre24"/>。</p><div><pre class="programlisting">    # next state distribution
    next_distr_v, next_qvals_v = tgt_net.both(next_states_v)
    next_actions = next_qvals_v.max(1)[1].data.cpu().numpy()
    next_distr = tgt_net.apply_softmax(next_distr_v).data.cpu().numpy()</pre></div><p class="calibre8">为了计算<a id="id302" class="calibre1"/>概率的对数，我们使用PyTorch函数<code class="literal">log_softmax</code>，它以数值和稳定的方式执行<code class="literal">log</code>和<code class="literal">softmax</code>。训练循环和以前一样，除了<code class="literal">ptan.DQNAgent</code>创建中的一个例外，它需要使用函数<code class="literal">qvals()</code>，而不是模型本身。</p><div><pre class="programlisting">    next_best_distr = next_distr[range(batch_size), next_actions]
    dones = dones.astype(np.bool)
    proj_distr = common.distr_projection(next_best_distr, rewards, dones, Vmin, Vmax, N_ATOMS, gamma)</pre></div><p class="calibre8"><a id="ch07lvl2sec25" class="calibre1"/>结果</p><div><pre class="programlisting">    distr_v = net(states_v)
    state_action_values = distr_v[range(batch_size), actions_v.data]
    state_log_sm_v = F.log_softmax(state_action_values, dim=1)
    proj_distr_v = torch.tensor(proj_distr).to(device)
    loss_v = -state_log_sm_v * proj_distr_v
    return loss_v.sum(dim=1).mean()</pre></div><p class="calibre8">结果图<a id="id303" class="calibre1"/>如下，上面的线对应于基本DQN，下面的线来自C51训练。</p><p class="calibre8">图17:与基本DQN(上图)相比，分类DQN(下图)的收敛</p><div><pre class="programlisting">    agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), selector, device=device)</pre></div></div></div></body></html>


<html>
  <head>
    <title>Categorical DQN</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec25" class="calibre1"/>Results</h2></div></div></div><p class="calibre8">正如你所看到的，绝对DQN是收敛动态的唯一方法，它在开始时比经典的DQN更糟糕。然而，有一个因素保护了这种新方法:Pong是一个太简单的游戏，无法得出结论。在《分类DQN》的论文中，作者报告了超过一半的游戏在雅达利基准测试中的最新分数(Pong不在其中)。</p><div><img src="img/00175.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">在训练期间研究概率分布的动态可能是有趣的。该代码有两个标志，<code class="literal">SAVE_STATES_IMG</code>和<code class="literal">SAVE_TRANSITIONS_IMG</code>(默认禁用)，可以在训练期间保存概率分布图像。例如，下图显示了训练开始时(30k帧后)一个状态的所有六个动作的概率分布。</p></div></div><p class="calibre10">图18:训练开始时的概率分布</p><p class="calibre8">As you can see, categorical DQN is the only method of convergence dynamic, which, in the beginning is <em class="calibre11">worse</em> than the classic DQN. However, there is one factor that protects this new method: Pong is too simple a game to draw conclusions. In the Categorical DQN paper, the authors reported state-of-the-art scores for more than half of the games from the Atari benchmark (Pong is not among them).</p><p class="calibre8">所有的分布<a id="id304" class="calibre1"/>都非常宽(因为网络还没有收敛),中间的峰值对应于网络期望从其行为中获得的负回报。500k帧训练后的相同状态如下图所示:</p><div><img src="img/00176.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图19:由训练好的网络产生的概率分布</p></div></div><p class="calibre10">现在我们可以看到不同的动作有不同的分布。第一个动作(对应于NOOP，即<em class="calibre11">什么都不做</em>动作)已经分配到左边，所以在这种状态下什么都不做通常会导致失败。第五个动作是RIGHTFIRE，其平均值<a id="id305" class="calibre1"/>向右移动，因此该动作导致更好的分数。</p><p class="calibre8"><a id="ch07lvl1sec17" class="calibre1"/>结合一切</p><div><img src="img/00177.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">我们现在已经看到了论文<em class="calibre11"> [1] Rainbow:结合深度强化学习</em>中提到的所有<a id="id306" class="calibre1"/> DQN改进。让我们将所有这些方法结合成一个混合方法。首先，我们需要定义我们的网络架构以及促成这一架构的三种方法:</p></div></div><p class="calibre10"><strong class="calibre2">分类DQN </strong>:我们的网络会预测行动的价值概率分布。</p><p class="calibre8"><strong class="calibre2">决战DQN </strong>:我们的网络将有两条独立的路径进行价值状态分配和优势分配。在输出中，两条路径将被加在一起，提供动作的最终值概率分布。为了迫使优势分布具有零平均值，我们将减去每个原子中具有平均优势的分布。</p></div></div></body></html>


<html>
  <head>
    <title>Combining everything</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><strong class="calibre2">noise net</strong>:我们在价值和优势路径中的线性层将是<code class="literal">nn.Linear</code>的嘈杂变体。</h1></div></div></div><p class="calibre8">除了网络架构的变化，我们将使用优先重放缓冲区来保存环境转换，并根据KL-divergence按比例对其进行采样。最后，我们将把贝尔曼方程展开到<strong class="calibre2"> n步</strong>，并使用<strong class="calibre2">双DQN </strong>动作选择过程来防止高估状态值。</p><div><ul class="itemizedlist"><li class="listitem"><a id="ch07lvl2sec26" class="calibre1"/>实施</li><li class="listitem">前面列出的<a id="id307" class="calibre1"/>修改可能看起来很复杂，但是，事实上，所有的方法都很好地相互配合。完整的示例在<code class="literal">Chapter07/08_dqn_rainbow.py file</code>中:</li><li class="listitem">像往常一样，我们为将要使用的所有方法定义了<a id="id308" class="calibre1"/>超参数(为了节省空间，省略了导入)。</li></ul></div><p class="calibre8">我们网络的构造者不应该让你感到惊讶，因为我们以前已经见过它了。它将决斗的DQN、诺伊辛尼特和绝对的DQN结合到一个建筑中。值网络路径预测输入状态的值的分布，从而为我们提供每个批次样本的N个原子的单个向量。优势路径为我们在游戏中的每一个行动产生分布。</p></div></body></html>


<html>
  <head>
    <title>Combining everything</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">向前传递产生动作的值分布，这类似于分类DQN中的Q值。由于PyTorch的张量传播，通过准确地重塑价值和优势路径的输出，我们的回报表达式变得非常简单。</h2></div></div></div><p class="calibre8">这个想法是让我们想要添加的所有<a id="id309" class="calibre1"/>值具有相同的维数。比如价值路径会被重塑成(<code class="literal">batch_size</code>、<code class="literal">1</code>、<code class="literal">N_ATOMS</code>)，所以二次元会广播到优势路径中的所有动作。我们需要减去的基线优势是通过计算每个原子对所有行为的平均优势获得的。<code class="literal">keepdim=True</code>参数要求<code class="literal">mean()</code>调用保持第二维度，这产生了(<code class="literal">batch_size</code>、<code class="literal">1</code>、<code class="literal">N_ATOMS</code>)的张量。因此，基线优势也将被广播。</p><div><pre class="programlisting"># n-step
REWARD_STEPS = 2

# priority replay
PRIO_REPLAY_ALPHA = 0.6
BETA_START = 0.4
BETA_FRAMES = 100000

# C51
Vmax = 10
Vmin = -10
N_ATOMS = 51
DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)</pre></div><p class="calibre8">前述函数用于将概率分布组合成Q值，而无需多次调用网络。</p><div><pre class="programlisting">class RainbowDQN(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(RainbowDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        conv_out_size = self._get_conv_out(input_shape)
        self.fc_val = nn.Sequential(
            dqn_model.NoisyLinear(conv_out_size, 512),
            nn.ReLU(),
            dqn_model.NoisyLinear(512, N_ATOMS)
        )

        self.fc_adv = nn.Sequential(
            dqn_model.NoisyLinear(conv_out_size, 512),
            nn.ReLU(),
            dqn_model.NoisyLinear(512, n_actions * N_ATOMS)
        )

        self.register_buffer("supports", torch.arange(Vmin, Vmax+DELTA_Z, DELTA_Z))
        self.softmax = nn.Softmax(dim=1)</pre></div><p class="calibre8">最终函数将<code class="literal">softmax</code>应用于输出概率分布。</p><div><pre class="programlisting">    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        batch_size = x.size()[0]
        fx = x.float() / 256
        conv_out = self.conv(fx).view(batch_size, -1)
        val_out = self.fc_val(conv_out).view(batch_size, 1, N_ATOMS)
        adv_out = self.fc_adv(conv_out).view(batch_size, -1, N_ATOMS)
        adv_mean = adv_out.mean(dim=1, keepdim=True)
        return val_out + adv_out - adv_mean</pre></div><p class="calibre8">我们的损失函数接受与我们在优先重放缓冲区中看到的相同的参数集。除了带有训练数据的批处理数组之外，我们还传递每个样本的权重。</p><p class="calibre8">这里我们使用一个小技巧<a id="id310" class="calibre1"/>来加快我们的计算速度。因为双DQN方法要求我们使用我们的主网络来选择动作，但是使用目标网络来获得这些动作的值(在我们的例子中，值分布),所以我们需要向我们的主网络传递当前状态和下一个状态。前面我们计算了两次调用的网络输出，在GPU上效率不是很高。现在，我们将当前状态和下一个状态连接成一个张量，并在一次网络传递中获得结果，稍后再拆分结果。我们需要计算Q值和原始值的分布，因为我们的动作选择策略仍然是贪婪的:我们选择具有最大Q值的动作。</p><div><pre class="programlisting">    def both(self, x):
        cat_out = self(x)
        probs = self.apply_softmax(cat_out)
        weights = probs * self.supports 
        res = weights.sum(dim=2)
        return cat_out, res

    def qvals(self, x):
        return self.both(x)[1]</pre></div><p class="calibre8">在前面的几行中，我们决定了在下一个状态中要采取的动作，并使用我们的目标网络获得这些动作的分布。所以，上面的<code class="literal">net/tgt_net</code>洗牌实现了双DQN方法。然后，我们将<code class="literal">softmax</code>应用于这些最佳行动的分布，并将数据复制到CPU中以执行贝尔曼预测。</p><div><pre class="programlisting">    def apply_softmax(self, t):
        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())</pre></div><p class="calibre8">在前面的代码中，我们使用贝尔曼方程计算投影分布。这个结果将作为我们KL-divergence的一个目标。</p><div><pre class="programlisting">def calc_loss(batch, batch_weights, net, tgt_net, gamma, device="cpu"):
    states, actions, rewards, dones, next_states = common.unpack_batch(batch)
    batch_size = len(batch)

    states_v = torch.tensor(states).to(device)
    actions_v = torch.tensor(actions).to(device)
    next_states_v = torch.tensor(next_states).to(device)
    batch_weights_v = torch.tensor(batch_weights).to(device)</pre></div><p class="calibre8">这里，我们获得了所采取措施的分布，并应用<code class="literal">log_softmax</code>来计算损失。</p><div><pre class="programlisting">    distr_v, qvals_v = net.both(torch.cat((states_v, next_states_v)))
    next_qvals_v = qvals_v[batch_size:]
    distr_v = distr_v[:batch_size]</pre></div><p class="calibre8">在函数的最后几行中，我们计算KL-divergence损失，将其乘以权重并返回两个量<a id="id311" class="calibre1"/>:将在优化器步骤中使用的组合损失和批处理的单个损失值，它们将在重放缓冲区中用作优先级。模块的其余部分包含初始化和训练循环，您应该很熟悉。</p><div><pre class="programlisting">    next_actions_v = next_qvals_v.max(1)[1]
    next_distr_v = tgt_net(next_states_v)
    next_best_distr_v = next_distr_v[range(batch_size), next_actions_v.data]
    next_best_distr_v = tgt_net.apply_softmax(next_best_distr_v)
    next_best_distr = next_best_distr_v.data.cpu().numpy()</pre></div><p class="calibre8">在前面的代码中，我们创建了我们需要的一切，包括我们的自定义网络、经验源、优先重放缓冲区和优化器。</p><div><pre class="programlisting">    dones = dones.astype(np.bool)
    proj_distr = common.distr_projection(next_best_distr, rewards, dones, Vmin, Vmax, N_ATOMS, gamma)</pre></div><p class="calibre8"><a id="ch07lvl2sec27" class="calibre1"/>战果</p><div><pre class="programlisting">    state_action_values = distr_v[range(batch_size), actions_v.data]
    state_log_sm_v = F.log_softmax(state_action_values, dim=1)</pre></div><p class="calibre8">这里显示了<a id="id312" class="calibre1"/>聚合代理的培训动态。</p><div><pre class="programlisting">    proj_distr_v = torch.tensor(proj_distr)
    loss_v = -state_log_sm_v * proj_distr_v
    loss_v = batch_weights_v * loss_v.sum(dim=1)
    return loss_v.mean(), loss_v + 1e-5</pre></div><p class="calibre8">图20:组合方法(浅色)与基本DQN(深色)的收敛动态</p><div><pre class="programlisting">if __name__ == "__main__":
    params = common.HYPERPARAMS['pong']
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")

    env = gym.make(params['env_name'])
    env = ptan.common.wrappers.wrap_dqn(env)

    writer = SummaryWriter(comment="-" + params['run_name'] + "-rainbow")
    net = RainbowDQN(env.observation_space.shape, env.action_space.n).to(device)
    tgt_net = ptan.agent.TargetNet(net)
    agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), ptan.actions.ArgmaxActionSelector(), device=device)

    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=REWARD_STEPS)
    buffer = ptan.experience.PrioritizedReplayBuffer(exp_source, params['replay_size'], PRIO_REPLAY_ALPHA)
    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])</pre></div><p class="calibre8">In the preceding code we create everything we need, including our custom network, experience source, prioritized replay buffer and optimizer.</p><div><pre class="programlisting">    frame_idx = 0
    beta = BETA_START

    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:
        while True:
            frame_idx += 1
            buffer.populate(1)
            beta = min(1.0, BETA_START + frame_idx * (1.0 - BETA_START) / BETA_FRAMES)

            new_rewards = exp_source.pop_total_rewards()
            if new_rewards:
                if reward_tracker.reward(new_rewards[0], frame_idx):
                    break

            if len(buffer) &lt; params['replay_initial']:
                continue

            optimizer.zero_grad()
            batch, batch_indices, batch_weights = buffer.sample(params['batch_size'], beta)
            loss_v, sample_prios_v = calc_loss(batch, batch_weights, net, tgt_net.target_model, params['gamma'] ** REWARD_STEPS, device=device)
            loss_v.backward()
            optimizer.step()
            buffer.update_priorities(batch_indices, sample_prios_v.data.cpu().numpy())

            if frame_idx % params['target_net_sync'] == 0:
                tgt_net.sync()</pre></div></div></div></body></html>


<html>
  <head>
    <title>Combining everything</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">如果你把我们所有的方法放在一个角度来看，在Pong游戏中，聚合代理没有最好的训练动态，例如决斗DQN或NoisyNets单独收敛稍快。然而，Pong并不复杂，由于其简单性和快速收敛，被选为本章的基准。作为额外的练习，您可以在Atari套件的不同游戏上检查这些方法。</h2></div></div></div><p class="calibre8">在<code class="literal">lib/common.py</code>模块中，你<a id="id313" class="calibre1"/>可以找到更接近研究人员在基准测试中使用的设置的超参数，但请记住，在复杂的游戏上达到最先进的结果可能需要50-100米的帧，这可能需要一周的训练。</p><div><img src="img/00178.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14"><a id="ch07lvl1sec18" class="calibre1"/>总结</p></div></div><p class="calibre10">在这一章中，我们走过并实现了自2015年第一篇DQN论文发表以来研究人员发现的许多DQN改进。这份清单远非完整。首先，对于方法列表，我用过论文，<em class="calibre11">【1】彩虹:结合深度强化学习的改进</em>，是DeepMind发表的，所以方法列表肯定是偏向DeepMind论文的。其次，RL现在非常活跃，几乎每天都有新的论文出现，这使得我们很难跟上，即使我们将自己局限于一种RL模型，如DQN。这一章的目的是给你一个该领域发展的不同观点的实际观点。</p><p class="calibre8">在下一章，我们将把我们的DQN知识应用到真实的股票交易场景中。</p><p class="calibre8"><a id="ch07lvl1sec19" class="calibre1"/>参考文献</p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><em class="calibre11"> Matteo Hessel，Joseph Modayil，Hado van Hasselt，Tom Schaul，Georg Ostrovski，Will Dabney，Dan Horgan，Bilal Piot，Mohammad Azar，David Silver，2017，Rainbow:结合深度强化学习的改进。arXiv:1710.02298 </em></h1></div></div></div><p class="calibre8"><em class="calibre11"> Sutton，R.S. 1988，用时间差异方法学习预测，机器学习3(1):9-44 </em></p><p class="calibre8"><em class="calibre11"> Hado Van Hasselt，Arthur Guez，David Silver，2015，采用双Q学习的深度强化学习。arXiv:1509.06461v3 </em></p></div></body></html>


<html>
  <head>
    <title>References</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><em class="calibre11"> Meire Fortunato、Mohammad Gheshlaghi Azar、Bilal Pilot、Jacob Menick、Ian Osband、Alex Graves、Vlad Mnih、Remi Munos、戴密斯·哈萨比斯、Olivier Pietquin、Charles Blundell、Shane Legg，2017年,《探索的嘈杂网络》, arXiv:1706.10295v1 </em></h1></div></div></div><div><ol class="orderedlist"><li class="listitem" value="1"><em class="calibre11"> Marc Bellemare，Sriram Srinivasan，Georg Ostrovski，Tom Schaus，David Saxton，Remi Munos 2016，统一基于计数的探索和内在动机arXiv:1606.01868v2 </em></li><li class="listitem" value="2"><em class="calibre11"> Jarryd Martin，Suraj Narayanan Sasikumar，Tom Everitt，Marcus Hutter，2017，基于计数的特征空间探索用于强化学习arXiv:1706.08090 </em></li><li class="listitem" value="3"><em class="calibre11"> Tom Schaul，John Quan，Ioannis Antonoglou，David Silver，2015年，优先体验回放arXiv:1511.05952 </em></li><li class="listitem" value="4"><em class="calibre11">王子瑜，汤姆·绍尔，马特奥·赫塞尔，哈多·范·哈瑟尔特，马克·兰托特，南多·德·弗莱塔斯，2015，深度强化学习的决斗网络架构arXiv:1511.06581 </em></li><li class="listitem" value="5"><em class="calibre11"> Marc G. Bellemare，Will Dabney，Rémi Munos，2017，强化学习的分布视角arXiv:1707.06887 </em></li><li class="listitem" value="6"><em class="calibre11">Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, Marcus Hutter, 2017, Count-Based Exploration in Feature Space for Reinforcement Learning arXiv:1706.08090</em></li><li class="listitem" value="7"><em class="calibre11">Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, 2015, Prioritized Experience Replay arXiv:1511.05952</em></li><li class="listitem" value="8"><em class="calibre11">Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas, 2015, Dueling Network Architectures for Deep Reinforcement Learning arXiv:1511.06581</em></li><li class="listitem" value="9"><em class="calibre11">Marc G. Bellemare, Will Dabney, Rémi Munos, 2017, A Distributional Perspective on Reinforcement Learning arXiv:1707.06887</em></li></ol><div/></div></div></body></html>
</body></html>