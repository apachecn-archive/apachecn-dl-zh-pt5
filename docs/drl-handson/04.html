<html><head/><body>
<html>
  <head>
    <title>Chapter 4. The Cross-Entropy Method</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>第四章。交叉熵方法</h1></div></div></div><p class="calibre8">在这一章中，我们将总结这本书的第一部分，并熟悉RL方法之一——交叉熵。尽管它不如RL实践者工具箱中的其他工具有名，如<strong class="calibre2">深度Q网</strong> ( <strong class="calibre2"> DQN </strong>)或优势演员评论家，但这种方法有自己的优势。最重要的如下:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">简单性</strong>:交叉熵方法真的很简单，这使得它成为一种可以遵循的直观方法。例如，它在PyTorch上的实现不到100行代码。</li><li class="listitem"><strong class="calibre2">良好的收敛性</strong>:在简单的环境中，不需要学习和发现复杂的、多步骤的策略，并且具有频繁奖励的短插曲，交叉熵通常非常有效。当然，许多实际问题不属于这一类，但有时它们属于这一类。在这种情况下，交叉熵(就其本身或作为更大系统的一部分)可能是最合适的。</li></ul></div><p class="calibre8">在接下来的章节中，我们将从交叉熵的实用方面开始，然后看看它在Gym中的两种环境(我们熟悉的CartPole和FrozenLake的“网格世界”)中是如何工作的。然后，在本章的最后，我们将看一看该方法的理论背景。这一部分是可选的，需要更多的概率和统计知识，但是如果你想了解为什么这个方法有效，你可以深入研究。</p></div></body></html>


<html>
  <head>
    <title>Chapter 4. The Cross-Entropy Method</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec28" class="calibre1"/>RL方法的分类</h1></div></div></div><p class="calibre8"><a id="id125" class="calibre1"/>交叉熵方法属于<em class="calibre11">无模型</em>和<em class="calibre11">基于策略</em>的方法类别。这些概念是新的，所以让我们花一些时间来探索它们。RL中的所有方法都可以分为不同的方面:</p><div><ul class="itemizedlist"><li class="listitem">无模型还是基于模型</li><li class="listitem">基于价值还是基于政策</li><li class="listitem">符合政策还是不符合政策</li></ul></div><p class="calibre8">还有其他方法可以对RL方法进行分类，但是现在我们只对前三种感兴趣。让我们来定义它们，因为您的问题细节会影响您对特定方法的决策。</p><p class="calibre8">术语<strong class="calibre2">无模型</strong>意味着该方法不建立环境或奖励的模型；它只是将观察与行动(或与行动相关的值)直接联系起来。换句话说，代理接受当前的观察并对其进行一些计算，结果就是它应该采取的行动。相反，<strong class="calibre2">基于模型的</strong>方法试图预测下一次观察和/或奖励会是什么。基于这种预测，代理人试图选择最好的可能行动，经常多次做出这样的预测，以看到未来越来越多的步骤。</p><p class="calibre8">这两类方法都有优缺点，但通常纯基于模型的方法用于确定性环境，如有严格规则的棋盘游戏。另一方面，无模型方法通常更容易训练，因为很难用丰富的观察数据建立复杂环境的良好模型。本书中描述的所有方法都来自无模型类别，因为这些方法在过去几年中一直是最活跃的研究领域。直到最近，研究人员才开始将两个世界的好处结合起来(例如，参考DeepMind关于智能体想象力的论文。这种方法将在<a class="calibre1" title="Chapter 17. Beyond Model-Free – Imagination" href="part0124_split_000.html#3M85O1-ce551566b6304db290b61e4d70de52ee">第17章</a>、<em class="calibre11">超越无模型-想象</em>中描述。</p><p class="calibre8">从另一个角度来看，<strong class="calibre2">基于策略的</strong>方法直接逼近代理的策略，即代理在每一步应该执行什么动作。策略通常由可用动作的概率分布来表示。相反，该方法可以是基于值的。在这种情况下，代理计算每个可能动作的值，并选择具有最佳值的动作，而不是动作的概率。这两类方法同样受欢迎，我们将在本书的下一部分讨论基于值的方法。政策方法将是第三部分的主题。</p><p class="calibre8">方法的第三个重要分类是<strong class="calibre2">符合策略的</strong>与<strong class="calibre2">不符合策略的</strong>。我们将在本书的第二部分和第三部分详细讨论这种区别，但是现在，将非策略解释为该方法在旧的历史数据上学习的能力就足够了(由代理的先前版本获得，或者由人类演示记录，或者仅由同一代理在几集之前看到)。</p><p class="calibre8">因此，我们的交叉熵方法是无模型的、基于策略的和基于策略的，这意味着:</p><div><ul class="itemizedlist"><li class="listitem">它没有建立任何环境模型；它只是告诉代理每一步要做什么</li><li class="listitem">它近似于代理的策略</li><li class="listitem">它需要从环境中获得新的数据</li></ul></div></div></div></body></html>


<html>
  <head>
    <title>Practical cross-entropy</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec29" class="calibre1"/>实用交叉熵</h1></div></div></div><p class="calibre8">交叉熵方法<a id="id127" class="calibre1"/>描述分为两个不相等的部分:实践和理论。实际部分本质上是直观的，而对<em class="calibre11">为什么</em>交叉熵起作用的理论解释以及正在发生的事情更加复杂。</p><p class="calibre8">你可能还记得，RL中最核心、最棘手的事情是代理人，它试图通过与环境交流来积累尽可能多的总回报。在实践中，我们遵循一种常见的ML方法，并用某种非线性可训练函数代替代理的所有复杂性，该函数将代理的输入(来自环境的观察)映射到一些输出。这个函数产生的输出的细节可能取决于一个特定的方法或一系列方法，如前一节所述(例如基于值的方法和基于策略的方法)。由于我们的交叉熵方法是基于策略的，我们的非线性函数(神经网络)产生<em class="calibre11">策略</em>，它基本上为每个观察指出代理应该采取的动作。</p><div><img src="img/00029.jpeg" alt="Practical cross-entropy" class="calibre9"/><div><p class="calibre14">图1:RL的高级方法</p></div></div><p class="calibre10"> </p><p class="calibre8">在实践中，策略通常表示为动作的概率分布，这使得它非常类似于分类问题，类的数量等于我们可以执行的动作的数量。这种抽象使得我们的代理非常简单:它需要将一个观察从环境传递到网络，获得动作的概率分布，并使用概率分布执行随机采样以获得要执行的动作。这种随机抽样给我们的代理增加了随机性，这是一件好事，因为在训练开始时，我们的权重是随机的，代理的行为也是随机的。在代理获得一个要发布的动作后，它向环境触发该动作，并获得下一个观察和对上一个动作的奖励。然后循环继续。</p><p class="calibre8">在代理人的一生中，它的经验是作为插曲出现的。每一集都是代理人从环境中获得的一系列观察结果，它发布的行动，以及对这些行动的奖励。想象一下，我们的经纪人演了好几集这样的戏。对于每一集，我们都可以计算代理人要求的总报酬。可以打折，也可以不打折，但为了简单起见，我们假设打折因子gamma = 1，也就是说只是每集所有本地奖励的总和。这个总报酬显示了《T2》这一集对代理人有多好。让我们用一个图表来说明这一点，它包含四集(注意，不同的集对<img src="img/00030.jpeg" alt="Practical cross-entropy" class="calibre24"/>、<img src="img/00031.jpeg" alt="Practical cross-entropy" class="calibre24"/>和<img src="img/00032.jpeg" alt="Practical cross-entropy" class="calibre24"/>有不同的值):</p><div><img src="img/00033.jpeg" alt="Practical cross-entropy" class="calibre9"/><div><p class="calibre14">图2:带有观察、行动和奖励的示例情节</p></div></div><p class="calibre10">每一个单元格都代表了特工在这一集里的一步。由于环境的随机性和代理人选择采取行动的方式，有些情节会比其他情节更好。交叉熵方法的核心是扔掉不好的剧集，在更好的剧集上训练。因此，该方法的步骤如下:</p><p class="calibre8">使用我们当前的模型和环境播放<em class="calibre11"> N </em>集。</p><div><ol class="orderedlist"><li class="listitem" value="1">计算每集的总奖励，并确定一个奖励范围。通常，我们使用所有奖励的某个百分比，例如第50或第70。</li><li class="listitem" value="2">扔掉所有低于界限的有奖励的剧集。</li><li class="listitem" value="3">将观察结果作为输入，将发布的行动作为期望的输出，对剩余的“精华”片段进行培训。</li><li class="listitem" value="4">从第一步开始重复，直到我们对结果满意为止。</li><li class="listitem" value="5">所以，这就是关于交叉熵方法的描述。通过前面的程序，我们的神经网络学习如何重复动作，这导致了更大的奖励，不断地将边界移动得越来越高。尽管这种方法简单，但它在简单的环境中工作得很好，易于实现，并且对超参数变化非常鲁棒，这使它成为一种理想的基线方法。现在让我们将它应用到我们的CartPole环境中。</li></ol><div/></div><p class="calibre8"><a id="ch04lvl1sec30" class="calibre1"/>柱子上的交叉熵</p></div></body></html>


<html>
  <head>
    <title>Cross-entropy on CartPole</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">这个例子的全部代码在<code class="literal">Chapter04/01_cartpole.py</code>中，但是下面是最重要的部分。我们模型的<a id="id129" class="calibre1"/>核心是一个单隐层神经网络，有ReLU和128个隐藏神经元(这是绝对任意的)。其他超参数也几乎是随机设置的，并且不进行调整，因为该方法是鲁棒的并且收敛非常快。</h1></div></div></div><p class="calibre8">我们在文件的顶部定义了常数，它们包括隐藏层中神经元的数量，我们在每次迭代中播放的剧集数量(16)，以及我们用于精英剧集过滤的剧集总回报的百分比。我们将采用第70个百分位数，这意味着我们将把前30%的剧集按奖励排序:</p><div><pre class="programlisting">HIDDEN_SIZE = 128
BATCH_SIZE = 16
PERCENTILE = 70</pre></div><p class="calibre8">我们的网络没有什么特别的；它将来自环境的单个观察作为输入向量，并为我们可以执行的每个动作输出一个数字。网络的输出是动作的概率分布，因此一种简单的方法是在最后一层之后加入softmax非线性。然而，在前面的网络中，我们没有应用softmax来增加训练过程的数值稳定性。我们将使用PyTorch类<code class="literal">nn.CrossEntropyLoss</code>，它将softmax和交叉熵结合在一个更稳定的表达式中，而不是先计算softmax(使用取幂运算)然后计算交叉熵损失(使用概率的对数)。<code class="literal">CrossEntropyLoss</code>需要来自网络的原始的、非标准化的值(也称为logits ),这样做的缺点是，每次我们需要从网络输出中获取概率时，我们都需要记住应用softmax。</p><div><pre class="programlisting">class Net(nn.Module):
    def __init__(self, obs_size, hidden_size, n_actions):
        super(Net, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, n_actions)
        )

    def forward(self, x):
        return self.net(x)</pre></div><p class="calibre8">这里我们将定义两个助手类，它们被命名为来自标准库中<code class="literal">collections</code>包的元组:</p><div><pre class="programlisting">Episode = namedtuple('Episode', field_names=['reward', 'steps'])
EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])</pre></div><p class="calibre8"><code class="literal">EpisodeStep</code>:这将用于表示我们的代理在该集中执行的单个步骤，它存储了对环境的观察和代理完成的操作。我们将使用精英剧集中的剧集步骤作为训练数据。</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">Episode</code>:存储为总未打折奖励和<code class="literal">EpisodeStep</code>集合的单集。</li><li class="listitem">让我们来看一个生成包含剧集的批处理的函数:</li></ul></div><p class="calibre8">前面的函数接受<a id="id130" class="calibre1"/>环境(Gym库中的<code class="literal">Env</code>类实例)、我们的神经网络，以及它应该在每次迭代中生成的剧集数。<code class="literal">batch</code>变量将用于累积我们的批处理(这是一个<code class="literal">Episode</code>实例的列表)。我们还为当前剧集及其步骤列表声明了一个奖励计数器(<code class="literal">EpisodeStep</code>对象)。然后，我们重置我们的环境以获得第一个观察结果，并创建一个softmax层，它将用于将网络的输出转换为行动的概率分布。我们的准备工作到此为止；因此，我们准备开始环境循环:</p><div><pre class="programlisting">def iterate_batches(env, net, batch_size):
    batch = []
    episode_reward = 0.0
    episode_steps = []
    obs = env.reset()
    sm = nn.Softmax(dim=1)</pre></div><p class="calibre8">在每一次迭代中，我们将当前的观察结果转换为PyTorch张量，并将其传递给网络以获得行动概率。这里有几点需要注意:</p><div><pre class="programlisting">    while True:
        obs_v = torch.FloatTensor([obs])
        act_probs_v = sm(net(obs_v))
        act_probs = act_probs_v.data.numpy()[0]</pre></div><p class="calibre8">PyTorch中的所有<code class="literal">nn.Module</code>实例都需要一批数据项，我们的网络也是如此，因此我们将观察值(CartPole中的四个数字的向量)转换为大小为1 × 4的张量(为了实现这一点，我们在单元素列表中传递一个观察值)。</p><div><ul class="itemizedlist"><li class="listitem">由于我们没有在网络输出中使用非线性，它输出原始动作得分，我们需要通过softmax函数输入这些得分。</li><li class="listitem">我们的网络和softmax层都返回跟踪梯度的张量，因此我们需要通过访问<code class="literal">tensor.data</code>字段，然后将张量转换为NumPy数组来解包。该数组将具有与输入相同的二维结构，批次维度在轴0上，因此我们需要获取第一个批次元素以获得动作概率的一维向量:<div> <pre class="programlisting">        action = np.random.choice(len(act_probs), p=act_probs)         next_obs, reward, is_done, _ = env.step(action)</pre> </div></li><li class="listitem">现在我们有了动作的概率分布，我们可以通过使用NumPy函数<code class="literal">random.choice()</code>对这个分布进行采样，使用这个分布来获得当前步骤的实际动作。在此之后，我们将把这个动作传递给环境，以获得我们的下一次观察、我们的奖励以及剧集结束的指示:</li></ul></div><p class="calibre8">奖励被添加到当前<a id="id131" class="calibre1"/>集的总奖励中，我们的集步骤列表也增加了一个(观察、动作)对。注意，我们保存了用于选择动作的观察，而不是由环境作为动作的结果返回的观察。这些是你需要记住的微小但重要的细节。</p><div><pre class="programlisting">        episode_reward += reward
        episode_steps.append(EpisodeStep(observation=obs, action=action))</pre></div><p class="calibre8">这就是我们在当前一集结束时处理这种情况的方式(在掷骰子的情况下，尽管我们做了努力，但当棍子落下时，这一集就结束了)。我们将最终完成的剧集添加到批处理中，保存总奖励(因为剧集已经完成，我们已经累积了所有奖励)和我们已经采取的步骤。然后，我们重置总奖励累计器，并清除步骤列表。之后，我们重置环境，重新开始。</p><div><pre class="programlisting">        if is_done:
            batch.append(Episode(reward=episode_reward, steps=episode_steps))
            episode_reward = 0.0
            episode_steps = []
            next_obs = env.reset()
            if len(batch) == batch_size:
                yield batch
                batch = []</pre></div><p class="calibre8">如果我们的批处理达到了期望的剧集数，我们使用<code class="literal">yield</code>将它返回给调用者进行处理。我们的函数是一个生成器，所以每次执行<code class="literal">yield</code>操作符时，控制被转移到外部迭代循环，然后在<code class="literal">yield</code>行之后继续。如果您不熟悉Python的生成器函数，请参考Python文档。处理后，我们将清理该批次:</p><p class="calibre8">循环的最后一步，也是非常重要的一步，是将从环境中获得的一个观察值赋给我们当前的观察变量。之后一切无限重复:我们把观察传到网上，采样要执行的动作，要求环境处理动作，记住这个处理的结果。</p><div><pre class="programlisting">        obs = next_obs</pre></div><p class="calibre8">在这个功能逻辑中需要理解的一个非常重要的事实是，我们网络的训练和我们剧集的生成是在同时<em class="calibre11">执行的。它们不是完全并行的，但是每当我们的循环积累了足够多的片段(16)时，它就将控制传递给这个函数调用者，这个函数调用者应该使用梯度下降来训练网络。因此，当返回<code class="literal">yield</code>时，网络将有不同的、稍微好一点的(我们希望)行为。</em></p><p class="calibre8">我们不需要探索适当的<a id="id132" class="calibre1"/>同步，因为我们的训练和数据收集活动是在同一个执行线程中进行的，但是您需要理解那些从网络训练到其利用的不断跳跃。</p><p class="calibre8">好了，现在我们需要定义另一个函数，我们将准备好切换到训练循环:</p><p class="calibre8">这个函数是交叉熵方法的核心:根据给定的一批剧集和百分位值，它计算一个边界奖励，用于筛选精英剧集进行训练。为了获得边界奖励，我们使用NumPy的percentile函数，它从值列表和所需的百分位中计算百分位的值。然后我们会计算均值奖励，只用于监控。</p><div><pre class="programlisting">def filter_batch(batch, percentile):
    rewards = list(map(lambda s: s.reward, batch))
    reward_bound = np.percentile(rewards, percentile)
    reward_mean = float(np.mean(rewards))</pre></div><p class="calibre8">接下来，我们将过滤掉我们的剧集。对于该批次中的每一集，我们将检查该集的总回报是否高于我们的界限，如果是，我们将填充我们将训练的观察和行动列表。</p><div><pre class="programlisting">    train_obs = []
    train_act = []
    for example in batch:
        if example.reward &lt; reward_bound:
            continue
        train_obs.extend(map(lambda step: step.observation, example.steps))
        train_act.extend(map(lambda step: step.action, example.steps))</pre></div><p class="calibre8">作为函数的最后一步，我们将把我们的观察和行动从精英剧集转换成张量，并返回一个四元组:观察、行动、奖励的边界和平均奖励。最后两个值将仅用于将它们写入TensorBoard，以检查我们代理的性能。</p><div><pre class="programlisting">    train_obs_v = torch.FloatTensor(train_obs)
    train_act_v = torch.LongTensor(train_act)
    return train_obs_v, train_act_v, reward_bound, reward_mean</pre></div><p class="calibre8">现在，将所有内容粘合在一起的最后一段代码主要由训练循环组成，如下所示:</p><p class="calibre8">一开始，我们将创建所有<a id="id133" class="calibre1"/>需要的对象:环境、我们的神经网络、目标函数、优化器和TensorBoard的摘要编写器。注释行创建一个监视器来记录您的代理的表现视频。</p><div><pre class="programlisting">

if __name__ == "__main__":
    env = gym.make("CartPole-v0")
    # env = gym.wrappers.Monitor(env, directory="mon", force=True)
    obs_size = env.observation_space.shape[0]
    n_actions = env.action_space.n

    net = Net(obs_size, HIDDEN_SIZE, n_actions)
    objective = nn.CrossEntropyLoss()
    optimizer = optim.Adam(params=net.parameters(), lr=0.01)
    writer = SummaryWriter()</pre></div><p class="calibre8">在训练循环中，我们将迭代我们的批处理(这是一个<code class="literal">Episode</code>对象的列表)，然后我们使用<code class="literal">filter_batch</code>函数执行精英剧集的过滤。结果是观察和采取行动的变量、用于过滤的奖励边界和平均奖励。之后，我们将网络的梯度归零，并将观察结果传递给网络，从而获得其动作得分。这些分数被传递给目标函数，该函数计算网络输出和代理采取的动作之间的交叉熵。这样做的目的是为了加强我们的网络来执行那些“精英”行动，这些行动已经带来了很好的回报。然后，我们将计算损耗的梯度，并要求优化器调整我们的网络。</p><div><pre class="programlisting">    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):
        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)
        optimizer.zero_grad()
        action_scores_v = net(obs_v)
        loss_v = objective(action_scores_v, acts_v)
        loss_v.backward()
        optimizer.step()</pre></div><p class="calibre8">循环的其余部分主要是进度监控。在控制台上，我们显示迭代次数、损失、批次的平均回报和回报边界。我们还将相同的值写入TensorBoard，以获得代理学习表现的漂亮图表。</p><div><pre class="programlisting">        print("%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f" % (
            iter_no, loss_v.item(), reward_m, reward_b))
        writer.add_scalar("loss", loss_v.item(), iter_no)
        writer.add_scalar("reward_bound", reward_b, iter_no)
        writer.add_scalar("reward_mean", reward_m, iter_no)</pre></div><p class="calibre8">循环中的最后一个检查是我们的批量剧集的平均奖励的比较。当这变得大于<code class="literal">199</code>时，我们<a id="id134" class="calibre1"/>停止我们的训练。为什么是<code class="literal">199</code>？在Gym中，当最后100集的平均奖励大于195时，认为解决了CartPole环境，但我们的方法收敛得如此之快，以至于100集通常是我们需要的。经过适当培训的代理可以无限长地平衡棍子(获得任何数量的分数)，但CartPole中的剧集长度被限制在200步(如果您查看CartPole的环境变量，您可能会注意到<code class="literal">TimeLimit</code>包装器，它在200步后停止剧集)。考虑到这一点，我们将在批次中的平均奖励大于<code class="literal">199</code>后停止训练，这很好地表明我们的代理人作为专业人士知道如何平衡大棒。</p><div><pre class="programlisting">        if reward_m &gt; 199:
            print("Solved!")
            break
    writer.close()</pre></div><p class="calibre8">就是这样。所以让我们开始第一次RL训练吧！</p><p class="calibre8">代理解决环境一般不超过50批。我的实验显示了25到45集的内容，这是一个非常好的学习表现(记住，我们只需要每批播放16集)。TensorBoard显示我们的代理一直在进步，几乎每一批都达到了上限(有一些下降的时期，但大多数时间都有所改善)。</p><div><pre class="programlisting">rl_book_samples/Chapter04$ ./01_cartpole.py
[2017-10-04 12:44:39,319] Making new env: CartPole-v0
0: loss=0.701, reward_mean=18.0, reward_bound=21.0
1: loss=0.682, reward_mean=22.6, reward_bound=23.5
2: loss=0.688, reward_mean=23.6, reward_bound=25.5
3: loss=0.675, reward_mean=22.8, reward_bound=22.0
4: loss=0.658, reward_mean=31.9, reward_bound=34.0
.........
36: loss=0.527, reward_mean=135.9, reward_bound=168.5
37: loss=0.527, reward_mean=147.4, reward_bound=160.5
38: loss=0.528, reward_mean=179.8, reward_bound=200.0
39: loss=0.530, reward_mean=178.7, reward_bound=200.0
40: loss=0.532, reward_mean=192.1, reward_bound=200.0
41: loss=0.523, reward_mean=196.8, reward_bound=200.0
42: loss=0.540, reward_mean=200.0, reward_bound=200.0
Solved!</pre></div><p class="calibre8">图3:培训期间的损失、回报界限和回报</p><div><img src="img/00034.jpeg" alt="Cross-entropy on CartPole" class="calibre9"/><div><p class="calibre14">Figure 3: Loss, reward boundary, and reward during the training</p></div></div><p class="calibre10">为了检查我们的代理的运行情况，您可以在环境创建后通过取消注释下一行来启用<code class="literal">Monitor</code>。重启后(可能用<code class="literal">xvfb-run</code>提供虚拟X11显示)，我们的程序<a id="id135" class="calibre1"/>将创建一个<code class="literal">mon</code>目录，其中包含在不同训练步骤录制的视频:</p><p class="calibre8">正如你从输出中看到的，它<a id="id136" class="calibre1"/>将代理活动的定期记录转换成单独的视频文件，这可以让你了解你的代理的会话是什么样子的。</p><div><pre class="programlisting">rl_book_samples/Chapter04$ xvfb-run -s "-screen 0 640x480x24" ./01_cartpole.py
[2017-10-04 13:52:23,806] Making new env: CartPole-v0
[2017-10-04 13:52:23,814] Creating monitor directory mon
[2017-10-04 13:52:23,920] Starting new video recorder writing to mon/openaigym.video.0.4430.video000000.mp4
[2017-10-04 13:52:25,229] Starting new video recorder writing to mon/openaigym.video.0.4430.video000001.mp4
[2017-10-04 13:52:25,771] Starting new video recorder writing to mon/openaigym.video.0.4430.video000008.mp4
0: loss=0.682, reward_mean=18.9, reward_bound=20.5
[2017-10-04 13:52:26,297] Starting new video recorder writing to mon/openaigym.video.0.4430.video000027.mp4
1: loss=0.687, reward_mean=16.6, reward_bound=19.0
2: loss=0.677, reward_mean=21.1, reward_bound=21.0
[2017-10-04 13:52:26,964] Starting new video recorder writing to mon/openaigym.video.0.4430.video000064.mp4
3: loss=0.653, reward_mean=33.2, reward_bound=48.5
4: loss=0.642, reward_mean=37.4, reward_bound=42.5
.........
29: loss=0.561, reward_mean=111.6, reward_bound=122.0
30: loss=0.540, reward_mean=135.1, reward_bound=166.0
[2017-10-04 13:52:40,176] Starting new video recorder writing to mon/openaigym.video.0.4430.video000512.mp4
31: loss=0.546, reward_mean=147.5, reward_bound=179.5
32: loss=0.559, reward_mean=140.0, reward_bound=171.5
33: loss=0.558, reward_mean=160.4, reward_bound=200.0
34: loss=0.547, reward_mean=167.6, reward_bound=195.5
35: loss=0.550, reward_mean=179.5, reward_bound=200.0
36: loss=0.563, reward_mean=173.9, reward_bound=200.0
37: loss=0.542, reward_mean=162.9, reward_bound=200.0
38: loss=0.552, reward_mean=159.1, reward_bound=200.0
39: loss=0.548, reward_mean=189.6, reward_bound=200.0
40: loss=0.546, reward_mean=191.1, reward_bound=200.0
41: loss=0.548, reward_mean=199.1, reward_bound=200.0
Solved!</pre></div><p class="calibre8">图4:墨盒状态的可视化</p><div><img src="img/00035.jpeg" alt="Cross-entropy on CartPole" class="calibre9"/><div><p class="calibre14">Figure 4: Visualization of the CartPole state</p></div></div><p class="calibre10">现在让我们暂停一下，想想刚刚发生了什么。我们的神经网络已经学会了如何纯粹通过观察和奖励来玩环境，而不需要对观察到的<a id="id137" class="calibre1"/>值进行任何一个词的解释。环境可以很容易地不是一辆带棍子的手推车，而是，比如说，一个仓库模型，以产品数量作为观察，以赚到的钱作为奖励。我们的实现不依赖于环境细节。这就是RL模型的美妙之处，在下一节中，我们将看看如何将完全相同的方法应用于不同的环境，而不是健身房集合。</p><p class="calibre8"><a id="ch04lvl1sec31" class="calibre1"/>冰湖上的交叉熵</p></div></body></html>


<html>
  <head>
    <title>Cross-entropy on FrozenLake</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">我们将尝试使用交叉熵方法解决的下一个环境是FrozenLake。它的世界来自所谓的“网格世界”类别，当你的代理人生活在一个大小为4 × 4的网格中，可以向四个方向移动:上、下、左、右。代理总是从左上角开始，它的目标是到达网格的右下角。网格的固定单元格中有洞，如果你进入这些洞，这一集就结束了，你的奖励为零。如果代理到达目的地单元，则它获得奖励1.0，并且该集结束。</h1></div></div></div><p class="calibre8">让生活变得更复杂的是，这个世界很滑(毕竟这是一个冰冻的湖)，所以代理人的行动并不总是如预期的那样:有33%的机会它会滑向右边或左边。例如，您希望代理向左移动，但是有33%的可能性它确实会向左移动，有33%的可能性它会出现在上面的单元格中，有33%的可能性它会出现在下面的单元格中。正如我们将在本节末尾看到的，这使得进展变得困难。</p><p class="calibre8">图5:冰冻湖环境</p><div><img src="img/00036.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 5: The FrozenLake environment</p></div></div><p class="calibre10">让我们看看这种环境是如何在Gym中表现出来的:</p><p class="calibre8">我们的观察空间是离散的，这意味着它只是一个从0到15的数字。显然，这个数字是我们在网格中的当前位置。动作空间也是离散的，但是可以从零到三。在CartPole示例中，我们的网络需要一个数字向量。为了实现这一点，我们可以对离散输入应用传统的“一键编码”,这意味着网络的输入将有16个浮点数，除了我们要编码的索引之外，其他地方都是零。为了最小化代码中的变化，我们可以使用Gym中的<code class="literal">ObservationWrapper</code>类并实现我们的<code class="literal">DiscreteOneHotWrapper</code>类:</p><div><pre class="programlisting">&gt;&gt;&gt; e = gym.make("FrozenLake-v0")
[2017-10-05 12:39:35,827] Making new env: FrozenLake-v0
&gt;&gt;&gt; e.observation_space
Discrete(16)
&gt;&gt;&gt; e.action_space
Discrete(4)
&gt;&gt;&gt; e.reset()
0
&gt;&gt;&gt; e.render()

SFFF
FHFH
FFFH
HFFG</pre></div><p class="calibre8">有了应用于环境的包装器，观察空间和动作空间都与我们的CartPole解决方案100%兼容(源代码<code class="literal">Chapter04/02_frozenlake_naive.py</code>)。然而，通过<a id="id140" class="calibre1"/>启动它，我们可以看到，随着时间的推移，这并没有提高分数。</p><div><pre class="programlisting">class DiscreteOneHotWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super(DiscreteOneHotWrapper, self).__init__(env)
        assert isinstance(env.observation_space, gym.spaces.Discrete)
        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)

    def observation(self, observation):
        res = np.copy(self.observation_space.low)
        res[observation] = 1.0
        return res</pre></div><p class="calibre8">图6:在FrozenLake环境中，原始交叉熵编码缺乏收敛性</p><div><img src="img/00037.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 6: Lack of convergence of the original cross-entropy code in the FrozenLake environment</p></div></div><p class="calibre10">为了理解发生了什么，我们需要更深入地了解两种环境下的奖励结构。在CartPole中，环境的每一步都给我们1.0的回报，直到杆子落下的那一刻。所以，代理人平衡杆子的时间越长，获得的奖励就越多。由于我们代理人行为的随机性，不同的剧集长度不同，这给了我们一个非常正常的剧集奖励分布。在选择了奖励边界后，我们拒绝了不太成功的剧集，并学习了如何重复更好的剧集(通过对成功剧集的数据进行训练)。</p><p class="calibre8">如下图所示:</p><p class="calibre8">图7:在横竿环境中奖励的分布</p><div><img src="img/00038.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 7: Distribution of reward in the CartPole environment</p></div></div><p class="calibre10">在FrozenLake环境中，剧集和它们的奖励看起来是不同的。只有当我们达到目标时，我们才能获得1.0的奖励，这个奖励并不能说明每集有多好。是快速有效还是我们在随机进入最后一个牢房之前在湖上转了四圈？我们不知道，只是1.0奖励仅此而已。我们剧集的奖励分配也有问题。只有两种插曲可能，零奖励(失败)和一奖励(成功)，失败的插曲在训练的开始阶段会明显占优势。所以，我们对“精英”剧集的百分位数选择是完全错误的，给了我们糟糕的训练范例。这就是我们训练失败的原因。</p><p class="calibre8">图FrozenLake环境的奖励分布</p><div><img src="img/00039.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 8: Reward distribution of the FrozenLake environment</p></div></div><p class="calibre10">这个例子向我们展示了交叉熵方法的局限性:</p><p class="calibre8">为了训练，我们的剧集必须是有限的，最好是短的</p><div><ul class="itemizedlist"><li class="listitem">剧集的总报酬应该有足够的可变性来区分好的和坏的剧集</li><li class="listitem">没有关于代理是成功还是失败的中间指示</li><li class="listitem">在本书的后面，我们将熟悉解决这些限制的其他方法。现在，如果你对如何使用交叉熵解决FrozenLake感到好奇，这里有一个你需要做的代码调整列表(完整的例子在<code class="literal">Chapter04/03_frozenlake_tweaked.py</code>):</li></ul></div><p class="calibre8"><strong class="calibre2">更大批量的播放剧集</strong>:在CartPole中，每次迭代16集就足够了，但FrozenLake需要至少100集才能获得一些成功的剧集。</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">应用于奖励</strong>的折扣系数:为了使剧集的总奖励取决于剧集长度，并增加剧集的多样性，我们可以使用折扣系数为0.9或0.95的折扣总奖励。在这种情况下，较短剧集的奖励会高于较长剧集的奖励。</li><li class="listitem"><strong class="calibre2">更长时间保留“精英”剧集</strong>:在翻筋斗训练中，我们从环境中取样剧集，在最好的剧集上进行训练，然后扔掉。在FrozenLake中，成功的一集是非常罕见的动物，所以我们需要保留它们几个迭代来训练它们。</li><li class="listitem"><strong class="calibre2">降低学习率</strong>:这将给我们的网络时间平均更多的训练样本。</li><li class="listitem"><strong class="calibre2">更长的训练时间</strong>:由于成功情节的稀少，以及我们行动的随机结果，我们的网络很难获得在任何特定情况下最佳行为的想法。为了达到50%的成功剧集，需要大约5k的训练迭代。</li><li class="listitem">为了将所有这些整合到我们的代码中，我们<a id="id142" class="calibre1"/>需要更改<code class="literal">filter_batch</code>函数来计算折扣奖励并返回“精英”剧集供我们保留:</li></ul></div><p class="calibre8">然后，在训练循环中，我们将存储以前的“精英”剧集，以便在下一次训练迭代中将它们传递给前面的函数。</p><div><pre class="programlisting">def filter_batch(batch, percentile):
    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))
    reward_bound = np.percentile(disc_rewards, percentile)
    train_obs = []
    train_act = []
    elite_batch = []
    for example, discounted_reward in zip(batch, disc_rewards):
        if discounted_reward &gt; reward_bound:
            train_obs.extend(map(lambda step: step.observation, example.steps))
            train_act.extend(map(lambda step: step.action, example.steps))
            elite_batch.append(example)
    return elite_batch, train_obs, train_act, reward_bound</pre></div><p class="calibre8">代码的其余部分是相同的，除了学习率降低了10倍，并且<code class="literal">BATCH_SIZE</code>被设置为100。经过一段时间的耐心等待(新版本大约需要一个半小时来完成10k次迭代)，我们可以看到模型的训练在55%左右的已解决剧集中停止了改进。有很多方法可以解决这个问题(例如，通过应用熵损失正则化)，但是这些技术将在接下来的章节中讨论。</p><div><pre class="programlisting">full_batch = []
for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):
    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))
    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)
    if not full_batch:
        continue
    obs_v = torch.FloatTensor(obs)
    acts_v = torch.LongTensor(acts)
    full_batch = full_batch[-500:]</pre></div><p class="calibre8">图9:使用调整的交叉熵实现的FrozenLake的收敛</p><div><img src="img/00040.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 9: Convergence of FrozenLake with tweaked cross-entropy implementation</p></div></div><p class="calibre10">这里要注意的最后一点是FrozenLake环境中“光滑”的影响。我们每一个有33%概率的动作都被旋转了90°的动作所取代(例如，“向上”动作成功的概率为0.33，被“向左”动作取代的概率为0.33，被“向右”动作取代的概率为0.33)。</p><p class="calibre8">无滑动版本在<code class="literal">Chapter04/04_frozenlake_nonslippery.py</code>中，唯一的不同是在<a id="id143" class="calibre1"/>环境创建中(我们需要窥视Gym的核心来创建带有调整参数的环境实例):</p><p class="calibre8">效果很戏剧化！环境的非光滑版本可以在120-140次批量迭代中求解，这比噪声环境快100倍:</p><div><pre class="programlisting">    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)
    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)env = DiscreteOneHotWrapper(env)</pre></div><p class="calibre8">图10:frozen lake的非滑动版本的收敛</p><div><pre class="programlisting">rl_book_samples/Chapter04$ ./04_frozenlake_nonslippery.py
0: loss=1.379, reward_mean=0.010, reward_bound=0.000, batch=1
1: loss=1.375, reward_mean=0.010, reward_bound=0.000, batch=2
2: loss=1.359, reward_mean=0.010, reward_bound=0.000, batch=3
3: loss=1.361, reward_mean=0.010, reward_bound=0.000, batch=4
4: loss=1.355, reward_mean=0.000, reward_bound=0.000, batch=4
5: loss=1.342, reward_mean=0.010, reward_bound=0.000, batch=5
6: loss=1.353, reward_mean=0.020, reward_bound=0.000, batch=7
7: loss=1.351, reward_mean=0.040, reward_bound=0.000, batch=11
......
124: loss=0.484, reward_mean=0.680, reward_bound=0.000, batch=68
125: loss=0.373, reward_mean=0.710, reward_bound=0.430, batch=114
126: loss=0.305, reward_mean=0.690, reward_bound=0.478, batch=133
128: loss=0.413, reward_mean=0.790, reward_bound=0.478, batch=73
129: loss=0.297, reward_mean=0.810, reward_bound=0.478, batch=108
Solved!</pre></div><div><img src="img/00041.jpeg" alt="Cross-entropy on FrozenLake" class="calibre9"/><div><p class="calibre14">Figure 10: Convergence of the nonslippery version of FrozenLake</p></div></div><p class="calibre10"><a id="ch04lvl1sec32" class="calibre1"/>交叉熵方法的理论背景</p></div></body></html>


<html>
  <head>
    <title>Theoretical background of the cross-entropy method</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">这一部分是可选的，包含在<a id="id144" class="calibre1"/>中是为了让那些对这种方法的工作原理感兴趣的读者。如果你愿意，你可以参考关于交叉熵的原始论文，它将在本节的最后给出。</h1></div></div></div><p class="calibre8">交叉熵方法的基础在于重要抽样定理，该定理陈述如下:</p><p class="calibre8">The basis of the cross-entropy method lies in the importance sampling theorem, which states this:</p><div><img src="img/00042.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre9"/></div><p class="calibre10">在我们的RL例子中，<em class="calibre11"> H(x) </em>是某个策略<em class="calibre11"> x </em>得到的奖励值，<em class="calibre11"> p(x) </em>是所有可能策略的分布。我们不想通过搜索所有可能的策略来最大化我们的回报，相反，我们想找到一种方法来用<em class="calibre11"> q(x) </em>近似<em class="calibre11"> p(x)H(x) </em>，迭代地最小化它们之间的距离。两个概率分布之间的距离通过<strong class="calibre2">kull back-lei bler</strong>(<strong class="calibre2">KL</strong>)散度<a id="id145" class="calibre1"/>计算，如下:</p><p class="calibre8">In our RL case, <em class="calibre11">H(x)</em> is a reward value obtained by some policy <em class="calibre11">x</em> and <em class="calibre11">p(x)</em> is a distribution of all possible policies. We don't want to maximize our reward by searching all possible policies, instead we want to find a way to approximate <em class="calibre11">p(x)H(x)</em> by <em class="calibre11">q(x)</em>, iteratively minimizing the distance between them. The distance between two probability distributions is calculated by <strong class="calibre2">Kullback-Leibler</strong> (<strong class="calibre2">KL</strong>) divergence<a id="id145" class="calibre1"/> which is as follows:</p><div><img src="img/00043.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre9"/></div><p class="calibre10">KL中的第一项称为<strong class="calibre2">熵</strong>而<a id="id146" class="calibre1"/>不依赖于此，因此在最小化过程中可以省略。第二项称为<strong class="calibre2">交叉熵</strong>，是DL中非常常见的优化目标。</p><p class="calibre8">结合这两个公式，我们可以得到一个迭代算法，从<img src="img/00044.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre24"/>开始，每一步都在改进。这是一个对<em class="calibre11"> p(x)H(x) </em>的近似，但有一个更新:</p><p class="calibre8">Combining both formulas, we can get an iterative algorithm, which starts with <img src="img/00044.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre24"/> and on every step improves. This is an approximation of <em class="calibre11">p(x)H(x)</em> with an update:</p><div><img src="img/00045.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre9"/></div><p class="calibre10">这是一种通用的交叉熵<a id="id147" class="calibre1"/>方法，在我们的RL案例中可以大大简化。首先我们把我们的<em class="calibre11"> H(x) </em>换成一个指标函数，当该集的奖励在阈值以上时为1，奖励在阈值以下时为0。我们的策略更新将如下所示:</p><p class="calibre8">This is a generic cross-entropy <a id="id147" class="calibre1"/>method, which can be significantly simplified in our RL case. Firstly, we replace our <em class="calibre11">H(x)</em> with an indicator function, which is 1 when the reward for the episode is above the threshold and 0 if the reward is below. Our policy update will look like this:</p><div><img src="img/00046.jpeg" alt="Theoretical background of the cross-entropy method" class="calibre9"/></div><p class="calibre10">严格地说，前面的公式忽略了规范化项，但在实践中没有规范化项仍然有效。因此，方法很清楚:我们使用我们当前的策略(从一些随机的初始策略开始)对情节进行采样，并最小化最成功的样本和我们的策略的负对数可能性。</p><p class="calibre8">有一整本书专门讲这个方法，作者是<em class="calibre11"> Dirk P. Kroese </em>。更简短的描述可以在<em class="calibre11">德克·P·克罗斯</em>(<a class="calibre1" href="https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf">https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf</a>)的<em class="calibre11">交叉熵方法</em>论文中找到。</p><p class="calibre8"><a id="ch04lvl1sec33" class="calibre1"/>总结</p></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">在这一章中，我们熟悉了第一个RL方法交叉熵，它很简单，但是非常强大，尽管有它的局限性。我们将它应用于CartPole环境(取得了巨大的成功)和FrozenLake(取得了较小的成功)。这一章结束了这本书的介绍部分。</h1></div></div></div><p class="calibre8">在接下来的章节中，我们将探索更复杂，但更强大的深度强化工具。</p><p class="calibre8">In the upcoming chapters, we will explore more complex, but more powerful tools of deep RL.</p></div></body></html>
</body></html>