

# 十五、信任区域——TRPO、PPO 和 ACKTR

在这一章中，我们将看看用来提高随机策略梯度方法稳定性的方法。已经进行了一些尝试来使策略改进更加稳定，我们将集中讨论三种方法:**最近策略优化** ( **PPO** )、**信任区域策略优化** ( **TRPO** )和**演员评论家** ( **A2C** )使用**克罗内克因子信任区域** ( **ACKTR** )。

为了将它们与 A2C 基线进行比较，我们将使用 OpenAI 创建的 roboschool 库中的几个环境。



# 简介

我们将要了解的方法的总体动机是提高培训期间策略更新的稳定性。直觉上，这是一个两难的问题:一方面，我们希望尽可能快地训练，在**随机梯度下降** ( **SGD** )更新期间大步前进。另一方面，大量更新策略通常是一个坏主意，因为我们的策略是一个非常非线性的东西，所以大量更新可能会破坏我们刚刚学习的策略。在 RL 环境中，事情会变得更糟，因为一旦对策略进行了错误的更新，后续的更新将无法恢复。相反，坏的策略会给我们带来坏的经验样本，我们会在随后的培训步骤中使用，这可能会完全违反我们的策略。因此，我们想尽一切可能避免大的更新。一个天真的解决方案是在 SGD 期间使用小的学习率来小步前进，但是这将大大减慢收敛速度。

为了打破这种恶性循环，研究人员已经进行了几次尝试，以评估我们的策略更新对未来结果的影响。当然，这是一个非常笨拙的解释，但它可以帮助我们理解这个想法，这可能是有帮助的，因为那些方法是相当数学繁重的(尤其是 TRPO)。



# 机器人学校

为了用本章的方法进行实验，我们将使用 roboschool，它使用 PyBullet 作为物理引擎，有 13 个不同复杂度的环境。PyBullet 有类似的环境，但是在编写本文时，由于内部 OpenGL 问题，不可能创建同一个环境的几个实例。在本章中，我们将接触到两个问题:RoboschoolHalfCheetah-v1，它模拟了一个两条腿的生物，以及 roboschoollant-v1，它有四条腿。它们的状态和动作空间与我们在前一章看到的 Minitaur 环境非常相似:状态包括关节的特征，动作是这些关节的激活。两者的目标都是尽可能地移动，最大限度地减少能量消耗。

![Roboschool](img/00316.jpeg)

图 1:两个机器人学校环境的截图:机器人学校 HalfCheetah 和机器人学校

要安装 roboschool，你需要按照[https://github.com/openai/roboschool](https://github.com/openai/roboschool)上的说明操作。这需要在系统中安装额外的组件，并构建和使用经过修改的 PyBullet。安装 roboschool 之后，您应该能够在代码中使用`import roboschool`来访问新环境。

安装可能不是很容易和顺利，在我的例子中，需要导出目录名为`python-3.6.pc`的`PKG_CONFIG_PATH`变量。完整的命令是:

```py
export PKG_CONFIG_PATH=/home/shmuma/anaconda3/envs/rl_book_samples-0.4.0/lib/pkgconfig/

```

在这之后，roboschool 安装的最后一步没有出现任何问题。



# A2C 基线

为了建立基线结果，我们将使用 A2C 方法，与前一章中的代码非常相似。完整的源代码在文件`Chapter15/01_train_a2c.py`和`Chapter15/lib/model.py`中。这个基线和我们在上一章中使用的版本有一些不同。首先，有 16 个平行环境用于收集培训期间的经验。第二个区别是模型结构和我们执行探索的方式。为了说明它们，让我们看看模型和代理类。

演员和评论家都被放在不同的网络中，没有共享权重。他们遵循前一章中使用的方法，当时我们的评论家估计了行动的均值和方差，但现在，方差不是基础网络的单独头部，而是模型的单个参数。SGD 将在训练期间调整该参数，但它不依赖于观察。

```py
HID_SIZE = 64

class ModelActor(nn.Module):
    def __init__(self, obs_size, act_size):
        super(ModelActor, self).__init__()

        self.mu = nn.Sequential(
            nn.Linear(obs_size, HID_SIZE),
            nn.Tanh(),
            nn.Linear(HID_SIZE, HID_SIZE),
            nn.Tanh(),
            nn.Linear(HID_SIZE, act_size),
            nn.Tanh(),
        )
        self.logstd = nn.Parameter(torch.zeros(act_size))

    def forward(self, x):
        return self.mu(x)
```

actor 网络有两个 64 个神经元的隐藏层，每个都具有 tanh 非线性。方差被建模为单独的网络参数，并被解释为标准偏差的对数。

```py
class ModelCritic(nn.Module):
    def __init__(self, obs_size):
        super(ModelCritic, self).__init__()

        self.value = nn.Sequential(
            nn.Linear(obs_size, HID_SIZE),
            nn.ReLU(),
            nn.Linear(HID_SIZE, HID_SIZE),
            nn.ReLU(),
            nn.Linear(HID_SIZE, 1),
        )

    def forward(self, x):
        return self.value(x)
```

评论家网络也有两个同样大小的隐层，只有一个单一的输出值，这个输出值就是 *V(s)* 的估计，它是状态的贴现值。

```py
class AgentA2C(ptan.agent.BaseAgent):
    def __init__(self, net, device="cpu"):
        self.net = net
        self.device = device 

    def __call__(self, states, agent_states):
        states_v = ptan.agent.float32_preprocessor(states).to(self.device)

        mu_v = self.net(states_v)
        mu = mu_v.data.cpu().numpy()
        logstd = self.net.logstd.data.cpu().numpy()
        actions = mu + np.exp(logstd) * np.random.normal(size=logstd.shape)
        actions = np.clip(actions, -1, 1)
        return actions, agent_states
```

将状态转换为动作的代理也通过简单地从状态中获得预测的平均值并应用方差噪声来工作，方差由`logstd`参数的当前值决定。



## 结果

默认情况下，使用 RoboschoolHalfCheetah-v1 环境，但是要更改它，您可以传递带有所需环境 ID 的`-e`参数。以下是在 HalfCheetah 环境中训练期间从训练环境获得的平滑总奖励(plot **reward_100** )、平均测试分数(来自 10 次测试)和剧集长度的图表。

![Results](img/00317.jpeg)

图 HalfCheetah 上的 A2C 收敛

这个问题有一个局部最小策略，当模型静止不动，保持平衡 1000 步时，奖励大约 700 分。为了获得更大的回报，它需要发现向前跑可以获得更大的回报，这需要近 600 万次观察。总培训时间为 15 小时。

RoboschoolAnt-v1 速度较慢，因为需要更长的模拟过程，所以很可能再过一天的训练就能改进策略。但是，这些图表足以说明收敛稳定性和性能。

![Results](img/00318.jpeg)

图 Ant 上的 A2C 收敛



## 视频录制

像往常一样，有一个实用程序可以对训练好的模型进行基准测试，并录制一段代理运行的视频。它在文件`Chapter15/02_play.py`中，可以接受本章介绍的方法中的任何模型(因为 actor network 在所有方法中都是相同的)。您也可以使用`-e`命令选项更改环境名称。



# 最接近的策略优化

从历史上看，这个方法来自 OpenAI 团队，在 TRPO 之后很久才被提出(也就是从 2015 年开始)，但是 PPO 比 TRPO 简单很多，所以我们就从它开始。提出它的论文是由 John Schulman 等人提出的，名为*近似策略优化算法*，发表于 2017 年(arXiv:1707.06347)。

对经典的**异步优势演员评论家** ( **A3C** )方法的核心改进是改变用于估算 PG 的表达式。PPO 方法没有采用所采取行动的对数概率梯度，而是采用了不同的目标:新旧策略之间的比率与优势成比例。

在数学形式上，旧的 A3C 目标可以写成![Proximal Policy Optimization](img/00319.jpeg)。PPO 提出的新目标是![Proximal Policy Optimization](img/00320.jpeg)。改变目标的原因是与[第 4 章](part0030_split_000.html#SJGS1-ce551566b6304db290b61e4d70de52ee "Chapter 4. The Cross-Entropy Method")、*交叉熵方法*中的交叉熵方法相同:重要性抽样。但是，如果我们只是开始盲目地最大化这个值，它可能会导致对策略权重的非常大的更新。为了限制更新，使用了剪裁的物镜。如果我们把新旧策略的比率写成![Proximal Policy Optimization](img/00321.jpeg)，那么削减后的目标就可以写成![Proximal Policy Optimization](img/00322.jpeg)。这个目标将新旧策略之间的比率限制在区间![Proximal Policy Optimization](img/00323.jpeg)内，因此通过改变![Proximal Policy Optimization](img/00324.jpeg)我们可以限制更新的大小。

与 A3C 方法的另一个区别是我们评估优势的方式。在 A3C 论文中，从 T 步的有限时域估计中获得的优势的形式是:![Proximal Policy Optimization](img/00325.jpeg)。在 PPO 论文中，作者使用了更一般的估计形式，![Proximal Policy Optimization](img/00326.jpeg)其中![Proximal Policy Optimization](img/00327.jpeg)。最初的 A3C 估计是使用![Proximal Policy Optimization](img/00328.jpeg)提出的方法的特例。

PPO 方法也使用稍微不同的训练程序，当从环境中获得长序列的样品，然后在进行几个时期的训练之前，对整个序列进行优势估计。



## 实现

示例的代码放在两个源代码文件中:`Chapter15/04_train_ppo.py`和`Chapter15/lib/model.py`。演员、评论家和经纪人的职业与我们在 A2C 基线的职业完全相同。区别在于训练程序和我们计算优势的方式，但让我们从超参数开始。

```py
ENV_ID = "RoboschoolHalfCheetah-v1"
GAMMA = 0.99
GAE_LAMBDA = 0.95
```

`GAMMA`的值已经很熟悉了，但是`GAE_LAMBDA`是新的常数，它指定了优势估计器中的λ因子。PPO 文件中使用了`0.95`的值。

```py
TRAJECTORY_SIZE = 2049
LEARNING_RATE_ACTOR = 1e-4
LEARNING_RATE_CRITIC = 1e-3
```

该方法假设对于每个子迭代，将从环境中获得大量的转换(如在前面关于 PPO 的部分中所提到的，在训练期间，它在采样的训练批次上进行若干个时期)。我们还为演员和评论家使用了两个不同的优化器(因为他们没有共享的权重)。

```py
PPO_EPS = 0.2
PPO_EPOCHES = 10
PPO_BATCH_SIZE = 64
```

对于每批`TRAJECTORY_SIZE`样品，我们执行 PPO 目标的`PPO_EPOCHES`次迭代，最小批次为 64 个样品。值`PPO_EPS`指定新旧策略比率的剪辑值。

```py
TEST_ITERS = 1000
```

对于从环境中获得的每 1k 次观察，我们执行 10 集的测试，以获得当前策略的总回报和步骤数。下面的函数采用具有步长的轨迹，并计算演员的优势和评论家训练的参考值。我们的轨迹不是一个单一的事件，而是几个串联在一起的事件。

```py
def calc_adv_ref(trajectory, net_crt, states_v, device="cpu"):
    values_v = net_crt(states_v)
    values = values_v.squeeze().data.cpu().numpy()
```

作为第一步，我们要求评论家将状态转换成价值。

```py
    last_gae = 0.0
    result_adv = []
    result_ref = []
    for val, next_val, (exp,) in zip(reversed(values[:-1]), reversed(values[1:]), reversed(trajectory[:-1])):
```

这个循环加入了获得的值和经验值。对于每个轨迹步骤，我们需要当前值(从当前状态获得)和下一个后续步骤的值(使用贝尔曼方程进行估算)。我们还以相反的顺序遍历轨迹，以便能够在一个步骤中计算更近的优势值。

```py
        if exp.done:
            delta = exp.reward - val
            last_gae = delta
        else:
            delta = exp.reward + GAMMA * next_val - val
            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae
```

在每一步中，我们的操作都取决于该步的 done 标志。如果这是这一集的最后一步，我们没有事先的奖励要考虑(记住，我们是以相反的顺序处理轨迹)。所以，我们在这一步的 delta 值就是即时回报减去这一步的预测值。如果当前步骤不是最终步骤，delta 将等于直接奖励加上后续步骤的贴现值，再减去当前步骤的值。在经典的 A3C 方法中，该增量被用作优势估计，但在这里，使用平滑版本，因此优势估计(在`last_gae`变量中跟踪)被计算为具有折扣因子![Implementation](img/00329.jpeg)的增量之和。

```py
        result_adv.append(last_gae)
        result_ref.append(last_gae + val)
```

该函数的目标是为评论家计算优点和参考值，所以我们将它们保存在列表中。

```py
    adv_v = torch.FloatTensor(list(reversed(result_adv))).to(device)
    ref_v = torch.FloatTensor(list(reversed(result_ref))).to(device)
    return adv_v, ref_v
```

在训练循环中，我们使用 PTAN 库中的`ExperienceSource(steps_count=1)`类收集所需大小的轨迹。有了这样的配置，它为我们提供了来自元组`(state, action, reward, done)`环境中的个体步骤。

```py
            trajectory.append(exp)
            if len(trajectory) < TRAJECTORY_SIZE:
                continue

            traj_states = [t[0].state for t in trajectory]
            traj_actions = [t[0].action for t in trajectory]
            traj_states_v = torch.FloatTensor(traj_states).to(device)
            traj_actions_v = torch.FloatTensor(traj_actions).to(device)
            traj_adv_v, traj_ref_v = calc_adv_ref(trajectory, net_crt, device=device)
```

当我们有足够大的轨迹用于训练时(由上面的`TRAJECTORY_SIZE`超参数给出)，我们将状态和采取的行动转换成张量，并使用已经描述过的函数来获得优势和参考值。尽管我们的轨迹很长，但是我们的测试环境的观测值很短，所以在一个步骤中处理我们的批处理是很好的。在 Atari 帧的情况下，这样的批处理可能会导致 GPU 内存错误。

下一步，我们计算采取行动的概率的对数。该值将被用作 PPO 目标中的![Implementation](img/00330.jpeg)。此外，我们还对优势的均值和方差进行了归一化处理，以提高训练的稳定性。

```py
            mu_v = net_act(traj_states_v)
            old_logprob_v = calc_logprob(mu_v, net_act.logstd, traj_actions_v)
            traj_adv_v = (traj_adv_v - torch.mean(traj_adv_v)) / torch.std(traj_adv_v)
```

随后的两行删除了轨迹的最后一个条目，以反映我们的优势和参考值比轨迹长度短一步的事实(因为我们在`calc_adv_ref`函数内的循环中移动了值)。

```py
            trajectory = trajectory[:-1]
            old_logprob_v = old_logprob_v[:-1].detach()
```

当所有的准备工作完成后，我们在我们的轨道上进行几次训练。对于每一批，我们从相应的数组中提取部分，并分别进行评论家和演员训练。

```py
            for epoch in range(PPO_EPOCHES):
                for batch_ofs in range(0, len(trajectory), PPO_BATCH_SIZE):
                    states_v = traj_states_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    actions_v = traj_actions_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    batch_adv_v = traj_adv_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE].unsqueeze(-1)
                    batch_ref_v = traj_ref_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    batch_old_logprob_v = old_logprob_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
```

训练评论家，我们需要做的就是用事先计算好的参考值计算出**均方误差** ( **MSE** )损失。

```py
                    opt_crt.zero_grad()
                    value_v = net_crt(states_v)
                    loss_value_v = F.mse_loss(value_v.squeeze(-1), batch_ref_v)
                    loss_value_v.backward()
                    opt_crt.step()
```

在演员训练中，我们最小化被否定的裁剪目标:![Implementation](img/00331.jpeg)，其中![Implementation](img/00332.jpeg)。下面几行是这个公式的一个简单的实现。

```py
                    opt_act.zero_grad()
                    mu_v = net_act(states_v)
                    logprob_pi_v = calc_logprob(mu_v, net_act.logstd, actions_v)
                    ratio_v = torch.exp(logprob_pi_v - batch_old_logprob_v)
                    surr_obj_v = batch_adv_v * ratio_v
                    clipped_surr_v = batch_adv_v * torch.clamp(ratio_v, 1.0 - PPO_EPS, 1.0 + PPO_EPS)
                    loss_policy_v = -torch.min(surr_obj_v, clipped_surr_v).mean()
                    loss_policy_v.backward()
                    opt_act.step()
```



## 结果

在我们的两个测试环境中训练，PPO 方法比 A2C 方法有了很大的改进。下面的图表显示了在 RoboschoolHalfCheetah-v1 环境下的训练进度，当这些方法仅在两个小时的训练和 1.3 米的观察后就能够达到 1k 的分数，这比 A2C 用 10 米的观察和 15 个小时获得相同的结果要好得多。在 10 个小时的培训后，代理能够获得的最高奖励超过 2600 英镑，看起来它可以做得更好，因为只需要一些调整，如**学习率** ( **LR** )衰减。

![Results](img/00333.jpeg)

图 HalfCheetah 上的 PPO 聚合

在 robocoolant-v1 上，奖励的增加也几乎是线性的，在 20 小时的测试中，最大奖励是 1848，这也是对 A2C 方法的改进。

![Results](img/00334.jpeg)

图 Ant 环境中的 PPO



# 信任区域策略优化

TRPO 是 2015 年由伯克利的研究人员在 John Schulman 等人的论文中提出的，名为*信任区域策略优化* (arXiv:1502.05477)。本文朝着提高随机策略梯度优化的稳定性和一致性迈出了一步，并在各种控制任务中显示了良好的结果。

不幸的是，这篇论文和这个方法非常数学化，所以很难理解这个方法的细节。这同样适用于使用共轭梯度法有效解决约束优化问题的实现。

作为第一步，TRPO 方法定义了该州的折扣访问频率:![Trust Region Policy Optimization](img/00335.jpeg)。在该等式中，![Trust Region Policy Optimization](img/00336.jpeg)等于在采样轨迹的位置 I 处将遇到的状态 s 的采样概率。然后，TRPO 将优化目标定义为![Trust Region Policy Optimization](img/00337.jpeg)，其中![Trust Region Policy Optimization](img/00338.jpeg)是策略的期望折扣报酬，![Trust Region Policy Optimization](img/00339.jpeg)定义确定性策略。

为了解决大规模策略更新的问题，TRPO 定义了对策略更新的附加约束，表示为新旧策略之间的最大**kull back-lei bler**(**KL**)-偏差，可以写成![Trust Region Policy Optimization](img/00340.jpeg)。



## 实现

GitHub 或其他开源库上可用的大多数 TRPO 实现彼此非常相似，可能是因为它们都是从最初的约翰·舒尔曼 TRPO 实现发展而来。我的 TRPO 版本也没有太大的不同，它使用了实现共轭梯度法(TRPO 用来解决约束优化问题)的核心函数，这些函数来自这个库:[https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo)。

完整的示例在`Chapter15/03_train_trpo.py`和`Chapter15/lib/trpo.py`中，训练循环与 PPO 示例非常相似:我们对预定义长度的转换轨迹进行采样，并使用 PPO 部分中给出的平滑公式计算优势估计值(过去，该估计值首先在 TRPO 论文中提出)。接下来，我们使用 MSE 损失和计算的参考值进行一个训练步骤，并且进行一个 TRPO 更新步骤，该步骤包括通过使用共轭梯度法找到我们应该去的方向，并且沿着该方向进行线性搜索，以找到保持期望的 KL 散度的步骤。

以下是执行这两个步骤的训练循环片段:

```py
            # critic step
            opt_crt.zero_grad()
            value_v = net_crt(traj_states_v)
            loss_value_v = F.mse_loss(value_v.squeeze(-1), traj_ref_v)
            loss_value_v.backward()
            opt_crt.step()
```

为了执行 TRPO 步骤，我们需要提供两个函数:第一个函数将计算当前参与者策略的损失，它使用与 PPO 中新旧策略乘以优势估计相同的比率。第二个函数必须计算旧策略和当前策略之间的 KL-divergence。

```py
            # actor step
            def get_loss():
                mu_v = net_act(traj_states_v)
                logprob_v = calc_logprob(mu_v, net_act.logstd, traj_actions_v)
                action_loss_v = -traj_adv_v.unsqueeze(dim=-1) * torch.exp(logprob_v - old_logprob_v)
                return action_loss_v.mean()

            def get_kl():
                mu_v = net_act(traj_states_v)
                logstd_v = net_act.logstd
                mu0_v = mu_v.detach()
                logstd0_v = logstd_v.detach()
                std_v = torch.exp(logstd_v)
                std0_v = std_v.detach()
                kl = logstd_v - logstd0_v + (std0_v ** 2 + ((mu0_v - mu_v) ** 2) / (2.0 * std_v ** 2)) - 0.5
                return kl.sum(1, keepdim=True)

            trpo.trpo_step(net_act, get_loss, get_kl, TRPO_MAX_KL, TRPO_DAMPING, device=device)
```

换句话说，PPO 方法是 TRPO，它使用简单的策略比率剪裁来限制策略更新，而不是复杂的共轭梯度和线搜索。



## 结果

从我的实验来看，TRPO 显示出比 A2C 基线更好的结果，但表现比 PPO 差。很有可能这只是由于未经调整的复制粘贴 TRPO 的低级机制。

在 HalfCheetah 测试上，TRPO 经过 5 百万次的观察和 8 小时的训练，能够达到 1k 奖励。在 1300 万次观察中，它的回报翻了一番，这几乎花了它一天的时间。

![Results](img/00341.jpeg)

图 HalfCheetah 环境中的 TRPO

在机器人学校-v1 上的训练就不那么成功了。三个小时，1.5M 步，代理拿到了 700 的奖励，但是后来训练过程出现了分歧。

图 Ant 环境中的 TRPO

![Results](img/00342.jpeg)

Figure 7: TRPO on the Ant environment

A2C 使用 ACKTR



# 我们将比较的第三种方法使用不同的方法来解决 SGD 稳定性问题。在吴等人于 2017 年发表的名为【使用 Kronecker-Factored Approximation 进行深度强化学习的可扩展信赖域方法】的论文(arXiv:1708.05144)中，作者将二阶优化方法和信赖域方法相结合。

二阶方法的思想是通过取优化函数的二阶导数(换句话说，它的曲率)来改进传统的 SGD，以提高优化过程的收敛性。让事情变得更复杂的是，处理二阶导数通常需要你构建并求逆一个 Hessian 矩阵，这个矩阵可能会非常大，所以实用的方法通常会以某种方式近似它。该领域目前非常活跃，因为开发健壮的、可扩展的优化方法对于整个机器学习领域非常重要。

其中一种二阶方法被称为**克罗内克因子化的**近似(通常缩写为 **KFAC** )，由詹姆斯·马腾斯和罗杰·格罗斯在他们 2015 年发表的论文*用克罗内克因子化的近似曲率优化神经网络*中提出。然而，这种方法的详细描述超出了本书的范围。

实现



## 由于 KFAC 方法是最近才出现的，PyTorch 中没有实现该方法的优化器。唯一可用的 PyTorch 原型来自伊利亚·科斯特里科夫，在这里可以找到:[https://github.com/ikostrikov/pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)。TensorFlow 有另一个版本的 KFAC，它带有 OpenAI 基线，但在 PyTorch 上移植和测试它可能是一件很难的事情。

在我的实验中，我从上面的链接中提取了 KFAC，并将其应用到现有的代码中，这需要替换优化器并执行额外的`backward()`调用来收集 Fisher 信息。评论家的训练方式和在 A2C 一样。

完整的例子在`Chapter15/05_train_acktr.py`中，这里没有显示，因为它基本上与 A2C 相同。唯一的区别是使用了不同的优化器。

结果



## 在 RoboschoolAnt-v1 环境下，ACKTR 方法显示出比 A2C 更好的结果，但比 PPO 差。当然，这些结果必须有所保留，因为我在这里没有进行太多的超参数调优(这留给读者作为练习)，结果可能取决于实际环境，所以通常在手头的任务中尝试不同的方法是一种很好的做法。

图 HalfCheetah 上的 ACKTR

![Results](img/00343.jpeg)

Figure 8: ACKTR on HalfCheetah

在 HalfCheetah 环境下，ACKTR 表现得不太稳定，只能达到 1k 的奖励。

图 Ant 上的 ACKTR

![Results](img/00344.jpeg)

Figure 9: ACKTR on Ant

总结



# 在这一章中，我们检查了三种不同的方法，旨在提高随机策略梯度的稳定性，并将它们与 A2C 在两个连续控制问题上的实现进行了比较。使用上一章的方法(DDPG 和 D4PG)，他们创建了基本的工具来处理连续的控制域。

在下一章，我们将切换到最近流行的一组不同的 RL 方法:黑盒或无梯度方法。

In the next chapter, we'll switch to a different set of RL methods that have been becoming popular recently: black-box or gradient-free methods.