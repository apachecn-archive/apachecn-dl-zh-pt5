

# 零、前言

这本书的主题是强化学习——这是机器学习的一个子领域——专注于在复杂环境中学习最佳行为的一般性和挑战性问题。学习过程仅由奖励值和从环境中获得的观察来驱动。该模型非常通用，可以应用于许多实际情况，从玩游戏到优化复杂的制造过程。

由于灵活性和通用性，强化学习领域发展非常迅速，吸引了试图改进现有方法或创造新方法的研究人员以及对以最有效的方式解决问题感兴趣的实践者的大量关注。

这本书试图填补关于强化学习方法和途径的实用和结构化信息的明显缺乏。一方面，世界各地都有大量的研究活动，几乎每天都有新的研究论文发表，NIPS 或 ICLR 等深度学习会议的很大一部分都致力于 RL 方法。有几个大型研究小组专注于机器人、医学、多智能体系统等领域的 RL 方法应用。关于最近研究的信息随处可得，但是过于专业和抽象，不认真努力是无法理解的。更糟糕的是 RL 应用的实际情况，因为如何从研究论文中以数学形式描述的抽象方法到解决实际问题的工作实现并不总是显而易见的。这使得对该领域感兴趣的人很难直观地理解论文和会议讨论背后的方法和思想。有一些关于各种 RL 方面的非常好的博客帖子用工作示例进行了说明，但是博客帖子的有限格式允许作者仅描述一两种方法，而没有构建完整的结构化图片并显示不同方法之间的相互关系。这本书是我试图解决这个问题。

这本书的另一个方面是它面向实践。每种方法都适用于各种环境，从非常简单到非常复杂。我试图让例子变得清晰易懂，这是 PyTorch 的表现力和强大功能所带来的。另一方面，示例的复杂性和需求面向 RL 爱好者，而无法访问非常大的计算资源，例如 GPU 集群或非常强大的工作站。我相信，这将使充满乐趣和令人兴奋的 RL 领域面向更广泛的受众，而不仅仅是研究小组或大型人工智能公司。然而，它仍然是**深度**强化学习，因此，强烈建议使用 GPU。书中大约有一半的例子将从在 GPU 上运行它们中受益。除了 RL 中使用的传统中型环境示例，如 Atari 游戏或连续控制问题，本书包含三章(8、12 和 13)，其中包含更大的项目，说明 RL 方法如何应用于更复杂的环境和任务。这些例子仍然不是完整的真实项目(否则它们将占据单独的一本书)，而只是更大的问题，说明了 RL 范例如何应用于成熟基准之外的领域。

关于本书前三部分中的示例，另一件需要注意的事情是，我已经尝试让示例自包含，并且完整显示了源代码。有时这导致了代码片段的重复(例如，大多数方法中的训练循环都非常相似)，但是我相信给你直接跳到你想要学习的方法的自由比避免少量重复更重要。书中所有例子在 Github 上都有:[https://Github . com/packt publishing/Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)，欢迎大家叉出来，实验，投稿。



# 这本书是给谁的

主要目标受众是对机器学习有所了解，但对强化学习领域有兴趣的人。读者应该熟悉 Python 以及深度学习和机器学习的基础知识。理解统计学和概率将是一个优势，但不是绝对必要的理解书中的大部分材料。



# 这本书涵盖了什么

[第 1 章](part0012_split_000.html#BE6O2-ce551566b6304db290b61e4d70de52ee "Chapter 1. What is Reinforcement Learning?")，*什么是强化学习？*，包含对 RL 思想和主要形式模型的介绍。

[第 2 章](part0016_split_000.html#F8901-ce551566b6304db290b61e4d70de52ee "Chapter 2. OpenAI Gym")， *OpenAI Gym* ，使用开源的 library gym 向读者介绍了 RL 的实用方面。

[第 3 章](part0022_split_000.html#KVCC2-ce551566b6304db290b61e4d70de52ee "Chapter 3. Deep Learning with PyTorch")，*用 PyTorch 进行深度学习*，给出了 PyTorch 库的快速概览。

[第四章](part0030_split_000.html#SJGS1-ce551566b6304db290b61e4d70de52ee "Chapter 4. The Cross-Entropy Method")，*交叉熵方法*，给你介绍一种 RL 最简单的方法，给你 RL 方法和问题的感受。

[第 5 章](part0036_split_000.html#12AK82-ce551566b6304db290b61e4d70de52ee "Chapter 5. Tabular Learning and the Bellman Equation")、*表格学习和贝尔曼方程*，介绍了基于值的 RL 方法系列。

[第六章](part0043_split_000.html#190861-ce551566b6304db290b61e4d70de52ee "Chapter 6. Deep Q-Networks")、*深度 Q-Networks* ，描述了 DQN，基本的基于值的方法的扩展，允许解决复杂的环境。

[第七章](part0048_split_000.html#1DOR02-ce551566b6304db290b61e4d70de52ee "Chapter 7. DQN Extensions")， *DQN 扩展*，详细介绍了现代扩展到 DQN 方法，以提高其在复杂环境中的稳定性和收敛性。

[第八章](part0059_split_000.html#1O8H61-ce551566b6304db290b61e4d70de52ee "Chapter 8. Stocks Trading Using RL")、*利用 RL 进行股票交易*，是第一个将 DQN 方法应用于股票交易的实际项目。

[第 9 章](part0068_split_000.html#20R682-ce551566b6304db290b61e4d70de52ee "Chapter 9. Policy Gradients – An Alternative")，*策略梯度——另一种选择*，介绍了基于策略学习的另一系列 RL 方法。

[第十章](part0074_split_000.html#26I9K1-ce551566b6304db290b61e4d70de52ee "Chapter 10. The Actor-Critic Method")、*演员评论家法*，描述了 RL 中使用最广泛的一种方法。

[第十一章](part0081_split_000.html#2D7TI1-ce551566b6304db290b61e4d70de52ee "Chapter 11. Asynchronous Advantage Actor-Critic")，*异步优势 Actor-Critic* ，用并行环境通信扩展 Actor-Critic，提高稳定性和收敛性。

[第 12 章](part0087_split_000.html#2IV0U1-ce551566b6304db290b61e4d70de52ee "Chapter 12. Chatbots Training with RL")，*用 RL* 训练聊天机器人，是第二个项目，展示了如何将 RL 方法应用于 NLP 问题。

[第十三章](part0092_split_000.html#2NNJO1-ce551566b6304db290b61e4d70de52ee "Chapter 13. Web Navigation")，*网页导航*，又是一个长项目，将 RL 应用到网页导航中，使用 MiniWoB 的任务集。

[第 14 章](part0099_split_000.html#2UD7M1-ce551566b6304db290b61e4d70de52ee "Chapter 14. Continuous Action Space")，*连续动作空间*，用连续动作空间和各种方法描述环境的细节。

[第 15 章](part0107_split_000.html#361C61-ce551566b6304db290b61e4d70de52ee "Chapter 15. Trust Regions – TRPO, PPO, and ACKTR")、*信任区域——TRPO、PPO 和 ACKTR* ，是另一个关于描述“信任区域”方法集的连续动作空间的章节。

[第 16 章](part0114_split_000.html#3CN041-ce551566b6304db290b61e4d70de52ee "Chapter 16. Black-Box Optimization in RL")，*RL*中的黑盒优化，展示了另一组不使用显式渐变的方法。

[第 17 章](part0124_split_000.html#3M85O1-ce551566b6304db290b61e4d70de52ee "Chapter 17. Beyond Model-Free – Imagination")，*超越无模型想象*，介绍了基于模型的学习方法，使用了最近关于学习中想象的研究成果。

[第十八章](part0131_split_000.html#3STPM1-ce551566b6304db290b61e4d70de52ee "Chapter 18. AlphaGo Zero")、 *AlphaGo Zero* ，描述了 AlphaGo Zero 方法应用于游戏 Connect Four。



# 为了充分利用这本书

书中描述 RL 方法的所有章节都有相同的结构:一开始我们讨论了该方法的动机、理论基础及其背后的直觉。然后，我们跟随几个例子的方法适用于不同的环境与完整的源代码。所以，你可以用不同的方式使用这本书:

1.  为了快速熟悉一些方法，你可以只阅读相关章节的介绍性部分。
2.  为了更深入地理解方法的实现方式，你可以阅读代码和周围的注释。
3.  为了更好地熟悉这个方法(我认为这是最好的学习方法)，你应该使用提供的源代码作为参考点，尝试重新实现这个方法并使它工作。

无论如何，希望这本书对你有用！



## 下载示例代码文件

你可以从你在[http://www.packtpub.com](http://www.packtpub.com)的账户下载本书的示例代码文件。如果你在其他地方购买了这本书，你可以访问 http://www.packtpub.com/support[网站](http://www.packtpub.com/support)并注册，让文件直接通过电子邮件发送给你。

您可以按照以下步骤下载代码文件:

1.  登录或在 http://www.packtpub.com 注册。
2.  选择**支架**标签。
3.  点击**代码下载&勘误表**。
4.  在**搜索**框中输入图书名称，并按照屏幕上的说明进行操作。

下载文件后，请确保使用最新版本的解压缩或解压文件夹:

*   WinRAR / 7-Zip for Windows
*   适用于 Mac 的 Zipeg / iZip / UnRarX
*   用于 Linux 的 7-Zip / PeaZip

该书的代码包也托管在 GitHub 上，网址为[https://GitHub . com/packt publishing/Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)。我们在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)也有丰富的书籍和视频目录中的其他代码包。看看他们！



## 下载彩色图像

我们还提供了一个 PDF 文件，其中有本书中使用的截图/图表的彩色图像。可以在这里下载:[https://www . packtpub . com/sites/default/files/downloads/DeepReinforcementLearningHandsOn _ color images . pdf](https://www.packtpub.com/sites/default/files/downloads/DeepReinforcementLearningHandsOn_ColorImages.pdf)。



## 习惯用法

本书通篇使用了许多文本约定。

`CodeInText`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。比如说；"方法`get_observation()`应该将当前环境的观察结果返回给代理."

代码块设置如下:

```
    def get_actions(self):
        return [0, 1]
```

当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:

```
 def get_actions(self):
        return [0, 1]
```

任何命令行输入或输出都按如下方式编写:

```
$ xvfb-run -s "-screen 0 640x480x24" python 04_cartpole_random_monitor.py

```

**粗体**:表示一个新的术语，一个重要的单词，或者你在屏幕上看到的单词，例如，在菜单或对话框中，也像这样出现在文本中。例如:“实际上，它是一些代码，实现了一些**策略**。”



# 取得联系

我们随时欢迎读者的反馈。

**总体反馈**:发电子邮件`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`，在邮件主题中提及书名。如果您对本书的任何方面有疑问，请发邮件至`<[questions@packtpub.com](mailto:questions@packtpub.com)>`联系我们。

**勘误表**:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的图书，点击勘误表提交表格链接，输入详细信息。

盗版:如果您在互联网上遇到任何形式的我们作品的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过`<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有一个你擅长的主题，并且你有兴趣写书或投稿，请访问[http://authors.packtpub.com](http://authors.packtpub.com)。



## 评论

请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！

更多关于 Packt 的信息，请访问[packtpub.com](http://packtpub.com)。