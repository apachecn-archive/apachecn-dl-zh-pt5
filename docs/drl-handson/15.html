<html><head/><body>
<html>
  <head>
    <title>Chapter 15. Trust Regions – TRPO, PPO, and ACKTR</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch15" class="calibre1"/>第十五章。信任区域–TRPO、PPO和ACKTR</h1></div></div></div><p class="calibre8">在这一章中，我们将看看用来提高随机政策梯度方法稳定性的方法。已经进行了一些尝试来使策略改进更加稳定，我们将集中讨论三种方法:<strong class="calibre2">最近策略优化</strong> ( <strong class="calibre2"> PPO </strong>)、<strong class="calibre2">信任区域策略优化</strong> ( <strong class="calibre2"> TRPO </strong>)和<strong class="calibre2">演员-评论家</strong> ( <strong class="calibre2"> A2C </strong>)使用<strong class="calibre2">克罗内克因子信任区域</strong> ( <strong class="calibre2"> ACKTR </strong>)。</p><p class="calibre8">为了将它们与A2C基线进行比较，我们将使用OpenAI创建的roboschool库中的几个环境。</p></div></body></html>


<html>
  <head>
    <title>Chapter 15. Trust Regions – TRPO, PPO, and ACKTR</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch15lvl1sec68" class="calibre1"/>简介</h1></div></div></div><p class="calibre8">我们将要了解的方法的总体动机是提高培训期间策略更新的稳定性。直觉上，这是一个两难的问题:一方面，我们希望尽可能快地训练，在<strong class="calibre2">随机梯度下降</strong> ( <strong class="calibre2"> SGD </strong>)更新期间大步前进。另一方面，大量更新策略通常是一个坏主意，因为我们的策略是一个非常非线性的东西，所以大量更新可能会破坏我们刚刚学习的策略。在RL环境中，事情会变得更糟，因为一旦对策略进行了错误的更新，后续的更新将无法恢复。相反，坏的政策会给我们带来坏的经验样本，我们会在随后的培训步骤中使用，这可能会完全违反我们的政策。因此，我们想尽一切可能避免大的更新。一个天真的解决方案是在SGD期间使用小的学习率来小步前进，但是这将大大减慢收敛速度。</p><p class="calibre8">为了打破这种恶性循环，研究人员已经进行了几次尝试，以评估我们的政策更新对未来结果的影响。当然，这是一个非常笨拙的解释，但它可以帮助我们理解这个想法，这可能是有帮助的，因为那些方法是相当数学繁重的(尤其是TRPO)。</p></div></div></body></html>


<html>
  <head>
    <title>Roboschool</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">机器人学校</h1></div></div></div><p class="calibre8">为了用本章的方法进行实验，我们将使用roboschool，它使用PyBullet作为物理引擎，有13个不同复杂度的环境。PyBullet有类似的环境，但是在编写本文时，由于内部OpenGL问题，不可能创建同一个环境的几个实例。在本章中，我们将接触到两个问题:RoboschoolHalfCheetah-v1，它模拟了一个两条腿的生物，以及roboschoollant-v1，它有四条腿。它们的状态和动作空间与我们在前一章看到的Minitaur环境非常相似:状态包括关节的特征，动作是这些关节的激活。两者的目标都是尽可能地移动，最大限度地减少能量消耗。</p><div><img src="img/00316.jpeg" alt="Roboschool" class="calibre9"/><div><p class="calibre14">图1:两个机器人学校环境的截图:机器人学校HalfCheetah和机器人学校</p></div></div><p class="calibre10"> </p><p class="calibre8">要安装roboschool，你需要<a id="id641" class="calibre1"/>按照<a class="calibre1" href="https://github.com/openai/roboschool">https://github.com/openai/roboschool</a>上的说明操作。这需要在系统中安装额外的组件，并构建和使用经过修改的PyBullet。安装roboschool之后，您应该能够在代码中使用<code class="literal">import roboschool</code>来访问新环境。</p><p class="calibre8">安装可能不是很容易和顺利，在我的例子中，需要导出目录名为<code class="literal">python-3.6.pc</code>的<code class="literal">PKG_CONFIG_PATH</code>变量。完整的命令是:</p><div><pre class="programlisting">
<strong class="calibre2">export PKG_CONFIG_PATH=/home/shmuma/anaconda3/envs/rl_book_samples-0.4.0/lib/pkgconfig/</strong>
</pre></div><p class="calibre8">在这之后，roboschool安装的最后一步没有出现任何问题。</p></div></body></html>


<html>
  <head>
    <title>A2C baseline</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch15lvl1sec70" class="calibre1"/> A2C基线</h1></div></div></div><p class="calibre8">为了建立<a id="id642" class="calibre1"/>基线结果，我们将使用A2C方法，与前一章中的代码非常相似。完整的源代码在文件<code class="literal">Chapter15/01_train_a2c.py</code>和<code class="literal">Chapter15/lib/model.py</code>中。这个基线和我们在上一章中使用的版本有一些不同。首先，有16个平行环境用于收集培训期间的经验。第二个区别是模型结构和我们执行探索的方式。为了说明它们，让我们看看模型和代理类。</p><p class="calibre8">演员和评论家<a id="id643" class="calibre1"/>都被放在不同的网络中，没有共享权重。他们遵循前一章中使用的方法，当时我们的评论家估计了行动的均值和方差，但现在，方差不是基础网络的单独头部，而是模型的单个参数。SGD将在训练期间调整该参数，但它不依赖于观察。</p><div><pre class="programlisting">HID_SIZE = 64


class ModelActor(nn.Module):
    def __init__(self, obs_size, act_size):
        super(ModelActor, self).__init__()

        self.mu = nn.Sequential(
            nn.Linear(obs_size, HID_SIZE),
            nn.Tanh(),
            nn.Linear(HID_SIZE, HID_SIZE),
            nn.Tanh(),
            nn.Linear(HID_SIZE, act_size),
            nn.Tanh(),
        )
        self.logstd = nn.Parameter(torch.zeros(act_size))

    def forward(self, x):
        return self.mu(x)</pre></div><p class="calibre8">actor网络有两个64个神经元的隐藏层，每个都具有tanh非线性。方差被建模为单独的网络参数，并被解释为标准偏差的对数。</p><div><pre class="programlisting">class ModelCritic(nn.Module):
    def __init__(self, obs_size):
        super(ModelCritic, self).__init__()

        self.value = nn.Sequential(
            nn.Linear(obs_size, HID_SIZE),
            nn.ReLU(),
            nn.Linear(HID_SIZE, HID_SIZE),
            nn.ReLU(),
            nn.Linear(HID_SIZE, 1),
        )

    def forward(self, x):
        return self.value(x)</pre></div><p class="calibre8">批评家网络也有两个同样大小的隐层，只有一个单一的输出值，这个输出值就是<em class="calibre11"> V(s) </em>的估计，它是状态的贴现值。</p><div><pre class="programlisting">class AgentA2C(ptan.agent.BaseAgent):
    def __init__(self, net, device="cpu"):
        self.net = net
        self.device = device 

    def __call__(self, states, agent_states):
        states_v = ptan.agent.float32_preprocessor(states).to(self.device)

        mu_v = self.net(states_v)
        mu = mu_v.data.cpu().numpy()
        logstd = self.net.logstd.data.cpu().numpy()
        actions = mu + np.exp(logstd) * np.random.normal(size=logstd.shape)
        actions = np.clip(actions, -1, 1)
        return actions, agent_states</pre></div><p class="calibre8"><a id="id644" class="calibre1"/>将状态转换为动作的代理也通过简单地从状态中获得预测的平均值并应用方差噪声来工作，方差由<code class="literal">logstd</code>参数的当前值决定。</p></div></body></html>


<html>
  <head>
    <title>A2C baseline</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch15lvl2sec97" class="calibre1"/>结果</h2></div></div></div><p class="calibre8">默认情况下，使用<a id="id645" class="calibre1"/> RoboschoolHalfCheetah-v1环境，但是要更改它，您可以传递带有所需环境ID的<code class="literal">-e</code>参数。以下是在HalfCheetah环境中训练期间从训练环境获得的平滑总奖励(plot <strong class="calibre2"> reward_100 </strong>)、平均测试分数(来自10次测试)和剧集长度的图表。</p><div><img src="img/00317.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图HalfCheetah上的A2C收敛</p></div></div><p class="calibre10"> </p><p class="calibre8">这个问题有一个局部最小策略，当模型静止不动，保持平衡1000步时，奖励大约700分。为了获得更大的回报，它需要发现向前跑可以获得更大的回报，这需要近600万次观察。总培训时间为15小时。</p><p class="calibre8">RoboschoolAnt-v1速度较慢，因为<a id="id646" class="calibre1"/>需要更长的模拟过程，所以很可能再过一天的训练就能改进策略。但是，这些图表足以说明收敛稳定性和性能。</p><div><img src="img/00318.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图Ant上的A2C收敛</p></div></div><p class="calibre10"> </p></div></div></body></html>


<html>
  <head>
    <title>A2C baseline</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch15lvl2sec98" class="calibre1"/>视频录制</h2></div></div></div><p class="calibre8">像往常一样，有一个<a id="id647" class="calibre1"/>实用程序可以对训练好的模型进行基准测试，并录制一段代理运行的视频。它在文件<code class="literal">Chapter15/02_play.py</code>中，可以接受本章介绍的方法中的任何模型(因为actor network在所有方法中都是相同的)。您也可以使用<code class="literal">-e</code>命令选项更改环境名称。</p></div></div></body></html>


<html>
  <head>
    <title>Proximal Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch15lvl1sec71" class="calibre1"/>最接近的策略优化</h1></div></div></div><p class="calibre8">从历史上看，这个方法<a id="id648" class="calibre1"/>来自OpenAI团队，在TRPO之后很久才被提出(也就是从2015年开始)，但是PPO比TRPO简单很多，所以我们就从它开始。提出它的论文是由John Schulman等人提出的，名为<em class="calibre11">近似策略优化算法</em>，发表于2017年(arXiv:1707.06347)。</p><p class="calibre8">对经典的<strong class="calibre2">异步优势行动者-批评家</strong> ( <strong class="calibre2"> A3C </strong>)方法的核心<a id="id649" class="calibre1"/>改进是改变用于估算PG的表达式。PPO方法没有采用所采取行动的对数概率梯度，而是采用了不同的目标:新旧政策之间的比率与优势成比例。</p><p class="calibre8">在数学形式上，旧的A3C目标可以写成<img src="img/00319.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>。PPO提出的新目标是<img src="img/00320.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>。改变目标的原因是<a id="id650" class="calibre1"/>与<a class="calibre1" title="Chapter 4. The Cross-Entropy Method" href="part0030_split_000.html#SJGS1-ce551566b6304db290b61e4d70de52ee">第4章</a>、<em class="calibre11">交叉熵方法</em>中的交叉熵方法相同:重要性抽样。但是，如果我们只是开始盲目地最大化这个值，它可能会导致对策略权重的非常大的更新。为了限制更新，使用了剪裁的物镜。如果我们把新旧政策的比率写成<img src="img/00321.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>，那么削减后的目标就可以写成<img src="img/00322.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>。这个目标将新旧策略之间的比率限制在区间<img src="img/00323.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>内，因此通过改变<img src="img/00324.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>我们可以限制更新的大小。</p><p class="calibre8">与A3C方法的另一个区别是我们评估优势的方式。在A3C论文中，从T步的有限时域估计中获得的优势的形式是:<img src="img/00325.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>。在PPO论文中，作者使用了更一般的估计形式，<img src="img/00326.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>其中<img src="img/00327.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>。最初的A3C估计是使用<img src="img/00328.jpeg" alt="Proximal Policy Optimization" class="calibre24"/>提出的方法的特例。</p><p class="calibre8">PPO方法也使用稍微不同的训练程序，当从环境中获得长序列的样品，然后在进行几个时期的训练之前，对整个序列进行优势估计。</p></div></body></html>


<html>
  <head>
    <title>Proximal Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch15lvl2sec99" class="calibre1"/>实施</h2></div></div></div><p class="calibre8">示例的代码<a id="id651" class="calibre1"/>放在两个源代码文件中:<code class="literal">Chapter15/04_train_ppo.py</code>和<code class="literal">Chapter15/lib/model.py</code>。演员、评论家和经纪人的职业与我们在A2C基线的职业完全相同。区别在于训练程序和我们计算优势的方式，但让我们从超参数开始。</p><div><pre class="programlisting">ENV_ID = "RoboschoolHalfCheetah-v1"
GAMMA = 0.99
GAE_LAMBDA = 0.95</pre></div><p class="calibre8"><code class="literal">GAMMA</code>的值已经很熟悉了，但是<code class="literal">GAE_LAMBDA</code>是新的常数，它指定了优势估计器中的λ因子。PPO文件中使用了<code class="literal">0.95</code>的值。</p><div><pre class="programlisting">TRAJECTORY_SIZE = 2049
LEARNING_RATE_ACTOR = 1e-4
LEARNING_RATE_CRITIC = 1e-3</pre></div><p class="calibre8">该方法假设对于每个子迭代，将从环境中获得大量的转换(如在前面关于PPO的部分中所提到的，在训练期间，它在采样的训练批次上进行若干个时期)。我们还为演员和评论家使用了两个不同的优化器(因为他们没有共享的权重)。</p><div><pre class="programlisting">PPO_EPS = 0.2
PPO_EPOCHES = 10
PPO_BATCH_SIZE = 64</pre></div><p class="calibre8">对于每批<code class="literal">TRAJECTORY_SIZE</code>样品，我们执行PPO目标的<code class="literal">PPO_EPOCHES</code>次迭代，最小批次为64个样品。值<code class="literal">PPO_EPS</code>指定新旧策略比率的剪辑值。</p><div><pre class="programlisting">TEST_ITERS = 1000</pre></div><p class="calibre8">对于从环境中获得的每1k次观察，我们执行10集的测试，以获得当前策略的总回报和步骤数。下面的函数采用具有步长的轨迹，并计算演员的优势和评论家训练的参考值。我们的轨迹不是一个单一的事件，而是几个串联在一起的事件。</p><div><pre class="programlisting">def calc_adv_ref(trajectory, net_crt, states_v, device="cpu"):
    values_v = net_crt(states_v)
    values = values_v.squeeze().data.cpu().numpy()</pre></div><p class="calibre8">作为第一步，我们要求批评家将状态转换成价值。</p><div><pre class="programlisting">    last_gae = 0.0
    result_adv = []
    result_ref = []
    for val, next_val, (exp,) in zip(reversed(values[:-1]), reversed(values[1:]), reversed(trajectory[:-1])):</pre></div><p class="calibre8">这个循环加入了获得的值和经验值。对于每个轨迹步骤，我们需要当前值(从当前状态获得)和下一个后续步骤的值(使用贝尔曼方程进行<a id="id652" class="calibre1"/>估算)。我们还以相反的顺序遍历轨迹，以便能够在一个步骤中计算更近的优势值。</p><div><pre class="programlisting">        if exp.done:
            delta = exp.reward - val
            last_gae = delta
        else:
            delta = exp.reward + GAMMA * next_val - val
            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae</pre></div><p class="calibre8">在每一步中，我们的操作都取决于该步的done标志。如果这是这一集的最后一步，我们没有事先的奖励要考虑(记住，我们是以相反的顺序处理轨迹)。所以，我们在这一步的delta值就是即时回报减去这一步的预测值。如果当前步骤不是最终步骤，delta将等于直接奖励加上后续步骤的贴现值，再减去当前步骤的值。在经典的A3C方法中，该增量被用作优势估计，但在这里，使用平滑版本，因此优势估计(在<code class="literal">last_gae</code>变量中跟踪)被计算为具有折扣因子<img src="img/00329.jpeg" alt="Implementation" class="calibre24"/>的增量之和。</p><div><pre class="programlisting">        result_adv.append(last_gae)
        result_ref.append(last_gae + val)</pre></div><p class="calibre8">该函数的目标是为评论家计算优点和参考值，所以我们将它们保存在列表中。</p><div><pre class="programlisting">    adv_v = torch.FloatTensor(list(reversed(result_adv))).to(device)
    ref_v = torch.FloatTensor(list(reversed(result_ref))).to(device)
    return adv_v, ref_v</pre></div><p class="calibre8">在训练循环中，我们使用PTAN库中的<code class="literal">ExperienceSource(steps_count=1)</code>类收集所需大小的轨迹。有了这样的配置，它为我们提供了来自元组<code class="literal">(state, action, reward, done)</code>环境中的个体步骤。</p><div><pre class="programlisting">            trajectory.append(exp)
            if len(trajectory) &lt; TRAJECTORY_SIZE:
                continue

            traj_states = [t[0].state for t in trajectory]
            traj_actions = [t[0].action for t in trajectory]
            traj_states_v = torch.FloatTensor(traj_states).to(device)
            traj_actions_v = torch.FloatTensor(traj_actions).to(device)
            traj_adv_v, traj_ref_v = calc_adv_ref(trajectory, net_crt, device=device)</pre></div><p class="calibre8">当我们有足够大的轨迹用于训练时(由上面的<code class="literal">TRAJECTORY_SIZE</code>超参数给出)，我们将状态和采取的行动转换成张量，并使用已经描述过的函数来获得优势和参考值。尽管我们的轨迹很长，但是我们的测试环境的观测值很短，所以在一个步骤中处理我们的批处理是很好的。在Atari帧的情况下，这样的批处理可能会导致GPU内存错误。</p><p class="calibre8">下一步，我们计算采取行动的概率的对数<a id="id653" class="calibre1"/>。该值将被用作PPO目标中的<img src="img/00330.jpeg" alt="Implementation" class="calibre24"/>。此外，我们还对优势的均值和方差进行了归一化处理，以提高训练的稳定性。</p><div><pre class="programlisting">            mu_v = net_act(traj_states_v)
            old_logprob_v = calc_logprob(mu_v, net_act.logstd, traj_actions_v)
            traj_adv_v = (traj_adv_v - torch.mean(traj_adv_v)) / torch.std(traj_adv_v)</pre></div><p class="calibre8">随后的两行删除了轨迹的最后一个条目，以反映我们的优势和参考值比轨迹长度短一步的事实(因为我们在<code class="literal">calc_adv_ref</code>函数内的循环中移动了值)。</p><div><pre class="programlisting">            trajectory = trajectory[:-1]
            old_logprob_v = old_logprob_v[:-1].detach()</pre></div><p class="calibre8">当所有的准备工作完成后，我们在我们的轨道上进行几次训练。对于每一批，我们从相应的数组中提取部分，并分别进行评论家和演员训练。</p><div><pre class="programlisting">            for epoch in range(PPO_EPOCHES):
                for batch_ofs in range(0, len(trajectory), PPO_BATCH_SIZE):
                    states_v = traj_states_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    actions_v = traj_actions_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    batch_adv_v = traj_adv_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE].unsqueeze(-1)
                    batch_ref_v = traj_ref_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]
                    batch_old_logprob_v = old_logprob_v[batch_ofs:batch_ofs + PPO_BATCH_SIZE]</pre></div><p class="calibre8">训练评论家，我们需要做的就是用事先计算好的参考值计算出<strong class="calibre2">均方误差</strong> ( <strong class="calibre2"> MSE </strong>)损失。</p><div><pre class="programlisting">                    opt_crt.zero_grad()
                    value_v = net_crt(states_v)
                    loss_value_v = F.mse_loss(value_v.squeeze(-1), batch_ref_v)
                    loss_value_v.backward()
                    opt_crt.step()</pre></div><p class="calibre8">在演员训练中，我们最小化被否定的裁剪目标:<img src="img/00331.jpeg" alt="Implementation" class="calibre24"/>，其中<img src="img/00332.jpeg" alt="Implementation" class="calibre24"/>。下面几行是这个公式的一个简单的<a id="id654" class="calibre1"/>实现。</p><div><pre class="programlisting">                    opt_act.zero_grad()
                    mu_v = net_act(states_v)
                    logprob_pi_v = calc_logprob(mu_v, net_act.logstd, actions_v)
                    ratio_v = torch.exp(logprob_pi_v - batch_old_logprob_v)
                    surr_obj_v = batch_adv_v * ratio_v
                    clipped_surr_v = batch_adv_v * torch.clamp(ratio_v, 1.0 - PPO_EPS, 1.0 + PPO_EPS)
                    loss_policy_v = -torch.min(surr_obj_v, clipped_surr_v).mean()
                    loss_policy_v.backward()
                    opt_act.step()</pre></div></div></div></body></html>


<html>
  <head>
    <title>Proximal Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch15lvl2sec100" class="calibre1"/>结果</h2></div></div></div><p class="calibre8">在我们的两个测试<a id="id655" class="calibre1"/>环境中训练，PPO方法比A2C方法有了很大的改进。下面的图表显示了在RoboschoolHalfCheetah-v1环境下的训练进度，当这些方法仅在两个小时的训练和1.3米的观察后就能够达到1k的分数，这比A2C用10米的观察和15个小时获得相同的结果要好得多。在10个小时的培训后，代理能够获得的最高奖励超过2600英镑，看起来它可以做得更好，因为只需要一些调整，如<strong class="calibre2">学习率</strong> ( <strong class="calibre2"> LR </strong>)衰减。</p><div><img src="img/00333.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图HalfCheetah上的PPO聚合</p></div></div><p class="calibre10"> </p><p class="calibre8">在robocoolant-v1上，<a id="id656" class="calibre1"/>奖励的增加也几乎是线性的，在20小时的测试中，最大奖励是1848，这也是对A2C方法的改进。</p><div><img src="img/00334.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图Ant环境中的PPO</p></div></div><p class="calibre10"> </p></div></div></body></html>


<html>
  <head>
    <title>Trust Region Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch15lvl1sec72" class="calibre1"/>信任区域策略优化</h1></div></div></div><p class="calibre8">TRPO是2015年由<a id="id657" class="calibre1"/>伯克利的研究人员在John Schulman等人的论文中提出的，名为<em class="calibre11">信任区域策略优化</em> (arXiv:1502.05477)。本文朝着提高随机策略梯度优化的稳定性和一致性迈出了一步，并在各种控制任务中显示了良好的结果。</p><p class="calibre8">不幸的是，这篇论文和这个方法非常数学化，所以很难理解这个方法的细节。这同样适用于使用共轭梯度法有效解决约束优化问题的实现。</p><p class="calibre8">作为第一步，TRPO方法定义了该州的折扣访问频率:<img src="img/00335.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>。在该等式中，<img src="img/00336.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>等于在采样轨迹的位置I处将遇到的状态s的采样概率。然后，TRPO将优化目标定义为<img src="img/00337.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>，其中<img src="img/00338.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>是策略的期望折扣报酬，<img src="img/00339.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>定义确定性策略。</p><p class="calibre8">为了解决大规模策略更新的问题，TRPO定义了对策略更新的附加约束，表示为新旧策略之间的最大<strong class="calibre2">kull back-lei bler</strong>(<strong class="calibre2">KL</strong>)-偏差，可以写成<img src="img/00340.jpeg" alt="Trust Region Policy Optimization" class="calibre24"/>。</p></div></body></html>


<html>
  <head>
    <title>Trust Region Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch15lvl2sec101" class="calibre1"/>实施</h2></div></div></div><p class="calibre8">GitHub或其他开源库上可用的大多数TRPO <a id="id658" class="calibre1"/>实现彼此非常相似，可能是因为它们都是从最初的约翰·舒尔曼TRPO实现发展而来。我的TRPO版本也没有太大的不同，它使用了实现共轭梯度法(TRPO用来解决约束优化问题)的核心函数，这些函数来自这个库:<a class="calibre1" href="https://github.com/ikostrikov/pytorch-trpo">https://github.com/ikostrikov/pytorch-trpo</a>。</p><p class="calibre8">完整的示例在<code class="literal">Chapter15/03_train_trpo.py</code>和<code class="literal">Chapter15/lib/trpo.py</code>中，训练循环与PPO示例非常相似:我们对预定义长度的转换轨迹进行采样，并使用PPO部分中给出的平滑公式计算优势估计值(过去，该估计值首先在TRPO论文中提出)。接下来，我们使用MSE损失和计算的参考值进行一个训练步骤，并且进行一个TRPO更新步骤，该步骤包括通过使用共轭梯度法找到我们应该去的方向，并且沿着该方向进行线性搜索，以找到保持期望的KL散度的步骤。</p><p class="calibre8">以下是执行这两个步骤的<a id="id659" class="calibre1"/>训练循环片段:</p><div><pre class="programlisting">            # critic step
            opt_crt.zero_grad()
            value_v = net_crt(traj_states_v)
            loss_value_v = F.mse_loss(value_v.squeeze(-1), traj_ref_v)
            loss_value_v.backward()
            opt_crt.step()</pre></div><p class="calibre8">为了执行TRPO步骤，我们需要提供两个函数:第一个函数将计算当前参与者策略的损失，它使用与PPO中新旧策略乘以优势估计相同的比率。第二个函数必须计算旧策略和当前策略之间的KL-divergence。</p><div><pre class="programlisting">            # actor step
            def get_loss():
                mu_v = net_act(traj_states_v)
                logprob_v = calc_logprob(mu_v, net_act.logstd, traj_actions_v)
                action_loss_v = -traj_adv_v.unsqueeze(dim=-1) * torch.exp(logprob_v - old_logprob_v)
                return action_loss_v.mean()

            def get_kl():
                mu_v = net_act(traj_states_v)
                logstd_v = net_act.logstd
                mu0_v = mu_v.detach()
                logstd0_v = logstd_v.detach()
                std_v = torch.exp(logstd_v)
                std0_v = std_v.detach()
                kl = logstd_v - logstd0_v + (std0_v ** 2 + ((mu0_v - mu_v) ** 2) / (2.0 * std_v ** 2)) - 0.5
                return kl.sum(1, keepdim=True)

            trpo.trpo_step(net_act, get_loss, get_kl, TRPO_MAX_KL, TRPO_DAMPING, device=device)</pre></div><p class="calibre8">换句话说，PPO方法是TRPO，它使用简单的策略比率剪裁来限制策略更新，而不是复杂的共轭梯度和线搜索。</p></div></div></body></html>


<html>
  <head>
    <title>Trust Region Policy Optimization</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch15lvl2sec102" class="calibre1"/>结果</h2></div></div></div><p class="calibre8">从我的实验来看，TRPO显示出比A2C基线更好的结果，但表现比PPO差。很有可能这只是由于未经调整的复制粘贴TRPO的低级机制。</p><p class="calibre8">在HalfCheetah测试上，TRPO经过5百万次的观察和8小时的训练，能够达到1k奖励。在1300万次观察中，它的回报翻了一番，这几乎花了它一天的时间。</p><div><img src="img/00341.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">图HalfCheetah环境中的TRPO</p></div></div><p class="calibre10">在机器人学校-v1上的训练就不那么成功了。三个小时，1.5M步，代理拿到了700的奖励，但是后来训练过程出现了分歧。</p><p class="calibre8">图Ant环境中的TRPO</p><div><img src="img/00342.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 7: TRPO on the Ant environment</p></div></div><p class="calibre10"><a id="ch15lvl1sec73" class="calibre1"/> A2C使用ACKTR</p></div></div></body></html>


<html>
  <head>
    <title>A2C using ACKTR</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">我们将比较的第三种方法<a id="id661" class="calibre1"/>使用不同的方法来解决SGD稳定性问题。在吴等人于2017年发表的名为【使用Kronecker-Factored Approximation进行深度强化学习的可扩展信赖域方法】的论文(arXiv:1708.05144)中，作者将二阶优化方法和信赖域方法相结合。</h1></div></div></div><p class="calibre8">二阶方法的思想是通过取优化函数的二阶导数(换句话说，它的曲率)来改进传统的SGD，以提高优化过程的收敛性。让事情变得更复杂的是，处理二阶导数通常需要你构建并求逆一个Hessian矩阵，这个矩阵可能会非常大，所以实用的方法通常会以某种方式近似它。该领域目前非常活跃，因为开发健壮的、可扩展的优化方法对于整个机器学习领域非常重要。</p><p class="calibre8">其中一种二阶方法被称为<strong class="calibre2">克罗内克因子化的</strong>近似(通常缩写为<strong class="calibre2"> KFAC </strong>)，由詹姆斯·马腾斯和罗杰·格罗斯在他们2015年发表的论文<em class="calibre11">用克罗内克因子化的近似曲率优化神经网络</em>中提出。然而，这种方法的详细描述超出了本书的范围。</p><p class="calibre8"><a id="ch15lvl2sec103" class="calibre1"/>实施</p></div></body></html>


<html>
  <head>
    <title>A2C using ACKTR</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1">由于KFAC <a id="id662" class="calibre1"/>方法是最近才出现的，PyTorch中没有实现该方法的优化器。唯一可用的PyTorch原型来自伊利亚·科斯特里科夫，在这里可以找到:<a class="calibre1" href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr</a>。TensorFlow有另一个版本的KFAC，它带有OpenAI基线，但在PyTorch上移植和测试它可能是一件很难的事情。</h2></div></div></div><p class="calibre8">在我的实验中，我从上面的链接中提取了KFAC，并将其应用到现有的代码中，这需要替换优化器并执行额外的<code class="literal">backward()</code>调用来收集Fisher信息。评论家的训练方式和在A2C一样。</p><p class="calibre8">完整的例子在<code class="literal">Chapter15/05_train_acktr.py</code>中，这里没有显示，因为它基本上与A2C相同。唯一的区别是使用了不同的优化器。</p><p class="calibre8"><a id="ch15lvl2sec104" class="calibre1"/>结果</p></div></div></body></html>


<html>
  <head>
    <title>A2C using ACKTR</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2">在RoboschoolAnt-v1环境下，ACKTR方法显示出比A2C更好的结果，但比PPO差。当然，这些结果必须有所保留，因为我在这里没有进行太多的超参数调优(这留给读者作为练习)，结果可能取决于实际环境，所以通常在手头的任务中尝试不同的方法是一种很好的做法。</h2></div></div></div><p class="calibre8">图HalfCheetah上的ACKTR</p><div><img src="img/00343.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 8: ACKTR on HalfCheetah</p></div></div><p class="calibre10">在HalfCheetah <a id="id664" class="calibre1"/>环境下，ACKTR表现得不太稳定，只能达到1k的奖励。</p><p class="calibre8">图Ant上的ACKTR</p><div><img src="img/00344.jpeg" alt="Results" class="calibre9"/><div><p class="calibre14">Figure 9: ACKTR on Ant</p></div></div><p class="calibre10"><a id="ch15lvl1sec74" class="calibre1"/>总结</p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0">在这一章中，我们检查了三种不同的方法，旨在提高随机政策梯度的稳定性，并将它们与A2C在两个连续控制问题上的实现进行了比较。使用上一章的方法(DDPG和D4PG)，他们创建了基本的工具来处理连续的控制域。</h1></div></div></div><p class="calibre8">在下一章，我们将切换到最近流行的一组不同的RL方法:黑盒或无梯度方法。</p><p class="calibre8">In the next chapter, we'll switch to a different set of RL methods that have been becoming popular recently: black-box or gradient-free methods.</p></div></body></html>
</body></html>