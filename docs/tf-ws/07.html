<html><head/><body>


	
		<title>B16341_07_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-132"><a id="_idTextAnchor153"/> 7。卷积神经网络</h1>
		</div>
		<div><p class="callout-heading">概观</p>
			<p class="callout">在本章中，你将学习<strong class="bold">卷积神经网络</strong>(<strong class="bold">CNN</strong>)如何处理图像数据。您还将学习如何在图像数据上正确使用CNN。</p>
			<p class="callout">本章结束时，您将能够使用TensorFlow创建自己的CNN，用于对任何图像数据集进行分类和对象识别。</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor154"/>简介</h1>
			<p>本章涵盖CNN。CNN使用非常适合从图像中提取特征的卷积层。他们使用与手头任务相关的学习过滤器。简单来说，他们非常擅长在图像中寻找模式。</p>
			<p>在前一章中，您探索了正则化和超参数调整。您使用了L1和L2正则化，并在分类模型中添加了dropout，以防止在<code>connect-4</code>数据集上过度拟合。</p>
			<p>当你用CNN深入学习的时候，你将会改变很多。在本章中，你将学习CNN如何处理图像数据的基础知识，以及如何将这些概念应用到你自己的图像分类问题中。这才是TensorFlow真正的亮点。</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor155"/>CNN</h1>
			<p>CNN与你迄今为止建立的ann有许多共同的组成部分。关键区别在于网络中包含一个或多个卷积层。卷积层使用滤波器(也称为内核)对输入数据进行卷积。将<strong class="bold">卷积</strong>想象成<strong class="bold">图像转换器</strong>。你有一个输入图像，它通过CNN，给你一个输出标签。每一层都有独特的功能或特殊的能力来检测图像中的曲线或边缘等图案。CNN结合了深度神经网络和核卷积的能力来转换图像，并使这些图像边缘或曲线易于模型看到。CNN有三个关键部分:</p>
			<ul>
				<li><strong class="bold">输入图像</strong>:原始图像数据</li>
				<li><strong class="bold">滤镜/内核</strong>:图像转换机制</li>
				<li><strong class="bold">输出标签</strong>:图像分类</li>
			</ul>
			<p>下图是CNN的一个示例，其中图像在左侧输入网络，在右侧生成输出。图像分量在整个隐藏层中用更基本的分量来标识，例如在较早的隐藏层中标识的边缘。影像成分在隐藏图层中组合，形成数据集中可识别的要素。例如，在CNN中将图像分类为飞机或汽车时，可识别的特征可以是类似轮子或螺旋桨的过滤器。这些特征的组合将有助于确定图像是飞机还是汽车。</p>
			<p>最后，输出层是一个密集层，用于确定模型的具体输出。对于二元分类模型，这可以是具有一个具有sigmoid激活函数的单元的密集层。对于更复杂的多类分类，它可能是一个具有许多单元的密集图层，由类的数量和softmax激活函数确定，用于为呈现给模型的每个影像确定一个输出标注。</p>
			<div><div><img src="img/B16341_07_01.jpg" alt="Figure 7.1: CNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.1: CNN</p>
			<p>常见的CNN配置包括卷积层，随后是汇集层。这些层通常按此顺序成对使用(卷积和池化)。我们将在本章的后面讨论其原因，但现在，请将这些合并图层视为通过汇总过滤结果来减小输入图像的大小。</p>
			<p>在深入研究卷积层之前，首先需要了解从计算机的角度来看数据是什么样的。</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor156"/>形象再现</h1>
			<p>首先，考虑计算机如何处理图像。对计算机来说，图像就是数字。为了能够使用图像进行分类或对象识别，您需要了解模型如何将图像输入转换为数据。图像文件中的<strong class="bold">像素</strong>只是一段数据。</p>
			<p>在下图中，您可以看到数字8的灰度图像的像素值示例。对于<code>28x28</code>像素的图像，总共有<code>784</code>个像素。每个像素都有一个介于<code>0</code>和<code>255</code>之间的值，用于标识像素的明暗程度。在右侧，有一个大的列向量，其中列出了每个像素值。这被模型用来识别图像。</p>
			<div><div><img src="img/B16341_07_02.jpg" alt="Figure 7.2: Pixel values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.2:像素值</p>
			<p>现在您已经知道了输入数据的样子，是时候更仔细地了解卷积过程了，更具体地说，是卷积层。</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor157"/>卷积层</h1>
			<p>把卷积想象成一个有三个关键元素的图像转换器。首先是输入图像，然后是过滤器，最后是特征图。</p>
			<p>本节将依次讨论这些问题，让您对图像如何在卷积层中滤波有一个清晰的了解。卷积是在输入数据上通过一个过滤窗口的过程，这将产生一个激活图，称为二维数据的<code>3x3</code>，其中过滤器的具体值在训练过程中学习。滤波器以与滤波器大小相等的窗口大小通过输入数据，然后应用滤波器和输入数据部分的标量积，产生所谓的<strong class="bold">激活</strong>。当该过程使用相同的过滤器在整个输入数据上继续时，产生激活图，也称为<strong class="bold">特征图</strong>。</p>
			<p>下图说明了这一概念，图中有两个卷积层，生成了两组特征图。在从第一卷积层产生特征图之后，它们被传递到第二卷积层。第二卷积层的特征图被传递到分类器中:</p>
			<div><div><img src="img/B16341_07_03.jpg" alt="Figure 7.3: Convolution for classification&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.3:用于分类的卷积</p>
			<p>过滤器每次操作移动的距离或步数称为<code>0</code>。这被称为<strong class="bold">有效填充</strong>。</p>
			<p>让我们回顾一些关键词。有一个<code>2x2</code>内核。有<strong class="bold">步距</strong>，这是你移动内核的像素数。最后，无论是否添加像素，图像周围都有零填充。这确保了输出与输入的大小相同。</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor158"/>创建模型</h2>
			<p>从第一章开始，你会遇到不同类型的维度张量。需要注意的一件重要事情是，你只会和<code>Conv2D</code>一起工作。图层名<code>Conv2D</code>仅指<strong class="bold">滤镜</strong>或<strong class="bold">内核</strong>的移动。因此，如果你还记得卷积过程的描述，它只是在2D空间中滑动一个内核。因此，对于一个平面的正方形图像，内核只能在两个维度上滑动。</p>
			<p>当您实现<code>Conv2D</code>时，您需要传入某些参数:</p>
			<ol>
				<li>第一个参数是<code>filter</code>。过滤器是输出空间的维度。</li>
				<li>指定<code>strides</code>，这是内核移动的像素数量。</li>
				<li>然后，指定<code>padding</code>，通常是<code>valid</code>或<code>same</code>，这取决于您是否想要一个与输入尺寸相同的输出。</li>
				<li>最后还可以有<code>activation</code>。在这里，您将指定您想要应用到输出的激活类型。如果不指定激活，它只是一个线性激活。</li>
			</ol>
			<p>在继续之前，回忆一下第四章、<em class="italic">回归和分类模型</em>中的内容，密集层是指每个神经元都与前一层中的每个神经元相连。正如你在下面的代码中看到的，你可以很容易地用<code>model.add(Dense(32))</code>添加一个密集层。<code>32</code>是神经元的数量，后跟输入形状。AlexNet是一个具有多个卷积核的CNN的例子，它从图像中提取有趣的信息。</p>
			<div><div><img src="img/B16341_07_04.jpg" alt="Figure 7.4: AlexNet consists of five convolution layers and three connected layers &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.4: AlexNet由五个卷积层和三个连接层组成</p>
			<p class="callout-heading">注意</p>
			<p class="callout">AlexNet是由Alex Krizhevsky设计的一个CNN的名字。</p>
			<p>顺序模型可以用来构建CNN。可以使用不同的方法来添加层；这里，我们将使用使用模型的<code>add</code>方法向模型顺序添加层的框架，或者在模型被实例化时传入所有层的列表:</p>
			<pre>model = models.Sequential()
model.add(Dense(32, input_shape=(250,)))</pre>
			<p>下面是一个代码块，显示了您将在本章后面使用的代码:</p>
			<pre>our_cnn_model = models.Sequential([layers.Conv2D\
                                   (filters = 32, \
                                    kernel_size = (3,3),
                                    input_shape=(28, 28, 1)), \
                                   layers.Activation('relu'), \
                                   layers.MaxPool2D\
                                   (pool_size = (2, 2)), \
                                   layers.Conv2D\
                                   (filters = 64, \
                                    kernel_size = (3,3)), \
                                   layers.Activation('relu'), \
                                   layers.MaxPool2D\
                                   (pool_size = (2,2)), \
                                   layers.Conv2D\
                                   (filters = 64, \
                                    kernel_size = (3,3)), \
                                    layers.Activation('relu')])</pre>
			<p>当处理需要二维卷积的数据(如图像)时，使用<code>Conv2D</code>层。对于参数，将滤镜数量设置为<code>32</code>，后跟<code>3x3</code>像素的内核大小(示例中为<code>(3, 3)</code>)。在第一层，你总是需要指定<code>input_shape</code>的尺寸，高度，宽度和深度。<code>input_shape</code>是您将要使用的图像的大小。您还可以选择要在层的末端应用的激活功能。</p>
			<p>现在您已经学习了如何在模型中构建CNN层，您将在第一个练习中练习这样做。在本练习中，您将构建CNN的第一个结构，初始化模型，并向模型添加一个卷积层。</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor159"/>练习7.01:创建第一层来构建CNN</h2>
			<p>作为一名TensorFlow自由职业者，你被要求向你的潜在雇主展示几行代码，展示你可能如何构建CNN的第一层。他们要求您保持简单，但提供创建CNN层的最初几个步骤。在本练习中，您将完成创建CNN的第一步，即添加第一个卷积层。</p>
			<p>按照以下步骤完成本练习:</p>
			<ol>
				<li value="1">打开新的Jupyter笔记本。</li>
				<li>从<code>tensorflow.keras</code> : <pre>import tensorflow as tf from tensorflow.keras import models, layers</pre>导入TensorFlow库和<code>models</code>和<code>layers</code>类</li>
				<li>Check the TensorFlow version:<pre>print(tf.__version__)</pre><p>您应该得到以下输出:</p><pre>2.6.0</pre></li>
				<li>Now, use <code>models.Sequential</code> to create your model. The first layer (<code>Conv2D</code>) will require the number of nodes (<code>filters</code>), the filter size (<code>3,3</code>), and the shape of the input. <code>input_shape</code> for your first layer will determine the shape of your input images. Add a ReLU activation layer:<pre>image_shape = (300, 300, 3)
our_first_layer = models.Sequential([layers.Conv2D\
                                    (filters = 16, \
                                    kernel_size = (3,3), \
                                    input_shape = image_shape), \
                                    layers.Activation('relu')])</pre><p>很简单。你刚刚迈出了创建你的第一个CNN的第一步。</p></li>
			</ol>
			<p>现在，您将继续学习卷积层之后的层类型——池层。</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor160"/>汇集层</h1>
			<p>池化是一种通常添加到CNN的操作，通过减少来自其跟随的卷积层的输出中的像素数量来减少图像的维度。<strong class="bold">池层</strong>缩小输入图像以提高计算效率并减少参数数量以限制<strong class="bold">过拟合</strong>的风险。</p>
			<p><strong class="bold">池层</strong>紧跟卷积层，被认为是CNN结构的另一个重要部分。本节将重点介绍两种类型的池:</p>
			<ul>
				<li>最大池化</li>
				<li>平均池</li>
			</ul>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor161"/>最大池化</h2>
			<p>使用最大池，过滤器或内核仅保留输入矩阵中的最大像素值。为了更清楚地了解正在发生的事情，考虑下面的例子。假设您有一个<code>4x4</code>输入。max pooling的第一步是将<code>4x4</code>矩阵分成四个象限。每个象限的大小都是<code>2x2</code>。应用大小为<code>2</code>的过滤器。这意味着你的过滤器将看起来完全像一个<code>2x2</code>矩阵。</p>
			<p>首先将过滤器放在输入的顶部。对于最大池，该过滤器将查看其覆盖的<code>2x2</code>区域内的所有值。它将查找最大值，将该值发送到您的输出，并将其存储在要素地图的左上角。</p>
			<div><div><img src="img/B16341_07_05.jpg" alt="Figure 7.5: Max pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.5:最大池化</p>
			<p>然后，过滤器将向右移动并重复相同的过程，将值存储在<code>2x2</code>矩阵的右上角。一旦该操作完成，过滤器将向下滑动并从最左侧开始，再次重复相同的过程，寻找最大(或最大)值，然后将其存储在<code>2x2</code>矩阵的正确位置。</p>
			<p>回想一下，滑动被称为<code>2</code>。重复该过程，直到四个象限中的最大值分别为<code>8</code>、<code>5</code>、<code>7</code>和<code>5</code>。同样，为了获得这些数字，您使用了一个过滤器<code>2x2</code>并过滤掉了那个<code>2x2</code>矩阵中的最大数字。</p>
			<p>所以，在这种情况下，你的步幅是2，因为你移动了2个像素。这些是<code>filter</code>和<code>stride</code>是<code>2</code>。<em class="italic">图7.6 </em>显示了过滤器大小为3 x 3且<code>stride</code>为<code>1</code>时，最大池的实现可能是什么样子。</p>
			<p>图7.6 中的<em class="italic">有两个步骤。从特征图的左上方开始。使用<code>3x3</code>过滤器，您将查看以下数字:<code>2</code>、<code>8</code>、<code>2</code>、<code>5</code>、<code>4</code>、<code>9</code>、<code>8</code>、<code>4</code>和<code>6</code>，并选择最大值<code>9</code>。<code>9</code>将被放置在我们汇集的特征地图的左上角框中。如果步幅为<code>1</code>，您可以将滤镜向右滑动一个位置，如灰色所示。</em></p>
			<p>现在，从<code>8</code>、<code>2</code>、<code>1</code>、<code>4</code>、<code>9</code>、<code>6</code>、<code>4</code>、<code>6</code>和<code>4</code>中寻找最大值。同样，<code>9</code>是最大的值，所以在汇集的特征图的顶行的中间位置添加一个<code>9</code>(以灰色显示)。</p>
			<div><div><img src="img/B16341_07_06.jpg" alt="Figure 7.6: Pooled feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.6:汇集的要素地图</p>
			<p>前面的池大小是<code>(2, 2)</code>。它指定了您将缩小比例的因素。下面更详细地看一下你可以做些什么来实现<code>MaxPool2D</code>:</p>
			<pre>layers.MaxPool2D(pool_size=(2, 2), strides=None, \
                 padding='valid')</pre>
			<p><code>MaxPool2D</code>实例。代码片段使用池大小<code>2x2</code>初始化最大池层，并且未指定<code>stride</code>值，因此它将默认为池大小值。<code>padding</code>参数设置为<code>valid</code>，表示没有添加填充。下面的代码片段演示了它在CNN中的用法:</p>
			<pre>image_shape = (300, 300, 3)
our_first_model = models.Sequential([
    layers.Conv2D(filters = 16, kernel_size = (3,3), \
                  input_shape = image_shape), \
    layers.Activation('relu'), \
    layers.MaxPool2D(pool_size = (2, 2)), \
    layers.Conv2D(filters = 32, kernel_size = (3,3)), \
    layers.Activation('relu')])</pre>
			<p>在前面的示例中，用两个卷积层创建了一个顺序模型，每个层之后是一个ReLU激活函数，第一个卷积层的激活函数之后是一个max池层。</p>
			<p>既然您已经研究了最大池，让我们看看另一种类型的池:平均池。</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor162"/>平均池</h2>
			<p><strong class="bold">平均池</strong>的操作方式与最大池相似，但它不是提取过滤器中的最大重量值，而是计算平均值。然后，它将该值传递给特征映射。<em class="italic">图7.7 </em>强调了最大汇集和平均汇集之间的区别。</p>
			<p>在<em class="italic">图7.7 </em>中，考虑左边的<code>4x4</code>矩阵。左上象限中数字的平均值是<code>13</code>。这将是平均池值。如果是最大池，同一个左上象限会将<code>20</code>输出到其特征图，因为<code>20</code>是过滤器框架内的最大值。这是使用超参数对最大池和平均池进行的比较，其中<code>filter</code>和<code>stride</code>参数都设置为<code>2</code>:</p>
			<div><div><img src="img/B16341_07_07.jpg" alt="Figure 7.7: Max versus average pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.7:最大与平均池</p>
			<p>对于平均池，您可以使用<code>AveragePooling2D</code>代替<code>MaxPool2D</code>。</p>
			<p>要实现平均池代码，可以使用以下代码:</p>
			<pre>layers.AveragePooling2D(pool_size=(2, 2), strides=None, \
                        padding='valid')</pre>
			<p><code>AveragePooling2D</code>层。与最大池化类似，可以修改<code>pool_size</code>、<code>strides</code>和<code>padding</code>参数。下面的代码片段演示了它在CNN中的用法:</p>
			<pre>image_shape = (300, 300, 3)
our_first_model = models.Sequential([
    layers.Conv2D(filters = 16, kernel_size = (3,3), \
                  input_shape = image_shape), \
    layers.Activation('relu'), \
    layers.AveragePooling2D(pool_size = (2, 2)), \
    layers.Conv2D(filters = 32, kernel_size = (3,3)), \
    layers.Activation('relu')])</pre>
			<p>牢记使用池化图层的好处是一个好主意。其中一个好处是，如果对图像进行下采样，图像会缩小。这意味着你需要处理的数据更少，需要做的乘法也更少，这当然加快了速度。</p>
			<p>到目前为止，您已经创建了第一个CNN层，并学习了如何使用池层。现在，在下面的练习中，您将使用到目前为止所学的知识为CNN构建一个池层。</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor163"/>练习7.02:为CNN创建一个池层</h2>
			<p>你收到了一封来自潜在雇主的电子邮件，内容是你在<em class="italic">练习7.01 </em>、<em class="italic">中申请的TensorFlow自由职业工作，创建第一层以构建CNN </em>。电子邮件询问您是否可以展示您将如何为CNN编写池层代码。在本练习中，您将按照潜在雇主的要求，通过添加池层来构建基础模型:</p>
			<ol>
				<li value="1">打开新的Jupyter笔记本，导入TensorFlow库:<pre>import tensorflow as tf from tensorflow.keras import models, layers</pre></li>
				<li>使用<code>models.Sequential</code>创建您的模型。第一层，<code>Conv2D</code>，将需要节点的数量，过滤器的大小，以及张量的形状，就像之前的练习一样。接下来是激活层，神经网络末端的节点:<pre>image_shape = (300, 300, 3) our_first_model = models.Sequential([     layers.Conv2D(filters = 16, kernel_size = (3,3), \                   input_shape = image_shape), \     layers.Activation('relu')])</pre></li>
				<li>Now, add a <code>MaxPool2D</code> layer by using the model's <code>add</code> method:<pre>our_first_model.add(layers.MaxPool2D(pool_size = (2, 2))</pre><p>在这个模型中，您创建了一个带有卷积层的CNN，接着是一个ReLU激活函数，然后是一个max池层。这些模型用三种颜色通道拍摄大小为<code>300x300</code>的图像。</p></li>
			</ol>
			<p>现在你已经成功地添加了一个<code>MaxPool2D</code>层到你的CNN，下一步是添加一个<strong class="bold">展平层</strong>，这样你的模型就可以使用所有的数据了。</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor164"/>展平图层</h2>
			<p>添加展平层是一个重要的步骤，因为您需要为神经网络提供它可以处理的数据。请记住，在您执行卷积运算后，它仍然是多维的。因此，要将您的数据改回一维形式，您将使用展平层。要实现这一点，您需要将汇集的要素地图展平成一列，如下图所示。在<em class="italic">图7.8 </em>中，您可以看到，您从图表左侧的输入矩阵开始，使用最终汇集的特征图，并将其扩展为单个列向量:</p>
			<div><div><img src="img/B16341_07_08.jpg" alt="Figure 7.8: Flattening layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.8:展平图层</p>
			<p>下面是一个实现的展平层:</p>
			<pre>image_shape = (300, 300, 3)
our_first_model = models.Sequential([
    layers.Conv2D(filters = 16, kernel_size = (3,3), \
                  input_shape = image_shape), \
    layers.Activation('relu'), \
    layers.MaxPool2D(pool_size = (2, 2)), \
    layers.Conv2D(filters = 32, kernel_size = (3,3)), \
    layers.Activation('relu'), \
    layers.MaxPool2D(pool_size = (2, 2)), \
    layers.Flatten()])</pre>
			<p>这里，一个展平层作为最后一层添加到这个模型。现在，您已经创建了第一个CNN和池层，您将在接下来的练习中将所有的部分放在一起并构建一个CNN。</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor165"/>练习7.03:构建CNN</h2>
			<p>在<em class="italic">练习7.01 </em>、<em class="italic">创建第一层以构建CNN </em>和<em class="italic">练习7.02 </em>、<em class="italic">为CNN </em>创建池层的工作中，你被聘为自由职业者。现在你已经得到了这份工作，你的第一项任务是帮助你的初创公司制造原型产品，向投资者展示并筹集资金。该公司正试图开发一个马或人的分类器应用程序，他们希望你马上开始。他们告诉你，他们现在只需要分类器工作，很快就会有改进的空间。</p>
			<p>在本练习中，您将使用<code>horses_or_humans</code>数据集为模型构建一个卷积基础层。在该数据集中，图像不居中。目标图像在帧中的所有角度和不同位置显示。在本章中，你将继续在这个基础上一点一点地增加内容。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">可以使用<code>tensorflow_datasets</code>包下载数据集。</p>
			<ol>
				<li value="1">Import all the necessary libraries:<pre>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import RMSprop
from keras_preprocessing import image as kimage</pre><p>首先，需要导入TensorFlow库。您将使用<code>tensorflow_datasets</code>加载数据集，<code>tensorflow.keras.models</code>构建顺序张量流模型，<code>tensorflow.keras.layers</code>向CNN模型添加层，<code>RMSprop</code>作为优化器，<code>matplotlib.pyplot</code>和<code>matplotlib.image</code>用于一些快速可视化。</p></li>
				<li>Load your dataset from the <code>tensorflow_datasets</code> package:<pre>(our_train_dataset, our_test_dataset), \
dataset_info = tfds.load('horses_or_humans',\
                         split = ['train', 'test'],\
                         data_dir = 'content/',\
                         shuffle_files = True,\
                         with_info = True)
assert isinstance(our_train_dataset, tf.data.Dataset)</pre><p>在这里，您使用了作为<code>tfds</code>导入的<code>tensorflow_datasets</code>包。您使用了<code>tfds.load()</code>函数来加载<code>horses_or_humans</code>数据集。这是一个二进制影像分类数据集，包含两个类:马和人。</p><p class="callout-heading">注意</p><p class="callout">关于数据集的更多信息可以在https://laurencemoroney.com/datasets.html找到。</p><p class="callout">关于<code>tensorflow_datasets</code>包装的更多信息可以在<a href="https://www.tensorflow.org/datasets">https://www.tensorflow.org/datasets</a>找到。</p><p><code>split = ['train', 'test']</code>参数指定您想要加载的数据分割。在本例中，您正在将列车和测试分别装载到<code>our_train_dataset</code>和<code>our_test_dataset</code>中。指定<code>with_info = True</code>将数据集的元数据加载到<code>dataset_info</code>变量中。加载后，使用<code>assert</code>确保加载的数据集是<code>tf.data.Dataset</code>对象类的实例。</p></li>
				<li>View information about the dataset using the loaded metadata in <code>dataset_info</code>:<pre>image_shape = dataset_info.features["image"].shape
print(f'Shape of Images in the Dataset: \t{image_shape}')
print(f'Number of Classes in the Dataset: \
      \t{dataset_info.features["label"].num_classes}')
names_of_classes = dataset_info.features["label"].names
for name in names_of_classes:
    print(f'Label for class "{name}": \
          \t\t{dataset_info.features["label"].str2int(name)}')</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_07_09.jpg" alt="Figure 7.9: horses_or_humans dataset information&#13;&#10;"/></div><p class="figure-caption">图7.9:马或人类数据集信息</p></li>
				<li>Now, view the number of images in the dataset and its distribution of classes:<pre>print(f'T<a id="_idTextAnchor166"/>otal examples in Train Dataset: \
      \t{len(our_train_dataset)}')
pos_tr_samples = sum(i['label'] for i in our_train_dataset)
print(f'Horses in Train Dataset: \t\t{len(our_train_dataset) \
                                      - pos_tr_samples}')
print(f'Humans in Train Dataset: \t\t{pos_tr_samples}')
print(f'\nTotal examples in Test Dataset: \
      \t{len(our_test_dataset)}')
pos_ts_samples = sum(i['label'] for i in our_test_dataset)
print(f'Horses in Test Dataset: \t\t{len(our_test_dataset) \
                                     - pos_ts_samples}')
print(f'Humans in Test Dataset: \t\t{pos_ts_samples}') </pre><p>您应该得到以下输出:</p><div><img src="img/B16341_07_10.jpg" alt="Figure 7.10: horses_or_humans dataset distribution&#13;&#10;"/></div><p class="figure-caption">图7.10:马或人类数据集分布</p></li>
				<li>Now, view some sample images in the training dataset, using the <code>tfds.show_examples()</code> function:<pre>fig = tfds<a id="_idTextAnchor167"/>.show_examples(our_train_dataset, dataset_info)</pre><p>此函数用于交互式使用，它显示并返回训练数据集中的图像图。</p><p>您的输出应该如下所示:</p><div><img src="img/B16341_07_11.jpg" alt="Figure 7.11: Sample training images&#13;&#10;"/></div><p class="figure-caption">图7.11:样本训练图像</p></li>
				<li>View some sample images in the test dataset:<pre>fig = tfds.show_examples(our_test_dataset, dataset_info)</pre><p>您将获得以下输出:</p><div><img src="img/B16341_07_12.jpg" alt="Figure 7.12: Sample test images&#13;&#10;"/></div><p class="figure-caption">图7.12:样本测试图像</p></li>
				<li>最后，用<code>our_model = models.Sequential</code>创建你的模型。设置第一层<code>Conv2D</code>，设置<code>filters</code>为<code>16</code>。内核是<code>3x3</code>。使用ReLU激活。因为这是第一个卷积层，你还需要设置<code>input_shape</code>到<code>image_shape</code>，你正在处理的彩色图像的尺寸。现在，添加<code>MaxPool2D</code>池层。然后，添加另一个<code>Conv2D</code>和<code>MaxPool2D</code>对以获得更多的模型深度，接着是展平层和致密层:<pre>our_cnn_model = models.Sequential([     layers.Conv2D(filters = 16, kernel_size = (3,3), \                   input_shape = image_shape),\     layers.Activation('relu'),\     layers.MaxPool2D(pool_size = (2, 2)),\     layers.Conv2D(filters = 32, kernel_size = (3,3)),\     layers.Activation('relu'),\     layers.MaxPool2D(pool_size = (2, 2)),\     layers.Flatten(),\     layers.Dense(units = 512),\     layers.Activation('relu'),\     layers.Dense(units = 1),\     layers.Activation('sigmoid') ])</pre></li>
				<li>Compile the model with <code>RMSProp</code> for <code>optimizer</code> set to the recommended default of <code>0.001</code>, <code>loss</code> as <code>binary_crossentropy</code>, and <code>metrics</code> set to <code>acc</code> for accuracy. Print the model summary using the <code>summary()</code> method:<pre>our_cnn_model.compile(optimizer=RMSprop(learning_rate=0.001), \
                      loss='binary_crossentropy',\
                      metrics=['acc'], loss_weights=None,\
                      weighted_metrics=None, run_eagerly=None,\
                      steps_per_execution=None)
print(our_cnn_model.summary())</pre><p>这将打印模型摘要，包括层类型、输出形状和参数的详细信息:</p><div><img src="img/B16341_07_13.jpg" alt="Figure 7.13: Model summary&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图7.13:模型摘要</p>
			<p>在前面的截图中，您可以看到左侧列出了层和类型。这些层按照从第一层到最后一层、从上到下的顺序列出。输出形状显示在中间。指定层旁边列出了每个层的几个参数。在底部，您会看到总参数、可训练参数和不可训练参数的数量。</p>
			<p>您已经能够探索卷积层和池层了。现在让我们深入到使用图像数据时的另一个重要组成部分:图像增强。</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor168"/>图像增强</h1>
			<p>扩充的定义是通过使某物在尺寸或数量上更大而使其更好。这正是数据或图像增强所做的。您使用增强为模型提供更多版本的图像训练数据。请记住，数据越多，模型的性能就越好。通过<em class="italic">扩充</em>你的数据，你可以以某种方式转换你的图像，使模型在真实数据上更好地一般化。要做到这一点，您需要<em class="italic">转换</em>您可以随意处理的图像，以便您可以使用您的增强图像以及原始图像数据集来进行比其他方式更丰富多样的训练。这可以改善结果并防止过度拟合。看看下面三张图片:</p>
			<div><div><img src="img/B16341_07_14.jpg" alt="Figure 7.14: Augmented leopard images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.14:增强的豹纹图像</p>
			<p>很明显，这是三张照片中的同一只豹子。只是位置不同而已。由于卷积，神经网络仍然可以理解这一点。但是，通过使用图像增强，您可以提高模型学习<strong class="bold">平移不变性</strong>的能力。</p>
			<p>与大多数其他类型的图像数据不同，您可以移动、旋转和移动图像来改变原始图像。这就产生了更多的数据，有了CNN，更多的数据和数据变化将会产生一个性能更好的模型。为了能够创建这些图像增强，看看如何在TensorFlow中使用加载的<code>tf.data.Dataset</code>对象来实现这一点。您将使用<code>dataset.map()</code>函数将预处理图像增强函数映射到您的数据集，即<code>our_train_dataset</code>:</p>
			<pre>from tensorflow import image as tfimage
from tensorflow.keras.preprocessing import image as kimage</pre>
			<p>为此，您将使用<code>tensorflow.image</code>和<code>tensorflow.keras.preprocessing.image</code>包。这些软件包具有许多图像处理功能，可用于图像数据增强:</p>
			<pre>augment_dataset(image, label):
    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)
    image = tfimage.random_flip_left_right(image)
    return image, label</pre>
			<p>其他功能包括:</p>
			<ul>
				<li><code>kimage.random_rotation</code>:该功能允许您在指定的角度之间随机旋转图像。</li>
				<li><code>kimage.random_brightness</code>:该功能随机调节亮度等级。</li>
				<li><code>kimage.random_shear</code>:该函数应用剪切变换。</li>
				<li><code>kimage.random_zoom</code>:该功能随机缩放图像。</li>
				<li><code>tfimage.random_flip_left_right</code>:该功能随机水平翻转图像。</li>
				<li><code>tfimage.random_flip_up_down</code>:该功能随机垂直翻转图像。</li>
			</ul>
			<p>在下一步中，您将传递想要用<code>tf.data.Dataset.map()</code>函数增加的数据:</p>
			<pre>augment_dataset(image, label):
    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)
    image = tfimage.random_flip_left_right(image)
    return image, label    
our_train_dataset = our_train_dataset.map(augment_dataset)
model.fit(our_train_dataset,\
          epochs=50,\
          validation_data=our_test_dataset)</pre>
			<p>在前面的代码块中，使用<code>fit()</code>，您只需要传递您已经创建的生成器。您需要传入<code>epochs</code>值。如果你不这样做，发电机将永远不会停止。<code>fit()</code>函数返回历史记录(绘制每次迭代的损失等等)。</p>
			<p>你需要更多的功能添加到<code>our_train_dataset</code>中，然后才能在上面训练模型。使用<code>batch()</code>功能，您可以指定每批训练多少幅图像。使用<code>cache()</code>功能，您可以将数据集放入内存以提高性能。使用<code>shuffle()</code>功能，您可以将数据集的混洗缓冲区设置为数据集的整个长度，以实现真正的随机性。<code>prefetch()</code>功能也用于良好的性能:</p>
			<pre>our_train_dataset = our_train_dataset.cache()
our_train_dataset = our_train_dataset.map(augment_dataset)
our_train_dataset = our_train_dataset.shuffle\
                    (len(our_train_dataset))
our_train_dataset = our_train_dataset.batch(128)
our_train_dataset = our_train_dataset.prefetch\
                    (tf.data.experimental.AUTOTUNE)</pre>
			<p>既然您已经看到了如何在您的培训模型中实现增强，那么让我们更仔细地看看其中的一些转换在做什么。</p>
			<p>这里有一个<code>random_rotation</code>、<code>random_shift</code>和<code>random_brightnes</code>实现的例子。使用以下代码将图像随机旋转到指定的值:</p>
			<pre>image = kimage.random_rotation(image, rg = 135)</pre>
			<p>在<em class="italic">图7.15 </em>中，可以看到<code>random_rotation</code>的结果。</p>
			<div><div><img src="img/B16341_07_15.jpg" alt="Figure 7.15: Rotation range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.15:旋转范围</p>
			<p>这些图像被随机旋转了135度。</p>
			<p><code>random_shift</code>用于横向随机移动像素。注意下面代码中的<code>.15</code>,这意味着图像可以随机移动15个像素:</p>
			<pre>image = kimage.random_shift(image, wrg = 0.15, hrg = 0) </pre>
			<p>下图显示了图像宽度的随机调整，最多调整15个像素:</p>
			<div><div><img src="img/B16341_07_16.jpg" alt="Figure 7.16: Width shift range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.16:宽度移动范围</p>
			<p>这里再次使用了<code>random_shift</code>，随机调整高度15个像素:</p>
			<pre>image = kimage.random_shift(image, wrg = 0, hrg = 0.15)</pre>
			<p><em class="italic">图7.17 </em>显示了图像高度的随机调整，最多调整15个像素:</p>
			<div><div><img src="img/B16341_07_17.jpg" alt="Figure 7.17: Height shift range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.17:高度移动范围</p>
			<p>对于使用<code>random_brightness</code>的随机亮度等级，您将使用一个浮动值范围按百分比使图像变亮或变暗。任何低于<code>1.0</code>的东西都会使图像变暗。因此，在本例中，图像随机变暗10%到90%:</p>
			<pre>image = kimage.random_brightness(image, brightness_range=(0.1,0.9))</pre>
			<p>在下图中，您用<code>random_brightness</code>调节了亮度:</p>
			<div><div><img src="img/B16341_07_18.jpg" alt="Figure 7.18: Brightness range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.18:亮度范围</p>
			<p>现在您已经了解了一些图像增强选项，接下来看看如何使用批处理规范化来提高模型的性能。</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor169"/>批量归一化</h2>
			<p>在2015年，<strong class="bold">批次规范化</strong>，也称为<strong class="bold">批次规范</strong>，由<em class="italic"> Christian Szegedy </em>和<em class="italic"> Sergey Ioffe </em>提出。批处理范数是一种减少训练历元数量以提高性能的技术。批处理规范标准化小型批处理的输入，并“标准化”输入层。它最常用于卷积层之后，如下图所示:</p>
			<div><div><img src="img/B16341_07_19.jpg" alt="Figure 7.19: Batch norm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.19:批量定额</p>
			<p>下图显示了实现批处理规范化的一种常见方式。在以下示例中，您可以看到一个批次范数层跟随一个卷积层三次。然后是一个展平层，接着是两个致密层:</p>
			<div><div><img src="img/B16341_07_20.jpg" alt="Figure 7.20: Layer sequences&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.20:层序列</p>
			<p>批量范数有助于模型更好地泛化。对于batch norm训练的每个批次，模型都有不同的均值和标准差。因为批次平均值和标准偏差与真实的总体平均值和标准偏差略有不同，所以这些变化会作为您正在训练的噪声，使模型的总体性能更好。</p>
			<p>下面是一个<code>BatchNormalization</code>实现的例子。您可以简单地添加一个批处理规范层，然后添加一个激活层:</p>
			<pre>model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), use_bias=False))
model.add(layers.BatchNormalization())
model.add(layers.Activation("relu"))</pre>
			<p>到目前为止，您已经创建了一个CNN模型，并学习了如何利用图像增强。现在，在接下来的练习中，您将把所有东西放在一起，用一些额外的卷积层构建一个CNN。</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor170"/>练习7.04:用附加卷积层构建CNN</h2>
			<p>你的新雇主对你在<em class="italic">练习7.03 </em>、<em class="italic">打造CNN </em>中所取得的成绩很满意。既然<strong class="bold">最小可行产品</strong> ( <strong class="bold"> MVP </strong>)或原型已经完成，是时候构建一个更好的模型了。</p>
			<p>在本练习中，您将向模型中添加额外的ANN层。您将向之前创建的卷积基础层添加额外的层。您将再次使用<code>horses_or_humans</code>数据集。</p>
			<p>让我们开始吧。</p>
			<p>因为您正在扩展<em class="italic">练习7.03 </em>、<em class="italic">构建CNN </em>，并且使用相同的数据，所以从上一练习的最后一步开始:</p>
			<ol>
				<li value="1">创建一个函数来重新缩放图像，然后使用<code>map</code>方法将该函数应用于训练和测试数据集。继续使用数据集的<code>cache</code>、<code>shuffle</code>、<code>batch</code>和<code>prefetch</code>方法构建您的训练和测试数据集管道:<pre>normalization_layer = layers.Rescaling(1./255) our_train_dataset = our_train_dataset.map\                     (lambda x: (normalization_layer(x['image']), \                                                     x['label']), \                      num_parallel_calls = \                      tf.data.experimental.AUTOTUNE) our_train_dataset = our_train_dataset.cache() our_train_dataset = our_train_dataset.shuffle\                     (len(our_train_dataset)) our_train_dataset = our_train_dataset.batch(128) our_train_dataset = \ our_train_dataset.prefetch(tf.data.experimental.AUTOTUNE) our_test_dataset = our_test_dataset.map\                    (lambda x: (normalization_layer(x['image']), \                                                    x['label']),\                     num_parallel_calls = \                     tf.data.experimental.AUTOTUNE) our_test_dataset = our_test_dataset.cache() our_test_dataset = our_test_dataset.batch(32) our_test_dataset = our_test_dataset.prefetch\                    (tf.data.experimental.AUTOTUNE)</pre></li>
				<li>Fit the model. Specify the values of <code>epochs</code> and <code>validation_steps</code> and set <code>verbose</code> equal to <code>1</code>:<pre>history = our_cnn_model.fit\
          (our_train_dataset, \
          validation_data = our_test_dataset, \
          epochs=15, \
          validation_steps=8, \
          verbose=1)</pre><p>输出如下所示:</p><div><img src="img/B16341_07_21.jpg" alt="Figure 7.21: Model fitting process&#13;&#10;"/></div><p class="figure-caption">图7.21:模型拟合过程</p></li>
				<li>Take a batch from the test dataset and plot the first image from the batch. Convert the image to an array, then use the model to predict what the image shows:<pre>from matplotlib.pyplot import imshow
for images, lables in our_test_dataset.take(1):
    imshow(np.asarray(images[0]))
    image_to_test = kimage.img_to_array(images[0])
    image_to_test = np.array([image_to_test])
    prediction = our_cnn_model.predict(image_to_test)
    print(prediction)
    if prediction &gt; 0.5:
        print("Image is a human")
        else:
        print("Image is a horse")</pre><p>输出将包含以下详细信息:</p><div><img src="img/B16341_07_22.jpg" alt="Figure 7.22: Output of image test with its metadata&#13;&#10;"/></div><p class="figure-caption">图7.22:图像测试及其元数据的输出</p><p>对于预测，您有一张来自测试集的人的照片，以查看分类是什么。</p></li>
				<li>Take a look at what's happening with each successive layer. Do this by creating a list containing all names of the layers within the CNN and another list containing predictions on a random sample from each of the layers in the list created previously. Next, iterate through the list of names of the layers and their respective predictions and plot the features:<pre>layer_outputs = []
for layer in our_cnn_model.layers[1:]:
    layer_outputs.append(layer.output)
layer_names = []
for layer in our_cnn_model.layers:
    layer_names.append(layer.name)
features_model = models.Model(inputs = our_cnn_model.input, \
                              outputs = layer_outputs)
random_sample = our_train_dataset.take(1)
layer_predictions = features_model.predict(random_sample)
for layer_name, prediction in zip(layer_names, \
                                  layer_predictions):
    if len(prediction.shape) != 4:
        continue
    num_features = prediction.shape[-1]
    size = prediction.shape[1]
    grid = np.zeros((size, size * num_features))
    for i in range(num_features):
        img = prediction[0, :, :, i]
        img = ((((img - img.mean()) / img.std()) * 64) + 128)
        img = np.clip(img, 0, 255).astype('uint8')
        grid[:, i * size : (i + 1) * size] = img
    scale = 20. / num_features
    plt.figure(figsize=(scale * num_features, scale))
    plt.title(layer_name)
    plt.imshow(grid)</pre><p>您应该会得到如下所示的内容:</p><div><img src="img/B16341_07_23.jpg" alt="Figure 7.23: Transformation at different layers&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图7.23:不同层的转换</p>
			<p>现在，您已经创建了自己的CNN模型，并使用它来确定图像是马还是人，现在您将关注如何对图像进行分类，看它是否是特定的类。</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor171"/>二值图像分类</h1>
			<p>二元分类是分类模型最简单的方法，因为它将图像分为两类。在这一章中，我们从卷积运算开始，讨论了如何将它用作图像转换器。然后，您学习了池层的作用以及最大池和平均池的区别。接下来，我们还了解了拼合图层如何将合并的要素地图转换为单个列。然后，您学习了如何以及为什么使用图像增强，以及如何使用批量规范化。这些是CNN区别于其他ann的关键组成部分。</p>
			<p>在卷积基础层、池化层和标准化层之后，CNN的结构通常类似于迄今为止您已经建立的许多ann，具有一系列一个或多个密集层。与其他二进制分类器非常相似，二进制图像分类器终止于具有一个单元和一个sigmoid激活函数的密集层。为了提供更多的实用性，图像分类器可以用来分类两个以上的对象。这种分类器通常被称为对象分类器，您将在下一节中学习。</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor172"/>物体分类</h1>
			<p>在本节中，您将了解对象检测和分类。下一步是对包含两个以上类别的数据集进行影像分类。我们将涉及的用于对象分类的三种不同类型的模型是<strong class="bold">图像分类</strong>、<strong class="bold">带有定位的分类</strong>和<strong class="bold">检测</strong>:</p>
			<ul>
				<li><strong class="bold">图像分类</strong>:这涉及到用设定数量的类别进行训练，然后尝试确定这些类别中的哪一个显示在图像中。想想MNIST手写数据集。对于这些问题，您将使用传统的CNN。</li>
				<li><strong class="bold">定位分类</strong>:利用这种类型，模型试图预测物体在图像空间中的位置。对于这些型号，你使用一个简化的<strong class="bold">你只看一次</strong> ( <strong class="bold"> YOLO </strong>)或R-CNN。</li>
				<li><strong class="bold">检测</strong>:最后一种是检测。这是您的模型可以检测几个不同对象的地方，以及它们的位置。为此，你可以使用YOLO或R-CNN: <div> <img src="img/B16341_07_24.jpg" alt="Figure 7.24: Object classification types&#13;&#10;"/> </div></li>
			</ul>
			<p class="figure-caption">图7.24:对象分类类型</p>
			<p>现在，您将简要了解一下使用<code>Fashion-MNIST</code>数据集的影像分类。<code>Fashion-MNIST</code>由Zalando文章图片的数据集编译而成。Zalando是一家专注于时尚的电子商务公司，总部位于德国柏林。数据集由10个类组成，训练集包含60，000幅<code>28x28</code>灰度图像和10，000幅测试图像。</p>
			<ol>
				<li value="1">导入张量流:<pre>import tensorflow as tf</pre></li>
				<li>接下来，进行一些额外的导入，比如对于NumPy，Matplotlib，当然还有图层和模型。你会注意到这里你将使用额外的辍学层。如果你还记得，脱落层有助于防止过度拟合:<pre>import numpy as np import matplotlib.pyplot as plt import tensorflow_datasets as tfds from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \     Dropout, GlobalMaxPooling2D, Activation, Rescaling from tensorflow.keras.models import Model from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay import itertools import matplotlib.pyplot as plt</pre></li>
				<li>在他们决定包含的任何一个数据集中使用<code>tdfs</code>加载<code>Fashion-MNIST</code>数据集。其他还有<code>CIFAR-10</code>和<code>CIFAR-100</code>，仅举几个例子:<pre>(our_train_dataset, our_test_dataset), \ dataset_info = tfds.load(\                          'fashion_mnist'                           , split = ['train', 'test']                           , data_dir = 'content/FashionMNIST/'                           , shuffle_files = True                           , as_supervised = True                           , with_info = True) assert isinstance(our_train_dataset, tf.data.Dataset)</pre></li>
				<li>Check the data for its properties:<pre>image_shape = dataset_info.features["image"].shape
print(f'Shape of Images in the Dataset: \t{image_shape}')
num_classes = dataset_info.features["label"].num_classes
print(f'Number of Classes in the Dataset: \t{num_classes}')
names_of_classes = dataset_info.features["label"].names
print(f'Names of Classes in the Dataset: \t{names_of_classes}\n')
for name in names_of_classes:
    print(f'Label for class \
          "{name}":  \t\t{dataset_info.features["label"].\
          str2int(name)}')</pre><p>这将为您提供以下输出:</p><div><img src="img/B16341_07_25.jpg" alt="Figure 7.25: Details of properties for data&#13;&#10;"/></div><p class="figure-caption">图7.25:数据属性的详细信息</p></li>
				<li>Now, print the total examples of the train and test data:<pre>print(f'Total examples in Train Dataset: \
      \t{len(our_train_dataset)}')
print(f'Total examples in Test Dataset: \
      \t{len(our_test_dataset)}')</pre><p>这将为您提供以下输出:</p><div><img src="img/B16341_07_26.jpg" alt="Figure 7.26: Details of train and test datasets&#13;&#10;"/></div><p class="figure-caption">图7.26:训练和测试数据集的细节</p></li>
				<li>使用函数式API构建您的模型:<pre>input_layer = Input(shape=image_shape) x = Conv2D(filters = 32, kernel_size = (3, 3), \            strides=2)(input_layer) x = Activation('relu')(x) x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x) x = Activation('relu')(x) x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x) x = Activation('relu')(x) x = Flatten()(x) x = Dropout(rate = 0.2)(x) x = Dense(units = 512)(x) x = Activation('relu')(x) x = Dropout(rate = 0.2)(x) x = Dense(units = num_classes)(x) output = Activation('softmax')(x) our_classification_model = Model(input_layer, output)</pre></li>
				<li>Compile and fit your model. With <code>compile()</code> method, use <code>adam</code> as your optimizer, set the loss to <code>sparse_categorical_crossentropy</code>, and set the <code>accuracy</code> metric. Then, call <code>model.fit()</code> on your training and validation sets:<pre>our_classification_model.compile(
                   optimizer='adam', \
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'], loss_weights=None,
                   weighted_metrics=None, run_eagerly=None,
                   steps_per_execution=None
)
history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)</pre><p>这将产生以下输出:</p><div><img src="img/B16341_07_27.jpg" alt="Figure 7.27: Function returning history&#13;&#10;"/></div><p class="figure-caption">图7.27:函数返回历史</p></li>
				<li>Use <code>matplotlib.pyplot</code> to plot the loss and accuracy:<pre>def plot_trend_by_epoch(tr_values, val_values, title):
    epoch_number = range(len(tr_values))
    plt.plot(epoch_number, tr_values, 'r')
    plt.plot(epoch_number, val_values, 'b')
    plt.title(title)
    plt.xlabel('epochs')
    plt.legend(['Training '+title, 'Validation '+title])
    plt.figure()
hist_dict = history.history
tr_accuracy, val_accuracy = hist_dict['accuracy'], \
                            hist_dict['val_accuracy']
plot_trend_by_epoch(tr_accuracy, val_accuracy, "Accuracy")</pre><p>这将给出以下输出图:</p><div><img src="img/B16341_07_28.jpg" alt="Figure 7.28: Accuracy plot using matplotlib.pyplot &#13;&#10;"/></div><p class="figure-caption">图7.28:使用matplotlib.pyplot的精度图</p></li>
				<li>Plot the validation loss and training loss. Use the following code:<pre>tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']
plot_trend_by_epoch(tr_loss, val_loss, "Loss")</pre><p>这将给出以下输出图:</p><div><img src="img/B16341_07_29.jpg" alt="Figure 7.29: Validation loss and training loss&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图7.29:验证损失和培训损失</p>
			<p>从作为时期函数的精度和损耗曲线可以看出，精度提高，损耗降低。在验证集上，两者都开始趋于平稳，这是停止训练以防止过度适应训练数据集的好信号。</p>
			<p>在下一个练习中，您将构建一个CNN来将来自<code>CIFAR-10</code>数据集的图像分为10个不同的类别。</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor173"/>练习7.05:构建CNN</h2>
			<p>这家初创公司现在希望扩展其功能，处理更多的类和更大的图像数据集。你的挑战是准确预测图像的类别。</p>
			<p>您将使用的数据集是<code>CIFAR-10</code>数据集，该数据集包含10个类别的60，000张<code>32x32</code>彩色图像:飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船只和卡车。每个类有6，000幅图像，整个数据集包含50，000幅训练图像和10，000幅测试图像。</p>
			<p>有关数据集的更多信息可在<em class="italic">从微小图像中学习多层要素</em>(<a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">http://www . cs . Toronto . edu/~ kriz/Learning-Features-2009-tr . pdf</a>)、<em class="italic"> Alex Krizhevsky </em>、<em class="italic"> 2009 </em>:</p>
			<ol>
				<li value="1">启动新的Jupyter笔记本，导入TensorFlow库:<pre>import tensorflow as tf</pre></li>
				<li>导入所需的其他附加库:<pre>import numpy as np import matplotlib.pyplot as plt import tensorflow_datasets as tfds from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \     Dropout, GlobalMaxPooling2D, Activation, Rescaling from tensorflow.keras.models import Model from sklearn import metrics import confusion_matrix, \     ConfusionMatrixDisplay import itertools import matplotlib.pyplot as plt</pre></li>
				<li>直接从<code>tfds</code>加载<code>CIFAR-10</code>数据集，如下:<pre>(our_train_dataset, our_test_dataset), \ dataset_info = tfds.load('cifar10',\                          split = ['train', 'test'],\                          data_dir = 'content/Cifar10/',\                          shuffle_files = True,\                          as_supervised = True,\                          with_info = True) assert isinstance(our_train_dataset, tf.data.Dataset)</pre></li>
				<li>Print the properties of your dataset using the following code:<pre>image_shape = dataset_info.features["image"].shape
print(f'Shape of Images in the Dataset: \t{image_shape}')
num_classes = dataset_info.features["label"].num_classes
print(f'Number of Classes in the Dataset: \t{num_classes}')
names_of_classes = dataset_info.features["label"].names
print(f'Names of Classes in the Dataset: \t{names_of_classes}\n')
for name in names_of_classes:
    print(f'Label for class "{name}": \
          \t\t{dataset_info.features["label"].str2int(name)}')
print(f'Total examples in Train Dataset: \
      \t{len(our_train_dataset)}')
print(f'Total examples in Test Dataset: \
      \t{len(our_test_dataset)}')</pre><p>这将给出以下带有属性和类数量的输出:</p><div><img src="img/B16341_07_30.jpg" alt="Figure 7.30: Number of classes&#13;&#10;"/></div><p class="figure-caption">图7.30:类的数量</p></li>
				<li>建立训练和测试数据管道，如<em class="italic">练习7.03 </em>、<em class="italic">建立CNN </em> : <pre>normalization_layer = Rescaling(1./255) our_train_dataset = our_train_dataset.map\                     (lambda x, y: (normalization_layer(x), y),\                      num_parallel_calls = \                      tf.data.experimental.AUTOTUNE) our_train_dataset = our_train_dataset.cache() our_train_dataset = our_train_dataset.shuffle\                     (len(our_train_dataset)) our_train_dataset = our_train_dataset.batch(128) our_train_dataset = our_train_dataset.prefetch\                     (tf.data.experimental.AUTOTUNE) our_test_dataset = our_test_dataset.map\                    (lambda x, y: (normalization_layer(x), y),\                     num_parallel_calls = \                     tf.data.experimental.AUTOTUNE) our_test_dataset = our_test_dataset.cache() our_test_dataset = our_test_dataset.batch(1024) our_test_dataset = our_test_dataset.prefetch\                    (tf.data.experimental.AUTOTUNE)</pre>所示</li>
				<li>使用函数式API构建模型。设置形状、层类型、步幅和激活功能:<pre>input_layer = Input(shape=image_shape) x = Conv2D(filters = 32, \            kernel_size = (3, 3), strides=2)(input_layer) x = Activation('relu')(x) x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x) x = Activation('relu')(x) x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x) x = Activation('relu')(x) x = Flatten()(x) x = Dropout(rate = 0.5)(x) x = Dense(units = 1024)(x) x = Activation('relu')(x) x = Dropout(rate = 0.2)(x) x = Dense(units = num_classes)(x) output = Activation('softmax')(x) our_classification_model = Model(input_layer, output)</pre></li>
				<li>Compile and fit your model. Be sure to use your GPU for this, if possible, as it will speed up the process quite a bit. If you decide not to use the GPU and your machine has difficulty in terms of computation, you can decrease the number of epochs accordingly:<pre>our_classification_model.compile(
                      optimizer='adam', \
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'], loss_weights=None,
                      weighted_metrics=None, run_eagerly=None,
                      steps_per_execution=None
)
print(our_classification_model.summary())
history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)</pre><p>该函数将返回以下历史记录:</p><div><img src="img/B16341_07_31.jpg" alt="Figure 7.31: Fitting the model&#13;&#10;"/></div><p class="figure-caption">图7.31:拟合模型</p></li>
				<li>Get a visual representation of the model's performance by plotting your loss and accuracy per epoch:<pre>def plot_trend_by_epoch(tr_values, val_values, title):
    epoch_number = range(len(tr_values))
    plt.plot(epoch_number, tr_values, 'r')
    plt.plot(epoch_number, val_values, 'b')
    plt.title(title)
    plt.xlabel('epochs')
    plt.legend(['Training '+title, 'Validation '+title])
    plt.figure()
hist_dict = history.history
tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']
plot_trend_by_epoch(tr_loss, val_loss, "Loss")</pre><p>这将产生以下情节:</p><div><img src="img/B16341_07_32.jpg" alt="Figure 7.32: Loss plot&#13;&#10;"/></div><p class="figure-caption">图7.32:损失图</p></li>
				<li>Next, get an accuracy plot by using the following code:<pre>tr_accuracy, val_accuracy = hist_dict['accuracy'], \
                            hist_dict['val_accuracy']
plot_trend_by_epoch(tr_accuracy, val_accuracy, "Accuracy")</pre><p>这将给出以下情节:</p><div><img src="img/B16341_07_33.jpg" alt="Figure 7.33: Accuracy plot&#13;&#10;"/></div><p class="figure-caption">图7.33:精度图</p></li>
				<li>Plot the confusion matrix without normalization: <pre>test_labels = []
test_images = []
for image, label in tfds.as_numpy(our_test_dataset.unbatch()):
    test_images.append(image)
    test_labels.append(label)
test_labels = np.array(test_labels)
predictions = our_classification_model.predict(our_test_dataset).argmax(axis=1)
conf_matrix = confusion_matrix(test_labels, predictions)
disp = ConfusionMatrixDisplay(conf_matrix, \
                              display_labels = names_of_classes)
fig = plt.figure(figsize = (12, 12))
axis = fig.add_subplot(111)
disp.plot(values_format = 'd', ax = axis)</pre><p>这将产生以下输出:</p><div><img src="img/B16341_07_34.jpg" alt="Figure 7.34: Confusion matrix without normalization&#13;&#10;"/></div><p class="figure-caption">图7.34:没有归一化的混淆矩阵</p></li>
				<li>Use the following code to plot the confusion matrix with normalization:<pre>conf_matrix = conf_matrix.astype\
              ('float') / conf_matrix.sum(axis=1) \
              [:, np.newaxis]
disp = ConfusionMatrixDisplay(\
       conf_matrix, display_labels = names_of_classes)
fig = plt.figure(figsize = (12, 12))
axis = fig.add_subplot(111)
disp.plot(ax = axis)</pre><p>输出将如下所示:</p><div><img src="img/B16341_07_35.jpg" alt="Figure 7.35: Confusion matrix with normalization&#13;&#10;"/></div><p class="figure-caption">图7.35:归一化的混淆矩阵</p></li>
				<li>Take a look at one of the images that the model got wrong. Plot one of the incorrect predictions with the following code:<pre>incorrect_predictions = np.where(predictions != test_labels)[0]
index = np.random.choice(incorrect_predictions)
plt.imshow(test_images[index])
print(f'True label: {names_of_classes[test_labels[index]]}')
print(f'Predicted label: {names_of_classes[predictions[index]]}')</pre><p>输出将如下所示:</p><div><img src="img/B16341_07_36.jpg" alt="Figure 7.36: True versus predicted results&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图7.36:真实结果与预测结果的对比</p>
			<p>你会注意到上面写着<code>True label: bird</code>和<code>Predicted label: cat</code>。这意味着模型预测这个图像是一只猫，但它是一只鸟。由于分辨率只有<code>32x32</code>，图像模糊；不过，成绩还不错。公平地说，人类很难识别图像是一只狗还是一只猫。</p>
			<p>现在你已经完成了这一章，是时候用<em class="italic">活动7.01 </em>、<em class="italic">构建具有更多ANN层的CNN</em>来测试你所学的一切了，在这里你将构建一个具有额外ANN层的CNN。</p>
			<h2 id="_idParaDest-151">活动7.01:用M个<a id="_idTextAnchor174"/>矿层构建CNN</h2>
			<p>到目前为止，你为之工作的初创公司很喜欢你的工作。他们让你创建一个新的模型，能够对100个不同类别的图像进行分类。</p>
			<p>在这个活动中，你将把你所学的一切用于用<code>CIFAR-100</code>构建你自己的分类器。<code>CIFAR-100</code>是<code>CIFAR-10</code>数据集的更高级版本，有100个类，常用于机器学习研究中的性能基准测试。</p>
			<ol>
				<li value="1">开始一个新的Jupyter笔记本。</li>
				<li>导入TensorFlow库。</li>
				<li>导入您将需要的其他库，包括NumPy、Matplotlib、Input、Conv2D、Dense、Flatten、Dropout、GlobalMaxPooling2D、Activation、Model、confusion_matrix和itertools。</li>
				<li>Load the <code>CIFAR-100</code> dataset directly from <code>tensorflow_datasets</code> and view its properties from the metadata, and build a train and test data pipeline:<div><img src="img/B16341_07_37.jpg" alt="Figure 7.37: Properties of the CIFAR-100 dataset&#13;&#10;"/></div><p class="figure-caption">图7.37:CIFAR-100数据集的属性</p></li>
				<li>创建一个函数来缩放图像。然后，通过重新缩放、缓存、混排、批处理和预取图像来构建测试和训练数据管道。</li>
				<li>使用<code>Conv2D</code>和<code>Flatten</code>等功能API构建模型。</li>
				<li>Compile and fit the model using <code>model.compile</code> and <code>model.fit</code>:<div><img src="img/B16341_07_38.jpg" alt="Figure 7.38: Model fitting&#13;&#10;"/></div><p class="figure-caption">图7.38:模型拟合</p></li>
				<li>Plot the loss with <code>plt.plot</code>. Remember to use the history collected during the <code>model.fit()</code> procedure:<div><img src="img/B16341_07_39.jpg" alt="Figure 7.39: Loss versus epochs&#13;&#10;"/></div><p class="figure-caption">图7.39:损失与时期</p></li>
				<li>Plot the accuracy with <code>plt.plot</code>:<div><img src="img/B16341_07_40.jpg" alt="Figure 7.40: Accuracy versus epochs&#13;&#10;"/></div><p class="figure-caption">图7.40:准确度与时期</p></li>
				<li>为数据集中的不同类指定标签。</li>
				<li>Display a misclassified example with <code>plt.imshow</code>:<p> </p><div><img src="img/B16341_07_41.jpg" alt="Figure 7.41: Wrong classification example&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图7.41:错误的分类示例</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这个活动的解决方案可以通过<a href="B16341_Solution_ePub.xhtml#_idTextAnchor272">这个链接</a>找到。</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor175"/>总结</h1>
			<p>本章介绍了CNN。我们回顾了神经元、层、模型架构和张量等核心概念，以理解如何创建有效的CNN。</p>
			<p>您学习了卷积运算，并探索了内核和特性映射。我们分析了如何组装CNN，然后探讨了不同类型的池层以及何时应用它们。</p>
			<p>然后，您学习了stride操作，以及如果需要，如何使用填充在图像周围创建额外的空间。然后，我们深入研究了扁平化层，以及它如何能够将数据转换为下一层的1D数组。在最后的活动中，你把你学到的所有东西都拿来测试，因为你遇到了几个分类问题，包括<code>CIFAR-10</code>甚至<code>CIFAR-100</code>。</p>
			<p>完成这一章后，你就可以自信地用CNN来解决图像分类问题了。</p>
			<p>在下一章中，您将了解预训练模型，以及如何通过在预训练模型上添加人工神经网络层并根据您自己的训练数据微调权重，将它们用于您自己的应用。</p>
		</div>
	

</body></html>