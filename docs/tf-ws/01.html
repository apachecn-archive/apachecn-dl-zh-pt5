<html><head/><body>


	
		<title>B16341_01_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-17"><a id="_idTextAnchor016"/> 1。TensorFlow机器学习简介</h1>
		</div>
		<div><p class="callout-heading">概观</p>
			<p class="callout">在本章中，您将学习如何使用TensorFlow: tensors创建、利用和应用线性变换到编程的基本构件。然后你将利用张量来理解与神经网络相关的复杂概念，包括张量整形、转置和乘法。</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>简介</h1>
			<p><strong class="bold">机器学习</strong> ( <strong class="bold"> ML </strong>)已经渗透到很多人不知道的日常生活的各个方面。从你日常社交媒体的推荐到你在线搜索的结果，它们都是由机器学习算法驱动的。这些算法开始于解决利基问题的研究环境，但随着它们的可访问性扩大，它们的应用也扩大到了更广泛的用例。所有类型的研究人员和企业都认识到使用模型来优化各自运营的各个方面的价值。医生可以使用机器学习来决定诊断和治疗方案，零售商可以使用ML在正确的时间将正确的产品送到他们的商店，娱乐公司可以使用ML向他们的客户提供个性化的推荐。</p>
			<p>在数据时代，机器学习模型已被证明是任何数据驱动型公司的宝贵资产。大量可用的数据允许创建强大而精确的模型来完成各种任务，从回归到分类、推荐到时间序列分析，甚至生成艺术，其中许多将在本次研讨会中讨论。所有这些都可以用TensorFlow来构建、训练和部署。</p>
			<p>TensorFlow API具有大量的功能，这使得它在所有构建机器学习模型或使用张量(多维数值数组)的机器学习从业者中很受欢迎。对于研究人员来说，TensorFlow由于其高级定制和灵活性，是创建新的机器学习应用程序的合适选择。对于开发人员来说，TensorFlow是机器学习库的绝佳选择，因为它可以轻松地将模型从开发环境部署到生产环境。结合起来，TensorFlow的灵活性和易于部署性使该库成为许多寻求使用各种不同数据源构建高性能机器学习模型并在生产环境中复制学习结果的从业者的明智选择。</p>
			<p>本章提供了tensor flow API的实用介绍。您将学习如何执行与机器学习相关的数学运算，这将为您使用TensorFlow构建高性能ML模型打下坚实的基础。您将首先学习基本操作，例如如何使用API创建变量。接下来，您将学习如何执行线性变换，如加法，然后再继续更高级的任务，包括张量乘法。</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>在TensorFlow中实现人工神经网络</h1>
			<p>TensorFlow提供的高级灵活性非常适合创建<strong class="bold">人工神经网络</strong>(<strong class="bold">ann</strong>)。人工神经网络是受大脑中神经元连通性启发的算法，旨在复制人类学习的过程。它们由层组成，信息通过这些层从输入传播到输出。</p>
			<p><em class="italic">图1.1 </em>显示了人工神经网络的可视化表示。输入层位于左侧，在本例中，它有两个特征(<code>X</code> 1和<code>X</code> 2)。输入层连接到第一个隐藏层，它有三个单元。来自前一层的所有数据被传递到第一个隐藏层中的每个单元。然后，数据被传递到第二个隐藏层，它也有三个单元。同样，来自前一层的每个单元的信息被传递到第二隐藏层的每个单元。最后，来自第二个隐藏层的所有信息被传递到输出层，该层有一个单元，代表每组输入要素的一个数字。</p>
			<div><div><img src="img/B16341_01_01.jpg" alt="Figure 1.1: A visual representation of an ANN with two hidden layers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.1:具有两个隐藏层的人工神经网络的可视化表示</p>
			<p>人工神经网络已被证明在学习大型非结构化数据集(如音频、图像和文本数据)的复杂和非线性关系方面是成功的。虽然结果可能令人印象深刻，但人工神经网络的配置有很大的可变性。例如，层数、每层的大小以及应该使用哪个非线性函数是决定ann配置的一些因素。TensorFlow提供的类和函数不仅非常适合构建和训练ann，而且该库还提供了一套工具来帮助在训练过程中可视化和调试ann。</p>
			<p>与传统的机器学习算法(如线性和逻辑回归)相比，当提供大量数据时，人工神经网络可以胜过它们。人工神经网络是有利的，因为它们可以输入非结构化数据，并且不一定需要特征工程。数据预处理可能是一个耗时的过程。因此，如果有大量的数据，许多从业者更喜欢人工神经网络。</p>
			<p>各行各业的许多公司都利用TensorFlow为其应用构建人工神经网络。由于TensorFlow得到了谷歌的支持，该公司将该库用于其大部分机器学习应用程序的研究、开发和生产。然而，还有许多其他公司也使用该库。Airbnb、可口可乐、优步和通用电气医疗保健等公司都利用该图书馆完成各种任务。人工神经网络的使用特别有吸引力，因为如果有足够的数据和适当的训练，它们可以达到非常高的精度。例如，GE Healthcare使用TensorFlow构建人工神经网络来识别特定的解剖结构，而不管磁共振图像的方向如何，以提高速度和准确性。通过使用人工神经网络，他们可以在几秒钟内达到99%以上的识别解剖结构的准确性，而不管头部旋转，否则将需要一名训练有素的专业人员花费更多的时间。</p>
			<p>虽然使用人工神经网络的公司数量巨大，但人工神经网络可能不是解决所有商业问题的最合适的选择。在这种环境下，您必须回答以下问题，以确定人工神经网络是否是最合适的选择:</p>
			<ul>
				<li>这个问题有数值解吗？包括ann在内的机器学习算法，根据输入数据生成预测的数值结果。例如，机器学习算法可以预测给定数量，例如给定位置和先前天气条件的城市的温度，或者给定先前股票价格的股票价格，或者将图像标记到给定数量的类别中。在每一个例子中，都会根据提供的数据生成一个数字输出，如果有足够多的标记数据，模型可以运行得很好。然而，当期望的结果更抽象，或者需要创造性时，例如创作一首新歌，那么机器学习算法可能不是最合适的选择，因为可能没有明确定义的数值解。</li>
				<li><strong class="bold">有足够的适当标记的数据来训练一个模型吗？</strong>对于有监督的学习任务，你必须至少有一些带标签的数据来训练一个模型。例如，如果要建立一个模型来预测给定公司的金融股票数据，首先需要历史培训数据。如果问题公司上市时间不长，可能没有足够的培训数据。人工神经网络通常需要大量数据。当处理图像时，人工神经网络通常需要数百万个训练样本来开发精确、鲁棒的模型。当决定哪种算法适合于给定的任务时，这可能是要考虑的决定性因素。</li>
			</ul>
			<p>现在你已经知道TensorFlow是什么了，考虑TensorFlow的以下优点和缺点。</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>张量流的优势</h2>
			<p>以下是使用TensorFlow的一些主要优势，许多从业者在决定是否将该库用于机器学习目的时会考虑这些优势:</p>
			<ul>
				<li><strong class="bold">库管理</strong>:有一个庞大的从业者社区维护TensorFlow库，通过频繁的新发布来保持库的更新，以帮助修复错误，添加新的函数和类来反映该领域的最新进展，并添加对多种编程语言的支持。</li>
				<li><strong class="bold">流水线</strong> : TensorFlow支持端到端的模型生产，从支持GPU处理的高度并行化环境中的模型开发到一套模型部署工具。还有，TensorFlow中有轻量级的库，用于在移动和嵌入式设备上部署训练好的TensorFlow模型，比如<strong class="bold">物联网</strong> ( <strong class="bold"> IoT </strong>)设备。</li>
				<li><strong class="bold">社区支持</strong>:使用和支持该库的从业者社区是巨大的，他们相互支持，正因为如此，那些刚接触该库的从业者很容易获得他们想要的结果。</li>
				<li><strong class="bold">开源</strong> : TensorFlow是一个开源库，它的代码库可供任何人使用，并针对自己的应用进行修改。</li>
				<li><strong class="bold">支持多种语言</strong>:虽然这个库原本是为Python设计的，但是现在可以用JavaScript来训练和部署模型。</li>
			</ul>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>张量流的缺点</h2>
			<p>以下是使用TensorFlow的一些缺点:</p>
			<ul>
				<li><strong class="bold">计算速度</strong>:由于TensorFlow的主要编程语言是Python，所以这个库的计算速度不如其他语言(如C++)的速度快。</li>
				<li><strong class="bold">陡峭的学习曲线</strong>:与其他机器学习库(如Keras)相比，学习曲线更加陡峭，这使得新手在给定示例代码之外创建自己的模型具有挑战性。</li>
			</ul>
			<p>现在您已经理解了什么是TensorFlow，下一节将演示如何使用Python来使用TensorFlow库。</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Python中的TensorFlow库</h1>
			<p>通过导入某些库，可以在Python中使用TensorFlow。您可以使用<code>import</code>语句导入Python中的库:</p>
			<pre>import tensorflow as tf</pre>
			<p>在前面的命令中，您已经导入了TensorFlow库，并使用了简写方式<code>tf</code>。</p>
			<p>在下一个练习中，您将学习如何导入TensorFlow库并检查其版本，以便您可以利用该库提供的类和函数，这是使用该库时重要且必要的第一步。</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>练习1.01:验证您的TensorFlow版本</h2>
			<p>在本练习中，您将加载TensorFlow并检查您的系统上安装了哪个版本。</p>
			<p>执行以下步骤:</p>
			<ol>
				<li>打开一个Jupyter笔记本，通过在终端中键入<code>jupyter notebook</code>来实现这个练习。</li>
				<li>通过在Jupyter单元格中输入以下代码来导入TensorFlow库:<pre>import tensorflow as tf</pre></li>
				<li>Verify the version of TensorFlow using the following command:<pre>tf.__version__</pre><p>这将导致以下输出:</p><pre>'2.6.0'</pre><p>从前面的输出可以看出，TensorFlow的版本是<code>2.6.0</code>。</p><p class="callout-heading">注意</p><p class="callout">如果您没有使用<em class="italic">前言</em>中提供的步骤设置环境，系统上的版本可能会有所不同。</p></li>
			</ol>
			<p>在本练习中，您成功导入了TensorFlow。您还检查了系统上安装了哪个版本的TensorFlow。</p>
			<p>这项任务可以在Python中的任何导入库上完成，对于调试和引用文档非常有用。</p>
			<p>使用TensorFlow的潜在应用有很多，它已经取得了令人印象深刻的成果，例如Airbnb和GE Healthcare等公司的成果，Airbnb使用TensorFlow对其平台上的图像进行分类，GE health care使用TensorFlow在磁共振成像上识别大脑的解剖结构。要学习如何为自己的应用创建强大的模型，首先必须学习基本的数学原理和运算，这些原理和运算构成了可以在TensorFlow中实现的机器学习模型。数学运算可能会让新用户望而生畏，但是全面理解它们是如何操作的是制作高性能模型的关键。</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>张量简介</h1>
			<p>张量可以被认为是人工神经网络的核心组件，在整个训练过程中学习的输入数据、输出预测和权重都是张量。信息通过一系列线性和非线性变换传播，将输入数据转化为预测。本节演示了如何对张量应用线性变换，如加法、转置和乘法。还存在其他线性变换，如旋转、反射和剪切。然而，它们在人工神经网络中的应用并不常见。</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>标量、向量、矩阵和张量</h2>
			<p>张量可以表示为多维数组。张量跨越的维数称为张量的秩。秩为<code>0</code>、<code>1</code>和<code>2</code>的张量经常被使用，并且有它们自己的名字，分别是<strong class="bold">标量</strong>、<strong class="bold">向量</strong>和<strong class="bold">矩阵</strong>，尽管术语<em class="italic">张量</em>可以用来描述它们中的每一个。<em class="italic">图1.2 </em>显示了各种秩的张量的一些例子。从左到右是标量、向量、矩阵和三维张量，其中每个元素代表一个不同的数，下标代表元素在张量中的位置:</p>
			<div><div><img src="img/B16341_01_02.jpg" alt="Figure 1.2: A visual representation of a scalar, vector, matrix, and tensor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.2:标量、向量、矩阵和张量的可视化表示</p>
			<p>标量、向量、矩阵和张量的正式定义如下:</p>
			<ul>
				<li><strong class="bold">标量</strong>:标量由单个数字组成，使其成为零维数组。它是零阶张量的一个例子。标量没有任何轴。例如，一个对象的宽度是一个标量。</li>
				<li><strong class="bold">向量</strong>:向量是一维数组，是一阶张量的一个例子。它们可以被认为是值的列表。向量有一个轴。由宽度、高度和深度表示的给定对象的大小是向量场的一个例子。</li>
				<li><strong class="bold">矩阵</strong>:矩阵是有两个轴的二维数组。它们是二阶张量的一个例子。矩阵可以用来存储几个物体的大小。矩阵的每个维度包括每个对象的大小(宽度、高度、深度),另一个矩阵维度用于区分对象。</li>
				<li><code>3</code>以上。张量可以用来存储许多物体的大小和它们随时间的位置。矩阵的第一维包括每个对象的大小(宽度、高度、深度)，第二维用于区分对象，第三维描述这些对象随时间的位置。</li>
			</ul>
			<p>可以使用TensorFlow库中的<code>Variable</code>类创建张量，并传递一个表示张量的值。标量可以传递浮点或整数，向量可以传递浮点或整数列表，矩阵可以传递浮点或整数嵌套列表，等等。以下命令演示了<code>Variable</code>类的使用，其中传递了张量的预期值列表以及需要明确定义的任何其他属性:</p>
			<pre>tensor1 = tf.Variable([1,2,3], dtype=tf.int32, \
                      name='my_tensor', trainable=True)</pre>
			<p>得到的<code>Variable</code>对象有几个可能被经常调用的属性，如下所示:</p>
			<ul>
				<li><code>dtype</code>:对象的数据类型<code>Variable</code>(对于上面定义的张量，数据类型为<code>tf.int32</code>)。该属性的默认值由传递的值决定。</li>
				<li><code>shape</code>:物体的<code>Variable</code>的维数和各维的长度(对于上面定义的张量，形状为<code>[3]</code>)。该属性的默认值也由传递的值决定。</li>
				<li><code>name</code>:物体的名称<code>Variable</code>(对于上面定义的张量，张量的名称定义为<code>'my_tensor'</code>)。该属性的默认值为<code>Variable</code>。</li>
				<li><code>trainable</code>: This attribute indicates whether the <code>Variable</code> object can be updated during model training (for the tensor defined above, the <code>trainable</code> parameter is set to <code>true</code>). The default for this attribute is <code>true</code>.<p class="callout-heading">注意</p><p class="callout">你可以在这里阅读更多关于<code>Variable</code>物体的属性:<a href="https://www.tensorflow.org/api_docs/python/tf/Variable">https://www.tensorflow.org/api_docs/python/tf/Variable</a>。</p></li>
			</ul>
			<p><code>Variable</code>对象的<code>shape</code>属性可以如下调用:</p>
			<pre>tensor1.shape</pre>
			<p><code>shape</code>属性给出了张量的形状，即它是标量、矢量、矩阵等等。前面命令的输出将是<code>[3]</code>,因为张量有一个一维，沿该维有三个值。</p>
			<p>张量的秩可以在TensorFlow中使用<code>rank</code>函数来确定。可以通过将张量作为单个参数传递给函数来使用它，结果将是一个整数值:</p>
			<pre>tf.rank(tensor1)</pre>
			<p>以下命令的输出将是一个表示输入秩的零维整数张量。在这种情况下，<code>tensor1</code>的秩将是<code>1</code>，因为张量只有一维。</p>
			<p>在下面的练习中，您将学习如何使用TensorFlow的<code>Variable</code>类创建各种等级的张量。</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>练习1.02:在TensorFlow中创建标量、向量、矩阵和张量</h2>
			<p>A区和B区三个不同政党的不同候选人的投票情况如下:</p>
			<div><div><img src="img/B16341_01_03.jpg" alt="Figure 1.3: Votes cast for different candidates of three different political &#13;&#10;parties in districts A and B"/>
				</div>
			</div>
			<p class="figure-caption">图1.3:A区和B区三个不同政党的不同候选人的投票情况</p>
			<p>您需要执行以下操作:</p>
			<ul>
				<li>创建一个标量来存储<code>A</code>区<code>X</code>政党<code>Candidate 1</code>的选票，即<code>4113</code>，并检查其形状和排名。</li>
				<li>创建一个向量来表示地区<code>A</code>中政党<code>X</code>的三个不同候选人的投票比例，并检查其形状和等级。</li>
				<li>创建一个矩阵来代表政党<code>X</code>和<code>Y</code>的三个不同候选人的投票，并检查其形状和等级。</li>
				<li>创建一个张量来代表三个政党在两个不同地区的三个不同候选人的投票，并检查其形状和等级。</li>
			</ul>
			<p>执行以下步骤来完成本练习:</p>
			<ol>
				<li value="1">导入张量流库:<pre>import tensorflow as tf</pre></li>
				<li>Create an integer variable using TensorFlow's <code>Variable</code> class and pass <code>4113</code> to represent the number of votes cast for a particular candidate. Also, pass <code>tf.int16</code> as a second argument to ensure that the input number is an integer datatype. Print the result:<pre>int_variable = tf.Variable(4113, tf.int16)
int_variable</pre><p>这将导致以下输出:</p><pre>&lt;tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4113&gt;</pre><p>在这里，您可以看到所创建变量的属性，包括名称、<code>Variable:0</code>、形状、数据类型和张量的NumPy表示。</p></li>
				<li>Use TensorFlow's <code>rank</code> function to print the rank of the variable created:<pre>tf.rank(int_variable)</pre><p>这将导致以下输出:</p><pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;</pre><p>从张量的NumPy表示中可以看出，所创建的整数变量的秩是<code>0</code>。</p></li>
				<li>Access the integer variable of the rank by calling the <code>numpy</code> attribute:<pre>tf.rank(int_variable).numpy()</pre><p>这将导致以下输出:</p><pre>0</pre><p>标量的秩是<code>0</code>。</p><p class="callout-heading">注意</p><p class="callout">可以调用<code>rank</code>函数结果的所有属性，包括<code>shape</code>和<code>dtype</code>属性。</p></li>
				<li>Call the <code>shape</code> attribute of the integer to find the shape of the tensor:<pre>int_variable.shape</pre><p>这将导致以下输出:</p><pre>TensorShape([])</pre><p>前面的输出表明张量的形状没有大小，它代表一个标量。</p></li>
				<li>Print the <code>shape</code> of the scalar variable as a Python list:<pre>int_variable.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[]</pre></li>
				<li>Create a <code>vector</code> variable using TensorFlow's <code>Variable</code> class. Pass a list for the vector to represent the proportion of votes cast for three different candidates, and pass in a second argument for the datatype as <code>tf.float32</code> to ensure that it is a <code>float</code> datatype. Print the result:<pre>vector_variable = tf.Variable([0.23, 0.42, 0.35], \
                              tf.float32)
vector_variable</pre><p>这将导致以下输出:</p><pre>&lt;tf.Variable 'Variable:0' shape(3,) dtype=float32, 
numpy=array([0.23, 0.42, 0.35], dtype=float32)&gt;</pre><p>您可以看到shape和NumPy属性不同于前面创建的标量变量。形状现在是<code>(3,)</code>，表示张量是一维的，沿着那个维度有三个元素。</p></li>
				<li>Print the rank of the <code>vector</code> variable using TensorFlow's <code>rank</code> function as a NumPy variable:<pre>tf.rank(vector_variable).numpy()</pre><p>这将导致以下输出:</p><pre>1</pre><p>这里可以看到向量变量的秩是<code>1</code>，确认这个变量是一维的。</p></li>
				<li>Print the shape of the <code>vector</code> variable as a Python list:<pre>vector_variable.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[3]</pre></li>
				<li>Create a matrix variable using TensorFlow's <code>Variable</code> class. Pass a list of lists of integers for the matrix to represent the votes cast for three different candidates in two different districts. This matrix will have three columns representing the candidates, and two rows representing the districts. Pass in a second argument for the datatype as <code>tf.int32</code> to ensure that it is an integer datatype. Print the result:<pre>matrix_variable = tf.Variable([[4113, 7511, 6259], \
                               [3870, 6725, 6962]], \
                              tf.int32)
matrix_variable</pre><p>这将导致以下输出:</p><div><img src="img/B16341_01_04.jpg" alt="Figure 1.4: The output of the TensorFlow variable&#13;&#10;"/></div><p class="figure-caption">图1.4:张量流变量的输出</p></li>
				<li>Print the rank of the matrix variable as a NumPy variable:<pre>tf.rank(matrix_variable).numpy()</pre><p>这将导致以下输出:</p><pre>2</pre><p>这里可以看到矩阵变量的秩是<code>2</code>，证实了这个变量是二维的。</p></li>
				<li>Print the shape of the matrix variable as a Python list:<pre>matrix_variable.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[2, 3]</pre></li>
				<li>Create a tensor variable using TensorFlow's <code>Variable</code> class. Pass in a triple nested list of integers for the tensor to represent the votes cast for three different candidates in two different districts, for three political parties. Print the result:<pre>tensor_variable = tf.Variable([[[4113, 7511, 6259], \
                                [3870, 6725, 6962]], \
                               [[5102, 7038, 6591], \
                                [3661, 5901, 6235]], \
                               [[951, 1208, 1098], \
                                [870, 645, 948]]])
tensor_variable</pre><p>这将导致以下输出:</p><div><img src="img/B16341_01_05.jpg" alt="Figure 1.5: The output of the TensorFlow variable&#13;&#10;"/></div><p class="figure-caption">图1.5:张量流变量的输出</p></li>
				<li>Print the rank of the tensor variable as a NumPy variable:<pre>tf.rank(tensor_variable).numpy()</pre><p>这将导致以下输出:</p><pre>3</pre><p>这里可以看到张量变量的秩是<code>3</code>，证实了这个变量是三维的。</p></li>
				<li>Print the shape of the tensor variable as a Python list:<pre>tensor_variable.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[3, 2, 3]</pre><p>结果表明，所得张量的形状是一个列表对象。</p></li>
			</ol>
			<p>在本练习中，您已经使用TensorFlow的<code>Variable</code>类从政治投票数据中成功创建了各种等级的张量。首先，您创建了标量，它是秩为<code>0</code>的张量。接下来，您创建了向量，这是一个秩为<code>1</code>的张量。然后矩阵被创建，它是秩为<code>2</code>的张量。最后，创建了等级为<code>3</code>或更高的张量。您确认了使用TensorFlow的<code>rank</code>函数创建的张量的等级，并通过调用张量的<code>shape</code>属性验证了它们的形状。</p>
			<p>在下一节中，您将使用张量加法合并张量以创建新的张量。</p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>张量加法</h1>
			<p>张量可以加在一起创建新的张量。你将在本章中使用矩阵的例子，但是这个概念可以扩展到任何秩的张量。在称为广播的过程中，矩阵可以在一定条件下添加到标量、向量和其他矩阵中。播是指对不同形状的张量进行数组运算的过程。</p>
			<p>如果两个矩阵具有相同的形状，它们可以相加(或相减)。对于这种矩阵-矩阵相加，结果矩阵由输入矩阵的元素相加来确定。因此，合成矩阵将具有与两个输入矩阵相同的形状。可以将矩阵<code>Z = [Z</code> ij <code>]</code>定义为矩阵和<code>Z = X + Y</code>，其中<code>z</code>ij =<code>x</code>ij<code>+</code>ij<code>y</code>ij和<code>Z</code>中的每个元素都是<code>X</code>和<code>Y</code>中相同元素的和。</p>
			<p>矩阵加法是可交换的，也就是说<code>X</code>和<code>Y</code>的顺序并不重要，也就是<code>X + Y = Y + X</code>。矩阵加法也是关联的，这意味着即使加法的顺序不同，或者即使运算应用多次，也能获得相同的结果，即<code>X + (Y + Z) = (X + Y) + Z</code>。</p>
			<p>同样的矩阵加法原理也适用于标量、向量和张量。下图显示了一个示例:</p>
			<div><div><img src="img/B16341_01_06.jpg" alt="Figure 1.6: A visual example of matrix-matrix addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.6:矩阵-矩阵相加的可视化示例</p>
			<p>标量也可以添加到矩阵中。这里，矩阵的每个元素被单独添加到标量中，如图<em class="italic">图1.7 </em>所示:</p>
			<div><div><img src="img/B16341_01_07.jpg" alt="Figure 1.7: A visual example of matrix-scalar addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.7:矩阵标量加法的可视化例子</p>
			<p>加法是一种重要的变换，可以应用于张量，因为这种变换经常发生。例如，开发人工神经网络的一个常见转换是向层添加一个偏差。这是当一个相同大小的神经网络层的常数张量阵列被添加到该层。因此，了解如何以及何时将这个看似简单的变换应用于张量是很重要的。</p>
			<p>张量加法可以在TensorFlow中执行，方法是使用<code>add</code>函数并将张量作为参数传入，或者简单地使用如下的<code>+</code>运算符:</p>
			<pre>tensor1 = tf.Variable([1,2,3])
tensor2 = tf.Variable([4,5,6])
tensor_add1 = tf.add(tensor1, tensor2)
tensor_add2 = tensor1 + tensor2</pre>
			<p>在下面的练习中，您将在TensorFlow中对标量、向量和矩阵执行张量加法。</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>练习1.03:在TensorFlow中执行张量加法</h2>
			<p>A区和B区三个不同政党的不同候选人的投票情况如下:</p>
			<div><div><img src="img/B16341_01_08.jpg" alt="Figure 1.8: Votes cast for different candidates of three different political &#13;&#10;parties in districts A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.8:A区和B区三个不同政党的不同候选人的投票情况</p>
			<p>你的必要任务如下:</p>
			<ul>
				<li>存储a区政党X的总投票数。</li>
				<li>存储a区每个政党的总投票数。</li>
				<li>存储两个地区中每个政党的投票总数。</li>
			</ul>
			<p>执行以下步骤来完成练习:</p>
			<ol>
				<li value="1">导入张量流库:<pre>import tensorflow as tf</pre></li>
				<li>使用TensorFlow的<code>Variable</code>类创建三个标量变量，以表示投给A区政党X的三名候选人的选票:<pre>int1 = tf.Variable(4113, tf.int32) int2 = tf.Variable(7511, tf.int32) int3 = tf.Variable(6529, tf.int32)</pre></li>
				<li>创建一个新变量来存储A区政党X的总投票数:<pre>int_sum = int1+int2+int3</pre></li>
				<li>Print the result of the sum of the two variables as a NumPy variable:<pre>int_sum.numpy()</pre><p>这将导致以下输出:</p><pre>18153</pre></li>
				<li>创建三个向量来表示A区不同政党的投票数，每个向量有一行三列:<pre>vec1 = tf.Variable([4113, 3870, 5102], tf.int32) vec2 = tf.Variable([7511, 6725, 7038], tf.int32) vec3 = tf.Variable([6529, 6962, 6591], tf.int32)</pre></li>
				<li>创建一个新变量来存储A区每个政党的总票数:<pre>vec_sum = vec1 + vec2 + vec3</pre></li>
				<li>Print the result of the sum of the two variables as a NumPy array:<pre>vec_sum.numpy()</pre><p>这将导致以下输出:</p><pre>array([18153, 17557, 18731])</pre></li>
				<li>Verify that the vector addition is as expected by performing the addition of each element of the vector:<pre>print((vec1[0] + vec2[0] + vec3[0]).numpy())
print((vec1[1] + vec2[1] + vec3[1]).numpy())
print((vec1[2] + vec2[2] + vec3[2]).numpy())</pre><p>这将导致以下输出:</p><pre>18153
17557
18731</pre><p>您可以看到，对三个向量的<code>+</code>操作只是向量的元素相加。</p></li>
				<li>创建三个矩阵来存储每个地区每个政党候选人的选票:<pre>matrix1 = tf.Variable([[4113, 3870, 5102], \                        [3611, 951, 870]], tf.int32) matrix2 = tf.Variable([[7511, 6725, 7038], \                        [5901, 1208, 645]], tf.int32) matrix3 = tf.Variable([[6529, 6962, 6591], \                        [6235, 1098, 948]], tf.int32)</pre></li>
				<li>Verify that the three tensors have the same shape:<pre>matrix1.shape == matrix2.shape == matrix3.shape</pre><p>这将导致以下输出:</p><pre>True</pre></li>
				<li>创建一个新变量来存储两个地区中每个政党的投票总数:<pre>matrix_sum = matrix1 + matrix2 + matrix3</pre></li>
				<li>Print the result of the sum of the two variables as a NumPy array:<pre>matrix_sum.numpy()</pre><p>这将产生以下输出，代表各地区每个候选人和每个政党的总票数:</p><div><img src="img/B16341_01_09.jpg" alt="Figure 1.9: The output of the matrix summation as a NumPy variable&#13;&#10;"/></div><p class="figure-caption">图1.9:矩阵求和作为一个NumPy变量的输出</p></li>
				<li>Verify that the tensor addition is as expected by performing the addition of each element of the vector:<pre>print((matrix1[0][0] + matrix2[0][0] + matrix3[0][0]).numpy())
print((matrix1[0][1] + matrix2[0][1] + matrix3[0][1]).numpy())
print((matrix1[0][2] + matrix2[0][2] + matrix3[0][2]).numpy())
print((matrix1[1][0] + matrix2[1][0] + matrix3[1][0]).numpy())
print((matrix1[1][1] + matrix2[1][1] + matrix3[1][1]).numpy())
print((matrix1[1][2] + matrix2[1][2] + matrix3[1][2]).numpy())</pre><p>这将导致以下输出:</p><pre>18153
17557
18731
15747
3257
2463</pre><p>你可以看到,<code>+</code>操作相当于三个矩阵的元素相加。</p></li>
			</ol>
			<p>在本练习中，您成功地对代表政治候选人投票的数据执行了张量加法。可通过使用<code>+</code>操作来应用转换。您还验证了加法是逐元素执行的，并且确保变换有效的一种方法是张量具有相同的秩和形状。</p>
			<p>在下面的活动中，您将在TensorFlow中进一步练习张量加法。</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>活动1.01:在TensorFlow中执行张量加法</h2>
			<p>您所在的公司有三个分支机构，每个分支机构有两名销售人员，每个分支机构销售三种产品。您需要对张量求和，以表示各地区每种产品的总收入。</p>
			<div><div><img src="img/B16341_01_10.jpg" alt="Figure 1.10: Number of different products sold by each salesperson at different locations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.10:每个销售人员在不同地点销售的不同产品的数量</p>
			<p>您将采取的步骤如下:</p>
			<ol>
				<li value="1">导入TensorFlow库。</li>
				<li>使用TensorFlow的<code>Variable</code>类创建两个标量来表示所有销售人员在<code>Location X</code>的<code>Product A</code>总收入。第一个变量的值为<code>2706</code>，第二个变量的值为<code>2386</code>。</li>
				<li>Create a new variable as the sum of the scalars and print the result.<p>您应该得到以下输出:</p><pre>5092</pre></li>
				<li>使用TensorFlow的<code>Variable</code>类创建一个值为<code>[2706, 2799, 5102]</code>的向量和一个值为<code>95</code>的标量。</li>
				<li>Create a new variable as the sum of the scalar with the vector to represent the sales goal for <code>Salesperson 1</code> at <code>Location X</code> and print the result.<p>您应该得到以下输出:</p><div><img src="img/B16341_01_11.jpg" alt="Figure 1.11: The output of the integer-vector summation as a NumPy variable&#13;&#10;"/></div><p class="figure-caption">图1.11:整数向量求和作为NumPy变量的输出</p></li>
				<li>使用TensorFlow的<code>Variable</code>类创建三个等级为2的张量，代表每个销售人员、产品和地点的收入。第一个张量的值为<code>[[2706, 2799, 5102], [2386, 4089, 5932]]</code>，第二个张量的值为<code>[[5901, 1208, 645], [6235, 1098, 948]]</code>，第三个张量的值为<code>[[3908, 2339, 5520], [4544, 1978, 4729]]</code>。</li>
				<li>创建一个新变量作为矩阵的和，并打印结果:<div> <img src="img/B16341_01_12.jpg" alt="Figure 1.12: The output of the matrix summation as a NumPy variable&#13;&#10;"/> </div></li>
			</ol>
			<p class="figure-caption">图1.12:矩阵求和作为一个NumPy变量的输出</p>
			<p class="callout-heading">注意</p>
			<p class="callout">此活动的解决方案可通过<a href="B16341_Solution_ePub.xhtml#_idTextAnchor250">此链接</a>找到。</p>
			<p>在下一节中，你将学习如何改变张量的形状和秩。</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>整形</h1>
			<p>有些运算，比如加法，只有满足一定条件才能应用于张量。整形是修改张量形状的一种方法，这样就可以进行这样的操作。整形将张量的元素重新排列成不同大小的张量。只要元素总数不变，任何大小的张量都可以改变形状。</p>
			<p>例如，一个<code>(4x3)</code>矩阵可以被改造成一个<code>(6x2)</code>矩阵，因为它们总共有<code>12</code>个元素。尺寸的等级或数量也可以在整形过程中改变。例如，秩等于<code>2</code>的<code>(4x3)</code>矩阵可以被整形为秩等于<code>3</code>的<code>(3x2x2)</code>张量。也可以将<code>(4x3)</code>矩阵整形为<code>(12x1)</code>向量，其中秩从<code>2</code>变为<code>1</code>。</p>
			<p><em class="italic">图1.13 </em>展示了张量整形。左边是一个形状为<code>(3x2)</code>的张量，它可以被整形为形状等于<code>(2x3)</code>、<code>(6)</code>或<code>(6x1)</code>的张量。这里，元素的数量，即六个，保持不变，尽管张量的形状和秩已经改变:</p>
			<div><div><img src="img/B16341_01_13.jpg" alt="Figure 1.13: Visual representation of reshaping a (3x2) tensor to tensors of different shapes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.13:将一个(3x2)张量整形为不同形状的张量的可视化表示</p>
			<p>张量整形可在TensorFlow中进行，方法是使用<code>reshape</code>函数，并将张量和新张量的所需形状作为参数传入:</p>
			<pre>tensor1 = tf.Variable([1,2,3,4,5,6])
tensor_reshape = tf.reshape(tensor1, shape=[3,2])</pre>
			<p>这里，创建了一个新的张量，它具有与原始张量相同的元素；但是，形状是<code>[3,2]</code>而不是<code>[6]</code>。</p>
			<p>下一节介绍张量变换，这是另一种改变张量形状的方法。</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>张量换位</h2>
			<p>当张量被转置时，张量中的元素以特定的顺序重新排列。转置操作通常表示为张量上的一个<code>T</code>上标。张量中每个元素的新位置可以由<code>(x</code> 12…k <code>)</code> T = <code>x</code> k…21确定。对于秩等于<code>2</code>的矩阵或张量，行变成列，反之亦然。矩阵转置的示例如图<em class="italic">图1.14 </em>所示。任何秩的张量都可以被转置，通常形状会因此而改变:</p>
			<div><div><img src="img/B16341_01_14.jpg" alt="Figure 1.14: A visual representation of tensor transposition on a (3x2) matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.14 :( 3 x2)矩阵上张量变换的可视化表示</p>
			<p>下图显示了矩阵<code>A</code>和<code>B</code>的矩阵转置属性:</p>
			<div><div><img src="img/B16341_01_15.jpg" alt="Figure 1.15: Tensor transposition properties where X and Y are tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.15:张量转置属性，其中X和Y是张量</p>
			<p>如果一个张量的转置等价于原张量，则称这个张量是对称的。</p>
			<p>张量转置可以在TensorFlow中执行，方法是使用其<code>transpose</code>函数，并将张量作为唯一参数传入:</p>
			<pre>tensor1 = tf.Variable([1,2,3,4,5,6])
tensor_transpose = tf.transpose(tensor1)</pre>
			<p>当转置一个张量时，只有一种可能的结果；然而，根据输出的期望形状，对张量进行整形具有多种可能的结果。</p>
			<p>在下面的练习中，使用TensorFlow在张量上演示了整形和变换。</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>练习1.04:在TensorFlow中执行张量整形和变换</h2>
			<p>在本练习中，您将学习如何使用TensorFlow库执行张量整形和变换。</p>
			<p>执行以下步骤:</p>
			<ol>
				<li value="1">导入TensorFlow库，并使用TensorFlow的<code>Variable</code>类:<pre>import tensorflow as tf matrix1 = tf.Variable([[1,2,3,4], [5,6,7,8]])</pre>创建一个两行四列的矩阵</li>
				<li>Verify the shape of the matrix by calling the <code>shape</code> attribute of the matrix as a Python list:<pre>matrix1.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[2, 4]</pre><p>你看到矩阵的形状是<code>[2,4]</code>。</p></li>
				<li>Use TensorFlow's <code>reshape</code> function to change the matrix to a matrix with four rows and two columns by passing in the matrix and the desired new shape:<pre>reshape1 = tf.reshape(matrix1, shape=[4, 2])
reshape1</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_01_16.jpg" alt="Figure 1.16: The reshaped matrix&#13;&#10;"/></div><p class="figure-caption">图1.16:重塑的矩阵</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <code>shape</code> attribute as a Python list:<pre>reshape1.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[4, 2]</pre><p>在这里，你可以看到矩阵的形状已经变成了你想要的形状，<code>[4,2]</code>。</p></li>
				<li>Use TensorFlow's <code>reshape</code> function to convert the matrix into a matrix with one row and eight columns. Pass the matrix and the desired new shape as parameters to the <code>reshape</code> function:<pre>reshape2 = tf.reshape(matrix1, shape=[1, 8])
reshape2</pre><p>您应该得到以下输出:</p><pre>&lt;tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8]])&gt;</pre></li>
				<li>Verify the shape of the reshaped matrix by calling the <code>shape</code> attribute as a Python list:<pre>reshape2.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[1, 8]</pre><p>前面的输出确认了整形后矩阵的形状为<code>[1, 8]</code>。</p></li>
				<li>Use TensorFlow's <code>reshape</code> function to convert the matrix into a matrix with eight rows and one column, passing the matrix and the desired new shape as parameters to the <code>reshape</code> function:<pre>reshape3 = tf.reshape(matrix1, shape=[8, 1])
reshape3</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_01_17.jpg" alt="Figure 1.17: Reshaped matrix of shape (8, 1)&#13;&#10;"/></div><p class="figure-caption">图1.17:形状(8，1)的重塑矩阵</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <code>shape</code> attribute as a Python list:<pre>reshape3.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[8, 1]</pre><p>前面的输出确认了整形后矩阵的形状为<code>[8, 1]</code>。</p></li>
				<li>Use TensorFlow's <code>reshape</code> function to convert the matrix to a tensor of size <code>2x2x2</code>. Pass the matrix and the desired new shape as parameters to the reshape function:<pre>reshape4 = tf.reshape(matrix1, shape=[2, 2, 2])
reshape4</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_01_18.jpg" alt="Figure 1.18: Reshaped matrix of shape (2, 2, 2)&#13;&#10;"/></div><p class="figure-caption">图1.18:形状(2，2，2)的重塑矩阵</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <code>shape</code> attribute as a Python list:<pre>reshape4.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[2, 2, 2]</pre><p>前面的输出确认了整形后矩阵的形状为<code>[2, 2, 2]</code>。</p></li>
				<li>Verify that the rank has changed using TensorFlow's <code>rank</code> function and print the result as a NumPy variable:<pre>tf.rank(reshape4).numpy()</pre><p>这将导致以下输出:</p><pre>3</pre></li>
				<li>Use TensorFlow's <code>transpose</code> function to convert the matrix of size <code>2X4</code> to a matrix of size <code>4x2</code>:<pre>transpose1 = tf.transpose(matrix1)
transpose1</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_01_19.jpg" alt="Figure 1.19: Transposed matrix&#13;&#10;"/></div><p class="figure-caption">图1.19:转置矩阵</p></li>
				<li>Verify that the <code>reshape</code> function and the <code>transpose</code> function create different resulting matrices when applied to the given matrix:<pre>transpose1 == reshape1</pre><div><img src="img/B16341_01_20.jpg" alt="Figure 1.20: Verification that transposition and reshaping produce different results&#13;&#10;"/></div><p class="figure-caption">图1.20:证实变换和整形产生不同的结果</p></li>
				<li>Use TensorFlow's <code>transpose</code> function to transpose the reshaped matrix in <em class="italic">step 9</em>:<pre>transpose2 = tf.transpose(reshape4)
transpose2</pre><p>这将导致以下输出:</p><div><img src="img/B16341_01_21.jpg" alt="Figure 1.21: The output of the transposition of the reshaped tensor&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图1.21:整形张量的转置输出</p>
			<p>这个结果显示了在对张量进行整形和转置后，得到的张量是如何出现的。</p>
			<p>在这个练习中，你已经通过整形或换位成功地修改了张量的形状。你学习了在整形和转置操作之后张量的形状和秩是如何变化的。</p>
			<p>在下面的活动中，您将测试如何使用TensorFlow重塑和转置张量。</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>活动1.02:在TensorFlow中执行张量整形和变换</h2>
			<p>在本练习中，你需要模拟24名学生的分组，以完成班级项目。每个结果整形或转置张量的维数将代表每个组的大小。</p>
			<p>执行以下步骤:</p>
			<ol>
				<li value="1">导入TensorFlow库。</li>
				<li>Create a one-dimensional tensor with 24 monotonically increasing elements using the <code>Variable</code> class to represent the IDs of the school children. Verify the shape of the matrix.<p>您应该得到以下输出:</p><pre>[24]</pre></li>
				<li>Reshape the matrix so that it has 12 rows and 2 columns using TensorFlow's <code>reshape</code> function representing 12 pairs of school children. Verify the shape of the new matrix.<p>您应该得到以下输出:</p><pre>[12, 2]</pre></li>
				<li>Reshape the original matrix so that it has a shape of <code>3x4x2</code> using TensorFlow's <code>reshape</code> function representing 3 groups of 4 sets of pairs of school children. Verify the shape of the new tensor.<p>您应该得到以下输出:</p><pre>[3, 4, 2]</pre></li>
				<li>验证这个新张量的秩是<code>3</code>。</li>
				<li>Transpose the tensor created in <em class="italic">step 3</em> to represent 2 groups of 12 students using TensorFlow's <code>transpose</code> function. Verify the shape of the new tensor.<p>您应该得到以下输出:</p><pre>[2, 12]</pre><p class="callout-heading">注意</p><p class="callout">这个活动的解决方案可以通过<a href="B16341_Solution_ePub.xhtml#_idTextAnchor252">这个链接</a>找到。</p></li>
			</ol>
			<p>在本节中，您将了解到人工神经网络的一些基本组成部分——张量。您还学习了张量的一些基本操作，如加法、换位和整形。您通过使用TensorFlow库中的函数实现了这些概念。</p>
			<p>在下一个主题中，您将通过讲述与人工神经网络相关的另一个重要变换(张量乘法)来扩展您对线性变换的理解。</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>张量乘法</h1>
			<p>张量乘法是在构建和训练ann的过程中经常使用的另一种基本运算，因为信息通过一系列加法和乘法从输入到结果在网络中传播。加法的规则简单直观，而张量的规则更复杂。张量乘法不仅仅是简单的逐元素乘法。相反，执行更复杂的过程，包括每个张量的整个行/列之间的点积，以计算结果张量的每个元素。这一节将解释乘法如何适用于二维张量或矩阵。但是，更高阶的张量也可以相乘。</p>
			<p>给定一个矩阵，<code>X = [x</code>ij<code>]</code>m×n，和另一个矩阵，<code>Y = [y</code>ij<code>]</code>n×p，两个矩阵的乘积是<code>Z = XY = [z</code>ij<code>]</code>m×p，每个元素，<code>z</code> ij按元素定义为<img src="img/B16341_01_21a.png" alt="Formula"/>。所得矩阵的形状与矩阵乘积的外部尺寸相同，或者与第一矩阵的行数和第二矩阵的列数相同。为了使乘法有效，矩阵乘积的内部维数必须匹配，或者第一个矩阵中的列数和第二个矩阵中的列数必须对应。</p>
			<p>矩阵乘法的内外维概念如下图所示，其中<code>X</code>代表第一个矩阵，<code>Y</code>代表第二个矩阵:</p>
			<div><div><img src="img/B16341_01_22.jpg" alt="Figure 1.22: A visual representation of inner and outer dimensions in matrix multiplication&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.22:矩阵乘法中内部和外部维度的可视化表示</p>
			<p>与矩阵加法不同，矩阵乘法是不可交换的，这意味着乘积中矩阵的顺序很重要:</p>
			<div><div><img src="img/B16341_01_23.jpg" alt="Figure 1.23: Matrix multiplication is non-commutative&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.23:矩阵乘法是不可交换的</p>
			<p>例如，假设您有以下两个矩阵:</p>
			<div><div><img src="img/B16341_01_24.jpg" alt="Figure 1.24: Two matrices, X and Y&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.24:两个矩阵，X和Y</p>
			<p>构造乘积的一种方法是首先用矩阵<code>X</code>乘以<code>Y</code>:</p>
			<div><div><img src="img/B16341_01_25.jpg" alt="Figure 1.25: Visual representation of matrix X multiplied by Y, X•Y &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.25:矩阵X乘以Y，X Y的直观表示</p>
			<p>这产生了一个<code>2x2</code>矩阵。构建产品的另一种方法是先有<code>Y</code>，再乘以<code>X</code>:</p>
			<div><div><img src="img/B16341_01_26.jpg" alt="Figure 1.26: Visual representation of matrix Y multiplied by X, Y•X&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.26:矩阵Y乘以X，y X的直观表示</p>
			<p>在这里，您可以看到由产品<code>YX</code>形成的矩阵是一个<code>3x3</code>矩阵，并且与由产品<code>XY</code>形成的矩阵非常不同。</p>
			<p>张量乘法可在TensorFlow中执行，方法是使用<code>matmul</code>函数，并按照张量相乘的顺序将张量作为自变量传入:</p>
			<pre>tensor1 = tf.Variable([[1,2,3]])
tensor2 = tf.Variable([[1],[2],[3]])
tensor_mult = tf.matmul(tensor1, tensor2)</pre>
			<p>张量乘法也可以通过使用<code>@</code>操作符实现，如下所示:</p>
			<pre>tensor_mult = tensor1 @ tensor2</pre>
			<p>标量-张量乘法要简单得多，它只是张量中每个元素乘以标量的乘积，因此<code>λX = [λx</code> ij…k <code>]</code>，其中<code>λ</code>是标量，<code>X</code>是张量。</p>
			<p>标量乘法可通过使用<code>matmul</code>函数或使用<code>*</code>运算符在TensorFlow中实现:</p>
			<pre>tensor1 = tf.Variable([[1,2,3]])
scalar_mult = 5 * tensor1</pre>
			<p>在以下练习中，您将使用TensorFlow库执行张量乘法。</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>练习1.05:在TensorFlow中执行张量乘法</h2>
			<p>在本练习中，您将使用TensorFlow的<code>matmul</code>函数和<code>@</code>运算符在TensorFlow中执行张量乘法。在本练习中，您将使用三明治零售商的数据示例，这些数据代表各种三明治的配料以及不同配料的成本。您将使用矩阵乘法来确定每个三明治的成本。</p>
			<p><strong class="bold">三明治食谱</strong>:</p>
			<div><div><img src="img/B16341_01_27.jpg" alt="Figure 1.27: Sandwich recipe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.27:三明治食谱</p>
			<p><strong class="bold">配料详情</strong>:</p>
			<div><div><img src="img/B16341_01_28.jpg" alt="Figure 1.28: Ingredient details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.28:成分细节</p>
			<p><strong class="bold">销售预测</strong>:</p>
			<div><div><img src="img/B16341_01_29.jpg" alt="Figure 1.29: Sales projections&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.29:销售预测</p>
			<p>执行以下步骤:</p>
			<ol>
				<li value="1">导入TensorFlow库:<pre>import tensorflow as tf</pre></li>
				<li>Create a matrix representing the different sandwich recipes, with the rows representing the three different sandwich offerings and the columns representing the combination and number of the five different ingredients using the <code>Variable</code> class: <pre>matrix1 = tf.Variable([[1.0,0.0,3.0,1.0,2.0], \
                       [0.0,1.0,1.0,1.0,1.0], \
                       [2.0,1.0,0.0,2.0,0.0]], \
                      tf.float32)
matrix1</pre><p>您应该得到以下输出:</p><div><img src="img/B16341_01_30.jpg" alt="Figure 1.30: Matrix representing the number of ingredients needed to make sandwiches&#13;&#10;"/></div><p class="figure-caption">图1.30:代表制作三明治所需配料数量的矩阵</p></li>
				<li>Verify the shape of the matrix by calling the <code>shape</code> attribute of the matrix as a Python list:<pre>matrix1.shape.as_list()</pre><p>这将导致以下输出:</p><pre>[3, 5]</pre></li>
				<li>Create a second matrix representing the cost and weight of each individual ingredient in which the rows represent the five ingredients, and the columns represent the cost and weight of the ingredients in grams:<pre>matrix2 = tf.Variable([[0.49, 103], \
                       [0.18, 38], \
                       [0.24, 69], \
                       [1.02, 75], \
                       [0.68, 78]])
matrix2</pre><p>您应该会得到以下结果:</p><div><img src="img/B16341_01_31.jpg" alt="Figure 1.31: A matrix representing the cost and weight of each ingredient&#13;&#10;"/></div><p class="figure-caption">图1.31:代表每种成分的成本和重量的矩阵</p></li>
				<li>Use TensorFlow's <code>matmul</code> function to perform the matrix multiplication of <code>matrix1</code> and <code>matrix2</code>:<pre>matmul1 = tf.matmul(matrix1, matrix2)
matmul1</pre><p>这将导致以下输出:</p><div><img src="img/B16341_01_32.jpg" alt="Figure 1.32: The output of the matrix multiplication&#13;&#10;"/></div><p class="figure-caption">图1.32:矩阵乘法的输出</p></li>
				<li>创建一个矩阵，代表五家不同商店对三种三明治的销售预测:<pre>matrix3 = tf.Variable([[120.0, 100.0, 90.0], \                        [30.0, 15.0, 20.0], \                        [220.0, 240.0, 185.0], \                        [145.0, 160.0, 155.0], \                        [330.0, 295.0, 290.0]])</pre></li>
				<li>Multiply <code>matrix3</code> by the result of the matrix multiplication of <code>matrix1</code> and <code>matrix2</code> to give the expected cost and weight for each of the five stores:<pre>matmul3 = matrix3 @ matmul1
matmul3</pre><p>这将导致以下输出:</p><div><img src="img/B16341_01_33.jpg" alt="Figure 1.33: The output of matrix multiplication&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图1.33:矩阵乘法的输出</p>
			<p>乘法得到的张量显示了每个商店三明治的预期成本和所有配料的预期重量。</p>
			<p>在本练习中，您已经成功学习了如何使用几个运算符在TensorFlow中执行矩阵乘法。您使用了TensorFlow的<code>matmul</code>函数，以及简写的<code>@</code>运算符。每个将执行乘法；然而，<code>matmul</code>函数有几个不同的参数可以传递给函数，这使它更加灵活。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在这里阅读更多关于<code>matmul</code>功能的内容:<a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul">https://www.tensorflow.org/api_docs/python/tf/linalg/matmul</a>。</p>
			<p>在下一节中，您将探索一些与人工神经网络相关的其他数学概念。您将探索正向和反向传播，以及激活函数。</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>优化</h1>
			<p>在本节中，您将了解一些对训练机器学习模型至关重要的优化方法。优化是更新人工神经网络各层权重的过程，以便最小化人工神经网络预测值和训练数据真实值之间的误差。</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>正向传播</h2>
			<p>正向传播是信息通过人工神经网络传播的过程。诸如一系列张量乘法和加法的运算发生在网络的每一层，直到最终输出。前向传播在<em class="italic">图1.37 </em>中解释，显示了一个单隐层人工神经网络。输入数据有两个要素，而输出图层的每个输入记录只有一个值。</p>
			<p>隐藏层和输出的权重和偏差显示为具有适当索引的矩阵和向量。对于隐藏层，权重矩阵中的行数等于输入的特征数，列数等于隐藏层中的单元数。因此，<code>W1</code>有两行和三列，因为输入<code>X</code>有两个特征。同样，<code>W2</code>有三行一列，隐藏层有三个单元，输出大小为一。然而，偏差始终是一个向量，其大小等于该层中节点的数量，并被添加到输入和权重矩阵的乘积中。</p>
			<div><div><img src="img/B16341_01_34.jpg" alt="Figure 1.34: A single-layer artificial neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.34:单层人工神经网络</p>
			<p>执行正向传播的步骤如下:</p>
			<ol>
				<li value="1"><code>X</code> is the input to the network and the input to the hidden layer. First, the input matrix, <code>X</code>, is multiplied by the weight matrix for the hidden layer, <code>W1</code>, and then the bias, <code>b1</code>, is added:<p><code>z1 = X*W1 + b1</code></p><p>这里有一个例子，说明运算后得到的张量的形状。如果输入的大小是<code>nX2</code>，其中<code>n</code>是输入示例的数量，<code>W1</code>的大小是<code>2X3</code>，而<code>b1</code>的大小是<code>1X3</code>，则得到的矩阵<code>z1</code>的大小将是<code>nX3</code>。</p></li>
				<li><code>z1</code> is the output of the hidden layer, which is the <code>W2</code>, and the bias, <code>b2</code>, is added:<p><code>Y = z1 * W2 + b2</code></p><p>为了理解结果张量的形状，考虑下面的例子。如果输出层的输入<code>z1</code>的大小为<code>nX3</code>，<code>W2</code>的大小为<code>3X1</code>，而<code>b1</code>的大小为<code>1X1</code>，则得到的矩阵<code>Y</code>的大小将为<code>nX1</code>，表示每个训练示例的一个结果。</p></li>
			</ol>
			<p>该模型中的参数总数等于<code>W1</code>、<code>W2</code>、<code>b1</code>、<code>b2</code>中的元素数之和。因此，可以通过对权重矩阵和偏差中的每个参数中的元素求和来计算参数的数量，等于<code>6 + 3 + 3 + 1 = 13</code>。这些是在训练ANN的过程中需要学习的参数。</p>
			<p>在向前传播步骤之后，您必须评估您的模型，并将其与真实的目标值进行比较。这是使用损失函数来实现的。均方差，即真实值和预测值之间的平方差的平均值，是回归任务的损失函数的示例之一。一旦计算出损失，必须更新权重以减少损失，并且使用反向传播找到权重应该更新的量和方向。</p>
			<h2 id="_idParaDest-38">反向传播</h2>
			<p><code>loss</code>对预测输出的作用如下:</p>
			<p><code>loss = L(y_predicted)</code></p>
			<p>损失相对于模型参数的导数将告知您增加或减少模型参数是否会导致损失增加或减少。反向传播过程通过将微积分的链式法则从神经网络的输出层应用到输入层来实现，在每一层计算<code>loss</code>函数相对于模型参数的导数。</p>
			<p>微积分的链式法则是一种通过中间函数计算复合函数导数的技术。该函数的一般形式可以写成如下形式:</p>
			<p><code>dz/dx = dz/dy * dy/dx</code></p>
			<p>这里，<code>dz/dx</code>是复合函数，<code>y</code>是中间函数。在人工神经网络的情况下，复合函数是作为模型参数函数的损失，中间函数代表隐藏层。因此，损失相对于模型参数的导数可以通过将损失相对于预测输出的导数乘以预测输出相对于模型参数的导数来计算。</p>
			<p>在下一节中，您将了解如何在给定损失函数对每个权重的导数的情况下更新权重参数，以使损失最小化。</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>学习最佳参数</h2>
			<p>在本节中，您将看到如何迭代选择最佳权重。您知道，前向传播通过一系列张量加法和乘法在网络中传输信息，后向传播是了解损耗相对于每个模型权重的变化的过程。下一步是使用反向传播的结果来更新权重，以便它们根据损失函数来减少误差。这个过程被称为学习参数，并使用优化算法来实现。一种常用的优化算法叫做<strong class="bold">梯度下降</strong>。</p>
			<p>在学习最佳参数时，应用优化算法，直到损失函数达到最小值。你通常在给定的步数后或者当损失函数有一个可以忽略的变化时停止。如果将损失绘制为每个模型参数的函数，则损失函数的形状类似于凸形，只有一个最小值，并且优化函数的目标是找到该最小值。</p>
			<p>下图显示了特定特征的损失函数:</p>
			<div><div><img src="img/B16341_01_35.jpg" alt="Figure 1.35: A visual representation of the gradient descent algorithm finding &#13;&#10;the optimal parameter to minimize the loss&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.35:梯度下降算法寻找最佳参数以最小化损失的可视化表示</p>
			<p>首先，通过随机设置每个重量的参数来实现，如图中的<code>p</code> 1所示。然后计算该模型参数<code>l</code> 1的损失。反向传播步骤确定损失相对于模型参数的导数，并将确定模型应该在哪个方向上更新。下一个模型参数<code>p</code> 2等于当前模型参数减去学习率(<code>α</code>)乘以导数值。学习率是在模型训练过程之前设置的超参数。通过乘以导数值，当参数远离导数绝对值较大的最小值时，将采取较大的步长。然后计算损耗<code>l</code> 2，并且该过程继续，直到达到最小损耗<code>l</code> m，具有最佳参数<code>p</code> m</p>
			<p>总之，这些是优化算法为找到最佳参数而执行的迭代步骤:</p>
			<ol>
				<li value="1">使用正向传播和当前参数来预测整个数据集的输出。</li>
				<li>应用损失函数根据预测输出计算所有示例的损失。</li>
				<li>使用反向传播计算每层的权重和偏差的损失导数。</li>
				<li>使用导数值和学习率更新权重和偏差。</li>
			</ol>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>tensor flow中的优化器</h2>
			<p>TensorFlow中有几种不同的优化器。每一种都基于不同的优化算法，旨在达到损失函数的全局最小值。它们都基于梯度下降算法，尽管在实现上略有不同。TensorFlow中可用的优化器包括:</p>
			<ul>
				<li><strong class="bold">随机梯度下降</strong>(<strong class="bold">SGD</strong>):SGD算法将梯度下降应用于小批量的训练数据。在TensorFlow中使用优化器时，动量参数也是可用的，该优化器对计算的梯度应用指数平滑以加速训练。</li>
				<li><strong class="bold"> Adam </strong>:这种优化是一种SGD方法，基于一阶和二阶矩的连续自适应估计。</li>
				<li><strong class="bold">均方根传播</strong> ( <strong class="bold"> RMSProp </strong>):这是一个未发布的自适应学习率优化器。RMSprop在每步后寻找损失最小值时，将学习率除以梯度平方的平均值，这导致学习率呈指数衰减。</li>
				<li><strong class="bold"> Adagrad </strong>:该优化器具有特定于参数的学习率，该学习率根据训练过程中参数更新的频率而更新。随着参数接收到更多更新，每个后续更新的值都会变小。</li>
			</ul>
			<p>优化器的选择将影响训练时间和模型性能。每个优化器也有超参数，如初始学习率，这些参数必须在训练前选择，这些超参数的调整也会影响训练时间和模型性能。虽然TensorFlow中可用的其他优化器在这里没有明确说明(可以在这里找到:<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">https://www . tensor flow . org/API _ docs/python/TF/keras/optimizer</a>)，但上述那些在训练时间和模型性能方面都表现良好，是为您的模型选择优化器时的安全首选。TensorFlow中可用的优化器位于<code>tf.optimizers</code>模块中；例如，学习率等于<code>0.001</code>的Adam优化器可以初始化如下:</p>
			<pre>optimizer = tf.optimizer.adam(learning_rate=0.001)</pre>
			<p>在本主题中，您已经了解了实现梯度下降以计算模型训练的最佳参数的步骤。在梯度下降法中，每一个训练样本都用来学习参数。但是，当处理大量数据集时，例如图像和音频，您通常会分批处理，并在从每批中学习后进行更新。对批量数据使用梯度下降时，该算法称为SGD。SGD优化器以及一套其他高性能优化器在TensorFlow中很容易获得，包括Adam、RMSProp和Adagrad优化器等等。</p>
			<p>在下一节中，您将探索不同的激活函数，这些函数通常应用于每一层的输出。</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>激活功能</h1>
			<p>激活函数是数学函数，通常应用于ANN层的输出，以限制或限定层的值。值可能需要有界的原因是，如果没有激活函数，值和相应的梯度可能会爆炸或消失，从而使结果不可用。这是因为最终值是每个后续层的值的累积积。随着图层数量的增加，值和梯度爆炸到无穷大或消失为零的可能性也会增加。这个概念被称为<strong class="bold">爆炸和消失梯度问题</strong>。决定一个层中的节点是否应该被激活是激活函数的另一种用法，因此得名。常见的激活功能及其在<em class="italic">图1.36 </em>中的可视化表示如下:</p>
			<ul>
				<li><strong class="bold">步骤</strong>功能:高于某个阈值，该值为非零，否则为零。如图1.36a 中的<em class="italic">所示。</em></li>
				<li><strong class="bold">线性</strong>函数:<img src="img/B16341_01_35a.png" alt="Formula"/>，是输入值的标量乘法。如图1.36 b<em class="italic">所示。</em></li>
				<li><strong class="bold"> Sigmoid </strong>函数:<img src="img/B16341_01_35b.png" alt="Formula"/>，像平滑渐变的平滑阶梯函数。该激活函数对于分类是有用的，因为值从0到1被绑定。如图1.36c 所示。</li>
				<li><code>x=0</code>。如图<em class="italic">图1.36d </em>所示。</li>
				<li><code>0</code>。如图<em class="italic">图1.36e </em>所示。</li>
				<li><strong class="bold"> ELU </strong> ( <strong class="bold">指数线性单位</strong>)函数:<img src="img/B16341_01_35e.png" alt="Formula"/>，否则<img src="img/B16341_01_35f.png" alt="Formula"/>，其中<img src="img/B16341_01_35g.png" alt="Formula"/>为常数。</li>
				<li><strong class="bold"> SELU </strong> ( <strong class="bold">标度指数线性单位</strong>)函数:<img src="img/B16341_01_35h.png" alt="Formula"/>，否则<img src="img/B16341_01_35i.png" alt="Formula"/>，其中<img src="img/B16341_01_35j.png" alt="Formula"/>为常数。这在<em class="italic">图1.36f </em>中显示。</li>
				<li><strong class="bold">唰</strong>功能:<img src="img/B16341_01_35k.png" alt="Formula"/>。如图<em class="italic">图1.36g </em>所示:</li>
			</ul>
			<div><div><img src="img/B16341_01_36.jpg" alt="Figure 1.36: A visual representation of the common activation functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.36:常见激活功能的可视化表示</p>
			<p>通过利用<code>tf.keras.activations</code>模块中的激活函数，激活函数可应用于任何张量。例如，sigmoid激活函数可以如下应用于张量:</p>
			<pre>y=tf.keras.activations.sigmoid(x)</pre>
			<p>现在，让我们测试一下你在下面的活动中已经学到的知识。</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>活动1.03:应用激活功能</h2>
			<p>在本练习中，您将回忆起本章中使用的许多概念，并将激活函数应用于张量。您将使用汽车经销商销售的示例数据，应用这些概念，显示各种销售人员的销售记录，并突出显示净销售额为正的销售人员。</p>
			<p><strong class="bold">销售记录</strong>:</p>
			<div><div><img src="img/B16341_01_37.jpg" alt="Figure 1.37: Sales records&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.37:销售记录</p>
			<p><strong class="bold">车辆MSRP</strong>:</p>
			<div><div><img src="img/B16341_01_38.jpg" alt="Figure 1.38: Vehicle MSRPs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.38:车辆MSRPs</p>
			<p><strong class="bold">固定成本</strong>:</p>
			<div><div><img src="img/B16341_01_39.jpg" alt="Figure 1.39: Fixed costs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图1.39:固定成本</p>
			<p>执行以下步骤:</p>
			<ol>
				<li value="1">导入TensorFlow库。</li>
				<li>用值<code>[[-0.013, 0.024, 0.06, 0.022], [0.001, -0.047, 0.039, 0.016], [0.018, 0.030, -0.021, -0.028]]</code>创建一个<code>3x4</code>张量作为输入。该张量中的行表示各销售代表的销售额，列表示经销商处现有的各种车辆，值表示与MSRP的平均百分比差异。该值为正值或负值，取决于销售人员的销售价格是高于还是低于建议零售价。</li>
				<li>创建一个形状为<code>4x1</code>的<code>4x1</code>权重张量，其值<code>[[19995.95], [24995.50], [36745.50], [29995.95]]</code>代表汽车的MSRP。</li>
				<li>创建一个大小为<code>3x1</code>的偏差张量，其值<code>[[-2500.0], [-2500.0], [-2500.0]]</code>代表与每个销售人员相关的固定成本。</li>
				<li>Matrix multiply the input by the weight to show the average deviation from the MSRP on all cars and add the bias to subtract the fixed costs of the salesperson. Print the result.<p>您应该会得到以下结果:</p><div><img src="img/B16341_01_40.jpg" alt="Figure 1.40: The output of the matrix multiplication&#13;&#10;"/></div><p class="figure-caption">图1.40:矩阵乘法的输出</p></li>
				<li>Apply a ReLU activation function to highlight the net-positive salespeople and print the result.<p>您应该会得到以下结果:</p><div><img src="img/B16341_01_41.jpg" alt="Figure 1.41: The output after applying the activation function&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图1.41:应用激活功能后的输出</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这个活动的解决方案可以通过<a href="B16341_Solution_ePub.xhtml#_idTextAnchor253">这个链接</a>找到。</p>
			<p>在随后的章节中，你将会看到如何在你的人工神经网络中添加激活函数，或者在层之间添加，或者在定义层后直接应用。您将学习如何选择最合适的激活函数，这通常是通过超参数优化技术。激活函数是超参数的一个示例，超参数是在学习过程开始之前设置的参数，可以对其进行调整以找到模型性能的最佳值。</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>总结</h1>
			<p>在本章中，向您介绍了TensorFlow库。您学习了如何在Python编程语言中使用它。您创建了具有各种等级和形状的ann(张量)的构建块，使用TensorFlow对张量执行了线性变换，并对张量实现了加法、整形、转置和乘法，所有这些都是理解ann底层数学的基础。</p>
			<p>在下一章<a id="_idTextAnchor043"/>中，您将加深对tensors的理解，并学习如何加载各种类型的数据并对其进行预处理，以便适合在TensorFlow中训练ann。您将使用表格、可视和文本数据，所有这些数据都必须进行不同的预处理。通过使用视觉数据(即图像)，您还将学习如何使用内存无法容纳的训练数据。</p>
		</div>
		<div><div/>
		</div>
	

</body></html>