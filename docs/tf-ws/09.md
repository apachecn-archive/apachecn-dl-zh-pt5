

# 九、循环神经网络

概观

在这一章中，你将学习如何处理真正的顺序数据。您将扩展您对用于训练序列数据的**人工神经网络** ( **安**)模型和**循环神经网络** ( **RNN** )架构的了解。您还将学习如何为自然语言处理构建一个带有 LSTM 图层的 RNN 模型。

本章结束时，您将获得应用多个 LSTM 层来构建用于股票价格预测的 rnn 的实践经验。

# 简介

顺序数据是指每个数据点都依赖于前一个数据点的数据集。把它想象成一个句子，由一系列相互关联的单词组成。一个动词将与一个主语相联系，一个副词将与一个动词相联系。另一个例子是股票价格，某一天的价格与前几天的价格相关。传统的神经网络不适合处理这类数据。有一种特定类型的架构可以接收数据序列。本章将向你介绍这样的模型——被称为**循环神经网络** ( **RNNs** )。

RNN 模型是一种特定类型的深度学习架构，其中模型的输出反馈到输入中。这种类型的模型有它们自己的挑战(称为消失和爆炸渐变),将在本章后面讨论。

在许多方面，RNN 是大脑如何工作的代表。rnn 使用记忆来帮助他们学习。但是，如果信息只单向流动，他们如何做到这一点呢？为了理解这一点，你需要先回顾一下序列数据。这是一种需要工作内存来有效处理数据的数据。到目前为止，您只探索了非顺序模型，如感知器或 CNN。在这一章中，你将会看到像 RNN、LSTM 或 GRU 这样的序列模型

![Figure 9.1: Sequential versus non-sequential models
](img/B16341_09_01.jpg)

图 9.1:顺序与非顺序模型

# 顺序数据

顺序数据是按顺序发生的信息，与过去和未来的数据相关。时序数据的一个例子是时间序列数据；正如你所感知的，时间只朝一个方向行进。

假设你有一个球(如图*图 9.2* )，你想预测这个球下一步会去哪里。如果你事先不知道球的投掷方向，你只能猜测。然而，如果除了球的当前位置之外，你还知道它以前的位置，问题就简单多了。为了能够预测球的下一个位置，您需要顺序(或有序)形式的先前位置信息来预测未来事件。

![Figure 9.2: Direction of the ball
](img/B16341_09_02.jpg)

图 9.2:球的方向

RNNs 的工作方式允许信息序列在内存的帮助下保留值。

在下一节中，您将看到一些顺序数据的例子。

## 顺序数据的例子

顺序数据是一种特定类型的数据，其中每条信息的顺序都很重要，并且它们都相互依赖。

顺序数据的一个例子是金融数据，如股票价格。如果您想要预测给定股票的未来数据值，您需要使用以前的时间值。事实上，您将在*练习 9.01* 、*中进行股票预测，为序列数据训练一个人工神经网络——Nvidia 股票预测*。

音频和文本也可以被认为是顺序数据。音频可以分解成一系列声波，文本可以分解成一系列字符或单词。应该对声波或字符或单词序列进行处理，以传达想要的结果。除了这两个你每天都会遇到的例子之外，还有更多顺序处理可能有用的例子，从分析 EEG 等医学信号，预测股票价格，以及推断和理解基因组序列。有三类顺序数据:

*   **多对一**从多个输入产生一个输出。
*   **一对多**从一个输入产生多个输出。
*   **多对多**从多个输入产生多个输出。

![Figure 9.3: Categories of sequential data
](img/B16341_09_03.jpg)

图 9.3:顺序数据的类别

考虑另一个例子。假设您有一个包含一个句子或短语的语言模型，并且您正在尝试预测下一个单词，如下图所示:

![Figure 9.4: Sentence example
](img/B16341_09_04.jpg)

图 9.4:句子示例

假设给你单词`yesterday I took my car out for a…`，你想试着预测下一个单词`drive`。一种方法是建立深度神经网络，如前馈神经网络。然而，你会立即遇到一个问题。前馈网络只能将固定长度的输入向量作为其输入；您必须从一开始就指定输入的大小。

因此，您的模型需要一种能够处理可变长度输入的方法。一种方法是使用固定窗口。这意味着你强制输入向量只有一定的长度。例如，你可以把句子分成由两个连续单词组成的组(也称为**双字组**，并预测下一个单词。这意味着无论你想在哪里做下一个预测，你的模型都只会接受前两个词作为输入。您需要考虑如何用数字表示这些数据。一种方法是取一个固定长度的向量，在向量中为第一个单词分配一些空间，为第二个单词分配一些空间。在这些空格中，对每个单词的身份进行编码。然而，这是有问题的。

为什么？因为您只使用了可用信息的一部分(也就是说，只有两个连续的单词)。你只能访问有限的数据窗口，这些数据无法给出足够的上下文来准确预测下一个单词是什么。这意味着您无法有效地对长期依赖关系建模。这在像图 9.5 中*这样的句子中很重要，你显然需要句子中更早的信息来准确预测下一个单词。*

![Figure 9.5: Sentence example
](img/B16341_09_05.jpg)

图 9.5:句子示例

如果你只看过去的两三个单词，你就无法做出下一个预测，你知道这就是`Italian`。所以，这意味着你真的需要一种方法来从头到尾整合句子中的信息。

为此，您可以使用一组计数作为固定长度的向量，并使用整个句子。这种方法被称为**袋字**。

不管句子的身份是什么，你都有一个固定长度的向量，但是不同的是增加了这个词汇表的计数。您可以将此输入到您的模型中，作为生成预测的输入。

然而，这还有另一个大问题。仅使用计数意味着您会丢失所有顺序信息和所有关于先前历史的信息。

考虑*图 9.6* 。所以，这两个语义完全相反的句子，在这个单词包格式中会有完全相同的表达。这是因为它们有完全相同的单词列表，只是顺序不同。所以，很明显，这是行不通的。另一个想法可能是简单地扩展固定窗口。

![Figure 9.6: Bag of words example
](img/B16341_09_06.jpg)

图 9.6:单词袋示例

现在，考虑*图 9.7* 。你可以用这种方式表达你的句子，将句子输入到你的模型中，并生成你的预测。问题是，如果你将这个向量输入到一个前馈神经网络中，这些输入中的每一个，`yesterday I took my car`，都将有一个单独的权重将其连接到网络。因此，如果你在句子的开头反复看到单词`yesterday`，网络可能会知道`yesterday`代表一个时间或一个场景。然而，如果`yesterday`突然出现在固定长度向量的后面，在句子的末尾，网络可能很难理解`yesterday`的意思。这是因为在向量末尾的参数可能以前从未见过术语`yesterday`，并且从句子开始的参数没有在整个序列中共享。

![Figure 9.7: Sentence example
](img/B16341_09_07.jpg)

图 9.7:句子示例

因此，您需要能够处理可变长度的输入和长期依赖性，跟踪序列顺序，并拥有可以在整个序列中共享的参数。具体来说，您需要开发能够完成以下任务的模型:

*   处理可变长度的输入序列。
*   跟踪数据中的长期相关性。
*   维护有关序列顺序的信息。
*   在整个序列中共享参数。

对于一个信息只单向流动的模型，你如何做到这一点呢？你需要一种不同的神经网络。你需要一个递归模型。在下面的练习中，您将练习处理顺序数据。

## 练习 9.01:为序列数据训练人工神经网络 Nvidia 股票预测

在本练习中，您将构建一个简单的 ANN 模型来预测 Nvidia 股票价格。但是与前几章的例子不同，这次输入数据是连续的。因此，您需要手动进行一些处理来创建一个数据集，该数据集将包含给定一天的股票价格作为目标变量，以及前 60 天的价格作为特征。您需要在日期`2019-01-01`前后将数据分为训练集和测试集。

注意

你可以在这里找到`NVDA.csv`数据集:[https://packt.link/Mxi80](https://packt.link/Mxi80)。

1.  打开新的 Jupyter 或 Colab 笔记本。
2.  导入所需的库。使用`numpy`进行计算，`matplotlib`绘制可视化，`pandas`帮助处理数据集，`MinMaxScaler`在零和一之间缩放数据集:

    ```
    import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.preprocessing import StandardScaler, MinMaxScaler
    ```

3.  使用`read_csv()`函数读入 CSV 文件，并将数据集存储在 pandas 数据帧`data`中，以进行操作:

    ```
    import io data = pd.read_csv('NVDA.csv')
    ```

4.  Call the `head()` function on your data to take a look at the first five rows of your DataFrame:

    ```
    data.head()
    ```

    您应该得到以下输出:

    ![Figure 9.8: First five rows of output
    ](img/B16341_09_08.jpg)

    图 9.8:输出的前五行

    上表显示了原始数据。您可以看到每一行都代表一天，在这一天中，您有关于股票开市和收盘时的价格、最高价格、最低价格和调整后的收盘价的信息(例如，考虑股息或股票分割)。

5.  现在，分割训练数据。使用比`2019-01-01`早的所有数据，使用`Date`列作为您的训练数据。另存为`data_training`。使用`copy()`方法:

    ```
    data_training = data[data['Date']<'2019-01-01'].copy()
    ```

    将其保存在一个单独的文件中
6.  现在，拆分测试数据。使用比使用`Date`列的`2019-01-01`更新的所有数据。另存为`data_test`。使用`copy()`方法:

    ```
    data_test = data[data['Date']>='2019-01-01'].copy()
    ```

    将其保存在一个单独的文件中
7.  Use `drop()` to remove your `Date` and `Adj Close` columns in your DataFrame. Remember that you used the `Date` column to split your training and test sets, so the date information is not needed. Use `axis = 1` to specify that you also want to drop labels from your columns. To make sure it worked, call the `head()` function to take a look at the first five rows of the DataFrame:

    ```
    training_data = data_training.drop\
                    (['Date', 'Adj Close'], axis = 1)
    training_data.head()
    ```

    您应该得到以下输出:

    ![Figure 9.9: New training data
    ](img/B16341_09_09.jpg)

    图 9.9:新的训练数据

    这是删除这两列后应该得到的输出。

8.  Create a scaler from `MinMaxScaler` to scale `training_data` to numbers between zero and one. Use the `fit_transform` function to fit the model to the data and then transform the data according to the fitted model:

    ```
    scaler = MinMaxScaler()
    training_data = scaler.fit_transform(training_data)
    training_data
    ```

    您应该得到以下输出:

    ![Figure 9.10: Scaled training data
    ](img/B16341_09_10.jpg)

    图 9.10:缩放的训练数据

9.  将您的数据分成`X_train`和`y_train`数据集:

    ```
    X_train = [] y_train = []
    ```

10.  Check the shape of `training_data`:

    ```
    training_data.shape[0]
    ```

    您应该得到以下输出:

    ```
    868
    ```

    您可以看到在训练集中有 868 个观察值。

11.  创建一个包含前 60 天股票价格的训练数据集，以便可以预测第 61 天的收盘价。在这里，`X_train`将有两列。第一列将存储从 0 到 59 的值，第二列将存储从 1 到 60 的值。在`y_train`的第一列中，在索引 60 处存储第 61 个值，在第二列中，在索引 61 处存储第 62 个值。使用`for`循环在 60 个时间步长内创建数据:

    ```
    for i in range(60, training_data.shape[0]):     X_train.append(training_data[i-60:i])     y_train.append(training_data[i, 0])
    ```

12.  将`X_train`和`y_train`转换成 NumPy 数组:

    ```
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

13.  Call the `shape()` function on `X_train` and `y_train`:

    ```
    X_train.shape, y_train.shape
    ```

    您应该得到以下输出:

    ```
    ((808, 60, 5), (808,))
    ```

    前面的代码片段显示，准备好的训练集包含您保留的五个特征(`Open`、`Low`、`High`、`Close`和`Volume`)的`808`观察和`60`天的数据。

14.  Transform the data into a 2D matrix with the shape of the sample (the number of samples and the number of features in each sample). Stack the features for all 60 days on top of each other to get an output size of `(808, 300)`. Use the following code for this purpose:

    ```
    X_old_shape = X_train.shape
    X_train = X_train.reshape(X_old_shape[0], \
                              X_old_shape[1]*X_old_shape[2]) 
    X_train.shape
    ```

    您应该得到以下输出:

    ```
    (808, 300)
    ```

15.  现在，建立一个人工神经网络。为此，您将需要一些额外的库。使用`Sequential`初始化神经网络，`Input`添加输入层，`Dense`添加密集层，`Dropout`帮助防止过拟合:

    ```
    from tensorflow.keras import Sequential from tensorflow.keras.layers import Input, Dense, Dropout
    ```

16.  通过调用`regressor_ann = Sequential()`初始化神经网络。

    ```
    regressor_ann = Sequential()
    ```

17.  添加一个`shape`为`300` :

    ```
    regressor_ann.add(Input(shape = (300,)))
    ```

    的输入图层
18.  然后，添加第一个密集层。将其设置为`512`单位，这将是输出空间的维度。使用 ReLU 激活功能。最后，添加一个删除层，在训练过程中删除 20%的单元，以防止过度拟合:

    ```
    regressor_ann.add(Dense(units = 512, activation = 'relu')) regressor_ann.add(Dropout(0.2))
    ```

19.  添加另一个有`128`个单位的密集层，ReLU 为激活函数，dropout 为`0.3` :

    ```
    regressor_ann.add(Dense(units = 128, activation = 'relu')) regressor_ann.add(Dropout(0.3))
    ```

20.  添加另一个单位为`64`的密集层，ReLU 为激活函数，一个`0.4` :

    ```
    regressor_ann.add(Dense(units = 64, activation = 'relu')) regressor_ann.add(Dropout(0.4))
    ```

    的漏点
21.  再次，添加另一个具有`128`单位的密集层，ReLU 作为激活函数，以及`0.3` :

    ```
    regressor_ann.add(Dense(units = 16, activation = 'relu')) regressor_ann.add(Dropout(0.5))
    ```

    的删除
22.  添加一个单位的最终致密层:

    ```
    regressor_ann.add(Dense(units = 1))
    ```

23.  Check the summary of the model:

    ```
    regressor_ann.summary()
    ```

    您将获得关于模型层和参数的有价值的信息。

    ![Figure 9.11: Model summary
    ](img/B16341_09_11.jpg)

    图 9.11:模型摘要

24.  使用`compile()`方法配置您的模型进行训练。选择 Adam 作为您的优化器，并使用均方差来衡量您的损失函数:

    ```
    regressor_ann.compile(optimizer='adam', \                       loss = 'mean_squared_error')
    ```

25.  Finally, fit your model and set it to run on `10` epochs. Set your batch size to `32`:

    ```
    regressor_ann.fit(X_train, y_train, epochs=10, batch_size=32)
    ```

    您应该得到以下输出:

    ![Figure 9.12: Training the model
    ](img/B16341_09_12.jpg)

    图 9.12:训练模型

26.  Test and predict the stock price and prepare the dataset. Check your data by calling the `head()` method:

    ```
    data_test.head()
    ```

    您应该得到以下输出:

    ![Figure 9.13: First five rows of a DataFrame
    ](img/B16341_09_13.jpg)

    图 9.13:数据帧的前五行

27.  使用`tail(60)`方法创建一个`past_60_days`变量，它由训练集中最近 60 天的数据组成。用`append()`功能将`past_60_days`变量添加到测试数据中。将`True`分配给`ignore_index` :

    ```
    past_60_days = data_training.tail(60) df = past_60_days.append(data_test, ignore_index = True)
    ```

28.  Now, prepare your test data for predictions by repeating what you did for the training data in *steps 8* to *15*:

    ```
    df = df.drop(['Date', 'Adj Close'], axis = 1)
    inputs = scaler.transform(df) 
    X_test = []
    y_test = []
    for i in range(60, inputs.shape[0]):
        X_test.append(inputs[i-60:i])
        y_test.append(inputs[i, 0])
    X_test, y_test = np.array(X_test), np.array(y_test)
    X_old_shape = X_test.shape
    X_test = X_test.reshape(X_old_shape[0], \
                            X_old_shape[1] * X_old_shape[2])
    X_test.shape, y_test.shape
    ```

    您应该得到以下输出:

    ```
    ((391, 300), (391,))
    ```

29.  通过在`X_test` :

    ```
    y_pred = regressor_ann.predict(X_test)
    ```

    上调用`predict()`方法来测试对你的股票价格的一些预测
30.  Before looking at the results, reverse the scaling you did earlier so that the number you get as output will be at the correct scale using the `StandardScaler` utility class that you imported with `scaler.scale_`:

    ```
    scaler.scale_
    ```

    您应该得到以下输出:

    ![Figure 9.14: Using StandardScaler
    ](img/B16341_09_14.jpg)

    图 9.14:使用标准缩放器

31.  Use the first value in the preceding array to set your scale in preparation for the multiplication of `y_pred` and `y_test`. Recall that you are converting your data back from your earlier scale, in which you converted all values to between zero and one:

    ```
    scale = 1/3.70274364e-03
    scale 
    ```

    您应该得到以下输出:

    ```
    270.0700067909643
    ```

32.  将`y_pred`和`y_test`乘以`scale`，将您的数据转换回正确的值:

    ```
    y_pred = y_pred*scale y_test = y_test*scale
    ```

33.  Review the real Nvidia stock price and your predictions:

    ```
    plt.figure(figsize=(14,5))
    plt.plot(y_test, color = 'black', label = "Real NVDA Stock Price")
    plt.plot(y_pred, color = 'gray',\
             label = 'Predicted NVDA Stock Price')
    plt.title('NVDA Stock Price Prediction')
    plt.xlabel('time')
    plt.ylabel('NVDA Stock Price')
    plt.legend()
    plt.show()
    ```

    您应该得到以下输出:

    ![Figure 9.15: Real Nvidia stock price versus your predictions
    ](img/B16341_09_15.jpg)

图 9.15:真实的 Nvidia 股票价格与你的预测

在上图中，您可以看到您的训练模型能够捕捉到 Nvidia 股票价格的一些趋势。注意到预测值与实际值相差很大。从这个结果可以明显看出，人工神经网络不适用于序列数据。

在本练习中，您看到了简单人工神经网络无法处理序列数据。在下一节中，您将了解循环神经网络，它旨在从时序数据的时间维度中学习。然后，在*练习 9.02* 、*使用 LSTM 层 Nvidia 股票预测构建 RNNs】中，您将使用 RNNs 对相同的 Nvidia 股票价格数据集执行预测，并比较您的结果。*

# 循环神经网络

约翰·霍普菲尔德于 1982 年首次提出了类似递归的神经网络。他这样做有两个动机:

*   数据的顺序处理
*   神经元连通性建模

本质上，RNN 在每个时间步处理输入数据，并将信息存储在其内存中，用于下一步。信息首先被转换成机器可以处理的向量。然后，RNN 一次处理一个向量序列。当它处理每一个向量时，它通过前一个隐藏状态。隐藏状态保留上一步的信息，相当于一种记忆。它通过用压缩`-1`和`1`之间的值的双曲正切函数组合输入和先前的隐藏状态来实现这一点。

本质上，这就是 RNN 的功能。rnn 不需要大量的计算，并且对于短序列工作得很好。

![Figure 9.16: RNN data flow
](img/B16341_09_16.jpg)

图 9.16: RNN 数据流

现在将注意力转向将神经网络应用于涉及数据顺序处理的问题。您已经了解了为什么这类任务需要与您目前所看到的完全不同类型的网络架构。

## RNN 建筑

本节将介绍 RNNs 背后的关键原理，它们与您目前所学的内容有何根本不同，以及 RNN 计算实际上是如何工作的。

但在此之前，请后退一步，考虑之前讨论过的标准前馈神经网络。

在前向神经网络中，数据只沿一个方向传播，即从输入到输出。

因此，您需要一种不同的网络架构来处理顺序数据。rnn 特别适合处理有一系列输入而不是单个输入的情况。这对于传播一系列数据以给出单个输出的问题非常有用。

例如，假设您正在训练一个模型，该模型将一系列单词作为输入，并输出与该序列相关联的情感。类似地，考虑这样的情况，不是返回单个输出，而是有一个输入序列并通过网络传播它们，其中序列中的每个时间步长都生成一个输出。

简而言之，rnn 是一种网络，它提供了一种机制来长期保存以前处理过的数据，并使用它来进行未来预测。

![Figure 9.17: RNN computation
](img/B16341_09_17.jpg)

图 9.17: RNN 计算

在上图中，在由 t 表示的某个时间步长，RNN 将`X` t 作为输入，在该时间步长，它计算预测值`Y` t，这是网络的输出。

除了这个输出，它还保存了一个内部状态，称为更新，`H` t。这个来自时间步`t`的内部状态可以用来补充下一个时间步`t+1`的输入。所以，基本上，它提供了关于上一步到下一步的信息。这种机制被称为**循环**，因为信息在网络中从一个时间步传递到下一个时间步。

这里到底发生了什么？这是通过使用简单的递归关系来处理顺序数据来实现的。RNNs 保持内部状态，`H` t，并将其与下一个输入数据，`X` t+1 结合，进行预测，`Y` t+1，并存储新的内部状态，`H` t+1。关键思想是状态更新是先前状态时间步长以及网络正在接收的当前输入的组合。

值得注意的是，在这个计算中，在每个时间步使用的是`W`的相同函数`f`和相同的参数集，这些参数集是您在训练过程中学习到的。为了更好地理解这些网络是如何工作的，请浏览 RNN 算法:

1.  首先初始化你的 RNN 和网络的隐藏状态。你可以指出一个你有兴趣预测下一个单词的句子。RNN 计算简单地由他们循环通过这个句子中的单词组成。
2.  在每一个时间步，你把你正在考虑的当前单词，以及你的 RNN 先前的隐藏状态输入到网络中。然后，这可以为序列中的下一个单词生成预测，并使用该信息来更新其隐藏状态。
3.  最后，当你遍历完句子中的所有单词后，你对那个丢失单词的预测就是 RNN 在最后一个时间步的输出。

如下图所示，这种 RNN 计算包括内部状态更新和形式输出向量。

![Figure 9.18: RNN data flow
](img/B16341_09_18.jpg)

图 9.18: RNN 数据流

给定输入向量`X` t，RNN 应用一个函数来更新它的隐藏状态。这个函数只是一个标准的神经网络操作。它包括乘以权重矩阵和应用非线性激活函数。关键的区别在于，在这种情况下，您将输入向量`X` t 和之前的状态作为这个函数`H` t-1 的输入。

接下来，将非线性激活函数(如 tanh)应用于上一步。你有这两个权重矩阵，最后，你的输出，`y` t，在一个给定的时间步，是这个内部状态的一个修改的，转换的版本。

当你遍历完句子中的所有单词后，你对那个缺失单词的预测就是 RNN 在最后一个时间步的输出，在所有单词都被输入模型后。因此，如上所述，RNN 计算包括内部状态更新和形式输出向量。

另一种表示 rnn 的方式是随时间展开它们的模块。你可以把 rnn 想象成同一个网络有多个副本，每个副本把一个消息传递给它的后代。

![Figure 9.19: Computational graph with time
](img/B16341_09_19.jpg)

图 9.19:随时间变化的计算图

在这个表示中，您可以使您的权重矩阵显式化，从将输入转换为用于将先前的隐藏状态转换为当前的隐藏状态的`H`权重开始，最后将隐藏状态转换为输出。

值得注意的是，你在每个时间步使用相同的权重矩阵。根据这些输出，您可以计算每个时间步的损失。损耗的计算将完成你在网络中的正向传播。最后，要定义总损失，只需将所有单个时间步骤的损失相加。由于你的损失取决于每个时间步，这意味着，在训练网络时，你也必须把时间作为一个组成部分。

现在，您已经对这些 RNN 的构造和功能有了一些了解，您可以通过一个简单的示例来了解如何在 TensorFlow 中从头开始实现 RNN。

下面的片段使用了来自`keras.models.Sequential`的一个简单的 RNN。您将单位数指定为`1`，并将第一个输入维度设置为`None`，因为 RNN 可以处理任意数量的时间步长。默认情况下，简单的 RNN 使用 tanh 激活:

```
model = keras.models.Sequential([
                                 keras.layers.SimpleRNN\
                                 (1, input_shape=[None, 1]) 
])
```

前面的代码创建了一个具有单个神经元的单层。

这很容易。现在你需要叠加一些额外的循环层。代码是相似的，但是这里有一个关键的区别。你会注意到除了最后一层以外的所有层上的`return_sequences=True`。这是为了确保输出是 3D 数组。如您所见，前两层各有`20`单元:

```
model = keras.models.Sequential\
        ([Keras.layers.SimpleRNN\
          (20, return_sequences=True, input_shape=[None, 1]), \
          Keras.layers.SimpleRNN(20, return_sequences=True), \
          Keras.layers.SimpleRNN(1)])
```

RNN 被定义为一个图层，您可以通过从图层类继承它来构建它。你也可以将你的权重矩阵和 RNN 单元格的隐藏状态初始化为零。

这里的关键步骤是定义调用函数，它描述了在给定输入`X`的情况下，如何通过网络进行前向传递。为了分解这个调用函数，首先要根据前面讨论的等式更新隐藏状态。

取之前的隐藏状态和输入`X`，乘以相关的权重矩阵，将它们加在一起，然后通过一个非线性，比如双曲正切(tanh)。

然后，输出只是隐藏状态的转换版本，在每个时间步，您都返回当前输出和更新的隐藏状态。

TensorFlow 通过内置的密集层使其变得简单。这同样适用于注册护士。TensorFlow 已经用简单的 RNN 层实现了这些类型的 RNN 单元。但这种类型的层有一些限制，如消失梯度。在探索不同类型的递归层之前，您将在下一节中研究这个问题。

## 消失渐变问题

如果你仔细观察梯度是如何在这个重复模块链中流动的，你会发现在每个时间步之间你需要执行矩阵乘法。这意味着梯度的计算——即损失对参数的导数，一直追溯到初始状态——需要多次重复乘以权重矩阵，以及重复使用激活函数的导数。

您可能会遇到两种特别成问题的情况:爆炸渐变问题或消失渐变问题。

爆炸梯度问题是当梯度由于矩阵乘法运算而不断变得越来越大时，你就不能再优化它们了。减轻这种情况的一种方法是执行所谓的渐变裁剪。这相当于缩小大梯度，使它们的值更小，更接近`1`。

你也可以有相反的问题，你的梯度太小。这就是所谓的消失梯度问题。这是当你进行这些重复的乘法时，梯度变得越来越小(接近于`0`，并且你不能再训练网络。这是一个非常现实的问题，当谈到培训 RNNs。

例如，考虑这样一个场景，你一直将一个数乘以一个介于 0 和 1 之间的数。当你不断重复这个过程时，这个数字会不断缩小，直到最后消失，变成 0。当梯度发生这种情况时，很难将错误进一步传播回过去，因为梯度变得越来越小。

考虑前面的语言模型中的例子，您试图预测下一个单词。如果你试图预测下面短语中的最后一个单词，那么下一个单词会是什么就相对清楚了。在关键的相关信息(比如“鱼”这个词)和需要预测的地方之间没有太大的差距。

![Figure 9.20: Word prediction 
](img/B16341_09_20.jpg)

图 9.20:单词预测

然而，在其他情况下，需要更多的上下文，如下面的例子。来自句子早期的信息`She lived in Spain`表明`she speaks fluent`之后的句子的下一个单词最有可能是一种语言的名称`Spanish`。

![Figure 9.21: Sentence example
](img/B16341_09_21.jpg)

图 9.21:句子示例

但是你需要`Spain`的上下文，它在这个句子中位于更早的位置，能够填补相关的空白，识别哪种语言是正确的。随着语义上重要的单词之间的差距越来越大，rnn 变得越来越无法将这些点连接起来，并将这些相关的信息链接在一起。这是由于消失梯度问题。

你如何缓解这种情况？第一个技巧很简单。您可以选择 tanh 或 sigmoid 作为激活函数。这两个函数的导数都小于`1`。

另一个简单的技巧是初始化网络参数的权重。事实证明，初始化单位矩阵的权重有助于防止它们在反向传播期间过快地收缩到零。

但是最后一个也是最可靠的解决方案是使用一个稍微复杂一点的递归单元，它可以更有效地跟踪数据中的长期依赖关系。它可以通过控制传递什么信息以及使用什么信息来更新其内部状态来做到这一点。具体来说，这是门控细胞的概念，就像在 LSTM 层，这是下一节的重点。

## 长短期记忆网络

LSTMs 非常适合学习长期依赖性和克服消失梯度问题。它们是序列数据的高性能模型，被深度学习社区广泛使用。

LSTMs 具有链状结构。在 LSTM 中，重复单元包含不同的相互作用层。关键是这些层相互作用，有选择地控制细胞内的信息流。

LSTM 的关键构建模块是一种称为门的结构，其功能是使 LSTM 能够选择性地添加或删除其细胞状态的信息。门由一个类似 s 形的神经网络层组成。

![Figure 9.22: LSTM architecture
](img/B16341_09_22.jpg)

图 9.22: LSTM 建筑

花一点时间想想这样的门在 LSTM 会有什么作用。在这种情况下，sigmoid 函数会强制其输入在`0`和`1`之间。你可以把这种机制想象成捕捉有多少通过大门的信息应该被保留。它在零和一之间。这有效地控制了信息的流动。

LSTMs 通过四个简单的步骤处理信息:

1.  LSTM 的第一步是决定哪些信息将从细胞状态中丢弃，以忘记不相关的历史。这是先前内部状态`H` t-1 和输入`X` t 的函数，因为其中一些信息可能不重要。
2.  接下来，LSTM 决定新信息的哪一部分是相关的，并使用它来将该信息存储在其单元状态中。
3.  然后，它获取先验信息的相关部分以及当前输入，并使用这些信息来有选择地更新其单元状态。
4.  最后，它返回一个输出，这就是所谓的输出门，它控制单元状态中编码的信息发送到网络。![Figure 9.23: LSTM processing steps
    ](img/B16341_09_23.jpg)

图 9.23: LSTM 处理步骤

对于 LSTMs 来说，这里的关键要点是它们如何调节信息流和存储的顺序。同样，LSTMs 的操作如下:

*   忘记无关的历史
*   存储新内容和重要内容
*   使用其内部存储器来更新内部状态
*   生成输出

LSTMs 的一个重要特性是，所有这些不同的门控和更新机制共同创建了一个内部单元状态`C`，它允许梯度随时间不间断地流动。你可以把它想象成细胞状态的高速公路，梯度可以不间断地流动。这使你能够缓解和减轻消失梯度的问题，这是标准的 RNNs。

LSTMs 能够独立于输出内容来维护这种独立的单元状态，并且它们通过忘记不相关的历史、存储相关的新信息、选择性地更新它们的单元状态，然后返回过滤版本作为输出，来使用门来控制信息流。

就训练和 LSTM 而言，关键点在于保持分离的独立单元状态允许 LSTM 随时间反向传播的有效训练，这将在后面讨论。

现在，您已经了解了 RNNs 的基本工作原理、时间反向传播算法以及 LSTM 架构的一些知识，您可以将其中的一些概念应用到下面的示例中。

考虑以下 LSTM 模型:

```
regressor = Sequential()
regressor.add(LSTM(units= 50, activation = 'relu', \
                   return_sequences = True, \
                   input_shape = (X_train.shape[1], 5)))
regressor.add(Dropout(0.2))
regressor.add(LSTM(units= 60, activation = 'relu', \
                   return_sequences = True))
regressor.add(Dropout(0.3))
regressor.add(LSTM(units= 80, activation = 'relu', \
                   return_sequences = True))
regressor.add(Dropout(0.4))
regressor.add(LSTM(units= 120, activation = 'relu'))
regressor.add(Dropout(0.5))
regressor.add(Dense(units = 1))
```

首先，您已经通过调用`regressor = Sequential()`初始化了一个神经网络。同样，需要注意的是，在最后一行中您省略了`return_sequences = True`,因为它是最终输出:

```
regressor = Sequential()
```

然后，LSTM 层被添加。首先，设置 LSTM 层为`50`单位。使用 relu 激活函数并指定训练集的形状。最后在脱落层加上`regressor.add(Dropout(0.2)`。`0.2`意味着 20%的图层将被移除。设置`return_sequences = True`，允许返回上一次输出。

同样，添加三个以上的 LSTM 层和一个密集层到 LSTM 模型。

现在，您已经熟悉了处理顺序数据的基本概念，是时候使用一些真实数据来完成下面的练习了。

## 练习 9.02:使用 LSTM 图层构建 RNN–Nvidia 股票预测

在本练习中，您将使用与*练习 9.01* 、*为序列数据训练人工神经网络相同的数据集——Nvidia 股票预测*。你还是会尝试根据之前 60 天的数据来预测英伟达股价。但这次，你将训练一个 LSTM 模特。您需要在`2019-01-01`日期前后将数据分成训练集和测试集。

注意

你可以在这里找到`NVDA.csv`数据集:[https://packt.link/Mxi80](https://packt.link/Mxi80)。

在应用以下代码之前，您需要像在*练习 9.01* 、*为序列数据训练 ANN-Nvidia 股票预测* ( *步骤 1* 到 *15* )中一样准备数据集:

1.  开始建造 LSTM。为此，您将需要一些额外的库。使用`Sequential`初始化神经网络，`Dense`添加密集层，`LSTM`添加 LSTM 层，`Dropout`帮助防止过度拟合:

    ```
    from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, LSTM, Dropout
    ```

2.  通过调用`regressor = Sequential()`初始化神经网络。添加四个 LSTM 层，每个层有`50`、`60`、`80`和`120`个单元。使用一个 ReLU 激活功能，将`True`分配给`return_sequences`，除了最后一个 LSTM 层。将训练集的形状提供给第一个 LSTM 图层。最后，添加 20%、30%、40%和 50%漏失的漏失层:

    ```
    regressor = Sequential() regressor.add(LSTM(units= 50, activation = 'relu',\                    return_sequences = True,\                    input_shape = (X_train.shape[1], 5))) regressor.add(Dropout(0.2)) regressor.add(LSTM(units= 60, activation = 'relu', \               return_sequences = True)) regressor.add(Dropout(0.3)) regressor.add(LSTM(units= 80, activation = 'relu', \               return_sequences = True)) regressor.add(Dropout(0.4)) regressor.add(LSTM(units= 120, activation = 'relu')) regressor.add(Dropout(0.5)) regressor.add(Dense(units = 1))
    ```

3.  Check the summary of the model using the `summary()` method:

    ```
    regressor.summary()
    ```

    您应该得到以下输出:

    ![Figure 9.24: Model summary
    ](img/B16341_09_24.jpg)

    图 9.24:模型总结

    从上图中可以看出，摘要提供了关于所有模型层和参数的有价值的信息。这是一种很好的方法，可以确保您的图层按照您希望的顺序排列，并且具有正确的输出形状和参数。

4.  使用`compile()`方法配置您的模型进行训练。选择 Adam 作为您的优化器，并使用均方差来衡量您的损失函数:

    ```
    regressor.compile(optimizer='adam', loss = 'mean_squared_error')
    ```

5.  Fit your model and set it to run on `10` epochs. Set your batch size equal to `32`:

    ```
    regressor.fit(X_train, y_train, epochs=10, batch_size=32)
    ```

    您应该得到以下输出:

    ![Figure 9.25: Training the model
    ](img/B16341_09_25.jpg)

    图 9.25:训练模型

6.  Test and predict the stock price and prepare the dataset. Check your data by calling the `head()` function:

    ```
    data_test.head()
    ```

    您应该得到以下输出:

    ![Figure 9.26: First five rows of the DataFrame
    ](img/B16341_09_26.jpg)

    图 9.26:数据帧的前五行

7.  Call the `tail(60)` method to look at the last 60 days of data. You will use this information in the next step:

    ```
    data_training.tail(60)
    ```

    您应该得到以下输出:

    ![Figure 9.27: Last 10 rows of the DataFrame
    ](img/B16341_09_27.jpg)

    图 9.27:数据帧的最后 10 行

8.  使用`tail(60)`方法创建`past_60_days`变量:

    ```
    past_60_days = data_training.tail(60)
    ```

9.  使用`append()`功能将`past_60_days`变量添加到您的测试数据中。将`True`设置为`ignore_index`。删除`Date`和`Adj Close`列，因为您不需要这些信息:

    ```
    df = past_60_days.append(data_test, ignore_index = True) df = df.drop(['Date', 'Adj Close'], axis = 1)
    ```

10.  Check the DataFrame to make sure that you successfully dropped `Date` and `Adj Close` by using the `head()` function:

    ```
    df.head()
    ```

    您应该得到以下输出:

    ![Figure 9.28: Checking the first five rows of the DataFrame
    ](img/B16341_09_28.jpg)

    图 9.28:检查数据帧的前五行

11.  Use `scaler.transform` from `StandardScaler` to perform standardization on inputs:

    ```
    inputs = scaler.transform(df)
    inputs
    ```

    您应该得到以下输出:

    ![Figure 9.29: DataFrame standardization
    ](img/B16341_09_29.jpg)

    图 9.29:数据帧标准化

    从前面的结果可以看出，标准化后，现在所有值都接近`0`。

12.  将您的数据分成`X_test`和`y_test`数据集。创建一个包含前 60 天股票价格的测试数据集，以便可以测试第 61 天的收盘价。在这里，`X_test`将有两列。第一列将存储从 0 到 59 的值。第二列将存储从 1 到 60 的值。在`y_test`的第一列中，在索引 60 处存储第 61 个值，在第二列中，在索引 61 处存储第 62 个值。使用`for`循环在 60 个时间步长内创建数据:

    ```
    X_test = [] y_test = [] for i in range(60, inputs.shape[0]):     X_test.append(inputs[i-60:i])     y_test.append(inputs[i, 0])
    ```

13.  Convert `X_test` and `y_test` into NumPy arrays:

    ```
    X_test, y_test = np.array(X_test), np.array(y_test)
    X_test.shape, y_test.shape
    ```

    您应该得到以下输出:

    ```
    ((391, 60, 5), (391,))
    ```

    前面的结果显示有`391`个观察值，每个观察值都有最近`60`天以下五个特征的数据:`Open`、`High`、`Low`、`Close`和`Volume`。另一方面，目标变量包含`391`值。

14.  通过调用`regressor.predict(X_test)` :

    ```
    y_pred = regressor.predict(X_test)
    ```

    来测试一些股票价格的预测
15.  Before looking at the results, reverse the scaling you did earlier so that the number you get as output will be at the correct scale using the `StandardScaler` utility class that you imported with `scaler.scale_`:

    ```
    scaler.scale_
    ```

    您应该得到以下输出:

    ![Figure 9.30: Using StandardScaler
    ](img/B16341_09_30.jpg)

    图 9.30:使用标准缩放器

16.  Use the first value in the preceding array to set your scale in preparation for the multiplication of `y_pred` and `y_test`. Recall that you are converting your data back from the scale you did earlier when converting all values to between zero and one:

    ```
    scale = 1/3.70274364e-03
    scale
    ```

    您应该得到以下输出:

    ```
    270.0700067909643
    ```

17.  将`y_pred`和`y_test`乘以`scale`，将您的数据转换回正确的值:

    ```
    y_pred = y_pred*scale y_test = y_test*scale
    ```

18.  Use `y_pred` to view predictions for NVIDIA stock:

    ```
    y_pred
    ```

    您应该得到以下输出:

    ![Figure 9.31: Checking prediction
    ](img/B16341_09_31.jpg)

    图 9.31:检查预测

    上述结果显示了未来几天 Nvidia 股票的预测价格。

19.  Plot the real Nvidia stock price and your predictions:

    ```
    plt.figure(figsize=(14,5))
    plt.plot(y_test, color = 'black', label = "Real NVDA Stock Price")
    plt.plot(y_pred, color = 'gray',\
             label = 'Predicted NVDA Stock Price')
    plt.title('NVDA Stock Price Prediction')
    plt.xlabel('time')
    plt.ylabel('NVDA Stock Price')
    plt.legend()
    plt.show()
    ```

    您应该得到以下输出:

    ![Figure 9.32: NVIDIA stock price visualization
    ](img/B16341_09_32.jpg)

图 9.32:英伟达股价可视化

从*图 9.32* 中的灰色线可以看出，与黑线显示的实际股价相比，您的预测模型相当准确。

在本练习中，您为 Nvidia 股票预测构建了一个具有 LSTM 图层的 RNN，并完成了训练、测试和预测步骤。

现在，在下面的活动中测试你在本章中学到的知识。

## 活动 9.01:使用多个 LSTM 图层构建一个 RNN 来预测功耗

`household_power_consumption.csv`数据集包含与一个家庭 4 年来的电力消耗测量相关的信息，采样率为 1 分钟。您需要根据之前的测量结果预测给定时间内的功耗。

您的任务是调整具有额外 LSTM 图层的 RNN 模型，以预测分钟级别的家庭功耗。你将建立一个有三个 LSTM 层的 RNN 模型。

注意

你可以在这里找到数据集:[https://packt.link/qrloK](https://packt.link/qrloK)。

执行以下步骤来完成本练习:

1.  加载数据。
2.  通过将`Date`和`Time`列组合成一个单独的`Datetime`列来准备数据，然后可以使用该列对数据进行排序并填充缺失值。
3.  标准化数据，删除`Date`、`Time`、`Global_reactive_power`和`Datetime`列，因为预测不再需要它们。
4.  重塑给定分钟的数据，以包含前 60 分钟的值。
5.  将数据分为训练集和测试集，分别包含索引`217440`之前和之后的数据，索引`217440`对应于上个月的数据。
6.  定义并训练一个 RNN 模型，该模型由三个不同层的 LSTM 组成，具有`20`、`40`和`80`个单元，后面是`50%` dropout 和 ReLU 作为激活函数。
7.  使用训练好的模型对测试集进行预测。
8.  Compare the predictions against the actual values on the entire dataset.

    您应该得到以下输出:

    ![Figure 9.33: Expected output of Activity 9.01
    ](img/B16341_09_33.jpg)

图 9.33:活动 9.01 的预期产出

注意

此活动的解决方案可通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor280)找到。

在下一节中，您将学习如何将 RNNs 应用于文本。

# 自然语言处理

自然语言处理是一个快速发展的领域，既有挑战性又有回报。NLP 获取传统上机器很难理解的有价值的数据，并将其转化为可以使用的信息。这些数据可以采取句子、单词、字符、文本和音频等形式。为什么这对机器来说是如此困难的任务？要回答这个问题，请考虑下面的例子。

回想一下这两句话:*是什么就是什么*和*是什么就是什么*。这两个句子，虽然它们有完全相反的语义，但在这个单词包格式中会有完全相同的表示。这是因为它们有完全相同的单词，只是顺序不同。所以，你知道你需要使用一个序列模型来处理这个，但是还有什么呢？已经开发了几种工具和技术来解决这些问题。但是在这之前，您需要学习如何预处理顺序数据。

## 数据预处理

快速回顾一下，预处理通常包括训练模型所需的所有步骤。一些常见的步骤包括数据清理、数据转换和数据简化。更具体地说，对于自然语言处理，这些步骤可以是以下的全部、一些或没有:

*   标记化
*   填料
*   小写转换
*   删除停用词
*   删除标点符号
*   堵塞物

以下部分对您将使用的步骤进行了更深入的描述。现在，这里是每个步骤的概述:

*   **数据集清理**包括大小写转换、删除标点符号等等。
*   **记号化**是将一个字符序列分解成称为记号的特定单元。
*   **填充**是一种通过填充使不同大小的输入句子相同的方法。填充序列意味着确保序列具有一致的长度。
*   **词干化**是将单词截断到词干。例如，单词“rainy”和“raining”都有词干“rain”。

### 数据集清洗

在这里，您创建了`clean_text`函数，它返回一个包含单词的列表，这些单词已经被清理过了。您将使用`lower()`将所有文本保存为小写，并使用`utf8`对其进行编码以实现字符标准化:

```
def clean_text(txt):
    txt = "".join(v for v in txt if v not in string.punctuation)\
            .lower()
    txt = txt.encode("utf8").decode("ascii",'ignore')
    return txt 
corpus = [clean_text(x) for x in all_headlines]
```

### 生成序列和标记化

TensorFlow 提供了一个专门的类，用于生成一系列 N 元符号——来自`keras.preprocessing.text`的`Tokenizer`:

```
from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
```

一旦实例化了一个`Tokenizer()`，就可以使用`fit_on_texts()`方法从语料库中提取标记。该步骤将为语料库中的每个唯一单词分配一个整数索引:

```
tokenizer.fit_on_texts(corpus)
```

在语料库上训练完分词器之后，您可以使用`word_index`属性访问语料库中分配给每个单词的索引:

```
tokenizer.word_index
```

您可以使用`texts_to_sequences()`方法将句子转换成标记化版本:

```
tokenizer.texts_to_sequences([sentence])
```

您可以创建一个函数，使用下面的代码片段从输入语料库中生成 N 元语法的标记化句子序列:

```
def get_seq_of_tokens(corpus):
    tokenizer.fit_on_texts(corpus)
    all_words = len(tokenizer.word_index) + 1

    input_sequences = []
    for line in corpus:
        token_list = tokenizer.texts_to_sequences([line])[0]
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)
    return input_sequences, all_words
inp_sequences, all_words = get_seq_of_tokens(corpus)
inp_sequences[:10]
```

`get_seq_of_tokens()`函数在给定的语料库上训练一个`Tokenizer()`。然后，您需要遍历语料库的每一行，并将它们转换成标记化的对等词。最后，对于每一个标记化的句子，您可以从中创建不同的 N 元语法序列。

接下来，您将看到如何使用填充来处理可变的句子长度。

### 填充序列

如前所述，深度学习模型期望固定长度的输入。但是对于文本，句子的长度可以变化。克服这个问题的一个方法是将所有的句子转换成相同的长度。你需要设置句子的最大长度。然后，对于比这个阈值短的句子，可以添加填充，它会添加一个特定的令牌值来填充空白。另一方面，较长的句子将被截断以适应这种约束。您可以使用`pad_sequences()`来实现这一点:

```
from keras.preprocessing.sequence import pad_sequences
```

您可以创建`generate_padded_sequences`函数，它将接受`input_sequences`并生成其填充版本:

```
def generate_padded_sequences(input_sequences):
    max_sequence_len = max([len(x) for x in input_sequences])
    input_sequences = np.array(pad_sequences\
                               (input_sequences, \
                                maxlen=max_sequence_len, \
                                padding='pre'))
    predictors, label = input_sequences[:,:-1], \
                        input_sequences[:,-1]
    label = ku.to_categorical(label, num_classes=all_words)
    return predictors, label, max_sequence_len
predictors, label, max_sequence_len = generate_padded_sequences\
                                      (inp_sequences)
```

既然您已经知道如何处理原始文本，那么请看下一节中的建模步骤。

# 穿越时间的反向传播(BPTT)

序列模型有很多种。您已经使用了简单 rnn、深度 rnn 和 LSTMs。让我们来看几个用于 NLP 的附加模型。

请记住，您首先通过从输入到输出的网络进行正向传递来训练前馈模型。这是标准的前馈模型，其中各层紧密相连。为了训练这种模型，可以通过网络反向传播梯度，对网络中每个权重参数的损失进行求导。然后，可以调整参数，将损失降到最低。

但是在 RNNs 中，如前所述，通过网络的前向传递还包括及时前进，根据输入和先前状态更新单元状态，并生成输出`Y`。在该时间步，计算损失，最后将各个时间步的损失相加，得到总损失。

这意味着，误差不是在单个时间步长通过单个前馈网络反向传播，而是在每个单独的时间步长反向传播，最后跨越所有时间步长，从当前位置一直到序列的开头。

这就是为什么它被称为穿越时间的反向传播。正如你所看到的，所有的错误都及时流回到数据序列的开始。

Google Translate 是机器翻译的一个很好的例子，也是 RNNs 在行业中最强大和最广泛使用的应用之一。在机器翻译中，你用一种语言输入一个序列，任务是训练 RNN 用一种新的语言输出这个序列。这是通过采用具有编码器和解码器的双重结构来实现的，编码器将句子以其原始语言编码成状态向量。然后，它将该编码表示作为输入，并将其解码成一种新的语言。

但是这种方法有一个关键问题:所有输入编码器结构的内容都必须编码成一个向量。这在实践中会成为一个巨大的信息瓶颈，因为您可能有大量的文本需要翻译。为了解决这个问题，谷歌的研究人员开发了一种 RNN 的扩展，叫做注意力 T2。

现在，解码器不是只能访问最终的编码状态，而是可以访问原始句子中所有时间步长的状态。将编码器状态连接到解码器的这些向量的权重由网络在训练期间学习。这被称为注意力，因为当网络学习时，它将注意力放在输入句子的不同部分。

通过这种方式，它有效地捕捉到了对原始句子中重要信息的一种记忆访问。因此，近年来，随着注意力和门控细胞(如 LSTMs)等构建模块的出现，rnn 已经真正起飞，并在现实世界中得到相当成功的应用。

到目前为止，您应该已经对 rnn 的工作原理以及它们在处理顺序数据时为何如此强大有了一些了解。您已经看到了为什么以及如何使用 RNNs 通过定义这个递归关系来执行序列建模任务。您还了解了如何训练 rnn，并了解了 LSTMs 等门控细胞如何帮助我们建立长期依赖性模型。

在下面的练习中，您将看到如何使用 LSTM 模型来预测文本中的下一个单词。

## 练习 9.03:使用 LSTM 图层构建一个用于自然语言处理的 RNN

在本练习中，您将使用带有 LSTM 图层的 RNN 来预测新闻标题的最后一个单词。

`Articles.csv`数据集包含由新闻标题组成的原始文本。你将训练一个 LTSM 模型，它将预测给定句子的下一个单词。

注意

你可以在这里找到数据集:【https://packt.link/RQVoB】T4。

执行以下步骤来完成本练习:

1.  Import the libraries needed:

    ```
    from keras.preprocessing.sequence import pad_sequences
    from keras.layers import Embedding, LSTM, Dense, Dropout
    from keras.preprocessing.text import Tokenizer
    from keras.callbacks import EarlyStopping
    from keras.models import Sequential
    import keras.utils as ku 
    import pandas as pd
    import numpy as np
    import string, os 
    import warnings
    warnings.filterwarnings("ignore")
    warnings.simplefilter(action='ignore', category=FutureWarning)
    ```

    您应该得到以下输出:

    ```
    Using TensorFlow backend.
    ```

2.  Load the dataset locally by setting `curr_dir` to `content`. Create the `all_headlines` variable. Use a `for` loop to iterate over the files contained in the folder, and extract the headlines. Remove all headlines with the `Unknown` value. Print the length of `all_headlines`:

    ```
    curr_dir = '/content/'
    all_headlines = []
    for filename in os.listdir(curr_dir):
        if 'Articles' in filename:
            article_df = pd.read_csv(curr_dir + filename)
            all_headlines.extend(list(article_df.headline.values))
            break
    all_headlines = [h for h in all_headlines if h != "Unknown"]
    len(all_headlines)
    ```

    输出如下所示:

    ```
    831
    ```

3.  Create the `clean_text` method to return a list containing words once it has been cleaned. Save all text as lowercase with the `lower()` method and encode it with `utf8` for character standardization. Finally, output 10 headlines from your corpus:

    ```
    def clean_text(txt):
        txt = "".join(v for v in txt \
                      if v not in string.punctuation).lower()
        txt = txt.encode("utf8").decode("ascii",'ignore')
        return txt 
    corpus = [clean_text(x) for x in all_headlines]
    corpus[:10]
    ```

    您应该得到以下输出:

    ![Figure 9.34: Corpus
    ](img/B16341_09_34.jpg)

    图 9.34:语料库

4.  Use `tokenizer.fit` to extract tokens from the corpus. Each integer output corresponds with a specific word. With `input_sequences`, train features that will be a `list []`. With `token_list = tokenizer.texts_to_sequences`, convert each sentence into its tokenized equivalent. With `n_gram_sequence = token_list`, generate the N-gram sequences. Using `input_sequences.append(n_gram_sequence)`, append each N-gram sequence to the list of your features:

    ```
    tokenizer = Tokenizer()
    def get_seq_of_tokens(corpus):
        tokenizer.fit_on_texts(corpus)
        all_words = len(tokenizer.word_index) + 1
        input_sequences = []
        for line in corpus:
            token_list = tokenizer.texts_to_sequences([line])[0]
            for i in range(1, len(token_list)):
                n_gram_sequence = token_list[:i+1]
                input_sequences.append(n_gram_sequence)
        return input_sequences, all_words
    inp_sequences, all_words = get_seq_of_tokens(corpus)
    inp_sequences[:10]
    ```

    您应该得到以下输出:

    ![Figure 9.35: N-gram tokens
    ](img/B16341_09_35.jpg)

    图 9.35: N 元语法标记

5.  填充序列并获得`predictors`和`target`变量。使用`pad_sequence`填充序列并使其长度相等:

    ```
    def generate_padded_sequences(input_sequences):     max_sequence_len = max([len(x) for x in input_sequences])     input_sequences = np.array\                       (pad_sequences(input_sequences, \                                      maxlen=max_sequence_len, \                                      padding='pre'))     predictors, label = input_sequences[:,:-1], \                         input_sequences[:,-1]     label = ku.to_categorical(label, num_classes=all_words)     return predictors, label, max_sequence_len predictors, label, max_sequence_len = generate_padded_sequences\                                       (inp_sequences)
    ```

6.  Prepare your model for training. Add an input embedding layer with `model.add(Embedding)`. Add a hidden LSTM layer with `100` units and add a dropout of 10%. Then, add a dense layer with a softmax activation function. With the `compile` method, configure your model for training, setting your loss function to `categorical_crossentropy`, and use the Adam optimizer:

    ```
    def create_model(max_sequence_len, all_words):
        input_len = max_sequence_len - 1
        model = Sequential()

        model.add(Embedding(all_words, 10, input_length=input_len))

        model.add(LSTM(100))
        model.add(Dropout(0.1))

        model.add(Dense(all_words, activation='softmax'))
        model.compile(loss='categorical_crossentropy', \
                      optimizer='adam')
        return model
    model = create_model(max_sequence_len, all_words)
    model.summary()
    ```

    您应该得到以下输出:

    ![Figure 9.36: Model summary
    ](img/B16341_09_36.jpg)

    图 9.36:模型总结

7.  Fit your model with `model.fit` and set it to run on `100` epochs. Set `verbose` equal to `5`:

    ```
    model.fit(predictors, label, epochs=100, verbose=5)
    ```

    您应该得到以下输出:

    ![Figure 9.37: Training the model
    ](img/B16341_09_37.jpg)

    图 9.37:训练模型

8.  编写一个函数，该函数将接收输入文本、模型和要预测的下一个单词的数量。该功能将准备输入文本，以供模型预测下一个单词:

    ```
    def generate_text(seed_text, next_words, \                   model, max_sequence_len):     for _ in range(next_words):         token_list = tokenizer.texts_to_sequences\                      ([seed_text])[0]         token_list = pad_sequences([token_list], \                                    maxlen=max_sequence_len-1,\                                    padding='pre')         predicted = model.predict_classes(token_list, verbose=0)         output_word = ""         for word,index in tokenizer.word_index.items():             if index == predicted:                 output_word = word                 break         seed_text += " "+output_word     return seed_text.title()
    ```

9.  Output some of your generated text with the `print` function. Add your own words for the model to use and generate from. For example, in `the hottest new`, the integer `5` is the number of words output by the model:

    ```
    print (generate_text("the hottest new", 5, model,\
                         max_sequence_len))
    print (generate_text("the stock market", 4, model,\
                         max_sequence_len))
    print (generate_text("russia wants to", 3, model,\
                         max_sequence_len))
    print (generate_text("french citizen", 4, model,\
                         max_sequence_len))
    print (generate_text("the one thing", 15, model,\
                         max_sequence_len))
    print (generate_text("the coronavirus", 5, model,\
                         max_sequence_len))
    ```

    您应该得到以下输出:

    ![Figure 9.38: Generated text
    ](img/B16341_09_38.jpg)

图 9.38:生成的文本

在这个结果中，您可以看到您的模型为每个句子生成的文本。

在这个练习中，你已经成功地预测了一些新闻标题。不足为奇的是，有些可能不太令人印象深刻，但有些还不算太差。

现在，您已经掌握了关于 RNNs 的所有基本知识，尝试通过执行下一个活动来测试自己。

## 活动 9.02:构建预测推文情感的 RNN

`tweets.csv`数据集包含一个与航空公司相关的推文列表。每条推文都被分为积极、消极或中性情绪。

你的任务是为公司分析一份推文样本。你的目标是建立一个 RNN 模型，能够预测每条推文的情绪:积极或消极。

注意

你可以在这里找到`tweets.csv`:[https://packt.link/dVUd2](https://packt.link/dVUd2)。

执行以下步骤来完成本练习。

1.  导入必要的包。
2.  准备数据(合并`Date`和`Time`列，将其命名为`datetime`，对数据进行排序，并填充缺失值)。
3.  准备文本数据(标记单词并添加填充)。
4.  将数据集分为训练集和测试集，分别包含前 10，000 条推文和其余推文。
5.  定义并训练一个 RNN 模型，该模型由两个不同层的 LSTM 组成，分别具有`50`和`100`单元，后面是 20%的下降和 ReLU 作为激活函数。
6.  Make predictions on the testing set with the trained model.

    您应该得到以下输出:

    ![Figure 9.39: Expected output of Activity 9.02
    ](img/B16341_09_39.jpg)

图 9.39:活动 9.02 的预期产出

注意

此活动的解决方案可通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor281)找到。

# 总结

在这一章中，你探索了顺序数据的不同递归模型。您了解了每个连续的数据点都依赖于先前的数据点序列，例如自然语言文本。您还了解了为什么必须使用允许模型使用数据序列的模型，并按顺序生成下一个输出。

本章介绍了可以对序列数据进行预测的 RNN 模型。您观察到了 rnn 可以自我循环的方式，这允许模型的输出反馈到输入中。您回顾了使用这些模型所面临的挑战类型，例如渐变的消失和爆炸，以及如何解决它们。

在下一章中，您将学习如何在模型中使用自定义TensorFlow组件，包括损失函数和层。