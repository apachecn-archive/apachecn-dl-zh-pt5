<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>GAN: Generating New Images with CNN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">甘:用CNN创造新影像</h1>
                
            
            
                
<p class="mce-root">通常，神经网络需要带标签的示例来有效地学习。从无标签数据中学习的无监督学习方法还没有很好地工作。一个<strong>生成对抗网络</strong>，或者简称为<strong>甘</strong>，是无监督学习方法的一部分，但是基于可微分的生成网络。gan最早是由Ian Goodfellow等人在2014年发明的。从那以后，它们变得非常受欢迎。这是基于博弈论，有两个参与者或网络:一个发电机网络和b)一个鉴别器网络，两者相互竞争。这种基于双重网络博弈论的方法极大地改善了从无标签数据中学习的过程。发电机网络产生假数据并将其传递给鉴别器。鉴别器网络也看到真实数据，并预测它接收的数据是假的还是真实的。因此，生成器经过训练，可以轻松生成非常接近真实数据的数据，以欺骗鉴别器网络。鉴别器网络被训练来分类哪些数据是真的，哪些数据是假的。所以，最终，一个生成器网络学会产生非常非常接近真实数据的数据。甘将会在音乐和艺术领域大受欢迎。</p>
<p>根据Goodfellow的说法，“<em>你可以认为生成模型赋予了人工智能一种想象的形式</em></p>
<p class="mce-root">以下是GANs的几个例子:</p>
<ul>
<li>Pix2pix</li>
<li>CycleGAN</li>
</ul>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Pix2pix - Image-to-Image translation GAN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">Pix2pix图像到图像转换GAN</h1>
                
            
            
                
<p>这个网络使用一个<strong>条件生成对抗网络</strong> ( <strong> cGAN </strong>)从图像的输入和输出中学习映射。可以从原始文件中提取的一些示例如下:</p>
<div><img height="475" src="img/5bd742bd-4360-4fb0-acaf-595d92808a92.jpeg" width="450"/></div>
<p>cGANs的Pix2pix示例</p>
<p>在手提包的例子中，网络学习如何给黑白图像着色。这里，训练数据集的输入图像是黑白的，而目标图像是彩色的。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>CycleGAN </title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">CycleGAN</h1>
                
            
            
                
<p>CycleGAN也是一个图像到图像转换器，但没有输入/输出对。例如，要从绘画中生成照片，请将马图像转换为斑马图像:</p>
<div><img src="img/60b9b3b9-a03d-4a53-acbf-49563705690e.jpeg"/></div>
<p>在鉴别器网络中，使用压差非常重要。否则，可能会产生不良结果。</p>
<p>生成器网络将随机噪声作为输入，并产生逼真的图像作为输出。针对不同种类的随机噪声运行生成器网络会产生不同类型的真实图像。第二个网络被称为<strong>鉴别器网络</strong>，非常类似于常规的神经网络分类器。这个网络是在真实图像上训练的，尽管训练GAN与监督训练方法非常不同。在监督训练中，每个图像在显示给模型之前首先被标记。例如，如果输入是一只狗的图像，我们告诉模型这是一只狗。在生成模型的情况下，我们向模型显示大量图像，并要求它从相同的概率分布中制作更多类似的图像。实际上，第二个鉴别器网络有助于发电机网络实现这一点。</p>
<p class="mce-root">鉴别器从发生器网络输出图像是真的还是假的概率。换句话说，它试图为真实图像分配接近1的概率，为虚假图像分配接近0的概率。同时，发电机做相反的事情。它被训练成输出图像，该图像将具有由鉴别器接近1的概率。随着时间的推移，发生器会产生更真实的图像并欺骗鉴别器:</p>
<div><img height="349" src="img/31c3ec0c-b8e9-407f-9e05-9764f2b33178.png" width="543"/></div>
<p class="mce-root"/>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Training a GAN model</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">训练GAN模型</h1>
                
            
            
                
<p>在前面的章节中解释的大多数机器学习模型都是基于优化的，也就是说，我们在它的参数空间上最小化成本函数。gan是不同的，因为有两个网络:发生器G和鉴别器d。每个都有自己的成本。形象化GAN的一个简单方法是鉴别器的成本与发生器的成本成反比。在GAN中，我们可以定义一个值函数，生成器必须使其最小化，鉴别器必须使其最大化。生成模型的训练过程与监督训练方法非常不同。GAN对初始权重很敏感。所以我们需要使用批处理规范化。批处理规范化除了提高性能之外，还使模型稳定。这里，我们同时训练两个模型，生成模型和判别模型。生成模型G捕获数据分布，判别模型D估计样本来自训练数据而不是G的概率。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>GAN – code example</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">GAN–代码示例</h1>
                
            
            
                
<p>在下面的例子中，我们使用MNIST数据集和TensorFlow建立和训练GAN模型。这里，我们将使用ReLU激活函数的一个特殊版本，称为<strong>泄漏ReLU </strong>。输出是一种新型的手写数字:</p>
<p>漏ReLU是ReLU激活函数的一个变体，由公式<em>f(x)= max(α÷x，x </em> <em> ) </em>给出。因此，<em> x </em>的负值输出为<em> alpha * x </em>，正值<em> x </em>的输出为<em> x </em>。</p>
<pre>#import all necessary libraries and load data set<br/>%matplotlib inline<br/><br/>import pickle as pkl<br/>import numpy as np<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt<br/><br/>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets('MNIST_data')</pre>
<p>为了建立这个网络，我们需要两个输入，一个用于发生器，一个用于鉴别器。在下面的代码中，我们为鉴别器的<kbd>real_input</kbd>和生成器的<kbd>z_input</kbd>创建占位符，输入大小分别为<kbd>dim_real</kbd>和<kbd>dim_z</kbd>:</p>
<pre>#place holder for model inputs <br/>def model_inputs(dim_real, dim_z):<br/>    real_input = tf.placeholder(tf.float32, name='dim_real')<br/>    z_input = tf.placeholder(tf.float32, name='dim_z')<br/>    <br/>    return real_input, z_input</pre>
<p>这里，输入<kbd>z</kbd>是一个随机向量，生成器将这个向量转换成图像。然后，我们添加一个隐藏层，这是一个泄漏的ReLU层，以允许梯度倒流。泄漏ReLU就像一个正常的ReLU(负值发射零)，除了有一个小的非零输出为负输入值。使用<kbd>tanh</kbd> <kbd>sigmoid</kbd>功能时，发电机的性能会更好。发电机输出为<kbd>tanh</kbd>输出。因此，我们必须将MNIST图像重新缩放到-1和1之间，而不是0和1之间。有了这些知识，我们可以构建发电机网络:</p>
<pre>#Following code builds Generator Network<br/>def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):<br/>    ''' Build the generator network.<br/>    <br/>        Arguments<br/>        ---------<br/>        z : Input tensor for the generator<br/>        out_dim : Shape of the generator output<br/>        n_units : Number of units in hidden layer<br/>        reuse : Reuse the variables with tf.variable_scope<br/>        alpha : leak parameter for leaky ReLU<br/>        <br/>        Returns<br/>        -------<br/>        out: <br/>    '''<br/>    with tf.variable_scope('generator', reuse=reuse) as generator_scope: # finish this<br/>        # Hidden layer<br/>        h1 = tf.layers.dense(z, n_units, activation=None )<br/>        # Leaky ReLU<br/>        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_generator')<br/>        <br/>        # Logits and tanh output<br/>        logits = tf.layers.dense(h1, out_dim, activation=None)<br/>        out = tf.tanh(logits)<br/>        <br/>        return out</pre>
<p>鉴别器网络与发生器相同，除了输出层是一个<kbd>sigmoid</kbd>功能:</p>
<pre><br/>def discriminator(x, n_units=128, reuse=False, alpha=0.01):<br/>    ''' Build the discriminator network.<br/>    <br/>        Arguments<br/>        ---------<br/>        x : Input tensor for the discriminator<br/>        n_units: Number of units in hidden layer<br/>        reuse : Reuse the variables with tf.variable_scope<br/>        alpha : leak parameter for leaky ReLU<br/>        <br/>        Returns<br/>        -------<br/>        out, logits: <br/>    '''<br/>    with tf.variable_scope('discriminator', reuse=reuse) as discriminator_scope:# finish this<br/>        # Hidden layer<br/>        h1 = tf.layers.dense(x, n_units, activation=None )<br/>        # Leaky ReLU<br/>        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_discriminator')<br/>        <br/>        logits = tf.layers.dense(h1, 1, activation=None)<br/>        out = tf.sigmoid(logits)<br/>        <br/>        return out, logits</pre>
<p class="mce-root">要构建网络，请使用以下代码:</p>
<pre class="mce-root">#Hyperparameters<br/># Size of input image to discriminator<br/>input_size = 784 # 28x28 MNIST images flattened<br/># Size of latent vector to generator<br/>z_size = 100<br/># Sizes of hidden layers in generator and discriminator<br/>g_hidden_size = 128<br/>d_hidden_size = 128<br/># Leak factor for leaky ReLU<br/>alpha = 0.01<br/># Label smoothing <br/>smooth = 0.1</pre>
<p>我们希望在真实数据和虚假数据之间共享权重，因此我们需要重用变量:</p>
<pre>#Build the network<br/>tf.reset_default_graph()<br/># Create our input placeholders<br/>input_real, input_z = model_inputs(input_size, z_size)<br/><br/># Build the model<br/>g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)<br/># g_model is the generator output<br/><br/>d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)<br/>d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Calculating loss </title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">计算损失</h1>
                
            
            
                
<p>对于鉴别器，总损失是真实和伪造图像的损失之和。损失将是s形交叉熵，我们可以使用张量流<kbd>tf.nn.sigmoid_cross_entropy_with_logits</kbd>得到。然后，我们计算该批中所有图像的平均值。所以损失会是这样的:</p>
<pre>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))</pre>
<p>为了帮助鉴别器更好地概括，可以将<kbd>labels</kbd>从1.0降低到0.9，例如，使用参数<kbd>smooth</kbd> <em>。</em>这被称为<strong>标签平滑</strong>，通常与分类器一起使用以提高性能。伪数据的鉴别器损耗是相似的。<kbd>logits</kbd>是<kbd>d_logits_fake</kbd>，它是我们通过将发生器输出传递给鉴别器得到的。这些假的<kbd>logits</kbd>与全零的<kbd>labels</kbd>一起使用。请记住，我们希望鉴频器为真实图像输出1，为虚假图像输出0，因此我们需要设置损耗来反映这一点。</p>
<p>最后，发电机损耗是利用<kbd>d_logits_fake</kbd> <em>、</em>的假图像<kbd>logits</kbd>。但是现在的<kbd>labels</kbd>都是1。生成器试图欺骗鉴别器，所以它希望鉴别器输出假图像:</p>
<pre># Calculate losses<br/>d_loss_real = tf.reduce_mean(<br/>                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, <br/>                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))<br/>d_loss_fake = tf.reduce_mean(<br/>                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, <br/>                                                          labels=tf.zeros_like(d_logits_real)))<br/>d_loss = d_loss_real + d_loss_fake<br/><br/>g_loss = tf.reduce_mean(<br/>             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,<br/>                                                     labels=tf.ones_like(d_logits_fake)))</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Adding the optimizer</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">添加优化器</h1>
                
            
            
                
<p>我们需要分别更新生成器和鉴别器变量。因此，首先获取图中的所有变量，然后，正如我们前面解释的，我们只能从生成器作用域中获取生成器变量，类似地，从鉴别器作用域中获取鉴别器变量:</p>
<pre># Optimizers<br/>learning_rate = 0.002<br/><br/># Get the trainable_variables, split into G and D parts<br/>t_vars = tf.trainable_variables()<br/>g_vars = [var for var in t_vars if var.name.startswith('generator')]<br/>d_vars = [var for var in t_vars if var.name.startswith('discriminator')]<br/><br/>d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)<br/>g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</pre>
<p>要训练网络，请使用:</p>
<pre>batch_size = 100<br/>epochs = 100<br/>samples = []<br/>losses = []<br/># Only save generator variables<br/>saver = tf.train.Saver(var_list=g_vars)<br/>with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    for e in range(epochs):<br/>        for ii in range(mnist.train.num_examples//batch_size):<br/>            batch = mnist.train.next_batch(batch_size)<br/>            <br/>            # Get images, reshape and rescale to pass to D<br/>            batch_images = batch[0].reshape((batch_size, 784))<br/>            batch_images = batch_images*2 - 1<br/>            <br/>            # Sample random noise for G<br/>            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))<br/>            <br/>            # Run optimizers<br/>            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})<br/>            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})<br/>        <br/>        # At the end of each epoch, get the losses and print them out<br/>        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})<br/>        train_loss_g = g_loss.eval({input_z: batch_z})<br/>            <br/>        print("Epoch {}/{}...".format(e+1, epochs),<br/>              "Discriminator Loss: {:.4f}...".format(train_loss_d),<br/>              "Generator Loss: {:.4f}".format(train_loss_g)) <br/>        # Save losses to view after training<br/>        losses.append((train_loss_d, train_loss_g))<br/>        <br/>        # Sample from generator as we're training for viewing afterwards<br/>        sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br/>        gen_samples = sess.run(<br/>                       generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),<br/>                       feed_dict={input_z: sample_z})<br/>        samples.append(gen_samples)<br/>        saver.save(sess, './checkpoints/generator.ckpt')<br/><br/># Save training generator samples<br/>with open('train_samples.pkl', 'wb') as f:<br/>    pkl.dump(samples, f)</pre>
<p>一旦模型训练完成并保存，就可以可视化生成的数字(代码不在这里，但可以下载)。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Semi-supervised learning and GAN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">半监督学习和GAN</h1>
                
            
            
                
<p>因此，我们已经看到了GAN如何用于生成逼真的图像。在本节中，我们将了解GAN如何用于分类任务，在这些任务中，我们有较少的标记数据，但仍希望提高分类器的准确性。这里我们也将使用相同的<strong>街景门牌号</strong>或<strong> SVHN </strong>数据集对图像进行分类。和前面一样，这里我们也有两个网络，生成器G和鉴别器d。在这种情况下，鉴别器被训练成分类器。另一个变化是鉴频器的输出转到softmax函数，而不是<kbd>sigmoid</kbd>函数，如前所述。softmax函数返回标签的概率分布:</p>
<div><img height="207" src="img/8f617d78-0c02-4e22-9e9c-1e99923915dc.png" width="546"/></div>
<p>现在，我们将网络建模为:</p>
<p class="CDPAlignCenter CDPAlign"><em>总成本=标记数据成本+未标记数据成本</em></p>
<p>要获得标记数据的成本，我们可以使用<kbd>cross_entropy</kbd>函数:</p>
<pre>cost of labeled data  = cross_entropy ( logits, labels)<br/>cost of unlabeled data =   cross_entropy ( logits, real)</pre>
<p>然后我们可以计算所有类的总和:</p>
<pre>real prob = sum (softmax(real_classes))</pre>
<p>普通分类器处理带标签的数据。然而，基于半监督GAN的分类器工作在标记数据、真实的未标记数据和虚假图像上。这非常有效，也就是说，即使我们在训练过程中有较少的标记数据，也有较少的分类错误。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Feature matching</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">特征匹配</h1>
                
            
            
                
<p>特征匹配的思想是给生成器的成本函数增加一个额外的变量，以便补偿测试数据和训练数据中的绝对误差之间的差异。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Semi-supervised classification using a GAN example</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">使用GAN示例的半监督分类</h1>
                
            
            
                
<p>在本节中，我们将解释如何使用GAN通过半监督学习方法来构建分类器。</p>
<p>在监督学习中，我们有一组训练输入<kbd>X</kbd>和类别标签<kbd>y</kbd>。我们训练一个以<kbd>X</kbd>为输入，以<kbd>y</kbd>为输出的模型。</p>
<p>在半监督学习中，我们的目标仍然是训练一个以<kbd>X</kbd>为输入，生成<kbd>y</kbd>为输出的模型。然而，并不是所有的训练例子都有标签<kbd>y</kbd>。</p>
<p>我们使用SVHN数据集。我们将把GAN鉴别器变成11类鉴别器(0到9和一个假图像标签)。它将识别10种不同类别的真实SVHN数字，以及来自生成器的第11类虚假图像。鉴别器将在真实的标记图像、真实的未标记图像和虚假图像上进行训练。通过利用三个数据源而不是一个数据源，它将比只在一个数据源上训练的传统分类器更好地推广到测试集:</p>
<pre>def model_inputs(real_dim, z_dim):<br/>    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')<br/>    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')<br/>    y = tf.placeholder(tf.int32, (None), name='y')<br/>    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')<br/>    <br/>    return inputs_real, inputs_z, y, label_mask</pre>
<p>添加发电机:</p>
<pre>def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):<br/>    with tf.variable_scope('generator', reuse=reuse):<br/>        # First fully connected layer<br/>        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)<br/>        # Reshape it to start the convolutional stack<br/>        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))<br/>        x1 = tf.layers.batch_normalization(x1, training=training)<br/>        x1 = tf.maximum(alpha * x1, x1)<br/>        <br/>        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')<br/>        x2 = tf.layers.batch_normalization(x2, training=training)<br/>        x2 = tf.maximum(alpha * x2, x2)<br/>        <br/>        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')<br/>        x3 = tf.layers.batch_normalization(x3, training=training)<br/>        x3 = tf.maximum(alpha * x3, x3)<br/>        <br/>        # Output layer<br/>        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')<br/>        <br/>        out = tf.tanh(logits)<br/>        <br/>        return out</pre>
<p>添加鉴别器:</p>
<pre>def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):<br/>    with tf.variable_scope('discriminator', reuse=reuse):<br/>        x = tf.layers.dropout(x, rate=drop_rate/2.5)<br/>        <br/>        # Input layer is 32x32x3<br/>        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')<br/>        relu1 = tf.maximum(alpha * x1, x1)<br/>        relu1 = tf.layers.dropout(relu1, rate=drop_rate)<br/>        <br/>        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')<br/>        bn2 = tf.layers.batch_normalization(x2, training=True)<br/>        relu2 = tf.maximum(alpha * x2, x2)<br/>        <br/>        <br/>        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same')<br/>        bn3 = tf.layers.batch_normalization(x3, training=True)<br/>        relu3 = tf.maximum(alpha * bn3, bn3)<br/>        relu3 = tf.layers.dropout(relu3, rate=drop_rate)<br/>        <br/>        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same')<br/>        bn4 = tf.layers.batch_normalization(x4, training=True)<br/>        relu4 = tf.maximum(alpha * bn4, bn4)<br/>        <br/>        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same')<br/>        bn5 = tf.layers.batch_normalization(x5, training=True)<br/>        relu5 = tf.maximum(alpha * bn5, bn5)<br/>        <br/>        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same')<br/>        bn6 = tf.layers.batch_normalization(x6, training=True)<br/>        relu6 = tf.maximum(alpha * bn6, bn6)<br/>        relu6 = tf.layers.dropout(relu6, rate=drop_rate)<br/>        <br/>        x7 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=1, padding='valid')<br/>        # Don't use bn on this layer, because bn would set the mean of each feature<br/>        # to the bn mu parameter.<br/>        # This layer is used for the feature matching loss, which only works if<br/>        # the means can be different when the discriminator is run on the data than<br/>        # when the discriminator is run on the generator samples.<br/>        relu7 = tf.maximum(alpha * x7, x7)<br/>        <br/>        # Flatten it by global average pooling<br/>        features = raise NotImplementedError()<br/>        <br/>        # Set class_logits to be the inputs to a softmax distribution over the different classes<br/>        raise NotImplementedError()<br/>        <br/>        <br/>        # Set gan_logits such that P(input is real | input) = sigmoid(gan_logits).<br/>        # Keep in mind that class_logits gives you the probability distribution over all the real<br/>        # classes and the fake class. You need to work out how to transform this multiclass softmax<br/>        # distribution into a binary real-vs-fake decision that can be described with a sigmoid.<br/>        # Numerical stability is very important.<br/>        # You'll probably need to use this numerical stability trick:<br/>        # log sum_i exp a_i = m + log sum_i exp(a_i - m).<br/>        # This is numerically stable when m = max_i a_i.<br/>        # (It helps to think about what goes wrong when...<br/>        # 1. One value of a_i is very large<br/>        # 2. All the values of a_i are very negative<br/>        # This trick and this value of m fix both those cases, but the naive implementation and<br/>        # other values of m encounter various problems)<br/>        raise NotImplementedError()<br/>        <br/>        return out, class_logits, gan_logits, features</pre>
<p>计算损失:</p>
<pre>def model_loss(input_real, input_z, output_dim, y, num_classes, label_mask, alpha=0.2, drop_rate=0.):<br/>    """<br/>    Get the loss for the discriminator and generator<br/>    :param input_real: Images from the real dataset<br/>    :param input_z: Z input<br/>    :param output_dim: The number of channels in the output image<br/>    :param y: Integer class labels<br/>    :param num_classes: The number of classes<br/>    :param alpha: The slope of the left half of leaky ReLU activation<br/>    :param drop_rate: The probability of dropping a hidden unit<br/>    :return: A tuple of (discriminator loss, generator loss)<br/>    """<br/>    <br/>    <br/>    # These numbers multiply the size of each layer of the generator and the discriminator,<br/>    # respectively. You can reduce them to run your code faster for debugging purposes.<br/>    g_size_mult = 32<br/>    d_size_mult = 64<br/>    <br/>    # Here we run the generator and the discriminator<br/>    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)<br/>    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br/>    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data<br/>    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br/>    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples<br/>    <br/>    <br/>    # Here we compute `d_loss`, the loss for the discriminator.<br/>    # This should combine two different losses:<br/>    # 1. The loss for the GAN problem, where we minimize the cross-entropy for the binary<br/>    # real-vs-fake classification problem.<br/>    # 2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy<br/>    # for the multi-class softmax. For this one we use the labels. Don't forget to ignore<br/>    # use `label_mask` to ignore the examples that we are pretending are unlabeled for the<br/>    # semi-supervised learning problem.<br/>    raise NotImplementedError()<br/>    <br/>    # Here we set `g_loss` to the "feature matching" loss invented by Tim Salimans at OpenAI.<br/>    # This loss consists of minimizing the absolute difference between the expected features<br/>    # on the data and the expected features on the generated samples.<br/>    # This loss works better for semi-supervised learning than the tradition GAN losses.<br/>    raise NotImplementedError()<br/><br/>    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)<br/>    eq = tf.equal(tf.squeeze(y), pred_class)<br/>    correct = tf.reduce_sum(tf.to_float(eq))<br/>    masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))<br/>    <br/>    return d_loss, g_loss, correct, masked_correct, g_model</pre>
<p class="mce-root">添加优化器:</p>
<pre>def model_opt(d_loss, g_loss, learning_rate, beta1):<br/>    """<br/>    Get optimization operations<br/>    :param d_loss: Discriminator loss Tensor<br/>    :param g_loss: Generator loss Tensor<br/>    :param learning_rate: Learning Rate Placeholder<br/>    :param beta1: The exponential decay rate for the 1st moment in the optimizer<br/>    :return: A tuple of (discriminator training operation, generator training operation)<br/>    """<br/>    # Get weights and biases to update. Get them separately for the discriminator and the generator<br/>    raise NotImplementedError()<br/><br/>    # Minimize both players' costs simultaneously<br/>    raise NotImplementedError()<br/>    shrink_lr = tf.assign(learning_rate, learning_rate * 0.9)<br/>    <br/>    return d_train_opt, g_train_opt, shrink_lr</pre>
<p class="mce-root">构建网络模型:</p>
<pre>class GAN:<br/>    """<br/>    A GAN model.<br/>    :param real_size: The shape of the real data.<br/>    :param z_size: The number of entries in the z code vector.<br/>    :param learnin_rate: The learning rate to use for Adam.<br/>    :param num_classes: The number of classes to recognize.<br/>    :param alpha: The slope of the left half of the leaky ReLU activation<br/>    :param beta1: The beta1 parameter for Adam.<br/>    """<br/>    def __init__(self, real_size, z_size, learning_rate, num_classes=10, alpha=0.2, beta1=0.5):<br/>        tf.reset_default_graph()<br/>        <br/>        self.learning_rate = tf.Variable(learning_rate, trainable=False)<br/>        inputs = model_inputs(real_size, z_size)<br/>        self.input_real, self.input_z, self.y, self.label_mask = inputs<br/>        self.drop_rate = tf.placeholder_with_default(.5, (), "drop_rate")<br/>        <br/>        loss_results = model_loss(self.input_real, self.input_z,<br/>                                  real_size[2], self.y, num_classes,<br/>                                  label_mask=self.label_mask,<br/>                                  alpha=0.2,<br/>                                  drop_rate=self.drop_rate)<br/>        self.d_loss, self.g_loss, self.correct, self.masked_correct, self.samples = loss_results<br/>        <br/>        self.d_opt, self.g_opt, self.shrink_lr = model_opt(self.d_loss, self.g_loss, self.learning_rate, beta1)</pre>
<p>训练并保持模型:</p>
<pre>def train(net, dataset, epochs, batch_size, figsize=(5,5)):<br/>    <br/>    saver = tf.train.Saver()<br/>    sample_z = np.random.normal(0, 1, size=(50, z_size))<br/><br/>    samples, train_accuracies, test_accuracies = [], [], []<br/>    steps = 0<br/><br/>    with tf.Session() as sess:<br/>        sess.run(tf.global_variables_initializer())<br/>        for e in range(epochs):<br/>            print("Epoch",e)<br/>            <br/>            t1e = time.time()<br/>            num_examples = 0<br/>            num_correct = 0<br/>            for x, y, label_mask in dataset.batches(batch_size):<br/>                assert 'int' in str(y.dtype)<br/>                steps += 1<br/>                num_examples += label_mask.sum()<br/><br/>                # Sample random noise for G<br/>                batch_z = np.random.normal(0, 1, size=(batch_size, z_size))<br/><br/>                # Run optimizers<br/>                t1 = time.time()<br/>                _, _, correct = sess.run([net.d_opt, net.g_opt, net.masked_correct],<br/>                                         feed_dict={net.input_real: x, net.input_z: batch_z,<br/>                                                    net.y : y, net.label_mask : label_mask})<br/>                t2 = time.time()<br/>                num_correct += correct<br/><br/>            sess.run([net.shrink_lr])<br/>            <br/>            <br/>            train_accuracy = num_correct / float(num_examples)<br/>            <br/>            print("\t\tClassifier train accuracy: ", train_accuracy)<br/>            <br/>            num_examples = 0<br/>            num_correct = 0<br/>            for x, y in dataset.batches(batch_size, which_set="test"):<br/>                assert 'int' in str(y.dtype)<br/>                num_examples += x.shape[0]<br/><br/>                correct, = sess.run([net.correct], feed_dict={net.input_real: x,<br/>                                                   net.y : y,<br/>                                                   net.drop_rate: 0.})<br/>                num_correct += correct<br/>            <br/>            test_accuracy = num_correct / float(num_examples)<br/>            print("\t\tClassifier test accuracy", test_accuracy)<br/>            print("\t\tStep time: ", t2 - t1)<br/>            t2e = time.time()<br/>            print("\t\tEpoch time: ", t2e - t1e)<br/>            <br/>            <br/>            gen_samples = sess.run(<br/>                                   net.samples,<br/>                                   feed_dict={net.input_z: sample_z})<br/>            samples.append(gen_samples)<br/>            _ = view_samples(-1, samples, 5, 10, figsize=figsize)<br/>            plt.show()<br/>            <br/>            <br/>            # Save history of accuracies to view after training<br/>            train_accuracies.append(train_accuracy)<br/>            test_accuracies.append(test_accuracy)<br/>            <br/><br/>        saver.save(sess, './checkpoints/generator.ckpt')<br/><br/>    with open('samples.pkl', 'wb') as f:<br/>        pkl.dump(samples, f)<br/>    <br/>    return train_accuracies, test_accuracies, samples</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Deep convolutional GAN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">深度卷积GAN</h1>
                
            
            
                
<p><strong>深度卷积GAN </strong>，又称<strong> DCGAN </strong>，用于生成彩色图像。这里，我们在生成器和鉴别器中使用卷积层。我们还需要使用批量标准化来使GAN得到适当的训练。我们将在“深度神经网络的性能改进”一章中详细讨论批处理规范化。我们将在SVHN数据集上训练GAN下图显示了一个小例子。经过训练后，生成器将能够创建与这些图像几乎相同的图像。您可以下载该示例的代码:</p>
<div><img src="img/ca8ff0f4-19fc-465c-9b3e-aab5e4b9f1d7.png"/></div>
<p>谷歌街景门牌号视图</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Batch normalization</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">批量标准化</h1>
                
            
            
                
<p>批处理规范化是一种提高神经网络性能和稳定性的技术。其目的是归一化图层输入，使其平均值为零，方差为1。Sergey Ioffe和Christian Szegedy在2015年的论文<em>中介绍了批处理规范化，批处理规范化是使DCGANs工作的必要条件</em>。我们的想法是，不只是将网络的输入标准化，而是将网络中各层的输入标准化。它被称为<strong>批</strong> <strong>归一化</strong>，因为在训练期间，我们通过使用当前小批中的值的平均值和方差来归一化每一层的输入。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Summary</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们看到了GAN模型如何真实地展示了CNN的威力。我们学习了如何训练自己的生成模型，并看到了GAN的一个实际例子，它可以从绘画中生成照片，并将马变成斑马。</p>
<p>我们了解了GAN与其他判别模型的不同之处，并了解了为什么生成模型更受青睐。</p>
<p>下一章，我们将从零开始学习深度学习软件对比。</p>
<p class="mce-root"/>


            

            
        
    </body>

</html></body></html>