<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Object Detection and Instance Segmentation with CNN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">基于CNN的目标检测和实例分割</h1>
                
            
            
                
<p>到目前为止，在这本书里，我们一直大多使用<strong>卷积神经网络</strong>(<strong>CNN</strong>)进行分类。分类相对于图像中具有最大检测概率的实体，将整个图像分类到一个类别中。但是，如果不是一个而是多个感兴趣的实体，并且我们希望将图像与所有这些实体相关联，该怎么办呢？一种方法是使用标签而不是类，其中这些标签是倒数第二个Softmax分类图层的所有类，其概率高于给定阈值。然而，这里的检测概率因实体的大小和位置而变化很大，从下图中，我们实际上可以说，<em>该模型对所识别的实体就是所声明的实体有多大把握？</em>如果我们非常确信图像中存在一个实体，比如说一只狗，但是它在图像中的尺度和位置没有它的主人<em>人</em>实体那么突出怎么办？因此,<em>多类标签</em>是一种有效的方式，但对于这个目的来说不是最好的:</p>
<div><img height="194" src="img/6abbc29c-4951-427c-bc3e-f85cbefac003.jpeg" width="291"/></div>
<p>在本章中，我们将讨论以下主题:</p>
<ul>
<li>目标检测和图像分类的区别</li>
<li>用于对象检测的传统的非CNN方法</li>
<li>基于区域的CNN及其特点</li>
<li>快速R-CNN</li>
<li>更快的R-CNN</li>
<li>屏蔽R-CNN</li>
</ul>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>The differences between object detection and image classification</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">目标检测和图像分类的区别</h1>
                
            
            
                
<p>我们再举一个例子。您正在观看电影<em> 101只斑点狗</em>，您想知道在该电影的给定电影场景中，您实际上可以数出多少只斑点狗。图像分类最多只能告诉你至少有一只狗或一只达尔马提亚狗(取决于你训练分类器的等级)，但不能告诉你到底有多少只。</p>
<p>基于分类的模型的另一个问题是，它们不能告诉你图像中被识别的实体在哪里。很多时候，这很重要。比方说，你看到邻居家的狗<em> </em>和他(<em>人</em>)还有他家的猫在玩。你给它们拍了一张快照，想从那里提取狗的图像，在网上搜索它的品种或类似的狗。这里唯一的问题是搜索整个图像可能不起作用，如果不从图像中识别单个对象，您必须手动执行剪切-提取-搜索任务，如下图所示:</p>
<div><img height="205" src="img/8d289e70-4022-4ba7-b696-a5b2e1017574.jpeg" width="308"/></div>
<p>因此，您本质上需要一种技术，不仅能识别图像中的实体，还能告诉您它们在图像中的位置。这就是所谓的<strong>物体检测</strong>。对象检测为您提供图像中识别的所有实体的边界框和类别标签(以及检测概率)。该系统的输出可用于支持多个高级用例，这些用例处理特定类别的检测到的对象。</p>
<p>以脸书、谷歌照片和许多其他类似应用程序中的面部识别功能为例。在这种情况下，在你从一个聚会上拍摄的图像中识别出<em xmlns:epub="http://www.idpf.org/2007/ops">是谁之前，你需要检测图像中所有的人脸；然后你就可以把这些脸通过你的人脸识别/分类模块来获取/分类他们的名字。因此，对象检测中的对象命名法不限于语言实体，而是包括任何具有特定边界和足够数据来训练系统的事物，如下图所示:</em></p>
<div><img height="203" src="img/ea2c10b7-68bc-4eee-8901-c33a84738f3a.jpeg" width="305"/></div>
<p>现在，如果你想知道有多少出席你的派对的客人实际上是<strong>在享受</strong>它，你甚至可以运行一个<strong>笑脸</strong>或<strong>微笑探测器</strong>的对象检测。对于大多数可检测的人体部位(眼睛、面部、上身等)、流行的人类表情(如微笑)以及许多其他一般对象，都有非常强大且高效的对象检测器训练模型。所以，下次你在智能手机上使用<strong>微笑快门</strong>(当场景中的大多数人脸被检测到微笑时，自动点击图像的功能)，你就知道是什么驱动了这个功能。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Why is object detection much more challenging than image classification?</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">为什么物体检测比图像分类更具挑战性？</h1>
                
            
            
                
<p>从我们到目前为止对CNN和图像分类的理解，让我们尝试理解我们可以如何处理对象检测问题，并且这应该在逻辑上引导我们发现潜在的复杂性和挑战。为了简单起见，假设我们处理的是单色图像。</p>
<p>任何高水平的物体检测可以被认为是两个任务的组合(我们将在后面反驳这一点):</p>
<ul>
<li>获得正确的边界框(或尽可能多的边界框以便稍后过滤)</li>
<li>对该边界框中的对象进行分类(同时返回用于过滤的分类有效性)</li>
</ul>
<p>因此，对象检测不仅要满足图像分类(第二个目标)的所有挑战，还要面对找到正确的或尽可能多的边界框的新挑战。由于我们已经知道如何使用CNN进行图像分类，以及相关的挑战，我们现在可以专注于我们的第一项任务，并探索我们的方法有多有效(分类精度)和高效(计算复杂性)——或者说这项任务将有多具挑战性。</p>
<p>所以，我们从图像中随机生成边界框开始。即使我们不担心生成如此多的候选框的计算负荷，这些候选框在技术上被称为<strong>区域建议</strong>(我们作为对对象进行分类的建议发送的区域)，我们仍然需要一些机制来找到以下参数的最佳值:</p>
<ul>
<li>提取/绘制候选边界框的起始(或中心)坐标</li>
<li>候选边界框的长度</li>
<li>候选边框的宽度</li>
<li>跨越每个轴(水平轴<em> x </em>和垂直轴<em> y </em>从一个起点到另一个起点的距离)</li>
</ul>
<p>假设我们可以生成这样一个算法，它可以给出这些参数的最优值。然而，这些参数的一个值在大多数情况下，或者事实上，在一些一般情况下是否有效？根据我们的经验，我们知道每个对象都有不同的比例，所以我们知道这些盒子的固定值<em> L </em>和<em> W </em>是行不通的。此外，我们可以理解，同一个物体，比如说狗，可能会以不同的比例/尺度和位置出现在不同的图像中，就像我们前面的一些例子一样。所以这证实了我们的信念，我们不仅需要不同尺度的盒子，还需要不同大小的盒子。</p>
<p>让我们假设，修正前面的类比，我们想要在图像中每个起始坐标提取<em> N </em>个候选框，其中<em> N </em>包含了可能适合我们分类问题的大多数尺寸/比例。虽然这本身似乎是一项相当具有挑战性的工作，但让我们假设我们有这个神奇的数字，并且它远不是一个组合<em> L[1，l-image] x W[1，w-image] </em>(所有组合<em> L </em>和<em> W </em>，其中长度是1和实际图像长度之间的所有整数的集合，宽度是从1到图像的宽度)；这将引导我们找到每个坐标的<em>长*宽</em>框:</p>
<div><img height="170" src="img/c76e332d-472c-46bd-8e0d-4f51af7b7126.jpeg" width="256"/></div>
<p>然后，另一个问题是关于我们需要在我们的图像中访问多少个起始坐标，从那里我们将提取这些<em> N </em>个框，或者步幅。使用非常大的步幅将使我们提取子图像本身，而不是可以有效分类并用于实现我们早期示例中的一些目标的单个同质对象。相反，太短的步幅(比如，每个方向1个像素)可能意味着很多候选框。</p>
<p>从前面的例子中，我们可以理解，即使在假设放松了大多数限制后，我们也远远没有实现一个可以在智能手机中实时检测微笑自拍甚至是笑脸的系统(事实上，即使在一个小时后)。它也不能让我们的机器人和自动驾驶汽车在物体移动时识别它们(并通过避开它们来导航)。这种直觉应该有助于我们理解物体检测领域的进步，以及为什么它是一个如此有影响力的工作领域。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Traditional, nonCNN approaches to object detection</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">传统的非神经网络目标检测方法</h1>
                
            
            
                
<p>OpenCV和其他一些库迅速包含在智能手机、机器人项目和许多其他项目的软件包中，以提供特定对象(面部、微笑等)的检测功能和类似计算机视觉的好处，尽管在大量采用CNN之前就有一些限制。</p>
<p>基于CNN的对象检测和实例分割领域的研究为该领域提供了许多进步和性能增强，不仅实现了这些系统的大规模部署，还为许多新的解决方案开辟了道路。但在我们计划进入基于CNN的进步之前，最好先了解如何应对前面部分提到的挑战，以使物体检测成为可能(即使有所有限制)，然后我们将合乎逻辑地开始讨论不同的研究人员以及如何应用CNN来解决使用传统方法仍然存在的其他问题。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Haar features, cascading classifiers, and the Viola-Jones algorithm</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">Haar特征、级联分类器和Viola-Jones算法</h1>
                
            
            
                
<p>与CNN或最深度学习不同，后者以其自动生成更高概念特征的能力而闻名，这反过来又大大推动了分类器，在传统机器学习应用的情况下，这些特征需要由SME手工制作。</p>
<p>我们也可以从基于CPU的机器学习分类器的工作经验中了解到，它们的性能受到数据的高维度和太多功能的可用性的影响，无法应用于模型，特别是一些非常流行和复杂的分类器，如<strong>支持向量机</strong> ( <strong> SVM </strong>)，这些分类器在不久前还被认为是最先进的。</p>
<p>在本节中，我们将了解一些从科学和数学的不同领域中汲取灵感的创新想法，这些想法导致了上述一些挑战的解决方案，从而使非CNN系统中的实时对象检测的概念更加充实。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Haar Features</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">哈尔特征</h1>
                
            
            
                
<p class="mce-root">哈尔或类哈尔特征是具有不同像素密度的矩形的形成。Haar特征对检测区域中特定位置的相邻矩形区域中的像素强度求和。基于不同区域像素亮度总和的差异，他们对图像的不同子部分进行分类。</p>
<p>Haar-like特征的名称归因于数学术语Haar小波，Haar小波是一系列重新缩放的方形函数，它们一起形成小波族或基。</p>
<p>因为Haar-like特征对不同区域的像素强度之间的差异起作用，所以它们对单色图像最有效。这也是为什么前面和本节中使用的图像是单色的，以便更直观。</p>
<p class="mce-root">这些类别可分为以下三大类:</p>
<ul>
<li class="mce-root">两个矩形特征</li>
<li class="mce-root">三个矩形特征</li>
<li class="mce-root">四个矩形特征</li>
</ul>
<div><img height="215" src="img/b4eecaef-6d1f-4db9-b0f1-59ca9198ba09.png" width="215"/></div>
<p>类哈尔特征</p>
<p>通过一些简单的技巧，图像上不同强度的计算变得非常有效，并且可以以非常高的速度实时处理。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Cascading classifiers</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">级联分类器</h1>
                
            
            
                
<p>即使我们可以非常快速地从特定区域提取Haar特征，它也不能解决从图像中大量不同的地方提取这种特征的问题；这就是级联功能的概念发挥作用的地方。据观察，只有1/10，000的子区域在分类中对人脸是阳性的，但是我们必须提取所有特征并在所有区域中运行整个分类器。此外，观察到通过仅使用一些特征(级联的第一层中的两个)，分类器可以消除非常高比例的区域(级联的第一区域中的50%)。此外，如果样本仅由这些缩减区域样本组成，那么分类器只需要稍微多一点的特征(级联的第二层中的10个特征)就可以剔除更多的情况，等等。因此，我们在层中进行分类，从需要很低计算能力的分类器开始剔除大部分子区域，逐渐增加剩余子集所需的计算负荷，等等。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>The Viola-Jones algorithm</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">维奥拉-琼斯算法</h1>
                
            
            
                
<p>2001年，Paul Viola和Michael Jones提出了一个解决方案，可以很好地应对前面的一些挑战，但有一些限制。尽管这是一个有近二十年历史的算法，但迄今为止，或者至少直到最近，一些最受欢迎的计算机视觉软件都用来以某种形式嵌入它。这个事实使得在我们转到基于CNN的区域提议方法之前，理解这个非常简单但是强大的算法非常重要。</p>
<p>OpenCV是最流行的计算机视觉软件库之一，它使用级联分类器作为目标检测的主要模式，而Haar-featured-like级联分类器在OpenCV中非常流行。许多预训练的Haar分类器可用于多种类型的一般对象。</p>
<p>该算法不仅能够以高<strong> TPRs </strong> ( <strong>真阳性率</strong>)和低<strong> FPRs </strong> ( <strong>假阳性率</strong>)进行检测，还能够实时工作(每秒至少处理两帧)。</p>
<p>高TPR结合低FPR是确定算法鲁棒性的非常重要的标准。</p>
<p>他们提出的算法的限制如下:</p>
<ul>
<li>它只能用于检测，而不能识别人脸(他们提出了人脸的算法，尽管同样的算法可以用于许多其他物体)。</li>
<li>这些脸必须作为正面视图出现在图像中。无法检测到其他视图。</li>
</ul>
<p>该算法的核心是Haar (like)特征和级联分类器。Haar特性将在后面的小节中介绍。Viola-Jones算法使用Haar特征的子集来确定面部的一般特征，例如:</p>
<ul>
<li>眼睛(由两个矩形特征(水平)确定，眼睛上方的黑色水平矩形形成眉毛，下面是较亮的矩形)</li>
<li>鼻子(三矩形特征(垂直)，以鼻子为中心的亮矩形和鼻子两侧的一个暗矩形，形成太阳穴)，等等</li>
</ul>
<p>然后，这些快速提取的特征可以用于制作分类器来检测(区分)人脸(与非人脸)。</p>
<p>Haar特征，加上一些技巧，计算起来非常快。</p>
<div><img height="104" src="img/8614f6b9-ba3d-4666-9c6a-d7a7540fcdd6.jpg" width="398"/></div>
<p>Viola-Jones算法和Haar-like特征检测人脸</p>
<p>这些Haar-like特征然后被用在级联分类器中以加速检测问题而不损失检测的鲁棒性。</p>
<p>因此，Haar特征和级联分类器产生了一些非常健壮、有效和快速的上一代个体对象检测器。但是，为一个新的物体训练这些级联是非常耗时的，而且它们有很多限制，如前所述。这就是基于CNN的新一代物体探测器的作用。</p>
<p>在这一章中，我们只讨论了哈尔级联或哈尔特征(在非CNN类别中)的基础，因为它们在很长一段时间内一直占主导地位，并且是许多新类型的基础。鼓励读者也探索一些后来更有效的基于SIFT和HOG的特征/级联(相关论文在<em>参考文献</em>部分给出)。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>R-CNN – Regions with CNN features</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">r-CNN–具有CNN特色的地区</h1>
                
            
            
                
<p>在“为什么物体检测比图像分类更具挑战性？”一节中，我们使用了一种非CNN方法来绘制区域建议，并使用CNN进行分类，我们意识到这种方法效果不好，因为生成并输入CNN的区域不是最佳的。R-CNN或具有CNN特征的区域，顾名思义，完全颠倒了这个例子，并使用CNN生成特征，这些特征使用一种称为<strong> SVM </strong> ( <strong>支持向量机</strong>)的(非CNN)技术进行分类</p>
<p>R-CNN使用滑动窗口方法(很像我们之前讨论的，使用一些<em> L x W </em>和stride)来生成大约2000个感兴趣的区域，然后使用CNN将它们转换为用于分类的特征。请记住我们在“迁移学习”一章中讨论的内容—可以提取最后一个展平层(在分类或softmax层之前),以从基于一般化数据训练的模型中迁移学习，并进一步训练它们(与使用特定于域的数据从头开始训练的具有类似性能的模型相比，通常需要更少的数据)以对特定于域的模型建模。r-CNN也使用类似的机制来提高其对特定对象检测的有效性:</p>
<div><img height="142" src="img/6173be31-0eb7-4bce-9345-27e63b442d91.png" width="459"/></div>
<p>r-CNN–工作</p>
<p>R-CNN的原始论文声称，在PASCAL VOC 2012数据集上，它将<strong>平均精度</strong> ( <strong> mAP </strong>)相对于该数据上以前的最佳结果提高了30%以上，同时实现了53.3%的mAP。</p>
<p>我们在ImageNet数据上看到了图像分类练习(使用CNN)的非常高精度的数字。不要将该图与这里给出的比较统计数据一起使用，因为不仅所使用的数据集不同(因此不具有可比性)，而且手头的任务(分类与目标检测)也非常不同，并且目标检测比图像分类更具挑战性。</p>
<p>PASCAL <strong> VOC </strong> ( <strong>视觉对象挑战</strong>):每个研究领域都需要某种标准化数据集和标准KPI来比较不同研究和算法的结果。我们用于图像分类的数据集Imagenet不能用作对象检测的标准化数据集，因为对象检测需要(训练、测试和验证集)不仅标记有对象类别，还标记有其位置的数据。ImageNet不提供这种功能。因此，在大多数对象检测研究中，我们可能会看到使用标准化的对象检测数据集，如PASCAL VOC。到目前为止，PASCAL VOC数据集有4个变体，VOC2007、VOC2009、VOC2010和VOC2012。VOC2012是其中最新的(也是最丰富的)。</p>
<p>我们遇到的另一个地方是感兴趣区域的不同尺度(和位置)，使用区域的<em xmlns:epub="http://www.idpf.org/2007/ops">识别。这就是所谓的<strong xmlns:epub="http://www.idpf.org/2007/ops">本地化</strong>挑战；在R-CNN中，通过使用不同范围的感受野来解决这个问题，从195×195像素和32×32步长的区域开始，到较小的区域。</em></p>
<p>这种方法被称为使用区域的<strong xmlns:epub="http://www.idpf.org/2007/ops">识别。</strong></p>
<p>等一下！想起来了吗？我们说过将使用CNN从该区域生成要素，但是CNN使用恒定大小的输入来生成固定大小的展平图层。我们确实需要固定大小的特征(展平的向量大小)作为支持向量机的输入，但是这里输入区域的大小是变化的。那么这是怎么做到的呢？R-CNN使用一种叫做<strong>仿射图像扭曲</strong>的流行技术来计算来自每个区域提议的固定大小的CNN输入，而不管该区域的形状。</p>
<p>在几何中，仿射变换是仿射空间之间保持点、直线和平面的变换函数的名称。仿射空间是这样的结构，其概括了欧几里得空间的性质，同时仅保留了与平行性和各自尺度相关的性质。</p>
<p>除了我们已经提到的挑战之外，还有一个挑战值得一提。我们在第一步中生成的候选区域(我们在第二步中对其执行分类)不是非常准确，或者它们在所识别的对象周围缺乏紧密的边界。因此，我们在该方法中包括第三阶段，通过运行回归函数(称为<strong>边界框回归器</strong>)来识别分离的边界，从而提高边界框的准确性。</p>
<p>与早期的端到端非CNN方法相比，R-CNN被证明是非常成功的。但是它只使用CNN将区域转换成特征。正如我们所理解的，CNN对于图像分类也非常强大，但是因为我们的CNN只对输入区域图像起作用，而不对展平区域特征起作用，所以我们不能在这里直接使用它。在下一节中，我们将看到如何克服这个障碍。</p>
<p>从理解CNN在对象检测中的背景使用的角度来看，R-CNN是非常重要的，因为它是所有非基于CNN的方法的巨大飞跃。但是由于基于CNN的物体检测的进一步改进，正如我们接下来将讨论的，R-CNN现在没有被积极地研究，并且代码不再被维护。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Fast R-CNN – fast region-based CNN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">快速R-CNN–基于区域的快速CNN</h1>
                
            
            
                
<p>快速R-CNN，或快速基于区域的CNN方法，是对以前报道的R-CNN的改进。准确地说，与R-CNN相比，改进的统计数据是:</p>
<ul>
<li>训练速度提高9倍</li>
<li>评分/服务/测试速度快213倍(每张图像处理0.3秒)，忽略了花在区域提案上的时间</li>
<li>在PASCAL VOC 2012数据集上具有66%的较高mAP</li>
</ul>
<p>R-CNN使用更小的(五层)CNN，而Fast R-CNN使用更深的VGG16网络，这是其准确性提高的原因。此外，R-CNN很慢，因为它为每个对象提议执行一个ConvNet前向传递，而不共享计算:</p>
<div><img height="192" src="img/a12c5041-0ca5-492b-8a7f-12be6edd2eb9.png" width="448"/></div>
<p>快速R-CNN:工作</p>
<p>在快速R-CNN中，深度VGG16 CNN为所有阶段提供基本计算，即:</p>
<ul>
<li><strong>感兴趣区域</strong> ( <strong> RoI </strong>)计算</li>
<li>区域内容的分类对象(或背景)</li>
<li>用于增强边界框的回归</li>
</ul>
<p>在这种情况下，CNN的输入不是来自图像的原始(候选)区域，而是(完整的)实际图像本身；输出不是最后一个展平的图层，而是之前的卷积(贴图)图层。从如此生成的卷积图中，使用RoI汇集层(max-pooling的变体)来生成对应于每个对象提议的展平的固定长度RoI，然后使其通过一些<strong>完全连接的</strong> ( <strong> FC </strong>)层。</p>
<p>RoI pooling是max pooling(我们在本书最初章节中使用的)的变体，其中输出大小是固定的，输入矩形是一个参数。</p>
<p>RoI pooling图层使用max pooling将任何有效感兴趣区域内的要素转换为具有固定空间范围的小型要素地图。</p>
<p>倒数第二个FC层的输出用于以下两个方面:</p>
<ul>
<li>分类(SoftMax层)具有与对象提议一样多的类，背景+1个附加类(在该区域中找不到任何类)</li>
<li>产生四个数字(两个数字表示该对象框左上角的x，y坐标，接下来的两个数字对应于在该区域找到的该对象的高度和宽度)的回归变量集，这四个数字是使该特定对象的边界框精确所需的每个对象建议</li>
</ul>
<p class="mce-root">快速R-CNN取得的结果非常好。更重要的是使用强大的CNN网络为我们需要克服的所有三个挑战提供非常有效的功能。但是仍然有一些缺点，还有进一步改进的空间，我们将在下一节更快的R-CNN中了解。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Faster R-CNN – faster region proposal network-based CNN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">更快的R-CNN-更快的地区提案基于网络的CNN</h1>
                
            
            
                
<p>我们在前面的章节中看到，快速R-CNN显著降低了图像评分(测试)所需的时间，但这种降低忽略了生成区域建议所需的时间，这使用了一种单独的机制(尽管从CNN的卷积图中提取),并继续证明是一个瓶颈。此外，我们观察到，虽然所有三个挑战都是使用快速R-CNN中卷积映射的共同特征解决的，但它们使用的是不同的机制/模型。</p>
<p>更快的R-CNN改进了这些缺点，并提出了<strong>区域提议网络</strong> ( <strong> RPNs </strong>)的概念，将每张图像的评分(测试)时间降至0.2秒，甚至包括区域提议的时间。</p>
<p>快速R-CNN在每张图片0.3秒内完成评分(测试)，这也不包括相当于地区提案的流程所需的时间。</p>
<div><img height="271" src="img/f947c085-62de-43b0-bde7-f7a27bb31127.png" width="265"/></div>
<p>更快的R-CNN:工作-作为关注机制的区域提议网络</p>
<div><p>如前图所示，VGG16(或另一种)CNN直接作用于图像，产生卷积图(类似于Fast R-CNN中所做的)。这里的情况有所不同，现在有两个分支，一个馈入RPN，另一个馈入检测网络。这又是同一个CNN预测的扩展，导致一个<strong>完全卷积网络</strong> ( <strong> FCN </strong>)。RPN充当注意机制，并且还与检测网络共享全图像卷积特征。此外，由于网络中的所有部分都可以使用高效的基于GPU的计算，因此减少了所需的总时间:</p>
</div>
<div><img height="180" src="img/1600a405-9d97-4b14-aeaa-b7b322fb5d71.jpg" width="303"/></div>
<p>更快的R-CNN:工作-作为关注机制的区域提议网络</p>
<p>为了更好地理解注意力机制，请参考本书中关于CNN注意力机制的章节。</p>
<p>RPN以滑动窗口机制工作，其中窗口从共享卷积层滑过最后一个卷积图(很像CNN滤波器)。对于每一张幻灯片，滑动窗口都会产生<em>k(k = N<sub>Scale</sub>×N<sub>Size</sub>)</em>数量的锚框(类似于候选框)，其中<em> N <sub> Scale </sub> </em>是从滑动窗口的中心提取的<em>N<sub>Size</sub></em>Size(纵横比)的框的<em> size </em>的(金字塔状)数量，很像下图。</p>
<p>RPN通向平坦的FC层。这进而导致两个网络，一个用于预测每个<em> k </em>盒子的四个数字(确定盒子的坐标、长度和宽度，如在Fast R-CNN中)，另一个进入二项式分类模型，确定在该盒子中找到任何给定物体的目标或概率。来自RPN的输出引入检测网络，该网络在给定盒子的位置及其对象的情况下，检测k个盒子中的每一个中的对象的特定类别。</p>
<div><div><img src="img/ce65a2bc-2168-4e83-be93-176926aef7e0.png"/></div>
<p>更快的R-CNN:工作-提取不同的尺度和大小</p>
</div>
<p>这种架构中的一个问题是两个网络的训练，即区域提议和检测网络。我们了解到，CNN是使用跨所有层的反向传播来训练的，同时随着每次迭代减少层的损失。但是由于分成了两个不同的网络，我们一次只能反向传播到一个网络。为了解决这个问题，在保持另一个网络的权重不变的同时，在每个网络中反复进行训练。这有助于快速融合两个网络。</p>
<p>RPN架构的一个重要特征是，它对于两个函数都具有平移不变性，一个函数产生锚点，另一个函数产生锚点的属性(其坐标和对象)。由于平移不变性，在给定锚图的矢量图的情况下，逆向操作或产生图像的部分是可行的。</p>
<p>由于平移不变性，我们可以在CNN中的任一方向上移动，即从图像到(区域)建议，以及从建议到图像的相应部分。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Mask R-CNN – Instance segmentation with CNN</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">屏蔽R-CNN–CNN实例分割</h1>
                
            
            
                
<p>更快的R-CNN是当今物体探测领域最先进的东西。但是，在物体检测领域有一些问题是快速R-CNN无法有效解决的，这就是Mask R-CNN(快速R-CNN的一种发展)可以提供帮助的地方。</p>
<p>本节介绍实例分割的概念，它是本章中描述的标准对象检测问题和语义分割挑战的结合。</p>
<p>在应用于图像的语义分割中，目标是将每个像素分类到一组固定的类别中，而不区分对象实例。</p>
<p>还记得我们在直觉部分计算图片中狗的数量的例子吗？我们能够很容易地数出狗的数量，因为它们相距很远，没有重叠，所以基本上只要数出物体的数量就可以了。现在，以下面的图像为例，使用对象检测来计算西红柿的数量。这将是一项艰巨的任务，因为边界框将有太多的重叠，以至于很难将番茄的实例与框区分开来。</p>
<p>因此，本质上，我们需要走得更远，超越边界框，进入像素，以获得水平分离和识别。就像我们在对象检测中使用对象名称对边界框进行分类一样，在实例分割中，我们不仅使用特定的对象名称，还使用对象实例对每个像素进行分割/分类。</p>
<p>对象检测和实例分割可以被视为两个不同的任务，一个在逻辑上导致另一个，很像我们在对象检测的情况下发现的寻找区域提议和分类的任务。但在对象检测的情况下，特别是使用快速/更快的R-CNN等技术，我们发现如果我们有一种机制来同时完成它们，同时利用大量的计算和网络来完成这些任务，使任务无缝，将会更加有效。</p>
<div><img height="217" src="img/d580f08c-98db-4d19-9f25-fcffec41326e.jpeg" width="316"/></div>
<p>实例细分–直觉</p>
<p>Mask R-CNN是早期网络中覆盖的更快R-CNN的扩展，使用了更快R-CNN中使用的所有技术，并增加了一项内容，即网络中的一条附加路径，以并行地为每个检测到的对象实例生成分段掩码(或对象掩码)。此外，由于这种使用大部分现有网络的方法，它仅给整个处理增加了最小的开销，并且具有几乎等同于更快的R-CNN的评分(测试)时间。在应用于COCO2016挑战赛(使用COCO2015数据集)的所有单模型解决方案中，它具有最佳精度之一。</p>
<p>像，PASCAL VOC，COCO是另一个大规模标准(系列)数据集(来自微软)。除了对象检测，COCO还用于分割和字幕。COCO比许多其他数据集更广泛，最近许多关于对象检测的比较都是在此基础上进行的，以便进行比较。COCO数据集有三种变体，即COCO 2014、COCO 2015和COCO 2017。</p>
<p>在掩模R-CNN中，除了具有为每个锚框或RoI生成目标和定位的两个分支之外，还存在第三FCN，其接收RoI并以像素到像素的方式为给定锚框预测分割掩模。</p>
<p>但是仍然存在一些挑战。虽然更快的R-CNN确实证明了变换不变性(也就是说，我们可以从RPN的卷积图追踪到实际图像的像素图)，但卷积图与实际图像像素的结构不同。因此，网络输入和输出之间不存在像素间对齐，这对于我们利用该网络提供像素间掩蔽非常重要。为了解决这一挑战，Mask R-CNN使用了一个无量化层(在原始论文中称为RoIAlign ),帮助对齐精确的空间位置。这一层不仅提供精确的对准，而且有助于在很大程度上提高精度，因此Mask R-CNN能够胜过许多其他网络:</p>
<div><img height="207" src="img/50bfd31a-acb9-4003-8b02-bcdf236793f3.jpg" style="text-align: center;color: #333333;font-size: 1em" width="382"/></div>
<p>屏蔽R-CNN–实例分段屏蔽(说明性输出)</p>
<div><p>实例分割的概念非常强大，可以实现许多非常有影响力的用例，而这些用例仅靠对象检测是不可能实现的。</p>
<p>我们甚至可以使用实例分割来估计同一框架中的人体姿态，并消除它们。</p>
</div>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Instance segmentation in code</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">代码中的实例分段</h1>
                
            
            
                
<p>现在是把我们学到的东西付诸实践的时候了。我们将使用COCO数据集及其数据API，并使用脸书研究所的Detectron项目(参考资料中的链接)，该项目在Apache 2.0许可下提供了前面讨论的许多技术的Python实现。代码适用于Python2和Caffe2，因此我们需要一个具有给定配置的虚拟环境。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Creating the environment</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">创造环境</h1>
                
            
            
                
<p>安装了Caffe2的虚拟环境可以根据<em>参考资料</em>部分中Caffe2存储库链接上的<kbd>caffe2</kbd>安装说明来创建。接下来，我们将安装依赖项。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Installing Python dependencies (Python2 environment)</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">安装Python依赖项(Python2环境)</h1>
                
            
            
                
<p>我们可以安装Python依赖项，如下面的代码块所示:</p>
<p>Python 2X和Python 3X是两种不同风格的Python(或者更准确地说是CPython)，不是版本的传统升级，因此一种变体的库可能与另一种不兼容。本节使用Python 2X。</p>
<div><em>When we refer to the (interpreted) programming language Python, we need to refer to it with the specific interpreter (since it is an interpreted language as opposed to a compiled one like Java). The interpreter that we implicitly refer to as the Python interpreter (like the one you download from Python.org or the one that comes bundled with Anaconda) is technically called CPython, on which is the default byte-code interpreter of Python, which is written in C. But there are other Python interpreters also like Jython (build on Java), PyPy (written in Python itself - not so intuitive, right?), IronPython (.NET implementation of Python). </em></div>
<pre>pip install numpy&gt;=1.13 pyyaml&gt;=3.12 matplotlib opencv-python&gt;=3.2 setuptools Cython mock scipy</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Downloading and installing the COCO API and detectron library (OS shell commands)</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">下载并安装COCO API和detectron库(OS shell命令)</h1>
                
            
            
                
<p>然后，我们将下载并安装Python依赖项，如以下代码块所示:</p>
<pre># COCO API download and install<br/># COCOAPI=/path/to/clone/cocoapi<br/>git clone https://github.com/cocodataset/cocoapi.git $COCOAPI<br/>cd $COCOAPI/PythonAPI<br/>make install<br/><br/># Detectron library download and install<br/># DETECTRON=/path/to/clone/detectron
git clone https://github.com/facebookresearch/detectron $DETECTRON<br/>cd $DETECTRON/lib &amp;&amp; make</pre>
<p>或者，我们可以下载并使用环境的Docker映像(需要Nvidia GPU支持):</p>
<pre># DOCKER image build<br/>cd $DETECTRON/docker docker build -t detectron:c2-cuda9-cudnn7.<br/>nvidia-docker run --rm -it detectron:c2-cuda9-cudnn7 python2 tests/test_batch_permutation_op.py</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Preparing the COCO dataset folder structure</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">准备COCO数据集文件夹结构</h1>
                
            
            
                
<p>现在，我们将看到准备COCO数据集文件夹结构的代码，如下所示:</p>
<pre># We need the following Folder structure: coco [coco_train2014, coco_val2014, annotations]<br/>mkdir -p $DETECTRON/lib/datasets/data/coco<br/>ln -s /path/to/coco_train2014 $DETECTRON/lib/datasets/data/coco/<br/>ln -s /path/to/coco_val2014 $DETECTRON/lib/datasets/data/coco/<br/>ln -s /path/to/json/annotations $DETECTRON/lib/datasets/data/coco/annotations</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Running the pre-trained model on the COCO dataset</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">在COCO数据集上运行预训练模型</h1>
                
            
            
                
<p>我们现在可以在COCO数据集上实现预训练模型，如以下代码片段所示:</p>
<pre>python2 tools/test_net.py \
    --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \
    TEST.WEIGHTS https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-             FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \
    NUM_GPUS 1</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>References</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">参考</h1>
                
            
            
                
<ol>
<li>
<p>Paul Viola和Michael Jones，使用简单特征级联的快速目标检测，<em>计算机视觉和模式识别会议</em>，2001年。</p>
</li>
<li>
<p>Paul Viola和Michael Jones，鲁棒的实时目标检测，<em>国际计算机视觉杂志</em>，2001年。</p>
</li>
<li>
<p>Itseez2015opencv，opencv，<em>开源计算机视觉库</em>，Itseez，2015。</p>
</li>
<li>
<p>Ross B. Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，<em>用于精确对象检测和语义分割的丰富特征层次</em>，CoRR，arXiv:1311.2524，2013年。</p>
</li>
<li>
<p>Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik，<em>用于精确对象检测和语义分割的丰富特征层次</em>，计算机视觉和模式识别，2014年。</p>
</li>
<li>
<p>米（meter的缩写））Everingham，L. VanGool，C. K. I. Williams，J. Winn，A. Zisserman，<em>2012年PASCAL视觉对象类挑战赛</em>，VOC2012，结果。</p>
</li>
<li>
<p>D.洛。<em>尺度不变关键点的独特图像特征</em>，IJCV，2004。</p>
</li>
<li>
<p>名词（noun的缩写）达拉和b .特里格斯。<em>用于人体检测的方向梯度直方图</em>。2005年在CVPR。</p>
</li>
<li>
<p>Ross B. Girshick，快速R-CNN，CoRR，arXiv:1504.08083，2015。</p>
</li>
<li>
<p>Rbgirshick，fast-rcnn，GitHub，【https://github.com/rbgirshick/fast-rcnn】T2，2018年2月。</p>
</li>
<li>
<p>任，，何，罗斯·格希克，，快速R-CNN: <em>用区域建议网络实现实时目标检测</em>，CoRR，arXiv:1506.01497，2015。</p>
</li>
<li>
<p>任和，何和罗斯·吉斯克和，更快的R-CNN: <em>用区域建议网络实现实时目标检测</em>，<strong>神经信息处理系统进展</strong> ( <strong> NIPS </strong>)，2015。</p>
</li>
<li>
<p>Rbgirshick，py-faster-rcnn，GitHub，<a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a>，2018年2月。</p>
</li>
<li>
<p>罗斯·吉尔希克，伊利亚·拉多萨维奇，乔治娅·格基奥萨里，彼得·多尔，明凯·何，</p>
Detectron, GitHub, <a href="https://github.com/facebookresearch/Detectron">https://github.com/facebookresearch/Detectron</a>, Feb-2018.</li>
<li>
<p>宗-林逸、迈克尔·梅尔、塞尔日·j·贝隆吉、卢博米尔·d·布尔德夫、罗斯·b·吉尔希克、詹姆斯·海斯、皮埃特罗·佩罗娜、德瓦·拉曼南、彼得·多勒、c·劳伦斯·兹尼克，<em>微软可可:背景中的共同对象</em>，CoRR，arXiv:1405.0312，2014年。</p>
</li>
<li>
<p>明凯·何，乔治娅·格基奥萨里，彼得·多尔，罗斯·b·吉尔希克，面具R-CNN，CoRR，arXiv:1703.06870，2017。</p>
</li>
<li>
<p>陈良杰，Alexander Hermans，George Papandreou，Florian Schroff，王鹏，Hartwig Adam，MaskLab: <em>利用语义和方向特征改进对象检测的实例分割</em>，CoRR，arXiv:1712.04837，2017。</p>
</li>
<li>
<p>Anurag Arnab，Philip H. S. Torr，<em>使用动态实例化网络的逐像素实例分割</em>，CoRR，arXiv:1704.02386，2017。</p>
</li>
<li>
<p>Matterport，Mask_RCNN，GitHub，<a href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a>，2018年2月。</p>
</li>
<li>
<p>CharlesShang，FastMaskRCNN，GitHub，<a href="https://github.com/CharlesShang/FastMaskRCNN">https://github.com/CharlesShang/FastMaskRCNN</a>，2018年2月。</p>
</li>
<li>
<p>卡菲2，卡菲2，GitHub，<a href="https://github.com/caffe2/caffe2">https://github.com/caffe2/caffe2</a>，2018年2月。</p>
</li>
</ol>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Summary</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:b11f9755-9a88-4359-8ec0-f5132866b134" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们从对象检测任务背后的非常简单的直觉开始，然后进入非常高级的概念，例如实例分割，这是一个当代的研究领域。物体检测是零售、媒体、社交媒体、移动和安全领域许多创新的核心；使用这些技术为企业和社会消费创造非常有影响力和有利可图的功能有很大的潜力。</p>
<p>从算法的角度来看，本章从传奇的Viola-Jones算法及其底层机制开始，比如Haar特征和级联分类器。利用这种直觉，我们开始探索CNN世界中的物体检测算法，如R-CNN、快速R-CNN，直至最先进的快速R-CNN。</p>
<p>在这一章中，我们还奠定了基础，并介绍了一个非常新的和有影响力的研究领域，称为<strong>实例分割</strong>。我们还介绍了一些先进的基于方法的深度CNN，例如Mask R-CNN，用于实例分割的简单和高性能的实现。</p>
<p> </p>
<p class="mce-root"/>


            

            
        
    </body>

</html></body></html>