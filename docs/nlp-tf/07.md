

# 第七章。长短期记忆网络

在这一章中，我们将讨论一种更高级的 RNN 变体，称为**长短期记忆网络**(**lstm**)。LSTMs 广泛用于许多顺序任务(包括股市预测、语言建模和机器翻译)，并已被证明比其他顺序模型(例如，标准 rnn)执行得更好，尤其是在大量数据可用的情况下。LSTMs 设计得很好，避免了我们在前一章讨论的消失梯度问题。

消失梯度造成的主要实际限制是它阻止模型学习长期依赖性。然而，通过避免消失梯度问题，LSTMs 能够比普通 rnn 存储更长时间的记忆(数百个时间步长)。与那些仅保持单个隐藏状态的 rnn 相比，LSTMs 具有更多的参数，并且可以更好地控制在给定的训练步骤中存储什么和丢弃什么。例如，rnn 不能决定存储哪个存储器和丢弃哪个存储器，因为隐藏状态在每个训练步骤被强制更新。

具体来说，我们将在很高的层次上讨论什么是 LSTM，以及 lstm 的功能如何允许它们存储长期依赖关系。然后，我们将进入管理 LSTMs 的实际基础数学框架，并讨论一个例子来强调为什么每个计算都很重要。我们还将比较 lstm 和普通 rnn，并看到 lstm 具有更复杂的架构，允许它们在顺序任务中超过普通 rnn。重温消失梯度的问题，并通过一个例子来说明它，将使我们理解 LSTMs 如何解决这个问题。

此后，我们将讨论几种已经引入的技术，以改进由标准 LSTM 产生的预测(例如，在文本生成任务中改进生成文本的质量/多样性)。例如，一次生成几个预测而不是逐个预测，有助于提高生成的预测的质量。我们还将研究 **BiLSTMs** 或**双向 LSTMs** ，它们是标准 LSTM 的扩展，比标准 LSTM 具有更强的能力来捕获序列中存在的模式。

最后，我们将讨论两个最近的 LSTM 变种。首先，我们将看看**窥视孔连接**，它向 LSTM 门引入更多的参数和信息，使 lstm 能够更好地执行。接下来，我们将讨论**门控循环单元** ( **GRUs** )，它们越来越受欢迎，因为与 LSTMs 相比，它们具有更简单的结构，并且不会降低性能。

# 理解长短期记忆网络

在这一节中，我们将首先解释 LSTM 细胞内发生的事情。我们将会看到，除了状态之外，还有一个控制细胞内信息流的门控机制。然后，我们将完成一个详细的示例，看看每个门和状态如何在示例的不同阶段帮助实现所需的行为，最终导致所需的输出。最后，我们将比较 LSTM 和标准的 RNN，以了解 LSTM 和标准的 RNN 有什么不同。

## 什么是 LSTM？

LSTMs 可以被看作是 rnn 的一个更好的家族。LSTM 主要由五种不同的东西组成:

*   **单元状态**:这是LSTM 单元的内部单元状态(即存储器)
*   **隐藏状态**:该是用于计算预测的外部隐藏状态
*   **输入门**:该决定有多少电流输入被读入单元状态
*   **遗忘门**:该决定将多少先前的单元状态发送到当前单元状态
*   **输出门**:该决定有多少单元状态输出到隐藏状态

我们可以将 RNN 包装成如下的单元架构。该单元将输出依赖于(具有非线性激活函数)先前单元状态和当前输入的某个状态。然而，在 RNNs 中，单元状态总是随着每个输入而改变。这导致 rnn 的小区状态总是变化。对于存储长期依赖关系来说，这种行为是非常不可取的。

LSTMs 可以决定何时替换、更新或忘记存储在细胞状态下每个神经元中的信息。换句话说，LSTMs 配备了保持单元状态不变的机制(如果需要的话),使它们能够存储长期依赖关系。

这是通过引入门控机制来实现的。LSTMs 拥有单元需要执行的每个操作的门。这些门在 0 和 1 之间是连续的(通常是 sigmoid 函数),其中 0 表示没有信息流过该门，1 表示所有信息流过该门。LSTM 为细胞中的每个神经元使用一个这样的门。如前所述，这些门控制以下内容:

*   有多少电流输入被写入单元状态(输入门)
*   从前一个单元状态中遗忘了多少信息(遗忘门)
*   有多少信息从单元状态(输出门)输出到最终隐藏状态

*图 7.1* 说明了这一功能。每个门决定有多少不同的数据(例如，当前输入、先前的隐藏状态或先前的单元状态)流入状态(即，最终的隐藏状态或单元状态)。每条线的粗细表示有多少信息从该门流出或流入该门(在一些假设的场景中)。例如，在该图中，您可以看到输入门允许来自当前输入的比来自先前最终隐藏状态的多，其中遗忘门允许来自先前最终隐藏状态的比来自当前输入的多:

![What is an LSTM?](img/B08681_07_01.jpg)

图 7.1:LSTM 中数据流的抽象视图

## LSTMs 更详细

在这里，我们将带了解 LSTMs 的实际机制。我们将首先简要讨论 LSTM 单元格的整体视图，然后开始讨论发生在 LSTM 单元格内的每个操作，以及一个文本生成的例子。

如前所述，LSTMs 主要由以下三个门组成:

*   **输入门**:输出 0(当前输入未写入单元状态)和 1(当前输入完全写入单元状态)之间的值的门。Sigmoid 激活用于将输出压缩到 0 和 1 之间。
*   **遗忘门**:s 形门，输出 0(计算当前单元状态时完全遗忘前一单元状态)和 1(计算当前单元状态时完全读入前一单元状态)之间的值。
*   **输出门**:一个s 形门，输出 0(计算最终状态时完全丢弃当前单元状态)和 1(计算最终隐藏状态时完全使用当前单元状态)之间的值。

这可以是图 7.2 中的*所示的。这是一个非常高级的图，为了避免混乱，隐藏了一些细节。为了提高理解，我们给出了带循环和不带循环的 LSTMs。右侧的图描绘了具有环的 LSTM，而左侧的图显示了具有扩展环的相同 LSTM，因此模型中不存在环:*

![LSTMs in more detail](img/B08681_07_02.jpg)

图 7.2:具有循环链路(即环路)的 LSTM(右)具有扩展的循环链路的 LSTM(左)

现在，为了更好地理解 LSTMs，让我们考虑一个例子。我们将讨论实际的更新规则和公式，并通过一个例子来更好地理解 LSTMs。

现在让我们考虑一个从下面的句子开始生成文本的例子:

约翰给了玛丽一只小狗。

我们输出的故事应该是关于*约翰*、*玛丽*和*小狗*的。让我们假设我们的 LSTM 在给定的句子后输出两个句子:

约翰给了玛丽一只小狗。____________________._____________________.

下面是我们 LSTM 给出的输出:

约翰给了玛丽一只小狗。它叫得很大声。他们把它命名为露娜。

我们离输出这样真实的短语还很远。然而，LSTMs 可以学习诸如名词和代词之间的关系。例如，*它*与*小狗*有关，而*它们*到*约翰*和*玛丽*。然后，它应该学习名词/代词和动词之间的关系。比如对于 *it* 来说，动词的结尾要有一个 *s* 。我们在*图 7.3* 中说明了这些关系/依赖关系。两者我们都可以看到，长期的(例如*露娜* *→* *小狗*)和短期的(例如*它* *→* *吠叫*)依赖都存在于这个短语中。实线箭头表示名词和代词之间的联系，虚线箭头表示名词/代词和动词之间的联系:

![LSTMs in more detail](img/B08681_07_03.jpg)

图 7.3:LSTM 给出和预测的句子，单词之间的各种关系被突出显示

现在让我们考虑 LSTMs 如何使用它们的各种操作，对这样的关系和依赖关系建模，以输出有意义的文本，给定一个起始句子。

输入门( *i [t]* )将当前输入( *x [t]* )和之前的最终隐藏状态( *h [t-1]* )作为输入，计算 *i [t]* ，如下:

![LSTMs in more detail](img/B08681_07_04.jpg)

输入门， *i* *t* 可以理解为在具有 s 形激活的单隐藏层标准 RNN 的隐藏层执行的计算。请记住，我们计算标准 RNN 的隐藏状态如下:

![LSTMs in more detail](img/B08681_07_05.jpg)

因此，LSTM 的*I[t]的计算看起来非常类似于标准 RNN 的*h[t]的计算，除了激活函数的改变和偏置的增加。**

在计算之后， *i [t]* 的值 0 将意味着没有来自当前输入的信息将流向单元状态，其中值 1 意味着来自当前输入的所有信息将流向单元状态。

接下来，另一个值(称为**候选值**)被计算如下，该值被添加以稍后计算当前单元状态:

![LSTMs in more detail](img/B08681_07_06.jpg)

我们可以在*图 7.4* 中看到这些计算:

![LSTMs in more detail](img/B08681_07_07.jpg)

图 7.4。在 LSTM 中发生的所有计算(灰显)的上下文中，计算*I*t 和![LSTMs in more detail](img/B08681_07_08.jpg)(粗体)

在我们的例子中，在学习的最开始，输入门需要被高度激活。LSTM 输出的第一个字是 *it* 。同样为了这样做，LSTM 必须了解到*小狗*也被称为*它*。让我们假设我们的 LSTM 有五个神经元来存储状态。我们希望 LSTM 存储信息，即 *it* 指的是*小狗*。我们希望 LSTM(在不同的神经元中)学习的另一条信息是，当使用代词 *it* 时，现在时态动词应该在动词末尾有一个 *s* 。LSTM 需要知道的另一件事是*小狗会大声叫*。*图 7.5* 说明了如何将这些知识编码到 LSTM 的单元状态中。每个圆圈代表单个神经元(即隐藏单元)的细胞状态:

![LSTMs in more detail](img/B08681_07_09.jpg)

图 7.5:应该在单元格状态中编码以输出第一句话的知识

有了这些信息，我们可以输出第一个新句子:

约翰给了玛丽一只小狗。它叫得很大声。

接下来，遗忘门的计算如下:

![LSTMs in more detail](img/B08681_07_10.jpg)

遗忘门执行以下操作。遗忘门的值为 0 意味着没有来自*c[t-1]的信息将被传递来计算*c[t]，值为 1 意味着*c[t-1]的所有信息将传播到*c[t]的计算中。****

现在我们来看看遗忘之门是如何帮助预测下一句话的:

他们把它命名为露娜。

现在你可以看到，我们正在看的新关系是在约翰和玛丽之间。因此，我们不再需要关于*它*和动词*吠叫*如何表现的信息，因为主语是*约翰*和*玛丽*。我们可以使用遗忘门结合当前主题*它们*和相应的名为的动词*来替换存储在**当前主题**和**动词中的信息用于当前主题**神经元(参见*图 7.6* ):*

![LSTMs in more detail](img/B08681_07_11.jpg)

图 7.6:左起第三个神经元中的知识(它 *→* 叫)被新的信息(它们 *→* 命名)取代。

根据权重的值，我们在*图 7.7* 中说明了这种转换。我们不改变神经元维持*it**→**puppy*关系的状态，因为 *puppy* 在最后一句中是作为宾语出现的。这是通过设置连接*it**→**puppy*从 *c [t-1]* 到 *c [t]* 到 1 的权重来实现的。然后我们会用新的主语和动词替换维护当前主语和当前动词信息的神经元。这是通过将该神经元的*f[t]t*的遗忘权重设置为 0 来实现的。然后我们将当前主语和动词连接到对应状态神经元的 *i [t]* 的权重设置为 1。我们可以将![LSTMs in more detail](img/B08681_07_12.jpg)视为包含哪些新信息(例如来自当前输入*x[t]的新信息)应该被带入单元状态的实体:*

![LSTMs in more detail](img/B08681_07_13.jpg)

图 7.7:如何用前一状态*c*t-1 和候选值![LSTMs in more detail](img/B08681_07_14.jpg)计算单元格状态 *c* * t*

当前单元状态将更新如下:

![LSTMs in more detail](img/B08681_07_15.jpg)

换句话说，当前状态是以下各项的组合:

*   要忘记/记住来自先前细胞状态的什么信息
*   向当前输入添加/丢弃什么信息

接下来在*图 7.8* 中，我们强调了到目前为止我们针对 LSTM 内部发生的所有计算所进行的计算:

![LSTMs in more detail](img/B08681_07_16.jpg)

图 7.8:目前已覆盖的计算包括 *i* *t* 、 *f* *t* 、![LSTMs in more detail](img/B08681_07_08.jpg)和 *c* * t*

学习完完整状态后，它将看起来像*图 7.9* :

![LSTMs in more detail](img/B08681_07_18.jpg)

图 7.9:输出两个句子后，完整的单元格状态看起来像这样

接下来，我们将看看LSTM 单元的最终状态( *h [t]* )是如何计算的:

![LSTMs in more detail](img/B08681_07_19.jpg)![LSTMs in more detail](img/B08681_07_20.jpg)

在我们的示例中，我们希望输出下面的句子:

他们把它命名为露娜。

为此，我们不需要倒数第二个神经元来计算这句话，因为它包含关于小狗如何吠叫的信息，而这句话是关于小狗的名字。因此，我们可以在最后一句话的预测过程中忽略最后一个神经元(包含*吠- >大声*的关系)。这正是*o[t]t*所做的；在计算 LSTM 单元的最终输出时，它将忽略不必要的内存，只从单元状态中检索相关的内存。同样，在*图 7.10* 中，我们展示了一个 LSTM 细胞看起来的样子:

![LSTMs in more detail](img/B08681_07_21.jpg)

图 7.10:完整的 LSTM 是什么样子

这里，我们总结了所有与 LSTM 单元内发生的操作相关的方程式。

![LSTMs in more detail](img/B08681_07_22.jpg)![LSTMs in more detail](img/B08681_07_23.jpg)![LSTMs in more detail](img/B08681_07_24.jpg)![LSTMs in more detail](img/B08681_07_25.jpg)![LSTMs in more detail](img/B08681_07_26.jpg)![LSTMs in more detail](img/B08681_07_27.jpg)

现在，在更大的画面中，对于一个顺序学习问题，我们可以随着时间的推移展开 LSTM 细胞，以显示它们如何链接在一起，以便它们接收细胞的前一状态来计算下一状态，如图*图 7.11* 所示:

![LSTMs in more detail](img/B08681_07_28.jpg)

图 7.11:lstm 将如何随着时间的推移而关联

然而，这不足以做一些有用的事情。正如你所看到的，即使我们可以创建一个很好的 LSTMs 链，实际上能够模拟一个序列，我们仍然没有输出或预测。但是如果我们想使用 LSTM 实际学到的东西，我们需要一种方法从 LSTM 中提取最终输出。因此，我们将在 LSTM 的顶部固定一个 softmax 层(权重为 *W* *s* ，偏移为 *b* *s* )。最终输出通过下式获得:

![LSTMs in more detail](img/B08681_07_29.jpg)

现在，带有 softmax 图层的 LSTM 的最终图片看起来像*图 7.12* :

![LSTMs in more detail](img/B08681_07_30.jpg)

图 7.12:具有 softmax 输出图层的 LSTMs 随时间的变化

## lstm 与标准 rnn 有何不同

现在让我们研究 LSTMs 与标准 rnn 相比如何。与标准 RNN 相比，LSTM 具有更复杂的结构。其中一个主要区别是 LSTM 有两种不同的状态:单元状态*c*t 和最终隐藏状态*h[t]。然而，一个 RNN 只有一个隐藏状态*h[t]。下一个主要区别是，由于 LSTM 有三个不同的门，所以在计算最终隐藏状态*h[t]时，LSTM 对如何处理当前输入和前一个单元状态有更多的控制。***

拥有两种不同的状态是非常有利的。利用这种机制，即使当单元状态快速改变时，最终隐藏状态仍然会改变得更慢。因此，当单元状态正在学习短期和长期依赖性时，最终隐藏状态可以仅反映短期依赖性或者仅反映长期依赖性或者两者都反映。

接下来，门控机制由三个门组成:输入门、遗忘门和输出门:

*   *输入门*控制有多少电流输入被写入单元状态
*   *遗忘门*控制有多少先前单元状态被带到当前单元状态
*   最后，*输出门*控制有多少从单元状态传播到最终隐藏状态

很明显，这是一种更有原则的方法(尤其是与标准 RNNs 相比)，它允许更好地控制当前输入和先前小区状态对当前小区状态的贡献。此外，输出门可以更好地控制单元状态对最终隐藏状态的贡献。在*图 7.13* 中，我们比较了标准 RNN 和 LSTM 的示意图，以强调两种型号在功能上的差异。

总之，通过维护两种不同状态的设计，LSTM 可以学习短期和长期依赖关系，这有助于解决消失梯度的问题，我们将在下一节中讨论。

![How LSTMs differ from standard RNNs](img/B08681_07_31.jpg)

图 7.13:标准 RNN 和 LSTM 比色皿的对比



# 【LSTMs 如何解决消失梯度问题

正如我们之前讨论的，尽管 rnn 理论上是合理的，但实际上它们有一个严重的缺点。也就是说，当使用通过时间(**【BPTT】**)的**反向传播时，梯度迅速减小，这允许 us 仅传播几个时间步长的信息。因此，我们只能存储很少时间步的信息，因此只拥有短期记忆。这反过来限制了 rnn 在现实世界顺序任务中的有用性。**

通常有用和有趣的顺序任务(如股票市场预测或语言建模)需要学习和存储长期依赖性的能力。想一想下面预测下一个单词的例子:

约翰是个有天赋的学生。他是优等生，打橄榄球和板球。所有其他学生都羡慕 ______。

对我们来说，这是一项非常容易的任务。答案是约翰。然而，对于一个 RNN 人来说，这是一项艰巨的任务。我们正试图预测一个答案，它就在这篇文章的开头。此外，为了解决这个任务，我们需要一种方法来存储 RNN 的长期依赖关系。这正是 LSTMs 要解决的任务类型。

在第 6 章、*循环神经网络*中，我们讨论了在没有任何非线性函数存在的情况下，消失/爆炸梯度是如何出现的。我们现在将看到，即使存在非线性项，它仍然可能发生。为此，我们将看到衍生项![How LSTMs solve the vanishing gradient problem](img/B08681_07_32.jpg)如何用于标准 RNN 和 LSTM ( ![How LSTMs solve the vanishing gradient problem](img/B08681_07_33.jpg)用于 LSTM)网络。这个是导致渐变消失的关键术语，正如我们在前一章中了解到的。

让我们假设标准 RNN 的隐藏状态计算如下:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_34.jpg)

为了简化计算，我们可以忽略当前输入相关项，而专注于循环部分，这将给出以下等式:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_35.jpg)

如果我们为前面的等式计算![How LSTMs solve the vanishing gradient problem](img/B08681_07_36.jpg)，我们将得到以下结果:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_37.jpg)![How LSTMs solve the vanishing gradient problem](img/B08681_07_38.jpg)

现在让我们看看当![How LSTMs solve the vanishing gradient problem](img/B08681_07_39.jpg)或![How LSTMs solve the vanishing gradient problem](img/B08681_07_40.jpg)时会发生什么(这将随着学习的继续而发生)。在这两种情况下，![How LSTMs solve the vanishing gradient problem](img/B08681_07_41.jpg)将开始接近 0，导致渐变消失。甚至当![How LSTMs solve the vanishing gradient problem](img/B08681_07_42.jpg)时，对于乙状结肠激活，梯度最大(0.25)，当乘以许多时间步长时，总梯度变得相当小。此外，术语![How LSTMs solve the vanishing gradient problem](img/B08681_07_43.jpg)(可能是由于糟糕的初始化)也会导致渐变的爆炸或消失。然而，与由于![How LSTMs solve the vanishing gradient problem](img/B08681_07_44.jpg)或![How LSTMs solve the vanishing gradient problem](img/B08681_07_45.jpg)导致的梯度消失相比，由术语![How LSTMs solve the vanishing gradient problem](img/B08681_07_46.jpg)导致的梯度消失/爆炸相对容易解决(通过权重和梯度剪裁的仔细初始化)。

现在让我们来看看 LSTM 细胞。更具体地说，我们将查看单元状态，由以下等式给出:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_47.jpg)

这是在 LSTM 发生的所有忘记门应用的产品。然而，如果您以类似的方式为 LSTMs 计算![How LSTMs solve the vanishing gradient problem](img/B08681_07_48.jpg)(即，忽略![How LSTMs solve the vanishing gradient problem](img/B08681_07_49.jpg)项和*b*f，因为它们是非递归的)，我们会得到以下结果:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_50.jpg)

在这种情况下，尽管如果![How LSTMs solve the vanishing gradient problem](img/B08681_07_51.jpg)梯度将消失，另一方面，如果![How LSTMs solve the vanishing gradient problem](img/B08681_07_52.jpg)，导数将比在标准 RNN 中下降得更慢。因此，我们有一个选择，梯度不会消失。此外，由于使用了挤压功能，梯度不会因为![How LSTMs solve the vanishing gradient problem](img/B08681_07_53.jpg)变大而爆炸(这在梯度爆炸过程中很可能发生)。此外，当![How LSTMs solve the vanishing gradient problem](img/B08681_07_54.jpg)时，我们得到接近 1 的最大梯度，这意味着梯度不会像我们在 RNNs 中看到的那样快速下降(当梯度最大时)。最后，在推导中没有诸如![How LSTMs solve the vanishing gradient problem](img/B08681_07_55.jpg)这样的术语。然而，对于![How LSTMs solve the vanishing gradient problem](img/B08681_07_56.jpg)，推导更加复杂。让我们看看这些术语是否出现在![How LSTMs solve the vanishing gradient problem](img/B08681_07_57.jpg)的推导中。如果你计算它的导数，你会得到如下形式的结果:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_58.jpg)

一旦你解决了这个问题，你会得到这样的东西:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_59.jpg)

我们并不关心![How LSTMs solve the vanishing gradient problem](img/B08681_07_103.jpg)或者![How LSTMs solve the vanishing gradient problem](img/B08681_07_104.jpg)里面的内容，因为无论什么值，都会被(0，1)或者(-1，1)所有界。如果我们进一步简化符号，将![How LSTMs solve the vanishing gradient problem](img/B08681_07_60.jpg)、![How LSTMs solve the vanishing gradient problem](img/B08681_07_61.jpg)、![How LSTMs solve the vanishing gradient problem](img/B08681_07_62.jpg)和![How LSTMs solve the vanishing gradient problem](img/B08681_07_63.jpg)项替换为一些常见符号，如![How LSTMs solve the vanishing gradient problem](img/B08681_07_64.jpg)，我们会得到以下形式的内容:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_65.jpg)

或者，我们得到以下结果(假设外面的![How LSTMs solve the vanishing gradient problem](img/B08681_07_66.jpg)被方括号内的每个![How LSTMs solve the vanishing gradient problem](img/B08681_07_67.jpg)项吸收):

![How LSTMs solve the vanishing gradient problem](img/B08681_07_68.jpg)

这将给出以下内容:

![How LSTMs solve the vanishing gradient problem](img/B08681_07_69.jpg)

这意味着尽管术语![How LSTMs solve the vanishing gradient problem](img/B08681_07_70.jpg)不受任何术语![How LSTMs solve the vanishing gradient problem](img/B08681_07_71.jpg)的影响，但![How LSTMs solve the vanishing gradient problem](img/B08681_07_72.jpg)却不是。因此，我们必须小心初始化 LSTM 的权重，我们也应该使用梯度裁剪。

### 注意

然而，LSTMs 的 *h [t]* 不受消失梯度的影响并不像 RNNs 那样重要。因为 *c [t]*

## 改进 LSTMs

正如我们在学习 rnn 时已经看到的那样，拥有坚实的理论基础并不总是保证它们在实践中表现最佳。这是由于计算机数值精度的限制。对于 LSTMs 来说也是如此。拥有一个复杂的设计(允许对数据的长期相关性进行更好的建模)本身并不意味着 LSTM 会输出完美的现实预测。因此，已经开发了许多扩展来帮助 LSTMs 在预测阶段执行得更好。在这里，我们将讨论几个这样的改进:贪婪的采样，波束搜索，使用单词向量代替单词的一个热编码表示，以及使用双向 LSTMs。

## 贪婪采样

如果我们试图总是以最高的概率预测单词，LSTM 将倾向于产生非常单调的结果。例如，在切换到另一个单词之前，它会多次重复单词*到*。

解决这个问题的一个方法是使用**贪婪采样**，我们从那个集合中挑选预测最佳的 *n* 并采样。这有助于打破预测的单调性质。

让我们考虑前一个例子的第一句话:

约翰给了玛丽一只小狗。

假设，我们从第一个单词开始，想要预测接下来的四个单词:

*约翰 ____ ____ _ _____。*

如果我们试图确定性地选择样本，LSTM 可能会输出如下内容:

约翰把玛丽送给了约翰。

但是，通过从词汇表中的单词子集(最有可能的单词)中抽取下一个单词，LSTM 被迫改变预测，并可能输出以下内容:

约翰给了玛丽一只小狗。

或者，它会给出以下输出:

约翰给了小狗一只。

但是，即使贪婪采样有助于给生成的文本增加更多的变化，这种方法也不能保证输出总是真实的，尤其是在输出较长的文本序列时。现在，我们将看到一种更好的搜索技术，它实际上比预测提前了几个步骤。

## 光束搜索

**波束搜索**是帮助提高由 LSTM 产生的预测质量的一种方式。在这种情况下，通过解决搜索问题来找到预测。波束搜索的关键思想是一次产生 *b* 输出(即![Beam search](img/B08681_07_73.jpg))，而不是单个输出 *y [ t ]* 。这里， *b* 称为光束的**长度**，产生的 *b* 输出称为**光束**。更严格地说，我们选择具有最高联合概率![Beam search](img/B08681_07_74.jpg)的波束，而不是选择最高概率![Beam search](img/B08681_07_105.jpg)。在做出预测之前，我们对未来看得更远，这通常会带来更好的结果。

让我们通过前面的例子来理解波束搜索:

约翰给了玛丽一只小狗。

说，我们是一个字一个字的预测。最初我们有以下内容:

*约翰 ____ ____ _ _____。*

让我们假设我们的 LSTM 使用波束搜索生成了这个例句。那么每个单词的概率可能会像我们在图 7.13 中看到的那样。让我们假设波束长度![Beam search](img/B08681_07_75.jpg)，我们将在搜索的每个阶段考虑![Beam search](img/B08681_07_77.jpg)最佳候选。搜索树如下图所示:

![Beam search](img/B08681_07_107.jpg)

图 7.13:b = 2，n=3 时波束搜索的搜索空间

我们从单词约翰开始，得到词汇表中所有单词的概率。在我们的例子中，作为![Beam search](img/B08681_07_76.jpg)，我们为树的下一级挑选最好的三个候选人:**give**、 **Mary** 和 **puppy** 。(请注意，这些可能不是实际 LSTM 发现的候选，仅用作示例。)然后，从这些选择的候选者中，生长树的下一层。从那里，我们将挑选最好的三个候选，搜索将重复进行，直到我们到达树中的深度 *b* 。

给予最高联合概率的路径(即![Beam search](img/B08681_07_78.jpg))用粗箭头突出显示。此外，这是一个更好的预测机制，因为它会返回更高的概率，或奖励，例如约翰给玛丽的短语比约翰给玛丽的短语更高。

注意，在我们的例子中，贪婪采样和波束搜索产生的输出是相同的，这是一个包含五个单词的简单句子。然而，当我们将此缩放以输出一篇小短文时，情况并非如此。那么由波束搜索产生的结果将比由贪婪采样产生的结果更加真实并且语法正确。

## 使用词向量

提高 LSTM 性能的另一种流行的方法是使用字向量，而不是使用一位热编码向量作为 LSTM 的输入。我们通过一个例子来了解一下这个方法的价值。假设我们想从某个随机单词开始生成文本。在我们的例子中，应该是这样的:

*约翰 ____ ____ _ _____。*

我们已经用下列句子训练了我们的 LSTM:

约翰给了玛丽一只小狗。玛丽送给鲍勃一只小猫。

我们还假设我们有如图*图 7.15* 所示的单词向量:

![Using word vectors](img/B08681_07_79.jpg)

图 7.15:二维空间中假设的词向量拓扑

单词这些单词的嵌入，以它们的数字形式，可能看起来像下面这样:

*小猫:[0.5，0.3，0.2]*

*小狗:[0.49，0.31，0.25]*

*给了:[0.1，0.8，0.9]*

可以看到那个![Using word vectors](img/B08681_07_80.jpg)。但是，如果我们使用一键编码，它们将如下所示:

*小猫:[ 1，0，0，…]*

*小狗:[0，1，0，…]*

*给了:[0，0，1，…]*

然后，![Using word vectors](img/B08681_07_81.jpg)。正如我们已经看到的，一个热点编码的向量没有捕捉单词之间的正确关系，并且看到所有的单词彼此之间的距离相等。然而，词向量能够捕捉这种关系，并且更适合作为 LSTM 的特征。

使用单词向量，LSTM 将学会更好地利用单词之间的关系。例如，通过单词向量，LSTM 将学习以下内容:

约翰给了玛丽一只小猫。

这与以下内容非常接近:

约翰给了玛丽一只小狗。

此外，它与以下内容有很大不同:

约翰给了玛丽一个礼物。

然而，如果使用一位热编码矢量，情况就不是这样了。

## 双向 lstm(BiLSTM)

使 LSTM 双向是提高 LSTM 预测质量的另一种方式。我们的意思是用从开始到结束以及从结束到开始读取的数据来训练 LSTM。到目前为止，在 LSTM 的训练期间，我们将创建如下数据集:

考虑下面两句话:

约翰给了玛丽一个 _____。它叫得很大声。

然而，在这个阶段，在我们希望 LSTM 合理填充的一个句子中有数据丢失。

如果我们从头开始阅读直到缺失的单词，将如下所示:

约翰给了玛丽一个 _____。

这没有提供足够的关于丢失单词的上下文的信息来正确地填充该单词。但是，如果我们从两个方向阅读，结果将如下:

约翰给了玛丽一个 _____。

*_____。它叫得很大声。*

如果我们用这两部分创建数据，就足以预测丢失的单词应该是类似于*狗*或*小狗*的东西。因此，某些问题可以从两边读取数据中受益匪浅。此外，这增加了神经网络可用的数据量，并提高了其性能。

BiLSTMs 的另一个应用是神经机器翻译，我们将源语言的句子翻译成目标语言。由于一种语言到另一种语言的翻译之间没有特定的一致性，了解源语言的过去和未来可以极大地帮助更好地理解上下文，从而产生更好的翻译。例如，考虑将菲律宾语翻译成英语的翻译任务。在菲律宾语中，句子通常按照*动词-宾语-主语*的顺序书写，而在英语中，则是*主语-动词-宾语*。在这项翻译任务中，向前和向后阅读句子对翻译很有帮助。

BiLSTM 实际上是两个独立的 LSTM 网络。一个网络从头到尾学习数据，另一个网络从头到尾学习数据。在*图 7.16* 中，我们展示了一个 BiLSTM 网络的架构。

培训分两个阶段进行。首先，用从头到尾阅读文本所创建的数据来训练纯色网络。该网络代表用于标准 LSTMs 的正常培训程序。第二，用反向阅读文本产生的数据训练虚线网络。然后，在推断阶段，我们使用实线和虚线状态的信息(通过连接两种状态并创建向量)来预测丢失的单词:

![Bidirectional LSTMs (BiLSTM)](img/B08681_07_82.jpg)

图 7.16:bil STM 的示意图



# lstm 的其他变体

虽然我们主要关注标准 LSTM 架构，但是已经出现了许多变体，它们或者简化了标准 lstm 中的复杂架构，或者产生了更好的性能，或者两者兼而有之。我们将着眼于两个变种，介绍 LSTM 的细胞结构的结构修改:窥视孔连接和 GRUs。

## 窥视孔连接

**窥视孔连接**允许门不仅能看到当前输入和先前的最终隐藏状态，还能看到先前的单元状态。这增加了 LSTM 池中的砝码数量。具有这样的连接已经显示出产生更好的结果。这些方程看起来像这样:

![Peephole connections](img/B08681_07_83.jpg)![Peephole connections](img/B08681_07_84.jpg)![Peephole connections](img/B08681_07_86.jpg)![Peephole connections](img/B08681_07_87.jpg)![Peephole connections](img/B08681_07_88.jpg)![Peephole connections](img/B08681_07_89.jpg)

让我们简单地看一下这是如何帮助 LSTM 表现得更好的。到目前为止，门看到的是当前输入和最终隐藏状态，而不是单元状态。然而，在这种配置中，如果输出门接近零，即使当单元状态包含对于更好的性能至关重要的重要信息时，最终的隐藏状态将是接近零的。因此，在计算过程中，门不会考虑隐藏状态。将单元状态直接包含在门计算公式中，可以更好地控制单元状态，即使在输出门接近零的情况下，它也能很好地工作。

我们在*图 7.17* 中展示了带有窥视孔连接的 LSTM 的架构。我们将标准 LSTM 中的所有现有连接都显示为灰色，新添加的连接显示为黑色:

![Peephole connections](img/B08681_07_90.jpg)

图 7.17:带有窥视孔连接的 LSTM(窥视孔连接显示为黑色，而其他连接显示为灰色)

## 门控循环单元

GRUs 可以被视为标准 LSTM 架构的简化。正如我们已经看到的，LSTM 有三个不同的门和两个不同的状态。仅这个就需要大量的参数，即使对于小的状态大小。因此，科学家研究了减少参数数量的方法。gru 就是这样一个努力的结果。

与 LSTMs 相比，gru 有几个主要区别。

首先，GRUs 将两个状态，单元格状态和最终隐藏状态，合并成一个隐藏状态*h[t]。现在，作为这个没有两个不同状态的简单修改的副作用，我们可以去掉输出门。记住，输出门仅仅是决定有多少单元状态被读入最终隐藏状态。该操作大大减少了单元中的参数数量。*

接下来，GRUs 引入了一个重置门，当它接近 1 时，在计算当前状态时，会获取完整的先前状态信息。此外，当复位门接近 0 时，它在计算当前状态时会忽略先前的状态。

![Gated Recurrent Units](img/B08681_07_91.jpg)![Gated Recurrent Units](img/B08681_07_92.jpg)

然后，GRUs 将输入门和遗忘门合并成一个*更新门*。标准 LSTM 有两个门，称为输入门和遗忘门。输入门决定有多少当前输入被读入单元状态，而遗忘门决定有多少先前单元状态被读入当前单元状态。从数学上讲，这可以表示如下:

![Gated Recurrent Units](img/B08681_07_93.jpg)![Gated Recurrent Units](img/B08681_07_94.jpg)

GRUs 将这两个操作合并成一个门，称为更新门。如果更新门为 0，则先前单元状态的完整状态信息被推入当前单元状态，其中没有当前输入被读入该状态。如果更新门为 1，则所有的当前输入被读入当前单元状态，并且没有先前的单元状态被传播到当前单元状态。换句话说，输入门*I*T2【t】变成了遗忘门的逆，也就是![Gated Recurrent Units](img/B08681_07_95.jpg):

![Gated Recurrent Units](img/B08681_07_96.jpg)![Gated Recurrent Units](img/B08681_07_97.jpg)

现在让我们把所有的方程放到一个地方。GRU 的计算看起来像这样:

![Gated Recurrent Units](img/B08681_07_98.jpg)![Gated Recurrent Units](img/B08681_07_99.jpg)![Gated Recurrent Units](img/B08681_07_101.jpg)![Gated Recurrent Units](img/B08681_07_102.jpg)

这比 LSTMs 紧凑得多。在*图 7.18* 中，我们可以看到 GRU 单元格(左)和 LSTM 单元格(右)并排在一起:

![Gated Recurrent Units](img/B08681_07_106.jpg)

图 7.18:GRU(左)和标准 LSTM(右)的对比



# 总结

在本章中，您了解了 LSTM 网络。首先，我们讨论了什么是 LSTM 及其高级架构。我们还深入研究了 LSTM 中发生的详细计算，并通过一个示例讨论了计算。

我们看到 LSTM 主要由五种不同的东西组成:

*   **单元状态**:LSTM 单元的内部单元状态
*   **隐藏状态**:用于计算预测的外部隐藏状态
*   **输入门**:决定有多少电流输入被读入单元状态
*   **遗忘门**:决定将多少先前的单元状态发送到当前单元状态
*   **输出门**:决定有多少单元状态输出到隐藏状态

拥有如此复杂的结构使得 LSTMs 能够很好地捕捉短期和长期的依赖关系。

我们比较了 lstm 和普通 rnn，发现 lstm 实际上能够学习作为其结构固有部分的长期依赖性，而 rnn 不能学习长期依赖性。之后，我们讨论了 LSTMs 如何利用其复杂的结构求解消失梯度。

然后，我们讨论了几种改进 LSTMs 性能的扩展。首先，一个非常简单的技术，我们称之为贪婪采样，其中，我们不是总是输出最佳候选，而是从一组最佳候选中随机采样一个预测。我们看到这提高了生成文本的多样性。接下来，我们研究了一种更复杂的搜索技术，称为波束搜索。这样，我们不再预测未来的单个时间步，而是预测未来的几个时间步，并挑选出产生最佳联合概率的候选项。另一项改进是了解词向量如何帮助提高 LSTM 的预测质量。使用单词向量，LSTM 可以更有效地学习在预测期间替换语义相似的单词(例如，LSTM 可能会输出*猫*，而不是输出*狗*)，从而导致生成的文本更加真实和正确。我们考虑的最后一个扩展是 BiLSTMs 或双向 LSTMs。BiLSTMs 的一个流行应用是填充短语中缺失的单词。比尔斯特姆从两个方向阅读文本，从开头到结尾，从结尾到开头。这提供了更多的背景信息，因为我们在预测之前既要看过去又要看未来。

最后，我们讨论了普通 LSTMs 的两种变体:窥视孔连接和 GRUs。Vanillan LSTMs 在计算门时，只查看当前输入和隐藏状态。使用窥视孔连接，我们使门计算依赖于所有:当前输入、隐藏和单元状态。

gru 是普通 lstm 的一个更加优雅的变体，它简化了 lstm 而不影响性能。gru 只有两个门和一个状态，而 vanilla LSTMs 有三个门和两个状态。

在下一章中，我们将看到所有这些不同的架构和它们的实现，并看看它们在文本生成任务中的表现。