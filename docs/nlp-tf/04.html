<html><head/><body>

  
    <title>Chapter 4. Advanced Word2vec</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title"><a id="ch04" class="calibre7"/>第四章。高级Word2vec</h1></div></div></div><p class="calibre10">在<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第三章</a>、<em class="calibre15">Word2vec——学习单词嵌入</em>中，我们向大家介绍了Word2vec，学习单词嵌入的基础知识，以及两种常见的Word 2 vec算法:skip-gram和CBOW。在本章中，我们将讨论与Word2vec相关的几个主题，重点是这两个算法和扩展。</p><p class="calibre10">首先，我们将探索原始的skip-gram算法是如何实现的，以及它如何与我们在第3章、<em class="calibre15">Word 2 vec–学习单词嵌入</em>中使用的更现代的变体进行比较。我们将研究skip-gram和CBOW之间的差异，并观察这两种方法随时间的损失行为。我们还将利用我们的观察和现有的文献来讨论哪种方法效果更好。</p><p class="calibre10">我们将讨论对现有Word2vec方法的几个扩展，以提高性能。这些扩展包括使用更有效的采样技术来对负样本进行采样，以及在学习过程中忽略无信息的单词，等等。您还将学习一种新的单词嵌入学习技术，称为<strong class="calibre11">全局向量</strong> ( <strong class="calibre11"> GloVe </strong>)和<a id="id225"/>GloVe相对于skip-gram和CBOW的具体优势。</p><p class="calibre10">最后，您将学习如何使用Word2vec解决一个现实世界的问题:文档分类。我们将通过一个从单词嵌入中获取文档嵌入的简单技巧来了解这一点。</p><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec28" class="calibre7"/>原skip-gram算法</h1></div></div></div><p class="calibre10">到目前为止，本书中讨论的skip-gram算法<a id="id226"/>实际上是对Mikolov等人在2013年发表的原始论文中提出的原始skip-gram算法的改进。在本文中，该算法没有使用中间隐藏层来学习表示。相比之下，原始算法使用了两个不同的嵌入或投影层(图4.1 中的输入和输出嵌入)，并定义了一个源自嵌入本身的成本函数:</p><div><img alt="The original skip-gram algorithm" src="img/B08681_04_01.jpg" class="calibre12"/><div><p class="calibre10">图4.1:没有隐藏层的原始跳格算法</p></div></div><p class="calibre10">原始负<a id="id227"/>抽样损失定义如下:</p><div><img alt="The original skip-gram algorithm" src="img/B08681_04_02.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> v </em>是输入嵌入层，<em class="calibre15">v’</em>是输出单词嵌入层，<img alt="The original skip-gram algorithm" src="img/B08681_04_03.jpg" class="calibre27"/>对应于输入嵌入层中单词<em class="calibre15">w<sub class="calibre17">I</sub>T12】的嵌入向量，<img alt="The original skip-gram algorithm" src="img/B08681_04_04.jpg" class="calibre27"/>对应于输出嵌入层中单词<em class="calibre15">w<sub class="calibre17">I</sub>T17】的单词向量。</em></em></p><div><img alt="The original skip-gram algorithm" src="img/B08681_04_05.jpg" class="calibre12"/></div><p class="calibre10">就是噪声分布，我们从中抽取噪声样本(比如可以简单到<a id="id228"/>从词汇中统一抽取——<em class="calibre15">{ w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">，w </em> <em class="calibre15"> <sub class="calibre17"> j </sub> } </em>，就像我们在<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第三章</a>，<em class="calibre15">Word 2 vec——学习单词嵌入</em>)。最后，<em class="calibre15"> E </em>表示从k-负样本中获得的损失的期望(平均值)。如你所见，除了单词嵌入本身，这个等式中没有权重和偏差。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec35" class="calibre7"/>实现原skip-gram算法</h2></div></div></div><p class="calibre10">实现<a id="id229"/>原始的skip-gram算法不像我们已经实现的版本那样简单。这是因为损失函数需要使用张量流函数手工制作，因为没有内置的函数来计算损失，这与其他算法不同。</p><p class="calibre10">首先，让我们为以下内容定义占位符:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre11">输入数据</strong>:这是一个占位符，包含一批<code class="literal">[batch_size]</code>形状的目标词</li><li class="listitem"><strong class="calibre11">输出数据</strong>:这是一个占位符，包含该批目标词对应的上下文词，大小为<code class="literal">[batch_size, 1]</code> <div> <pre class="programlisting">train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])</pre> </div></li></ul></div><p class="calibre10">定义了输入和输出占位符后，我们可以使用TensorFlow内置的<code class="literal">candidate_sampler</code>对负样本进行采样，如以下代码所示:</p><div><pre class="programlisting">negative_samples, _, _ = tf.nn.log_uniform_candidate_sampler(
                                      train_labels, num_true=1,
                                      num_sampled=num_sampled,
                                      unique=True,
                                      range_max=vocabulary_size)</pre></div><p class="calibre10">在这里，我们对否定词进行统一采样，对不同的词没有任何特殊的偏好。<code class="literal">train_labels</code>是真样本，TensorFlow可以避免产生负样本。然后我们有了<code class="literal">num_true</code>的数量，它表示给定数据点的真实类的数量，也就是1。接下来是我们想要的一批数据的负样本数(<code class="literal">num_sampled</code>)。<code class="literal">unique</code>定义阴性样本是否应该是唯一的。最后，<code class="literal">range</code>定义了一个单词的最大id，这样采样器就不会产生任何无效的单词ID。</p><p class="calibre10">我们摆脱了最大权重和偏见。然后，我们引入两个嵌入层，一个用于输入数据，另一个用于输出数据。需要两个嵌入层，因为如果我们只有一个嵌入层，成本函数将不起作用，正如在<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第3章</a>、<em class="calibre15">Word 2 vec–学习单词嵌入</em>中所讨论的。</p><p class="calibre10">让我们嵌入对输入数据、输出数据和负样本的查找:</p><div><pre class="programlisting">in_embed = tf.nn.embedding_lookup(in_embeddings, train_dataset)
out_embed = tf.nn.embedding_lookup(out_embeddings, tf.reshape(
                                      train_labels,[-1]))
negative_embed = tf.nn.embedding_lookup(out_embeddings,
                                           negative_samples)</pre></div><p class="calibre10">接下来，我们将定义<a id="id230"/>损失函数，它是代码中最重要的部分。这段代码实现了我们前面讨论过的损失函数。然而，正如我们在损失函数<img alt="Implementing the original skip-gram algorithm" src="img/B08681_04_06.jpg" class="calibre27"/>中所定义的，我们不会一次计算文档中所有单词的损失。这是因为文档可能太大而无法完全放入内存。因此，我们在单个时间步计算小批量数据的损失。完整代码可在位于<code class="literal">ch4</code>文件夹中的<code class="literal">ch4_word2vec_improvements.ipynb</code>练习册中找到:</p><div><pre class="programlisting"># Computing the loss for the positive sample
loss = tf.reduce_mean(
    tf.log(
        tf.nn.sigmoid(
            tf.reduce_sum(
                tf.diag([1.0 for _ in range(batch_size)])*
                tf.matmul(out_embed,tf.transpose(in_embed)),
            axis=0)
        )
    )
)

# Computing loss for the negative samples
loss += tf.reduce_mean(
    tf.reduce_sum(
        tf.log(tf.nn.sigmoid(
            -tf.matmul(negative_embed,tf.transpose(in_embed)))),
        axis=0
    )
)</pre></div><div><div><h3 class="title4"><a id="note22" class="calibre7"/>注意</h3><p class="calibre16">Tensorflow通过从完整的softmax权重和偏差中定义一个更小的权重和偏差子集来实现<code class="literal">sampled_softmax_loss</code>,这些权重和偏差只需要处理当前的一批数据。此后，TensorFlow会以类似于标准softmax交叉熵计算的方式计算损失。然而，我们不能直接翻译这种方法来计算原始跳格损失，因为没有softmax权重和偏差。</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec36" class="calibre7"/>比较原始跳转图和改进的跳转图</h2></div></div></div><p class="calibre10">我们应该有一个很好的<a id="id231"/>理由来使用一个<a id="id232"/>隐藏层，而不是原来的skip-gram算法不使用隐藏层。因此，我们将在<em class="calibre15">图4.2 </em>中观察原始skip-gram算法和包含隐藏层的skip-gram算法的损失函数行为:</p><div><img alt="Comparing the original skip-gram with the improved skip-gram" src="img/B08681_04_07.jpg" class="calibre12"/><div><p class="calibre10">图4.2:原始跳格算法与改进的跳格算法</p></div></div><p class="calibre10">我们可以清楚地看到，<a id="id233"/>有一个隐藏层<a id="id234"/>会比没有隐藏层带来更好的性能。这也表明更深Word2vec模型往往性能更好。</p></div></div></div>



  
    <title>Comparing skip-gram with CBOW</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec29" class="calibre7"/>比较跳跃图和CBOW</h1></div></div></div><p class="calibre10">在查看<a id="id235"/>性能差异和调查原因之前，让我们提醒自己skip-gram和CBOW方法之间的根本区别。</p><p class="calibre10">如下图所示，给定一个上下文和一个目标单词，skip-gram只观察单个输入/输出元组中的目标单词和上下文的单个单词。然而，CBOW在单个样本中观察目标单词和上下文中的所有单词。例如，如果我们假设短语<em class="calibre15">狗对着邮差</em>吠叫，skip-gram会在单个时间步长看到一个输入输出元组，如<em class="calibre15"> ["狗"，" at"] </em>，而CBOW会看到一个输入输出元组<em class="calibre15"> ["狗"，"吠叫"，"邮差"，" at"] </em>。因此，在给定的一批数据中，CBOW比skip-gram接收到更多关于给定单词上下文的信息。接下来让我们看看这种差异如何影响这两种算法的性能。</p><p class="calibre10">如上图所示，与skip-gram算法相比，CBOW模型在给定时间可以访问更多的信息(输入),从而使CBOW在特定条件下表现更好。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec37" class="calibre7"/>性能对比</h2></div></div></div><p class="calibre10">现在，让我们绘制skip-gram和CBOW在<a id="id236"/>时间上的损失，我们在前面的<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第3章</a>、<em class="calibre15">Word 2 vec–学习单词嵌入</em>中训练了模型(参见<em class="calibre15">图4.4 </em>):</p><div><img alt="Performance comparison" src="img/B08681_04_10.jpg" class="calibre12"/><div><p class="calibre10">图4.5:损失减少:skip-gram对比CBOW</p></div></div><p class="calibre10">我们讨论过，与skip-gram算法相比，对于给定的输入-输出元组，CBOW可以访问关于给定目标词的上下文的更多信息。我们可以看到，与skip-gram模型相比，CBOW显示了损耗的快速下降。然而，损失本身并不能充分衡量绩效，因为损失会因过度适应训练数据而迅速减少。虽然有一些基准测试任务用于评估单词嵌入的质量(例如，单词类比任务)，但是我们将使用一种更简单的检查方法。让我们直观地检查一下学习到的嵌入，以确保skip-gram和CBOW在它们之间显示出显著的语义差异。为此，我们使用一种流行的可视化技术<a id="id237"/>，称为<strong class="calibre11">t-分布式随机邻居嵌入</strong> ( <strong class="calibre11"> t-SNE </strong>)。</p><div><div><h3 class="title4"><a id="note23" class="calibre7"/>注</h3><p class="calibre16">应当注意，损失的减少不是评估单词嵌入系统的性能的非常令人信服的度量，因为我们用来测量损失的采样softmax严重低估了完整的softmax损失。单词嵌入的表现通常根据单词类比任务来评估。一个典型的单词类比任务可能会这样问:</p><p class="calibre16">意识到没有意识到就像印象深刻到______________。</p><p class="calibre16">所以，一个好的嵌入设置应该用<em class="calibre15">不起眼的</em>来回答这个问题。这可以通过<code class="literal">embedding(impressive) - [embedding(aware) - embedding(unaware)]</code>给出的简单算术运算来计算。如果得到的向量有单词<code class="literal">unimpressive</code>的嵌入作为它的最近邻，那么你就获得了正确的答案。</p><p class="calibre16">有几个词语类比<a id="id238"/>测试数据集可用，例如下面的<a id="id239"/>:</p><div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre11">谷歌类比数据集</strong>:<a href="http://download.tensorflow.org/data/questions-words.txt">http://download.tensorflow.org/data/questions-words.txt</a></li><li class="listitem"><strong class="calibre11">更大的类比测试集(蝙蝠)</strong>:<a href="http://vsm.blackbird.pw/bats">http://vsm.blackbird.pw/bats</a></li></ul></div></div></div><p class="calibre10">在<em class="calibre15">图4.6 </em>中，我们<a id="id240"/>可以看到CBOW比skip-gram更倾向于将单词聚集在一起，在skip-gram中，单词似乎稀疏地分布在整个空间中。因此，我们可以说，与skip-gram相比，CBOW在视觉上更具吸引力，在这个特定的示例中:</p><div><img alt="Performance comparison" src="img/B08681_04_11.jpg" class="calibre12"/><div><p class="calibre10">图4.6:使用skip-gram和CBOW获得的单词向量的t-SNE可视化</p></div></div><div><div><h3 class="title4"><a id="note24" class="calibre7"/>注意</h3><p class="calibre16">我们将使用scikit-learn提供的t-SNE算法来计算低维表示，然后通过Matplotlib将其可视化。然而，TensorFlow通过其可视化框架TensorBoard提供了一个更加方便的嵌入可视化选项。你可以在位于<code class="literal">appendix</code>文件夹的<code class="literal">tensorboard_word_embeddings.ipynb</code>中找到这个练习。</p></div></div><div><div><h3 class="title4"><a id="tip02" class="calibre7"/>提示</h3><p class="calibre16"><strong class="calibre11">t-SNE——一次简短的旅行</strong></p><p class="calibre16">t-SNE是一种可视化<a id="id241"/>技术，可以在较低的二维空间中可视化高维数据(例如，图像和单词嵌入)。我们不会深入研究该技术背后的所有复杂数学，而只会在更直观的层面上理解算法是如何工作的。</p><p class="calibre16">让我们首先定义符号。<img alt="Performance comparison" src="img/B08681_04_12.jpg" class="calibre27"/>表示一个<em class="calibre15"> D </em>维数据点，而<img alt="Performance comparison" src="img/B08681_04_13.jpg" class="calibre27"/>是输入空间。例如，这可以是一个单词嵌入向量，类似于我们在<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第3章</a>、<em class="calibre15">Word 2 vec–学习单词嵌入</em>中所涉及的向量，而<em class="calibre15"> D </em>是嵌入大小。接下来，我们假设一个假想的二维空间<img alt="Performance comparison" src="img/B08681_04_14.jpg" class="calibre27"/>，其中<em class="calibre15">y<sub class="calibre17">I</sub>T14】表示与<em class="calibre15">x<sub class="calibre17">I</sub>T18】数据点对应的二维向量；<em class="calibre15"> X </em>和<em class="calibre15"> Y </em>一一对应。我们将<em class="calibre15"> Y </em>称为地图空间，<em class="calibre15">Y<sub class="calibre17">I</sub>T28】称为地图点。</em></em></em></p><p class="calibre16">现在让我们定义一个条件概率<img alt="Performance comparison" src="img/B08681_04_15.jpg" class="calibre27"/>，它定义了<em class="calibre15">x<sub class="calibre17">I</sub>T33】数据点选择<em class="calibre15">x<sub class="calibre17">j</sub>T37】作为其邻居的概率。<em class="calibre15">P</em><em class="calibre15"><sub class="calibre17">j | I</sub></em>当点<em class="calibre15"> x <sub class="calibre17"> j </sub> </em>远离<em class="calibre15"> x <sub class="calibre17"> i </sub> </em>时需要为低，反之亦然。对<img alt="Performance comparison" src="img/B08681_04_16.jpg" class="calibre27"/>的直观选择是以<em class="calibre15">x<sub class="calibre17">I</sub>T56】数据点为中心的高斯分布，方差为<img alt="Performance comparison" src="img/B08681_04_17.jpg" class="calibre27"/>。对于具有密集<a id="id242"/>邻域的数据点，方差较低，而对于具有稀疏邻域的数据点，方差较高。具体地，条件概率的公式由下式给出:</em></em></em></p><div><img alt="Performance comparison" src="img/B08681_04_18.jpg" class="calibre12"/></div></div></div><div><div><h3 class="title4"><a id="tip03" class="calibre7"/>提示</h3><p class="calibre16">同样，我们可以为空间中的图点<em class="calibre15">Y<sub class="calibre17">I</sub>T63】定义一个类似的条件概率<em class="calibre15"> Y </em>，<img alt="Performance comparison" src="img/B08681_04_19.jpg" class="calibre27"/>。</em></p><p class="calibre16">现在，为了获得高维空间<em class="calibre15"> X </em>、<img alt="Performance comparison" src="img/B08681_04_20.jpg" class="calibre27"/>和<img alt="Performance comparison" src="img/B08681_04_21.jpg" class="calibre27"/>的良好低维表示<em class="calibre15"> Y </em>应该演示类似的行为。也就是说，如果两个数据点在<em class="calibre15"> X </em>空间中相似，那么它们在<em class="calibre15"> Y </em>空间中也应该相似，反之亦然。因此，获得数据的良好二维表示的问题归结为最小化所有<img alt="Performance comparison" src="img/B08681_04_24.jpg" class="calibre27"/>的<img alt="Performance comparison" src="img/B08681_04_22.jpg" class="calibre27"/>和<img alt="Performance comparison" src="img/B08681_04_23.jpg" class="calibre27"/>之间的不匹配。</p><p class="calibre16">这个问题可以正式表述为最小化<img alt="Performance comparison" src="img/B08681_04_25.jpg" class="calibre27"/>和之间的Kullback-Leibler散度</p><div><img alt="Performance comparison" src="img/B08681_04_26.jpg" class="calibre12"/></div><p class="calibre16">由<img alt="Performance comparison" src="img/B08681_04_27.jpg" class="calibre27"/>表示。因此，我们问题的成本函数是:</p><div><img alt="Performance comparison" src="img/B08681_04_28.jpg" class="calibre12"/></div><p class="calibre16">同样，通过利用随机梯度下降来最小化这个<em class="calibre15"> C </em>成本，我们可以找到与<em class="calibre15"> X </em>紧密匹配的最优表示<em class="calibre15"> Y </em>。</p><p class="calibre16">直观上，这个过程可以被认为是所有数据点对之间连接的弹簧集合达到的平衡。<img alt="Performance comparison" src="img/B08681_04_29.jpg" class="calibre27"/>是弹簧在<em class="calibre15">x<sub class="calibre17">I</sub>T26】和<em class="calibre15">x<sub class="calibre17">j</sub>T30】数据点之间的刚度。因此，当<em class="calibre15">x<sub class="calibre17">I</sub>T34】和<em class="calibre15">x<sub class="calibre17">j</sub>T38】相似时，它们会在不相似时保持彼此靠近和远离。因此，特定数据点的<em class="calibre15"> C </em>作为作用在该数据点上的合力，并根据该合力使其吸引或排斥所有其他数据点。</em></em></em></em></p></div></div></div><div><div><div><div><h2 class="title3">skip-gram和CBOW谁是赢家？</h2></div></div></div><p class="calibre10">谈到性能，skip-gram和CBOW之间没有明确的赢家<a id="id244"/>。例如，论文<em class="calibre15">单词和短语的分布式表示及其组合性</em>，<em class="calibre15"> Mikolov等人</em>，<em class="calibre15"> 2013 </em>提出skip-gram在语义任务中工作得更好，而CBOW在句法任务中工作得更好。然而，skip-gram在大多数任务中表现得比CBOW更好，这与我们的发现相矛盾。</p><p class="calibre10">各种经验证据表明，与CBOW相比，skip-gram在大型数据集上工作得很好，这在单词和短语及其组合性的<em class="calibre15">分布式表示中得到支持</em>、<em class="calibre15">米科洛夫等人</em>、<em class="calibre15"> 2013 </em>和<em class="calibre15"> GloVe:单词表示的全局向量</em>、<em class="calibre15"> Pennington等人</em>、<em class="calibre15"> 2014 </em>，它们通常使用数十亿个单词的语料库。然而，我们的任务涉及几十万字，这是比较小的。出于这个原因，CBOW的表现可能会更好。</p><p class="calibre10">现在让我解释一下为什么我认为是这样。考虑下面两句话:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">天气不错</em></li><li class="listitem">这是辉煌的一天</li></ul></div><p class="calibre10">对于CBOW，输入-输出元组如下所示:</p><p class="calibre10"><em class="calibre15">[[它，是，尼斯，天]，一] </em></p><p class="calibre10"><em class="calibre15">[[它，是，辉煌，天]，一] </em></p><p class="calibre10">skip-gram的输入输出元组如下所示:</p><p class="calibre10"><em class="calibre15">【它，一个】，【是，一个】，【尼斯，一个】，【日，一个】</em></p><p class="calibre10"><em class="calibre15">【它，一】，【是，一】，【辉煌，一】，【天，一】</em></p><p class="calibre10">我们希望我们的模型理解<em class="calibre15"> nice </em>和<em class="calibre15"> brilliant </em>是稍微不同的东西(也就是说，brilliant意味着比nice更好)。这种意义上有细微差别的词叫做<em class="calibre15">细微差别</em>。我们可以看到，对于CBOW来说，它很有可能将<em class="calibre15">辉煌</em>和<em class="calibre15">美好</em>视为同一事物，因为它们的语义被周围的词(<em class="calibre15"> It </em>、<em class="calibre15">是</em>和<em class="calibre15"> day </em>)平均化，因为这些词也是输入的一部分。相比之下，对于skip-gram，单词<em class="calibre15"> nice </em>和<em class="calibre15"> brilliant </em>与<em class="calibre15"> It </em>、<em class="calibre15"> is </em>和<em class="calibre15"> day </em>分开出现，这使得skip-gram比CBOW更注意单词之间的细微差异(例如<em class="calibre15"> brilliant </em>和<em class="calibre15"> nice </em>)。</p><p class="calibre10">但是，请注意，我们的模型中有数百万个参数。要训练这样的模型，需要大量的数据。CBOW以某种方式规避了这个问题，它试图不专注于学习单词之间的细微差异，而是对给定上下文中的所有单词进行平均(例如，平均语义为<em class="calibre15">这是美好的一天</em>或<em class="calibre15">这是美好的一天</em>)。然而，skip-gram将学习更细致的表示，因为在CBOW中没有平均效应。为了学习细致的表达，skip-gram需要更多的数据。但是一旦提供了更多的数据，skip-gram很可能会胜过CBOW算法。</p><p class="calibre10">此外，注意，CBOW模型的单个输入大约等于skip-gram模型的<img alt="Which is the winner, skip-gram or CBOW?" src="img/B08681_04_30.jpg" class="calibre27"/>个输入，其中<em class="calibre15"> m </em>是上下文窗口大小。这是因为skip-gram的单个输入只包含一个单词，而CBOW的单个输入有<img alt="Which is the winner, skip-gram or CBOW?" src="img/B08681_04_31.jpg" class="calibre27"/>多个单词。因此，为了进行更公平的比较，如果我们对<em class="calibre15"> L </em>步运行CBOW，我们应该对<img alt="Which is the winner, skip-gram or CBOW?" src="img/B08681_04_32.jpg" class="calibre27"/>步运行skip-gram算法。</p><p class="calibre10">到目前为止，您已经了解了skip-gram最初是如何实现的——它有两个嵌入层(一个用于查找输入单词，另一个用于查找输出单词)。我们讨论了在<a href="ch03.html" title="Chapter 3. Word2vec – Learning Word Embeddings">第3章</a>、<em class="calibre15">Word 2 vec–学习单词嵌入</em>中讨论的跳格算法实际上是对原始跳格算法的改进。我们看到改进的skip-gram实际上优于原始算法。然后，我们比较了skip-gram和CBOW的性能，发现在我们的示例中，CBOW的性能更好。最后，我们讨论了CBOW性能优于skip-gram的一些原因。</p></div></div>



  
    <title>Extensions to the word embeddings algorithms</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec30" class="calibre7"/>单词嵌入算法的扩展</h1></div></div></div><p class="calibre10">Mikolov <a id="id246"/>等人在2013年发表的原始论文讨论了几种可以进一步提高单词嵌入学习算法性能的扩展。尽管最初引入它们是为了用于skip-gram，但是它们也可以扩展到CBOW。此外，正如我们已经看到的，在我们的例子中，CBOW优于skip-gram算法，我们将使用CBOW来理解所有的扩展。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec39" class="calibre7"/>使用一元分布进行负采样</h2></div></div></div><p class="calibre10">已经发现，当通过从某些分布而不是从均匀分布进行采样时，负采样的性能<a id="id247"/>结果更好。一种这样的分布是<a id="id248"/>单字母分布<strong class="calibre11">。一个单词的单字概率由下面的等式给出:</strong></p><div><img alt="Using the unigram distribution for negative sampling" src="img/B08681_04_33.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15">count</em>(<em class="calibre15">w<sub class="calibre17">I</sub></em>)是<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>在文档中出现的次数。对于某个常数<em class="calibre15"> Z </em>来说，当一元分布被扭曲为<img alt="Using the unigram distribution for negative sampling" src="img/B08681_04_34.jpg" class="calibre27"/>时，它已经显示出比均匀分布或标准一元分布提供更好的性能。</p><p class="calibre10">让我们用一个例子来<a id="id249"/>更好地理解单字分布<a id="id250"/>。考虑下面的句子:</p><p class="calibre10">鲍勃是个足球迷。他是学校足球队的成员。</p><p class="calibre10">这里，单词<em class="calibre15"> football </em>的单字概率如下:</p><div><img alt="Using the unigram distribution for negative sampling" src="img/B08681_04_35.jpg" class="calibre12"/></div><p class="calibre10">可以看出，常见单词的单字概率会更高。这些常用词往往是很没信息的词，比如<em class="calibre15">的</em>、<em class="calibre15">的一个</em>、<em class="calibre15">的就是</em>。因此，在成本优化期间，这种频繁的词将被更多地负采样，导致更多信息的词被更少地负采样。因此，这在优化期间在常用词和罕见词之间创建了一个平衡，从而带来更好的性能。</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec40" class="calibre7"/>实现基于单字的负采样</h2></div></div></div><p class="calibre10">在这里，我们将看到如何使用TensorFlow来实现基于单字的负采样:</p><div><pre class="programlisting">unigrams = [0 for _ in range(vocabulary_size)]
for word,w_count in count:
    w_idx = dictionary[word]
    unigrams[w_idx] = w_count*1.0/token_count
    word_count_dictionary[w_idx] = w_count</pre></div><p class="calibre10">这里，<code class="literal">count</code>是元组列表，其中每个元组由<code class="literal">(word ID, frequency)</code>组成。该算法计算每个单词的单字概率，并将其作为按单词索引排序的列表返回。(这是TensorFlow规定的unigrams的特定格式)。这可以在<code class="literal">ch4</code>文件夹中的<code class="literal">ch4_word2vec_improvements.ipynb</code>中作为练习使用。</p><p class="calibre10">接下来，我们计算到<a id="id252"/>嵌入查找，就像我们通常对CBOW所做的那样:</p><div><pre class="programlisting">train_dataset = tf.placeholder(tf.int32, shape=[batch_size,
    window_size*2])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

# Variables.
# embedding, vector for each word in the vocabulary
embeddings = tf.Variable(tf.random_uniform([vocabulary_size,
    embedding_size], -1.0, 1.0, dtype=tf.float32))
softmax_weights =
    tf.Variable(tf.truncated_normal([vocabulary_size,
    embedding_size],
    stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float32))
softmax_biases =
    tf.Variable(tf.zeros([vocabulary_size], dtype=tf.float32))

stacked_embedings = None

for i in range(2*window_size):
    embedding_i = tf.nn.embedding_lookup(embeddings,
    train_dataset[:,i])
    x_size,y_size = embedding_i.get_shape().as_list()
    if stacked_embedings is None:
        stacked_embedings =
            tf.reshape(embedding_i,[x_size,y_size,1])
    else:
        stacked_embedings =
            tf.concat(axis=2,values=[stacked_embedings,
            tf.reshape(embedding_i,[x_size,y_size,1])])
mean_embeddings = tf.reduce_mean(stacked_embedings,2,keepdims=False)</pre></div><p class="calibre10">接下来，我们将基于一元分布抽取负样本。为此，我们将使用TensorFlow内置函数<code class="literal">tf.nn.fixed_unigram_candidate_sampler</code>:</p><div><pre class="programlisting">candidate_sampler = tf.nn.fixed_unigram_candidate_sampler(
    true_classes = tf.cast(train_labels, dtype=tf.int64),
    num_true = 1, num_sampled = num_sampled, unique = True,
    range_max = vocabulary_size, distortion=0.75,
    num_reserved_ids=0, unigrams=unigrams, name='unigram_sampler')

loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights,
    biases=softmax_biases, inputs=mean_embeddings,
    labels=train_labels, num_sampled=num_sampled,
    num_classes=vocabulary_size, sampled_values=candidate_sampler))</pre></div><pre> sampling. Generally, the following steps take place:</pre><div><ol class="orderedlist"><li class="listitem1">定义变量、占位符和超参数。</li><li class="listitem1">对于每一批数据，发生以下情况:<div> <ol class="orderedlist1"> <li class="listitem1">通过查找上下文窗口的每个索引的嵌入并对它们求平均来计算平均输入嵌入矩阵</li> <li class="listitem1">通过负采样来计算损失，根据一元分布进行采样</li> <li class="listitem1">使用随机梯度下降</li> </ol> </div>优化神经网络】</li></ol></div><p class="calibre10">从前面的代码片段中提取的以下一行代码在此算法中起着最重要的作用，它根据扭曲的unigram分布生成负样本:</p><div><pre class="programlisting">candidate_sampler = tf.nn.fixed_unigram_candidate_sampler(
    true_classes = tf.cast(train_labels,dtype=tf.int64),
    num_true = 1, num_sampled = num_sampled, unique = True,
    range_max = vocabulary_size, distortion=0.75,
    num_reserved_ids=0, unigrams=unigrams, name='unigram_sampler')</pre></div><pre>t as 10,000:</pre><div><pre class="programlisting">subsampled_data = []
for w_i in data:
    p_w_i = 1 - np.sqrt(1e5/word_count_dictionary[w_i])

    if np.random.random() &lt; p_w_i:
        drop_count += 1
        drop_examples.append(reverse_dictionary[w_i])
    else:
        subsampled_data.append(w_i)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec43" class="calibre7"/>比较CBOW及其扩展</h2></div></div></div><p class="calibre10">在<em class="calibre15">图4.6 </em>中，我们将<a id="id256"/>看到CBOW、基于单字的<a id="id257"/>负采样CBOW—<strong class="calibre11">CBOW(单字)</strong>—以及基于单字的负采样和<a id="id258"/>子采样CBOW—<strong class="calibre11">CBOW(单字+子采样)</strong>的不同损失减少:</p><div><img alt="Comparing the CBOW and its extensions" src="img/B08681_04_38.jpg" class="calibre12"/><div><p class="calibre10">图4.6:原始CBOW和两个扩展CBOW的损失行为</p></div></div><p class="calibre10">有趣的是，我们看到<a id="id259"/>与仅使用基于单gram的负采样相比，使用单gram和子采样改进可以得到相似的整体外观损失值。然而，这不应该被误解为子采样在学习问题上缺乏优势。这种特殊行为的原因可以理解如下。与二次抽样一样，我们去掉了许多没有信息的单词，因此文本的质量提高了(就信息质量而言)。这反过来又增加了学习问题的难度。在之前的问题设置中，单词向量有机会在优化过程中利用大量无信息的单词，而在新的问题设置中，这样的机会很少。这导致较高的损失，但语义上健全的词向量。</p></div></div>



  
    <title>More recent algorithms extending skip-gram and CBOW</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec31" class="calibre7"/>扩展skip-gram和CBOW的最新算法</h1></div></div></div><p class="calibre10">我们已经看到，Word2vec <a id="id260"/>技术在捕捉单词的语义<a id="id261"/>方面非常强大。然而，它们也不是没有局限性。例如，他们没有注意上下文单词和目标单词之间的距离。但是，如果上下文单词离目标单词更远，它对目标单词的影响应该更小。因此，我们将讨论分别关注上下文中不同位置的技术。Word2vec的另一个限制是，在计算单词向量时，它只关注给定单词周围非常小的窗口。然而，实际上，应该考虑单词在整个语料库中共现的方式来计算好的单词向量。因此，我们将研究一种技术，它不仅查看单词的上下文，还查看单词的全局共现信息。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec44" class="calibre7"/>skip-gram算法的局限性</h2></div></div></div><p class="calibre10">之前讨论的skip-gram <a id="id262"/>算法及其所有变体忽略了给定上下文中上下文单词的定位。换句话说，skip-gram不利用上下文单词在上下文中的确切位置，而是平等地对待给定上下文中的所有单词。例如，让我们考虑一个句子:</p><p class="calibre10">狗对着邮递员吠叫。</p><p class="calibre10">让我们考虑一个大小为2的窗口和目标词，<em class="calibre15"/>叫了起来。那么单词<em class="calibre15">吠叫</em>的上下文将是<em class="calibre15">的</em>、<em class="calibre15">的狗</em>、<em class="calibre15">的</em>和<em class="calibre15">的</em>。同样，我们将组成四个数据点<em class="calibre15">("汪汪"，" the") </em>，<em class="calibre15">("汪汪"，"狗")</em>，<em class="calibre15">("汪汪"，" at") </em>，<em class="calibre15">("汪汪"，" the") </em>，其中元组的第一个元素是输入单词，第二个是输出单词。如果我们考虑这个集合中的两个数据点，<em class="calibre15">(“汪汪”、“the”)</em>和<em class="calibre15">(“汪汪”、“狗”)</em>，那么原始的skip-gram算法将在优化期间平等地对待这两个元组。换句话说，skip-gram忽略了上下文单词在上下文中的实际位置。然而，从语言学的角度来看，显然元组<em class="calibre15">("汪汪"，"狗")</em>比<em class="calibre15">("汪汪"，" the") </em>承载了更多的信息。本质上，结构化跳过程序算法试图解决这一限制。让我们在下一节看看这是如何解决的。</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec45" class="calibre7"/>结构化跳格算法</h2></div></div></div><p class="calibre10">结构化跳格算法<a id="id263"/>使用图4.7 中<em class="calibre15">所示的架构来解决前一节讨论的原始跳格算法的局限性:</em></p><div><img alt="The structured skip-gram algorithm" src="img/B08681_04_39.jpg" class="calibre12"/><div><p class="calibre10">图4.7。结构化跳格模型</p></div></div><p class="calibre10">如此处所示，结构化<a id="id264"/>跳过程序在优化过程中保留了上下文单词的结构或定位。然而，它提出了更高的内存需求，因为参数的数量线性地依赖于窗口大小。更准确地说，对于窗口大小<em class="calibre15"> m </em>(即，在一侧)，如果原始跳格模型在softmax层中具有<em class="calibre15"> P </em>参数，则结构化跳格算法将具有<img alt="The structured skip-gram algorithm" src="img/B08681_04_40.jpg" class="calibre27"/>参数，因为我们对于上下文窗口中的每个位置都具有一组<em class="calibre15"> P </em>参数。</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec46" class="calibre7"/>损失函数</h2></div></div></div><p class="calibre10">跳格模型的原始负采样<a id="id265"/> softmax损耗如下所示:</p><div><img alt="The loss function" src="img/B08681_04_41.jpg" class="calibre12"/></div><p class="calibre10">对于结构化跳过程序，我们使用以下损失:</p><div><img alt="The loss function" src="img/B08681_04_42.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="The loss function" src="img/B08681_04_43.jpg" class="calibre27"/>是使用与<em class="calibre15">w<sub class="calibre17">j</sub>T10】位置的索引相对应的第<em class="calibre15">p</em>T5】th</em>组softmax权重和softmax偏差来计算的。</p><p class="calibre10">这是如下面的代码所示实现的，该代码位于<code class="literal">ch4</code>文件夹的<code class="literal">ch4_word2vec_extended.ipynb</code>中。如我们所见，我们现在有<img alt="The loss function" src="img/B08681_04_44.jpg" class="calibre27"/> softmax权重和偏差，并且对应于每个上下文位置的嵌入向量通过它们对应的softmax权重和偏差传播。</p><p class="calibre10">首先，我们将定义输入和输出占位符:</p><div><pre class="programlisting">train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = [tf.placeholder(tf.int32, shape=[batch_size, 1]) for _ in range(2*window_size)]</pre></div><p class="calibre10">然后，我们将定义计算损失所需的计算，从训练输入和标签开始:</p><div><pre class="programlisting"># Variables.
embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size],
    -1.0, 1.0))
softmax_weights = [tf.Variable(
    tf.truncated_normal([vocabulary_size, embedding_size],
    stddev=0.5 / math.sqrt(embedding_size))) for _ in range(2*window_size)]
softmax_biases =
    [tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01)) for _ in range(2*window_size)]

# Model.
# Look up embeddings for inputs.
embed = tf.nn.embedding_lookup(embeddings, train_dataset)
# Compute the softmax loss, using a sample of 
# the negative labels each time.
loss = tf.reduce_sum(
    [
        tf.reduce_mean(tf.nn.sampled_softmax_loss(
            weights=softmax_weights[wi],
            biases=softmax_biases[wi], inputs=embed,
            labels=train_labels[wi], num_sampled=num_sampled,
            num_classes=vocabulary_size))
        for wi in range(window_size*2)
    ]
)</pre></div><p class="calibre10">结构化跳格图<a id="id266"/>解决了标准跳格图算法的一个重要限制，即在学习过程中注意上下文单词的位置。这是通过为上下文的每个位置引入单独的一组softmax权重和偏差来实现的。这导致了性能的提高，但是由于参数数量的增加，这需要很高的存储空间。接下来，我们将看到CBOW模型的类似扩展。</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec47" class="calibre7"/>连续窗口模式</h2></div></div></div><p class="calibre10">连续窗口模型<a id="id267"/>以类似于结构化跳格算法的方式扩展了CBOW算法。在最初的CBOW算法中，在通过softmax层传播之前，对为所有上下文单词找到的嵌入进行平均。然而，在连续窗口模型中，不是对嵌入进行平均，而是将它们连接起来，从而产生<img alt="The continuous window model" src="img/B08681_04_45.jpg" class="calibre27"/>长的嵌入向量，其中<img alt="The continuous window model" src="img/B08681_04_46.jpg" class="calibre27"/>是CBOW算法的原始嵌入大小。<em class="calibre15">图4.8 </em>说明了连续窗口模型:</p><div><img alt="The continuous window model" src="img/B08681_04_47.jpg" class="calibre12"/><div><p class="calibre10">图4.8:连续窗口模型</p></div></div><p class="calibre10">在本节中，我们讨论了skip-gram和CBOW的两个<a id="id268"/>扩展算法。这两种变体本质上利用了单词在上下文中的位置，而不是平等地对待给定上下文中的所有单词。接下来，我们将讨论一种新引入的单词嵌入学习算法，称为GloVe。我们将看到GloVe克服了skip-gram和CBOW的某些限制。</p></div></div>



  
    <title>GloVe – Global Vectors representation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec32" class="calibre7"/>手套-全局向量表示</h1></div></div></div><p class="calibre10">学习单词向量的方法分为两类:基于全局矩阵分解的方法或基于局部上下文窗口的方法。<strong class="calibre11">潜在语义分析</strong> ( <strong class="calibre11"> LSA </strong> ) <a id="id270"/>是基于全局矩阵分解的方法的一个例子，skip-gram和CBOW是基于局部上下文窗口的方法。LSA被用作一种文档分析技术，它将文档中的单词映射到称为<strong class="calibre11">概念</strong>的东西，这是文档中出现的常见单词模式。基于全局矩阵分解的方法有效地利用了语料库的全局统计数据(例如，单词在全局范围内的共现)，但是在单词类比任务中表现不佳。另一方面，基于上下文窗口的方法在单词类比任务中表现良好，但是没有利用语料库的全局统计，留下了改进的空间。GloVe试图获得两个世界的最佳结果——一种有效利用全局语料库统计数据的方法，同时以类似于skip-gram或CBOW的基于上下文窗口的方式优化学习模型。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec48" class="calibre7"/>理解手套</h2></div></div></div><p class="calibre10">在查看GloVe的实现细节之前，让我们花时间了解GloVe背后的基本思想。为此，让我们<a id="id272"/>考虑一个例子:</p><div><ol class="orderedlist"><li class="listitem1">考虑单词<img alt="Understanding GloVe" src="img/B08681_04_48.jpg" class="calibre27"/>和<img alt="Understanding GloVe" src="img/B08681_04_49.jpg" class="calibre27"/></li><li class="listitem1">定义一个任意的探测字<em class="calibre15"> k </em></li><li class="listitem1">将<img alt="Understanding GloVe" src="img/B08681_04_50.jpg" class="calibre27"/>定义为单词<em class="calibre15"> i </em>和<em class="calibre15"> k </em>彼此靠近出现的概率，将<img alt="Understanding GloVe" src="img/B08681_04_51.jpg" class="calibre27"/>定义为单词<em class="calibre15"> j </em>和<em class="calibre15"> k </em>一起出现的概率</li></ol></div><p class="calibre10">现在让我们来看看<img alt="Understanding GloVe" src="img/B08681_04_52.jpg" class="calibre27"/>实体对于<em class="calibre15"> k </em>的不同值如何表现。</p><p class="calibre10">对于<img alt="Understanding GloVe" src="img/B08681_04_53.jpg" class="calibre27"/>，与<em class="calibre15"> i </em>一起出现的可能性很大，因此<img alt="Understanding GloVe" src="img/B08681_04_54.jpg" class="calibre27"/>会高。但是，<em class="calibre15"> k </em>不会经常与<em class="calibre15"> j </em>一起出现，导致<img alt="Understanding GloVe" src="img/B08681_04_55.jpg" class="calibre27"/>低。因此，我们得到下面的表达式:</p><div><img alt="Understanding GloVe" src="img/B08681_04_56.jpg" class="calibre12"/></div><p class="calibre10">接下来，对于<img alt="Understanding GloVe" src="img/B08681_04_57.jpg" class="calibre27"/>，不太可能<a id="id273"/>出现在<em class="calibre15"> i </em>附近，因此会有低<img alt="Understanding GloVe" src="img/B08681_04_58.jpg" class="calibre27"/>；但是，由于<em class="calibre15"> k </em>与<em class="calibre15"> j </em>高度相关，因此<img alt="Understanding GloVe" src="img/B08681_04_59.jpg" class="calibre27"/>的值会很高。这导致了以下情况:</p><div><img alt="Understanding GloVe" src="img/B08681_04_60.jpg" class="calibre12"/></div><p class="calibre10">现在，对于像<img alt="Understanding GloVe" src="img/B08681_04_61.jpg" class="calibre27"/>这样的词，它与<em class="calibre15"> i </em>和<em class="calibre15"> j </em>都有很强的关系，或者<img alt="Understanding GloVe" src="img/B08681_04_62.jpg" class="calibre27"/>，其中<em class="calibre15"> i </em>和<em class="calibre15"> j </em>都有最小的相关性，我们得到这样:</p><div><img alt="Understanding GloVe" src="img/B08681_04_63.jpg" class="calibre12"/></div><p class="calibre10">可见，通过测量两个相互接近的词出现的频率来计算的<img alt="Understanding GloVe" src="img/B08681_04_64.jpg" class="calibre27"/>实体，是一种很好的测量词与词之间关系的手段。因此，它成为学习单词向量的一个很好的候选。因此，定义损失函数的良好起点如下所示:</p><div><img alt="Understanding GloVe" src="img/B08681_04_65.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> F </em>是一些函数。从这一点出发，<a id="id274"/>原始文件经过精心推导，得出如下损失函数:</p><div><img alt="Understanding GloVe" src="img/B08681_04_66.jpg" class="calibre12"/></div><p class="calibre10">这里的<img alt="Understanding GloVe" src="img/B08681_04_67.jpg" class="calibre27"/>、<img alt="Understanding GloVe" src="img/B08681_04_68.jpg" class="calibre27"/>是<em class="calibre15"> j </em>这个词在<em class="calibre15"> i </em>这个词的上下文中出现的频率。另外，<em class="calibre15">w<sub class="calibre17">I</sub>T35】和<em class="calibre15">b</em>T38】I</em>分别表示从输入嵌入中获得的单词<em class="calibre15"> i </em>的单词嵌入和偏差嵌入。并且，<img alt="Understanding GloVe" src="img/B08681_04_69.jpg" class="calibre27"/>和<img alt="Understanding GloVe" src="img/B08681_04_70.jpg" class="calibre27"/>分别表示从输出嵌入中获得的字<em class="calibre15"> j </em>的字嵌入和偏置嵌入。<img alt="Understanding GloVe" src="img/B08681_04_71.jpg" class="calibre27"/>是我们设定的一个超参数。除了初始化时的随机化之外，这两种嵌入的行为类似。在评估阶段，这两个嵌入被加在一起，导致性能的提高。</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec49" class="calibre7"/>实施手套</h2></div></div></div><p class="calibre10">在这一小节中，我们将讨论<a id="id275"/>实施GloVe的步骤。完整的代码可以在位于<code class="literal">ch4</code>文件夹中的<code class="literal">ch4_glove.ipynb</code>练习文件中找到。</p><p class="calibre10">首先，我们将定义输入和输出:</p><div><pre class="programlisting">train_dataset = tf.placeholder(tf.int32, shape=[batch_size],name='train_dataset')
train_labels = tf.placeholder(tf.int32, shape=[batch_size],name='train_labels')</pre></div><p class="calibre10">接下来，我们将定义嵌入层。我们有两个不同的嵌入层，一个用来查找输入单词，另一个用来查找输出单词。此外，我们将定义一个<em class="calibre15">偏差</em>嵌入，就像我们对softmax层的偏差一样:</p><div><pre class="programlisting">in_embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size],
    -1.0, 1.0), name='embeddings')
in_bias_embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size],0.0,0.01,
    dtype=tf.float32), name='embeddings_bias')

out_embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size],
    -1.0, 1.0), name='embeddings')
out_bias_embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size],0.0,0.01,
    dtype=tf.float32), name='embeddings_bias')</pre></div><p class="calibre10">现在，我们将查找给定输入和输出(标签)的<a id="id276"/>对应嵌入:</p><div><pre class="programlisting">embed_in = tf.nn.embedding_lookup(in_embeddings, train_dataset)
embed_out = tf.nn.embedding_lookup(out_embeddings, train_labels)
embed_bias_in = tf.nn.embedding_lookup(in_bias_embeddings, train_dataset)
embed_bias_out = tf.nn.embedding_lookup(out_bias_embeddings, train_labels)</pre></div><p class="calibre10">此外，我们将在成本函数中为<em class="calibre15">f(X</em><em class="calibre15">ij</em><em class="calibre15">)</em>(<code class="literal">weights_x</code>)和<img alt="Implementing GloVe" src="img/B08681_04_72.jpg" class="calibre27"/> ( <code class="literal">x_ij</code>)定义占位符:</p><div><pre class="programlisting">weights_x = tf.placeholder(tf.float32, shape=[batch_size], name='weights_x')
x_ij = tf.placeholder(tf.float32, shape=[batch_size], name='x_ij')</pre></div><p class="calibre10">最后，我们将使用前面定义的实体定义完整损失函数，如下所示:</p><div><pre class="programlisting">loss = tf.reduce_mean(
    weights_x * (tf.reduce_sum(embed_in*embed_out,axis=1) +
    embed_bias_in + embed_bias_out - tf.log(epsilon+x_ij))**2)</pre></div><p class="calibre10">在这一节，我们看了GloVe，另一个单词嵌入学习技术。GloVe相对于之前描述的Word2vec技术的主要优势在于，它关注语料库的全局和局部统计来学习嵌入。由于GloVe能够捕获关于单词的全局信息，它们往往会提供更好的性能，尤其是当语料库规模增加时。另一个优点是，与Word2vec技术不同，GloVe不近似成本函数(例如，Word2vec使用负采样)，而是<a id="id277"/>计算真实成本。这导致损耗的更好和更容易的优化。</p></div></div>



  
    <title>Document classification with Word2vec</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec33" class="calibre7"/>用Word2vec进行文档分类</h1></div></div></div><p class="calibre10">尽管Word2vec <a id="id278"/>给出了一种非常优雅的方式来学习<a id="id279"/>单词的数字表示，正如我们在定量(损失值)和定性(t-SNE嵌入)中看到的那样，学习单词表示本身不足以令人信服地实现单词向量在现实世界应用中的威力。单词嵌入在许多任务中被用作单词的特征表示，例如图像字幕生成和机器翻译。然而，这些任务涉及组合不同的学习模型(如<strong class="calibre11">卷积神经网络</strong>(<strong class="calibre11">CNN</strong>)<a id="id280"/>和<strong class="calibre11">长短期记忆</strong> ( <strong class="calibre11"> LSTM </strong>)模型<a id="id281"/>或两个LSTM模型)。这些将在后面的章节中讨论。为了理解单词嵌入在现实世界中的用法，让我们坚持一个更简单的任务——文档分类。</p><p class="calibre10">文档分类是自然语言处理中最受欢迎的任务之一。文档分类对于处理大量数据(如新闻网站、出版商和大学的数据)的人来说非常有用。因此，有趣的是看到学习单词向量如何通过嵌入整个文档而不是单词来适应真实世界的任务，例如文档分类。</p><p class="calibre10">该练习可在<code class="literal">ch4</code>文件夹(<code class="literal">ch4_document_embedding.ipynb</code>)中找到。</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec50" class="calibre7"/>数据集</h2></div></div></div><p class="calibre10">对于这个任务，我们将<a id="id282"/>使用一组已经组织好的文本文件。这些是英国广播公司的新闻报道。该集合中的每个文档都属于以下类别之一:<em class="calibre15">商业</em>、<em class="calibre15">娱乐</em>、<em class="calibre15">政治</em>、<em class="calibre15">体育</em>或<em class="calibre15">科技</em>。我们使用每个类别的250个文档。我们的词汇量将达到25，000。此外，出于可视化的目的，每个文档将由一个<code class="literal">&lt;type of document&gt;-&lt;id&gt;</code>标签表示。例如，<em class="calibre15">娱乐</em>部分的第50个文档将表示为<code class="literal">entertainment-50</code>。应该注意的是，与现实应用中正在分析的大型文本语料库相比，这是一个非常小的数据集。然而，这个小例子足以说明单词嵌入的威力。</p><p class="calibre10">以下是来自实际数据的几个简短片段:</p><div><blockquote class="blockquote"><p class="calibre10">商业</p><p class="calibre10">日本勉强躲过衰退</p><p class="calibre10">数据显示，在截至9月份的三个月里，日本经济徘徊在技术性衰退的边缘。</p><p class="calibre10">修正后的数据显示仅增长0.1%，与上一季度类似规模的收缩。按年度计算，数据显示年增长率仅为0.2%，...</p><p class="calibre10">技术</p><p class="calibre10">英国网民引领电视下载量</p><p class="calibre10">研究显示，英国电视观众引领了从网上非法下载美剧的潮流。</p><p class="calibre10">据报道，新一集《绝望的主妇》和《六尺之下》在美国播出几小时后就出现在网络上。网络追踪公司Envisional称，18%的下载者来自英国，电视节目的下载量在去年增长了150%....</p></blockquote></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec51" class="calibre7"/>使用单词嵌入对文档进行分类</h2></div></div></div><p class="calibre10">问题<a id="id283"/>主要是看单词嵌入方法<a id="id284"/>如skip-gram或CBOW是否可以扩展到分类/聚类文档。在这个例子中，我们将使用CBOW算法，因为它已经被证明对于较小的数据集比skip-gram执行得更好。</p><p class="calibre10">我们将采取以下方法:</p><div><ol class="orderedlist"><li class="listitem1">从所有的文本文件中提取数据，并像我们已经做的那样学习单词嵌入。</li><li class="listitem1">从已经训练的文档中提取随机的一组文档。</li><li class="listitem1">扩展所学习的嵌入以嵌入这些选择的文档。更具体地说，我们将通过属于文档中找到的所有单词的嵌入的平均值来表示文档。</li><li class="listitem1">使用t-SNE可视化技术来可视化所发现的文档嵌入，以查看单词嵌入是否可用于文档聚类或分类。</li><li class="listitem1">最后，可以使用诸如K-means之类的聚类算法来为每个文档分配标签。我们将在讨论实现的同时简要讨论K-means是什么。</li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec52" class="calibre7"/>实施——学习单词嵌入</h2></div></div></div><p class="calibre10">首先，我们将为训练数据、训练标签、有效数据(用于监控单词嵌入)和测试数据(用于计算测试文档的平均嵌入)定义几个<a id="id285"/>占位符:</p><div><pre class="programlisting"># Input data.
train_dataset = tf.placeholder(tf.int32,
    shape=[batch_size, 2*window_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

test_labels = tf.placeholder(tf.int32,
    shape=[batch_size], name='test_dataset')</pre></div><p class="calibre10">接下来，我们将为词汇表和softmax权重和偏差定义嵌入变量(用于计算测试文档的平均嵌入):</p><div><pre class="programlisting"># Variables.
# embedding, vector for each word in the vocabulary
embeddings = tf.Variable(tf.random_uniform([vocabulary_size,
    embedding_size], -1.0, 1.0, dtype=tf.float32))
softmax_weights = tf.Variable(
    tf.truncated_normal([vocabulary_size, embedding_size],
    stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float32))
softmax_biases = tf.Variable(
    tf.zeros([vocabulary_size], dtype=tf.float32))</pre></div><p class="calibre10">然后，我们像以前一样定义采样负softmax损失函数:</p><div><pre class="programlisting">loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights,
    biases=softmax_biases, inputs=mean_embeddings,
    labels=train_labels, num_sampled=num_sampled,
    num_classes=vocabulary_size))</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec53" class="calibre7"/>实现–从单词嵌入到文档嵌入</h2></div></div></div><p class="calibre10">为了从单词嵌入中获得良好的<a id="id286"/>文档嵌入，我们将把文档中找到的所有单词的平均嵌入作为文档嵌入。然而，我们将分批处理数据。因此，我们将使用以下内容来实现这一点。</p><p class="calibre10">对于每个文档，请执行以下操作:</p><div><ol class="orderedlist"><li class="listitem1">创建一个数据集，其中每个数据点都是属于文档的一个单词</li><li class="listitem1">对于从数据集中采样的小批量，通过对小批量中所有单词的嵌入向量求平均来返回平均嵌入向量</li><li class="listitem1">分批遍历测试文档，通过对小批量均值嵌入进行平均来获得文档嵌入</li></ol></div><p class="calibre10">我们将得到平均批次<a id="id287"/>嵌入如下:</p><div><pre class="programlisting">mean_batch_embedding = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, test_labels), axis=0)
mean_embeddings = tf.reduce_mean(stacked_embeddings, 2, keepdims=False)</pre></div><pre>tech-42 document:</pre><div><blockquote class="blockquote"><pre>sport-50 document:</pre><div><blockquote class="blockquote"><p class="calibre10">运动-50</p><p class="calibre10">国际田联等待希腊组合的回应</p><p class="calibre10">科斯塔斯·肯特里斯和卡特琳娜·塔努尚未对国际田径联合会(IAAF)的兴奋剂指控做出回应。</p><p class="calibre10">这对希腊夫妇在错过特拉维夫、芝加哥和雅典的一系列常规药检后被起诉。他们的截止日期是12月16日午夜，一位国际田联发言人说:“我们确信他们正在回复。”如果他们没有回应或他们的解释被拒绝，他们将被暂时禁止参加比赛。他们将面临希腊联邦的听证会，...</p></blockquote></div><p class="calibre10">对于<a id="id291"/>为什么<code class="literal">sport-50</code>被聚集在远离其他体育相关文章的地方，我们可以有所了解。我们再仔细看看另一份接近<em class="calibre15"> sport-50 </em>的文件，是，<code class="literal">entertainment-115</code>:</p><div><blockquote class="blockquote"><p class="calibre10">娱乐-115</p><p class="calibre10">说唱歌手史努比·道格起诉“强奸”</p><p class="calibre10">美国说唱歌手史努比·多格被一名化妆师起诉，要求赔偿2500万美元(1300万英镑)，该化妆师声称他和他的随从两年前对她下药并强奸了她。</p><p class="calibre10">这名妇女说，她在2003年录制了ABC网络上的吉米·基梅尔直播电视节目后遭到了侵犯。这位说唱歌手的发言人表示，这些指控是“不真实的”，该女子“滥用法律体系作为获取经济利益的手段”。美国广播公司表示，这些指控“没有法律依据”。这位明星还没有被警方起诉。</p></blockquote></div><p class="calibre10">因此，这一领域的文件似乎与各种犯罪或非法指控有关，而不是关于体育或娱乐。这导致这些文档远离其他典型的体育或娱乐相关文档而聚集在一起。</p></blockquote></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec56" class="calibre7"/>实施–使用K-means对文档进行聚类/分类</h2></div></div></div><p class="calibre10">到目前为止，我们已经能够直观地检查文档群。然而，这个<a id="id293"/>是不够的，因为如果我们有1000多个文档要进行聚类/分类，我们将不得不目测检查1000次。因此，我们需要更多自动化的方法来实现这一点。</p><p class="calibre10">我们可以使用K-means对这些文档进行聚类。K-means是一种简单但功能强大的技术，用于根据数据的相似性将数据分组(聚类)，以便相似的数据将在同一组中，而不同的数据将在不同的组中。K-means的工作方式如下:</p><div><ol class="orderedlist"><li class="listitem1">定义<em class="calibre15"> K </em>，要形成的簇的数量。我们将它设置为5，因为我们已经知道有五个类别。</li><li class="listitem1">形成K个随机质心，它们是群集的中心。</li><li class="listitem1">然后，我们将每个数据点分配到最近的聚类质心。</li><li class="listitem1">将所有数据点分配给某个聚类后，我们将重新计算聚类质心(即数据点的平均值)。</li><li class="listitem1">我们将以这种方式继续，直到质心移动变得小于某个阈值。</li></ol></div><p class="calibre10">我们将使用scikit-learn库来获得K-means算法。在代码中，如下所示:</p><div><pre class="programlisting">kmeans = KMeans(n_clusters=5, random_state=43643, max_iter=10000,
                   n_init=100, algorithm='elkan')</pre></div><p class="calibre10">最重要的超参数是<code class="literal">n_clusters</code>，它是我们想要形成的簇的数目。您可以试验其他超参数，看看它们对性能有什么样的影响。关于可能的超参数的解释可在<a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">http://sci kit-learn . org/stable/modules/generated/sk learn . cluster . k means . html</a>上找到。</p><p class="calibre10">然后我们可以将我们用来训练的文档(或任何其他文档)分类成类。我们将获得以下内容:</p><div><table border="1" class="calibre18"><colgroup class="calibre19"><col class="calibre20"/><col class="calibre20"/></colgroup><thead class="calibre21"><tr class="calibre22"><th valign="bottom" class="calibre23">
<p class="calibre10">标签</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">文档</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">0</code></p>
</td><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">'entertainment-207', 'entertainment-14', 'entertainment-232', 'entertainment-49', 'entertainment-191', 'entertainment-243', 'entertainment-240'</code></p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">1</code></p>
</td><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">'sport-145', 'sport-228', 'sport-141', 'sport-249'</code></p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">2</code></p>
</td><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">'sport-4', 'sport-43', 'entertainment-54', 'politics-214', 'politics-12', 'politics-165', 'sport-42', 'politics-203', 'politics-87', 'sport-33', 'politics-81', 'politics-247', 'entertainment-245', 'entertainment-22', 'tech-102', 'sport-50', 'politics-33', 'politics-28'</code></p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">3</code></p>
</td><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">'business-220', 'business-208', 'business-51', 'business-30', 'business-130', 'business-190', 'business-34', 'business-206'</code></p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">4</code></p>
</td><td valign="top" class="calibre25">
<p class="calibre10"><code class="literal">'business-185', 'business-238', 'tech-105', 'tech-99', 'tech-239', 'tech-227', 'tech-31', 'tech-131', 'tech-118', 'politics-10', 'tech-150', 'tech-165'</code></p>
</td></tr></tbody></table></div><p class="calibre10">它并不完美，但它很好地将属于不同类别的文档分类到不同的标签中。我们可以看到，娱乐相关文档有<code class="literal">0</code>标签，体育相关文档有<code class="literal">1</code>标签，商务相关文档有<code class="literal">3</code>标签，等等。</p><p class="calibre10">在本节中，您学习了<a id="id294"/>我们如何扩展单词嵌入来分类/聚类<a id="id295"/>文档。首先，你学习了单词嵌入，就像我们通常做的那样。然后，我们通过对文档中找到的所有单词的单词嵌入进行平均来创建文档嵌入。后来，我们使用这些文档嵌入对一些BBC新闻文章进行聚类/分类，这些文章分为以下几类:娱乐、科技、政治、商业和体育。在对文档进行聚类之后，我们看到文档被合理地聚类，使得属于一个类别的文档被彼此靠近地聚类。然而，有一些异常的文档。但是在分析了这些文件的文本内容之后，我们发现在这些文件以这种特殊方式表现的背后有一些合理的原因。</p></div></div>



  
    <title>Summary</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch04lvl1sec34" class="calibre7"/>总结</h1></div></div></div><p class="calibre10">在本章中，我们研究了skip-gram算法和CBOW算法之间的性能差异。为了进行比较，我们使用了一种流行的二维可视化技术，t-SNE，我们也向您简要介绍了这种技术，触及了该方法背后的基本直觉和数学。</p><p class="calibre10">接下来，我们向您介绍了Word2vec算法的几个扩展，这些扩展提高了它们的性能，随后介绍了几个基于skip-gram和CBOW算法的新算法。结构化skip-gram通过在优化过程中保留上下文单词的位置来扩展skip-gram算法，允许算法根据输入-输出之间的距离来处理输入-输出。相同的扩展可以应用于CBOW算法，这导致了连续窗口算法。</p><p class="calibre10">然后我们讨论了手套——另一种单词嵌入学习技术。GloVe将当前的Word2vec算法向前推进了一步，将全局统计数据融入到优化中，从而提高了性能。最后，我们讨论了使用单词嵌入的一个实际应用—文档聚类/分类。我们展示了单词嵌入是非常强大的，它允许我们将相关的文档很好地聚集在一起。</p><p class="calibre10">在下一章中，我们将继续讨论一个不同的深度网络家族，它在利用数据中存在的空间信息方面更加强大，称为<strong class="calibre11">卷积神经网络</strong>(<strong class="calibre11">CNN</strong>)。确切地说，我们将看到如何使用CNN来开发句子的空间结构，以将它们分成不同的类别。</p></div>
</div></body></html>