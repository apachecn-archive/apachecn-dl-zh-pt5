<html><head/><body>
<html>
  <head>
    <title>Appendix A. Mathematical Foundations and Advanced TensorFlow</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title"><a id="appA" class="calibre7"/>附录a .数学基础和高级张量流</h1></div></div></div><p class="calibre10">在这里，我们将讨论一些概念，这将有助于理解章节中提供的细节。首先，我们将讨论贯穿全书的几种数学数据结构，然后描述对这些数据结构执行的各种操作。接下来，我们将讨论概率的概念。概率在机器学习中起着至关重要的作用，因为它们通常可以让我们了解一个模型的预测有多不确定。此后，我们讨论TensorFlow中称为Keras的高级库，以及如何用TensorFlow中的seq2seq子库实现神经机器翻译器。最后，我们以如何使用TensorBoard作为单词嵌入的可视化工具的指南来结束这一部分。</p><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec92" class="calibre7"/>基本数据结构</h1></div></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec188" class="calibre7"/>标量</h2></div></div></div><p class="calibre10">与矩阵或向量不同，标量是一个单独的数字<a id="id876"/>。例如，1.3是一个标量。一个<a id="id877"/>标量可以用数学方法表示如下:<img alt="Scalar" src="img/B08681_12_01.jpg" class="calibre27"/></p><p class="calibre10">这里，<em class="calibre15"> R </em>是实数空间。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec189" class="calibre7"/>载体</h2></div></div></div><p class="calibre10">向量是数字的数组<a id="id878"/>。与元素没有顺序的集合不同，<a id="id879"/>向量的元素有一定的顺序。一个示例向量是[1.0，2.0，1.4，2.3]。在数学上，它可以表示如下:</p><div><img alt="Vectors" src="img/B08681_12_02.jpg" class="calibre12"/></div><div><img alt="Vectors" src="img/B08681_12_03.jpg" class="calibre12"/></div><p class="calibre10">或者，我们可以这样写:</p><div><img alt="Vectors" src="img/B08681_12_04.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> R </em>是实数空间，<em class="calibre15"> n </em>是向量中元素的数量。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec190" class="calibre7"/>矩阵</h2></div></div></div><p class="calibre10">矩阵可以被认为是标量集合的二维排列。换句话说，矩阵可以被认为是向量的向量。矩阵示例如下所示:</p><div><img alt="Matrices" src="img/B08681_12_05.jpg" class="calibre12"/></div><p class="calibre10">大小为<img alt="Matrices" src="img/B08681_12_06.jpg" class="calibre27"/>的更一般的矩阵可以数学定义如下:</p><div><img alt="Matrices" src="img/B08681_12_07.jpg" class="calibre12"/></div><p class="calibre10">并且:</p><div><img alt="Matrices" src="img/B08681_12_08.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> m </em>是矩阵的行数，<em class="calibre15"> n </em>是矩阵的列数，<em class="calibre15"> R </em>是实数空间。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec191" class="calibre7"/>矩阵的索引</h2></div></div></div><p class="calibre10">我们将使用<a id="id882"/>零索引符号(也就是说，索引从0开始)。</p><p class="calibre10">为了索引位于<em class="calibre15"> (i，j)<sup class="calibre26"/></em>位置的矩阵中的单个元素，我们使用以下符号:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_09.jpg" class="calibre12"/></div><p class="calibre10">参考之前定义的矩阵，我们得到以下结果:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_10.jpg" class="calibre12"/></div><p class="calibre10">我们从<em class="calibre15">到</em>这样索引一个元素:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_11.jpg" class="calibre12"/></div><p class="calibre10">我们将任意矩阵<em class="calibre15">的<a id="id883"/>单行表示为</em>，如下所示:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_12.jpg" class="calibre12"/></div><p class="calibre10">对于我们的示例矩阵，我们可以表示矩阵的第二行(索引为1 ),如下所示:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_13.jpg" class="calibre12"/></div><p class="calibre10">我们表示从任意矩阵<em class="calibre15"> A </em>的第<em class="calibre15"> (i，k)<sup class="calibre26"/></em>索引到第<em class="calibre15"> (j，l)<sup class="calibre26"/></em>索引的切片，如下所示:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_14.jpg" class="calibre12"/></div><p class="calibre10">在我们的示例矩阵中，我们可以表示从第一行第三列到第二行第四列的切片，如下所示:</p><div><img alt="Indexing of a matrix" src="img/B08681_12_15.jpg" class="calibre12"/></div></div></div></div></body></html>


<html>
  <head>
    <title>Special types of matrices</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec93" class="calibre7"/>特殊类型的矩阵</h1></div></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec192" class="calibre7"/>单位矩阵</h2></div></div></div><p class="calibre10">一个单位矩阵是<a id="id884"/>，其中它在<a id="id885"/>矩阵的对角线上等于1，在其他地方等于0。数学上，它可以表示如下:</p><div><img alt="Identity matrix" src="img/B08681_12_16.jpg" class="calibre12"/></div><p class="calibre10">这看起来像下面这样:</p><div><img alt="Identity matrix" src="img/B08681_12_17.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="Identity matrix" src="img/B08681_12_18.jpg" class="calibre27"/>。</p><p class="calibre10">当与另一个矩阵<em class="calibre15"> A </em>相乘时，单位矩阵给出了以下良好的性质:</p><div><img alt="Identity matrix" src="img/B08681_12_19.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec193" class="calibre7"/>对角矩阵</h2></div></div></div><p class="calibre10">对角线<a id="id886"/>矩阵是单位矩阵的更一般的情况，其中沿对角线的<a id="id887"/>值可以取任何值，非对角线值为零:</p><div><img alt="Diagonal matrix" src="img/B08681_12_20.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec194" class="calibre7"/>张量</h2></div></div></div><p class="calibre10">一个<em class="calibre15"> n </em>维<a id="id888"/>矩阵被称为<strong class="calibre11">张量</strong>。换句话说，一个任意维数的矩阵叫做张量。例如，四维张量可以表示为:</p><div><img alt="Tensors" src="img/B08681_12_21.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> R </em>是实数空间。</p></div></div></body></html>


<html>
  <head>
    <title>Tensor/matrix operations</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec94" class="calibre7"/>张量/矩阵运算</h1></div></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec195" class="calibre7"/>转置</h2></div></div></div><p class="calibre10">转置是为矩阵或张量定义的重要操作<a id="id890"/>。对于矩阵，转置的定义如下:</p><div><img alt="Transpose" src="img/B08681_12_22.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15">A<sub class="calibre17">T</sub>T35】表示<em class="calibre15"> A </em>的转置。</em></p><p class="calibre10">转置操作的示例<a id="id891"/>可以如下所示:</p><div><img alt="Transpose" src="img/B08681_12_23.jpg" class="calibre12"/></div><p class="calibre10">转置操作后:</p><div><img alt="Transpose" src="img/B08681_12_24.jpg" class="calibre12"/></div><p class="calibre10">对于张量来说，转置可以看作是改变维度的顺序。例如，让我们定义一个张量<em class="calibre15"> S </em>，如下所示:</p><div><img alt="Transpose" src="img/B08681_12_25.jpg" class="calibre12"/></div><p class="calibre10">现在转置操作(许多转置操作中的一个)可以定义如下:</p><div><img alt="Transpose" src="img/B08681_12_26.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec196" class="calibre7"/>乘法</h2></div></div></div><p class="calibre10">矩阵<a id="id892"/>乘法是线性代数中出现频率相当高的另一个重要运算。</p><p class="calibre10">给定矩阵<img alt="Multiplication" src="img/B08681_12_27.jpg" class="calibre27"/>和<img alt="Multiplication" src="img/B08681_12_28.jpg" class="calibre27"/>，A<em class="calibre15">和B<em class="calibre15">的乘积定义如下:</em></em></p><div><img alt="Multiplication" src="img/B08681_12_29.jpg" class="calibre12"/></div><p class="calibre10">在这里，<img alt="Multiplication" src="img/B08681_12_30.jpg" class="calibre27"/>。</p><p class="calibre10">考虑这个例子:</p><div><img alt="Multiplication" src="img/B08681_12_31.jpg" class="calibre12"/></div><div><img alt="Multiplication" src="img/B08681_12_32.jpg" class="calibre12"/></div><p class="calibre10">这给了</p><div><img alt="Multiplication" src="img/B08681_12_33.jpg" class="calibre12"/></div><p class="calibre10">，并且<em class="calibre15"> C </em>的值如下:</p><div><img alt="Multiplication" src="img/B08681_12_34.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec197" class="calibre7"/>逐元素乘法</h2></div></div></div><p class="calibre10">针对具有相同形状的两个矩阵，计算逐元素矩阵乘法(或哈达玛乘积)。给定矩阵<img alt="Element-wise multiplication" src="img/B08681_12_35.jpg" class="calibre27"/>和<img alt="Element-wise multiplication" src="img/B08681_12_36.jpg" class="calibre27"/>，将<em class="calibre15"> A </em>和<em class="calibre15"> B </em>的逐元素乘法定义如下:</p><div><img alt="Element-wise multiplication" src="img/B08681_12_37.jpg" class="calibre12"/></div><p class="calibre10">在这里，<img alt="Element-wise multiplication" src="img/B08681_12_38.jpg" class="calibre27"/></p><p class="calibre10">考虑这个例子:</p><div><img alt="Element-wise multiplication" src="img/B08681_12_39.jpg" class="calibre12"/></div><p class="calibre10">这给出了<img alt="Element-wise multiplication" src="img/B08681_12_40.jpg" class="calibre27"/>，并且<em class="calibre15"> C </em>的值如下:</p><div><img alt="Element-wise multiplication" src="img/B08681_12_41.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec198" class="calibre7"/>逆</h2></div></div></div><p class="calibre10">矩阵<em class="calibre15"> A </em>的逆用<em class="calibre15"> A </em> <em class="calibre15"> -1 </em>表示，其中满足以下条件:</p><div><img alt="Inverse" src="img/B08681_12_42.jpg" class="calibre12"/></div><p class="calibre10">如果我们试图解一个线性方程组，逆是非常有用的。考虑这个例子:</p><div><img alt="Inverse" src="img/B08681_12_43.jpg" class="calibre12"/></div><p class="calibre10">我们可以这样求解<em class="calibre15"> x </em>:</p><div><img alt="Inverse" src="img/B08681_12_44.jpg" class="calibre12"/></div><p class="calibre10">这可以写成，<img alt="Inverse" src="img/B08681_12_45.jpg" class="calibre27"/>用结合律(即<img alt="Inverse" src="img/B08681_12_46.jpg" class="calibre27"/>)。</p><p class="calibre10">接下来我们会得到<img alt="Inverse" src="img/B08681_12_47.jpg" class="calibre27"/>因为<img alt="Inverse" src="img/B08681_12_48.jpg" class="calibre27"/>，其中<em class="calibre15"> I </em>是单位矩阵。</p><p class="calibre10">最后，<img alt="Inverse" src="img/B08681_12_49.jpg" class="calibre27"/>因为<img alt="Inverse" src="img/B08681_12_50.jpg" class="calibre27"/>。</p><p class="calibre10">例如，多项式回归是一种回归技术，它使用线性方程组来解决回归问题。回归与分类相似，但回归模型输出的不是一个类，而是一个连续值。让我们来看一个示例问题:给定一所房子的卧室数量，我们将计算房子的房地产价值。形式上，多项式回归问题可以写成如下形式:</p><div><img alt="Inverse" src="img/B08681_12_51.jpg" class="calibre12"/></div><p class="calibre10">这里，</p><div><img alt="Inverse" src="img/B08681_12_52.jpg" class="calibre12"/></div><p class="calibre10">是第<em class="calibre15"> i <sup class="calibre26"> th </sup> </em>数据输入，其中<em class="calibre15">x<sub class="calibre17">I</sub>T36】是输入，<em class="calibre15">y<sub class="calibre17">I</sub>T40】是标签，<img alt="Inverse" src="img/B08681_12_53.jpg" class="calibre27"/>是数据中的噪声。在我们的例子中，<em class="calibre15"> x </em>是卧室的数量，y </em>是房子的价格。这可以写成如下的线性方程组<a id="id895"/>:</em></p><div><img alt="Inverse" src="img/B08681_12_54.jpg" class="calibre12"/></div><p class="calibre10">然而，<em class="calibre15">A<sup class="calibre26">-1</sup>T3并不存在于所有的<em class="calibre15"> A </em>中。为了矩阵的逆存在，需要满足某些条件。比如定义逆矩阵，<em class="calibre15"> A </em>需要是方阵(也就是<img alt="Inverse" src="img/B08681_12_55.jpg" class="calibre27"/>)。即使逆存在，我们也不能总是在封闭形式中找到它；有时只能用有限精度的计算机来近似计算。如果逆存在，有几种算法可以找到它，我们将在这里讨论。</em></p><div><div><h3 class="title4"><a id="note44" class="calibre7"/>注意</h3><p class="calibre16">当说<em class="calibre15"> A </em>需要是方阵求逆才能存在时，我指的是标准的求逆。存在逆运算的变体(例如，Moore-Penrose逆运算，也称为伪逆运算)</p><div><img alt="Inverse" src="img/B08681_12_101.jpg" class="calibre12"/></div><p class="calibre16">矩阵。</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec199" class="calibre7"/>寻找矩阵逆-奇异值分解(SVD)</h2></div></div></div><p class="calibre10">现在让我们看看如何使用SVD来寻找矩阵A的逆矩阵。SVD <a id="id897"/>将<em class="calibre15"> A </em>分解成三个不同的矩阵，如下所示:</p><div><img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_56.jpg" class="calibre12"/></div><p class="calibre10">这里<em class="calibre15"> U </em>的列称为左奇异向量，<em class="calibre15"> V </em>的列称为右奇异向量，<em class="calibre15"> D </em>(对角矩阵)的对角值称为奇异值。左奇异向量是<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_57.jpg" class="calibre27"/>的特征向量，右奇异向量是<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_58.jpg" class="calibre27"/>的特征向量。最后，奇异值是<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_59.jpg" class="calibre27"/>和<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_60.jpg" class="calibre27"/>的特征值的平方根。方阵<em class="calibre15"> A </em>的特征向量<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_61.jpg" class="calibre27"/>及其对应的特征值<img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_62.jpg" class="calibre27"/>满足以下条件:</p><div><img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_63.jpg" class="calibre12"/></div><p class="calibre10">然后，如果SVD存在，则<em class="calibre15"> A </em>的倒数由下式给出:</p><div><img alt="Finding the matrix inverse – Singular Value Decomposition (SVD)" src="img/B08681_12_64.jpg" class="calibre12"/></div><p class="calibre10">由于<em class="calibre15"> D </em>是对角的，<em class="calibre15"> D <sup class="calibre26"> -1 </sup> </em>就是<em class="calibre15"> D </em>的非零元素的逐元素倒数。奇异值分解是一种重要的矩阵分解技术，出现在机器学习的许多场合。以<a id="id898"/>为例，SVD用于计算<strong class="calibre11">主成分分析</strong> ( <strong class="calibre11"> PCA </strong>)，这是一种流行的数据降维技术(目的类似于我们在<a href="ch04.html" title="Chapter 4. Advanced Word2vec">第四章</a>、<em class="calibre15">高级Word2vec </em>中看到的t-SNE)。SVD的另一个更面向NLP的应用是文档排序。也就是说，当您想要获得最多的<a id="id899"/>相关文档(并根据与某个术语的相关性对它们进行排序，例如<em class="calibre15">足球</em>)时，可以使用SVD来实现这一点。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec200" class="calibre7"/>规范</h2></div></div></div><p class="calibre10">Norm用作<a id="id900"/>矩阵的<em class="calibre15">大小</em>的度量(即矩阵中的值)。第<em class="calibre15">p<sup class="calibre26">th</sup>T31】定额的计算和表示如下:</em></p><div><img alt="Norms" src="img/B08681_12_65.jpg" class="calibre12"/></div><p class="calibre10">例如，L2的标准是这样的:</p><div><img alt="Norms" src="img/B08681_12_66.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec201" class="calibre7"/>行列式</h2></div></div></div><p class="calibre10">方阵的行列式，用<img alt="Determinant" src="img/B08681_12_67.jpg" class="calibre27"/>表示，是矩阵所有特征值的乘积。行列式在很多方面都非常有用。例如，<em class="calibre15"> A </em>可逆当且仅当行列式非零。以下等式显示了对<img alt="Determinant" src="img/B08681_12_68.jpg" class="calibre27"/>矩阵的行列式的计算:</p><div><img alt="Determinant" src="img/B08681_12_69.jpg" class="calibre12"/></div></div></div></body></html>


<html>
  <head>
    <title>Probability</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec95" class="calibre7"/>概率</h1></div></div></div><p class="calibre10">接下来，我们将讨论与概率论相关的术语。概率论是机器学习的一个重要组成部分，因为用概率模型对数据建模允许我们得出关于模型对一些预测的不确定性的结论。考虑这个例子，我们在<a href="ch11.html" title="Chapter 11. Current Trends and the Future of Natural Language Processing">第11章</a>、<em class="calibre15">自然语言处理的当前趋势和未来</em>中执行情感分析，其中我们有给定电影评论的输出值(正/负)。尽管对于我们输入的任何样本，模型都输出在<code class="literal">0</code>和<code class="literal">1</code>之间的某个值(<code class="literal">0</code>表示负，<code class="literal">1</code>表示正)，但是模型不知道<em class="calibre15">如何不确定</em>它的答案。</p><p class="calibre10">让我们理解不确定性如何帮助我们做出更好的预测。例如，一个确定性的模型可能会错误地将评论的积极性<em class="calibre15">【我从未失去兴趣</em>】说成<code class="literal">0.25</code>(也就是说，更有可能是负面评论)。然而，概率模型将给出预测的平均值和标准偏差。例如，它会说，这个预测的均值为<code class="literal">0.25</code>，标准差为<code class="literal">0.5</code>。对于第二个模型，我们知道由于高标准偏差，预测很可能是错误的。然而，在确定性模型中，我们没有这种奢侈。这一特性对于关键的机器系统(例如，恐怖主义风险评估模型)尤其有价值。</p><p class="calibre10">要开发这样的概率机器学习模型(例如，贝叶斯逻辑回归、贝叶斯神经网络或高斯过程)，您应该熟悉基本的概率理论。因此，我们在这里提供一些基本的概率信息。</p><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec202" class="calibre7"/>随机变量</h2></div></div></div><p class="calibre10">随机<a id="id903"/>变量是一个可以随机取值的变量。同样，随机<a id="id904"/>变量表示为<em class="calibre15"> x <sub class="calibre17"> 1 </sub> </em>、<em class="calibre15"> x <sub class="calibre17"> 2 </sub> </em>等等。随机变量有两种类型:离散型和连续型。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec203" class="calibre7"/>离散随机变量</h2></div></div></div><p class="calibre10">离散随机变量是可以取离散随机值的变量。例如，<a id="id905"/>抛硬币的试验可以建模为一个随机<a id="id906"/>变量，也就是说，当你抛硬币时硬币落地的一面是一个离散变量，因为数值只能是<em class="calibre15">正面</em>或<em class="calibre15">反面</em>。或者，掷骰子得到的值也是离散的，因为这些值只能来自集合<em class="calibre15"> {1，2，3，4，5，6} </em>。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec204" class="calibre7"/>连续随机变量</h2></div></div></div><p class="calibre10">连续型<a id="id907"/>随机变量是<a id="id908"/>可以取任意实值的变量，即如果<em class="calibre15"> x </em>是连续型随机变量:</p><div><img alt="Continuous random variables" src="img/B08681_12_70.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> R </em>是实数空间。</p><p class="calibre10">例如，一个人的身高是一个连续的随机变量，因为它可以取任何真实值。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec205" class="calibre7"/>概率质量/密度函数</h2></div></div></div><p class="calibre10"><strong class="calibre11">概率质量函数</strong> ( <strong class="calibre11"> PMF </strong>)或<strong class="calibre11">概率密度函数</strong> ( <strong class="calibre11"> PDF </strong>)是一种显示随机变量可以取的不同<a id="id910"/>值的<a id="id909"/>概率分布的方式。对于离散<a id="id911"/>变量，定义一个PMF，对于连续变量，定义一个PDF。<em class="calibre15">图A.1 </em>显示了一个PMF示例:</p><div><img alt="The probability mass/density function" src="img/B08681_12_71.jpg" class="calibre12"/><div><p class="calibre10">A.1:概率质量函数(PMF)离散</p></div></div><p class="calibre10">前述的PMF可以通过<em class="calibre15">偏置的</em>模具来实现。在这个图中，我们可以看到，用这个骰子得到一个<strong class="calibre11"> 3 </strong>的概率很高。这样的图表可以通过运行<a id="id912"/>数量的试验(比如说100)然后计算每个面落在上面的次数来获得。最后，将每个计数除以试验次数<a id="id913"/>以获得标准化概率。请注意，所有概率的总和应为1，如下所示:</p><div><img alt="The probability mass/density function" src="img/B08681_12_72.jpg" class="calibre12"/></div><p class="calibre10">相同的<a id="id914"/>概念被扩展到连续随机变量以获得PDF。假设我们正试图对给定人口的某个身高的概率进行建模。与离散情况不同，我们没有计算概率的单个值，而是一个连续的值谱(在本例中，它从0延伸到2.4 m)。如果我们要为这个例子画一个像<em class="calibre15">图A.1 </em>中那样的图，我们需要把它想象成无限小的容器。比如我们求出一个人身高在0.0米-0.01米，0.01米-0.02米之间的概率密度，...，1.8米-1.81米，…等等。概率密度可以使用以下公式计算:</p><div><img alt="The probability mass/density function" src="img/B08681_12_73.jpg" class="calibre12"/></div><p class="calibre10">然后，我们将绘制<a id="id915"/>那些相互靠近的条以获得一条连续的曲线，如图<em class="calibre15">图A.2 </em>所示。请注意，给定条柱的概率密度可以大于1(因为它是密度)，但曲线下的面积必须为1:</p><div><img alt="The probability mass/density function" src="img/B08681_12_74.jpg" class="calibre12"/><div><p class="calibre10">图A.2:概率密度函数(PDF)连续</p></div></div><p class="calibre10">图A.2 中<em class="calibre15">所示的形状<a id="id916"/>被称为正态(或高斯)分布。它也被称为<em class="calibre15">钟形曲线</em>。我们之前给了<a id="id917"/>一个关于如何考虑连续概率密度函数的直观解释。更正式地说，正态分布的连续PDF有一个等式，定义如下。假设一个连续随机变量<em class="calibre15"> X </em>具有均值<img alt="The probability mass/density function" src="img/B08681_12_75.jpg" class="calibre27"/>和标准差<img alt="The probability mass/density function" src="img/B08681_12_76.jpg" class="calibre27"/>的正态分布。对于<em class="calibre15"> x </em>的任意值，<em class="calibre15"> X = x </em>的概率由以下公式给出:</em></p><div><img alt="The probability mass/density function" src="img/B08681_12_77.jpg" class="calibre12"/></div><p class="calibre10">如果您将这个量对所有可能的无限小的<a id="id918"/>dx值进行积分，您应该得到面积(对于有效的PDF，该面积需要为1 ),如下式所示:</p><div><img alt="The probability mass/density function" src="img/B08681_12_78.jpg" class="calibre12"/></div><p class="calibre10">任意<em class="calibre15"> a </em>、<em class="calibre15"> b </em>值的法线的<a id="id919"/>积分为<a id="id920"/>由以下公式给出:</p><div><img alt="The probability mass/density function" src="img/B08681_12_79.jpg" class="calibre12"/></div><p class="calibre10">(你可以在<a href="http://mathworld.wolfram.com/GaussianIntegral.html">http://mathworld.wolfram.com/GaussianIntegral.html</a>找到更多信息，或者参考<a href="https://en.wikipedia.org/wiki/Gaussian_integral">https://en.wikipedia.org/wiki/Gaussian_integral</a>进行不太复杂的讨论。)</p><p class="calibre10">利用这个，我们可以得到正态分布的积分，其中<img alt="The probability mass/density function" src="img/B08681_12_80.jpg" class="calibre27"/>和<img alt="The probability mass/density function" src="img/B08681_12_81.jpg" class="calibre27"/>:</p><div><img alt="The probability mass/density function" src="img/B08681_12_82.jpg" class="calibre12"/></div><p class="calibre10">这给出了<em class="calibre15"> x </em>的所有值的所有概率值的累加，并给出值1。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec206" class="calibre7"/>条件概率</h2></div></div></div><p class="calibre10">条件<a id="id922"/>概率表示一个事件发生的概率，假设另一个事件<a id="id923"/>发生。例如，给定两个随机变量<em class="calibre15"> X </em>和<em class="calibre15"> Y </em>，给定<em class="calibre15"> Y = y </em>，则<em class="calibre15"> X = x </em>的条件概率由以下公式表示:</p><div><img alt="Conditional probability" src="img/B08681_12_83.jpg" class="calibre12"/></div><p class="calibre10">这种概率的真实示例<a id="id924"/>如下:</p><div><img alt="Conditional probability" src="img/B08681_12_84.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec207" class="calibre7"/>联合概率</h2></div></div></div><p class="calibre10">给定两个随机<a id="id925"/>变量<em class="calibre15"> X </em>和<em class="calibre15"> Y </em>，我们将<em class="calibre15"> X = x </em>的概率与<em class="calibre15"> Y = y </em>一起称为<em class="calibre15"> X = x </em>和<em class="calibre15"> Y = y </em>的联合概率。该<a id="id926"/>由以下公式表示:</p><div><img alt="Joint probability" src="img/B08681_12_85.jpg" class="calibre12"/></div><p class="calibre10">如果<em class="calibre15"> X </em>和<em class="calibre15"> Y </em>是互斥事件，则该表达式简化为:</p><div><img alt="Joint probability" src="img/B08681_12_86.jpg" class="calibre12"/></div><p class="calibre10">这方面的一个真实示例如下:</p><div><img alt="Joint probability" src="img/B08681_12_87.jpg" class="calibre12"/></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec208" class="calibre7"/>边际概率</h2></div></div></div><p class="calibre10">边际概率<a id="id927"/>分布是随机变量子集的概率分布，给定所有<a id="id928"/>变量的联合概率分布。例如，考虑两个随机变量，<em class="calibre15"> X </em>和<em class="calibre15"> Y </em>存在，我们已经知道<img alt="Marginal probability" src="img/B08681_12_88.jpg" class="calibre27"/>并且我们想要计算<em class="calibre15"> P(x) </em>:</p><div><img alt="Marginal probability" src="img/B08681_12_89.jpg" class="calibre12"/></div><p class="calibre10">直观上，我们<a id="id929"/>将对<em class="calibre15"> Y </em>的所有可能值求和，有效地使得<em class="calibre15"> Y = 1 </em>的概率。这给了我们</p><div><img alt="Marginal probability" src="img/B08681_12_90.jpg" class="calibre12"/></div><p class="calibre10">。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec209" class="calibre7"/>贝叶斯法则</h2></div></div></div><p class="calibre10">如果我们已经知道<img alt="Bayes' rule" src="img/B08681_12_92.jpg" class="calibre27"/>和<img alt="Bayes' rule" src="img/B08681_12_93.jpg" class="calibre27"/>，贝叶斯法则给了我们一个计算<img alt="Bayes' rule" src="img/B08681_12_91.jpg" class="calibre27"/>的方法。我们可以很容易地得出贝叶斯法则如下:</p><div><img alt="Bayes' rule" src="img/B08681_12_94.jpg" class="calibre12"/></div><p class="calibre10">现在让我们看看中间和<a id="id931"/>右边的部分:</p><div><img alt="Bayes' rule" src="img/B08681_12_95.jpg" class="calibre12"/></div><div><img alt="Bayes' rule" src="img/B08681_12_96.jpg" class="calibre12"/></div><p class="calibre10">这是贝叶斯法则。简单来说，如下图:</p><div><img alt="Bayes' rule" src="img/B08681_12_97.jpg" class="calibre12"/></div></div></div></body></html>


<html>
  <head>
    <title>Introduction to Keras</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec96" class="calibre7"/>Keras简介</h1></div></div></div><p class="calibre10">这里我们将提供一个<a id="id933"/>对Keras的简单介绍，Keras是TensorFlow的一个子库，为实现深度学习算法提供了更多的高层函数。Keras使用基本的TensorFlow操作，在下面；然而，它为用户提供了一个更高级的、初学者友好的API。为了了解如何使用Keras，我们将看一个简单的例子。我们将概述如何使用Keras创建CNN。完整的练习可以在位于<code class="literal">appendix</code>文件夹的<code class="literal">keras_cnn.ipynb</code>中找到。</p><p class="calibre10">我们将首先确定我们将定义什么类型的模型。Keras有两种不同的API:顺序API和函数API。顺序API更简单，允许一层一层地设计模型。然而，顺序API在设计网络各层之间的组织和连接时具有有限的灵活性。另一方面，函数式API具有更大的灵活性，允许用户设计神经网络的具体细节。出于演示的目的，我们将使用Keras中的顺序API实现一个CNN。在这种情况下，顺序模型是一系列层(例如，输入层、卷积层和池层):</p><div><pre class="programlisting">model = Sequential()</pre></div><p class="calibre10">接下来，我们将逐一定义我们CNN的层。首先，我们将定义一个具有32个滤波器的卷积层，内核大小为3 × 3，非线性度为<em class="calibre15"> ReLU </em>。该层将接受大小为28 × 28 × 1的输入(即MNIST图像的大小):</p><div><pre class="programlisting">model.add(Conv2D(32, 3, activation='relu', input_shape=[28, 28, 1]))</pre></div><p class="calibre10">接下来，我们将定义一个最大池层。如果没有定义内核大小和步幅，它们默认为2(内核大小)和1(步幅):</p><div><pre class="programlisting">model.add(MaxPool2D())</pre></div><p class="calibre10">然后，我们将添加一个批量标准化层:</p><div><pre class="programlisting">model.add(BatchNormalization())</pre></div><p class="calibre10">一个批量归一化层(参考<em class="calibre15">批量归一化:通过减少内部协变量移位加速深度网络训练</em>、<em class="calibre15"> Ioffe </em>和<em class="calibre15"> Szegedy </em>、<em class="calibre15">国际机器学习会议</em>、<em class="calibre15"> 2015 </em>)归一化(即激活零均值和单位方差)前一层的输出。这是用于提高CNN性能的附加步骤，尤其是在计算机视觉应用中。请注意，我们在本章练习中没有使用<a id="id934"/>批处理规范化，因为与计算机视觉应用中使用的数量相比，批处理规范化并没有在NLP任务中大量使用。</p><p class="calibre10">接下来，我们将再添加两个卷积层，然后添加一个最大池层和一个批处理规范化层:</p><div><pre class="programlisting">model.add(Conv2D(64, 3, activation='relu'))
model.add(MaxPool2D())
model.add(BatchNormalization())
model.add(Conv2D(128, 3, activation='relu'))
model.add(MaxPool2D())
model.add(BatchNormalization())</pre></div><p class="calibre10">接下来，我们将展平输入，因为这是将输出馈入完全连接的层所必需的:</p><div><pre class="programlisting">model.add(Flatten())</pre></div><p class="calibre10">然后，我们将添加一个带有<code class="literal">256</code>隐藏单元的完全连接层、一个<em class="calibre15"> ReLU </em>激活层和一个带有<code class="literal">10</code> softmax单元的最终softmax输出层(即，用于MNIST的10个不同等级):</p><div><pre class="programlisting">model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))</pre></div><p class="calibre10">最后，我们将<em class="calibre15">编译</em>该模型，同时我们还告诉Keras使用<em class="calibre15">亚当</em>作为优化器，并使用分类交叉熵损失和输出度量作为模型的准确性:</p><div><pre class="programlisting">model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</pre></div><p class="calibre10">一旦定义了模型、损失和优化器，我们就可以如下运行Keras模型。</p><p class="calibre10">要训练模型，您可以使用以下命令:</p><div><pre class="programlisting">model.fit(x_train, y_train, batch_size = batch_size)</pre></div><p class="calibre10">这里，<code class="literal">x_train</code>和<code class="literal">y_train</code>是训练数据。而<code class="literal">batch_size</code>定义了批量大小。当您运行此程序时，培训进度将显示如下。</p><p class="calibre10">然后，使用以下方法评估模型:</p><div><pre class="programlisting">test_acc = model.evaluate(x_test, y_test, batch_size=batch_size)  </pre></div><p class="calibre10">这一行将再次输出进度条以及每个时期的测试损失和准确性。</p></div></body></html>


<html>
  <head>
    <title>Introduction to the TensorFlow seq2seq library</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec97" class="calibre7"/>tensor flow seq 2 seq文库简介</h1></div></div></div><p class="calibre10">我们在本书的所有<a id="id935"/>实现中使用了原始的TensorFlow API，以提高模型实际功能的透明度和更好的学习体验。然而，TensorFlow有各种各样的库，它们隐藏了实现的所有细粒度细节。这允许用户用更少的代码行实现序列到序列模型，如我们在第10章、<em class="calibre15">序列到序列学习-神经机器翻译</em>中看到的<strong class="calibre11">神经机器翻译</strong> ( <strong class="calibre11"> NMT </strong>)模型，而不用担心关于它们如何工作的更具体的技术细节。关于这些库的知识是很重要的，因为它们提供了一种更干净的方式<a id="id936"/>在生产代码中使用这些模型，或者在现有方法之外进行研究。因此，我们将快速介绍如何使用TensorFlow <code class="literal">seq2seq</code>库。这个代码可以在<code class="literal">seq2seq_nmt.ipynb</code>文件中作为练习使用。</p><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec210" class="calibre7"/>定义编码器和解码器的嵌入</h2></div></div></div><p class="calibre10">我们将首先定义<a id="id937"/>编码器输入、解码器输入和解码器输出占位符:</p><div><pre class="programlisting">enc_train_inputs = []
dec_train_inputs, dec_train_labels = [],[]
for ui in range(source_sequence_length):
    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='train_inputs_%d'%ui))

for ui in range(target_sequence_length):
    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='train_inputs_%d'%ui))
    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size],name='train_outputs_%d'%ui))</pre></div><p class="calibre10">接下来，我们将为所有编码器和解码器输入定义嵌入查找函数，以获得单词嵌入:</p><div><pre class="programlisting">encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]
encoder_emb_inp = tf.stack(encoder_emb_inp)

decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, src) for src in dec_train_inputs]
decoder_emb_inp = tf.stack(decoder_emb_inp)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec211" class="calibre7"/>定义编码器</h2></div></div></div><p class="calibre10">编码器由一个<a id="id938"/> LSTM单元作为其基本构建模块制成。然后，我们将定义<code class="literal">dynamic_rnn</code>，它以定义的LSTM单元作为输入，状态用零初始化。然后，我们将<code class="literal">time_major</code>参数设置为<code class="literal">True</code>，因为我们的数据将时间轴作为第一轴(即0轴)。换句话说，我们的数据具有<code class="literal">[sequence_length, batch_size, embeddings_size]</code>形状，其中依赖于时间的<code class="literal">sequence_length</code>位于第一轴。<code class="literal">dynamic_rnn</code>的好处是它能够处理动态大小的输入。您可以使用可选的<code class="literal">sequence_length</code>参数来定义批处理中每个句子的长度。例如，假设您有一个大小为<code class="literal">[3,30]</code>的批处理，其中有三个长度为[10，20，30]的句子(注意，我们用一个特殊的标记将短句子填充到30)。将值为[10，20，30]的张量作为<code class="literal">sequence_length</code>传递给T17会将计算出的超过每个句子长度的LSTM输出清零。对于单元状态，它不会清零，而是取在句子长度内计算的最后一个单元状态，并将该值复制到句子长度之外，直到达到30:</p><div><pre class="programlisting">encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)

encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp, initial_state=initial_state,
    sequence_length=[source_sequence_length for _ in range(batch_size)], 
    time_major=True, swap_memory=True)</pre></div><p class="calibre10"><code class="literal">swap_memory</code>选项允许TensorFlow在GPU和CPU之间交换推理过程中产生的张量，以防模型过于复杂而无法完全适合GPU。</p></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec212" class="calibre7"/>定义解码器</h2></div></div></div><p class="calibre10">解码器的定义类似于<a id="id940"/>编码器，但有一个称为<code class="literal">projection_layer</code>的额外层，它代表softmax输出层，用于对解码器做出的预测进行采样。我们还将定义一个<code class="literal">TrainingHelper</code>函数，它将解码器输入正确地馈送到解码器。在本例中，我们还定义了两种类型的解码器:a <code class="literal">BasicDecoder</code>和<code class="literal">BahdanauAttention</code>解码器。(注意机制在<a href="ch10.html" title="Chapter 10. Sequence-to-Sequence Learning – Neural Machine Translation">第10章</a>、<em class="calibre15">序列对序列学习-神经机器翻译</em>中讨论。)库中还有很多其他的解码器，比如<code class="literal">BeamSearchDecoder</code>和<code class="literal">BahdanauMonotonicAttention</code>:</p><div><pre class="programlisting">decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

projection_layer = Dense(units=vocab_size, use_bias=True)

helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, [target_sequence_length for _ in range(batch_size)], time_major=True)

if decoder_type == 'basic':
    decoder = tf.contrib.seq2seq.BasicDecoder(
        decoder_cell, helper, encoder_state,
        output_layer=projection_layer)
    
elif decoder_type == 'attention':
    decoder = tf.contrib.seq2seq.BahdanauAttention(
        decoder_cell, helper, encoder_state,
        output_layer=projection_layer)</pre></div><p class="calibre10">我们将使用动态解码来获得解码器的输出:</p><div><pre class="programlisting">outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, output_time_major=True,
    swap_memory=True
)</pre></div><p class="calibre10">接下来，我们将<a id="id941"/>定义逻辑、交叉熵损失和训练预测操作:</p><div><pre class="programlisting">logits = outputs.rnn_output

crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=dec_train_labels, logits=logits)
loss = tf.reduce_mean(crossent)

train_prediction = outputs.sample_id</pre></div><p class="calibre10">然后，我们将定义两个优化器，其中前10，000步使用<code class="literal">AdamOptimizer</code>，其余的优化过程使用普通随机变量<code class="literal">GradientDescentOptimizer</code>。这是因为，长期使用Adam optimizer会导致一些意想不到的行为。因此，我们将使用Adam为SGD优化器获得一个良好的初始位置，然后从那时起使用SGD:</p><div><pre class="programlisting">with tf.variable_scope('Adam'):
    optimizer = tf.train.AdamOptimizer(learning_rate)
with tf.variable_scope('SGD'):
    sgd_optimizer = tf.train.GradientDescentOptimizer(learning_rate)

gradients, v = zip(*optimizer.compute_gradients(loss))
gradients, _ = tf.clip_by_global_norm(gradients, 25.0)
optimize = optimizer.apply_gradients(zip(gradients, v))

sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))
sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 25.0)
sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))</pre></div><div><div><h3 class="title4"><a id="note45" class="calibre7"/>注意</h3><p class="calibre16">Bahar等人在一篇名为<em class="calibre15">神经机器翻译中优化算法的实证研究</em>，<em class="calibre15">布拉格数学语言学公报</em>，<em class="calibre15"> 2017 </em>的论文中对优化器在NMT训练中的表现进行了严格的评估。</p></div></div></div></div></body></html>


<html>
  <head>
    <title>Visualizing word embeddings with TensorBoard</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec98" class="calibre7"/>使用TensorBoard可视化单词嵌入</h1></div></div></div><p class="calibre10">当我们想在第3章、<em class="calibre15">Word 2 vec-学习单词嵌入</em>中<a id="id942"/>可视化单词嵌入时，我们用t-SNE算法手动实现了可视化。然而，你也可以使用TensorBoard来可视化单词<a id="id943"/>的嵌入。TensorBoard是TensorFlow提供的可视化工具。您可以使用TensorBoard在程序中可视化TensorFlow变量。这允许您查看各种变量随时间的变化情况(例如，模型损失/准确性)，以便您可以识别模型中的潜在问题。</p><p class="calibre10">TensorBoard使您能够将标量值和向量可视化为直方图。除此之外，TensorBoard还允许你可视化单词嵌入。因此，如果您需要分析嵌入是什么样子的话，它会将所有需要的代码实现从您身边带走。接下来我们将看到如何使用TensorBoard来可视化单词嵌入。本练习的代码在<code class="literal">appendix</code>文件夹的<code class="literal">tensorboard_word_embeddings.ipynb</code>中提供。</p><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec213" class="calibre7"/>启动张量板</h2></div></div></div><p class="calibre10">首先，我们将列出启动TensorBoard的步骤<a id="id944"/>。TensorBoard作为一个服务运行在一个特定的端口上(默认情况下，在<code class="literal">6006</code>)。要启动TensorBoard，您需要遵循以下步骤:</p><div><ol class="orderedlist"><li class="listitem1">打开命令提示符(Windows)或终端(Ubuntu/macOS)。</li><li class="listitem1">进入项目主目录。</li><li class="listitem1">如果您使用的是<code class="literal">python</code> virtuanenv，请激活安装了TensorFlow的虚拟环境。</li><li class="listitem1">确保可以通过Python看到TensorFlow库。为此，请遵循以下步骤:<div> <ol class="orderedlist1"> <li class="listitem1">键入<code class="literal">python3</code>，您将得到一个<code class="literal">&gt;&gt;&gt;</code> looking提示</li> <li class="listitem1"> Try <code class="literal">import tensorflow as tf</code> </li> <li class="listitem1">如果您可以成功运行此提示，您就没事了</li> <li class="listitem1">通过键入<code class="literal">exit()</code> </li> </ol> </div>退出<code class="literal">python</code>提示(即<code class="literal">&gt;&gt;&gt;</code></li><li class="listitem1">键入<code class="literal">tensorboard --logdir=models</code>:<div><ul class="itemizedlist2"><li class="listitem"><code class="literal">--logdir</code>选项指向您将创建数据的目录，以可视化</li> <li class="listitem">可选地，您可以使用<code class="literal">--port=&lt;port_you_like&gt;</code>来更改</li> </ul> </div>上运行的端口张量板</li><li class="listitem1">你现在应该得到以下消息:<div> <pre class="programlisting"> <strong class="calibre11">TensorBoard 1.6.0 at &lt;url&gt;;:6006 (Press CTRL+C to quit)</strong> </pre> </div></li><li class="listitem1">在网络浏览器中输入<code class="literal">&lt;url&gt;:6006</code>。此时，您应该能够看到一个橙色的仪表盘。您将没有任何东西可显示，因为我们还没有生成数据。</li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch11lvl2sec214" class="calibre7"/>通过TensorBoard保存单词嵌入和可视化</h2></div></div></div><p class="calibre10">首先，我们将下载并加载我们在<a href="ch09.html" title="Chapter 9. Applications of LSTM – Image Caption Generation">第9章</a>、<em class="calibre15">LSTM应用–图像字幕生成</em>中使用的50维手套嵌入。首先从<a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>下载手套嵌入文件(<code class="literal">glove.6B.zip</code>)并放入<code class="literal">appendix</code>文件夹。我们将在文件中加载前50，000个单词向量，稍后使用<a id="id946"/>来初始化TensorFlow变量。我们还将记录每个单词的单词串，因为我们稍后会将这些作为标签提供给<a id="id947"/>每个点以显示在TensorBoard上:</p><div><pre class="programlisting">vocabulary_size = 50000
pret_embeddings = np.empty(shape=(vocabulary_size,50),dtype=np.float32)

words = [] 

word_idx = 0
with zipfile.ZipFile('glove.6B.zip') as glovezip:
    with glovezip.open('glove.6B.50d.txt') as glovefile:
        for li, line in enumerate(glovefile):
            if (li+1)%10000==0: print('.',end='')
            line_tokens = line.decode('utf-8').split(' ')
            word = line_tokens[0]
            
            vector = [float(v) for v in line_tokens[1:]]
            assert len(vector)==50
            words.append(word)
            pret_embeddings[word_idx,:] = np.array(vector)
            word_idx += 1
            if word_idx == vocabulary_size:
                break</pre></div><p class="calibre10">现在，我们将定义与张量流相关的变量和运算。在此之前，我们将创建一个名为<code class="literal">models</code>的目录，用于存储变量:</p><div><pre class="programlisting">log_dir = 'models'

if not os.path.exists(log_dir):
    os.mkdir(log_dir)</pre></div><p class="calibre10">然后，我们将定义一个变量，这个变量<a id="id948"/>将用我们之前从文本文件中复制的单词嵌入进行初始化:</p><div><pre class="programlisting">embeddings = tf.get_variable('embeddings',shape=[vocabulary_size, 50],
                             initializer=tf.constant_initializer(pret_embeddings))</pre></div><p class="calibre10">接下来，我们将创建<a id="id949"/>一个会话，并初始化我们之前定义的变量:</p><div><pre class="programlisting">session = tf.InteractiveSession()
tf.global_variables_initializer().run()</pre></div><p class="calibre10">此后，我们将创建一个<code class="literal">tf.train.Saver</code>对象。<code class="literal">Saver</code>对象可用于将张量流变量保存到内存中，以便日后需要时可以恢复。在下面的代码中，我们将把嵌入变量保存到名为<code class="literal">model.ckpt</code>的<code class="literal">models</code>目录中:</p><div><pre class="programlisting">saver = tf.train.Saver({'embeddings':embeddings})
saver.save(session, os.path.join(log_dir, "model.ckpt"), 0)</pre></div><p class="calibre10">我们还需要保存元数据文件。元数据文件包含与单词嵌入相关联的标签/图像或其他类型的信息，因此当您将鼠标悬停在嵌入可视化上时，相应的点将显示它们所代表的单词/标签。元数据文件应该是<code class="literal">.tsv</code>(制表符分隔值)格式，并且应该包含<code class="literal">vocabulary_size + 1</code>行，其中第一行包含您要包含的信息的标题。在下面的代码中，我们将保存两条信息:单词字符串和每个单词的唯一标识符(即行索引):</p><div><pre class="programlisting">with open(os.path.join(log_dir,'metadata.tsv'), 'w',encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile, delimiter='\t',
                            quotechar='|', quoting=csv.QUOTE_MINIMAL)
    writer.writerow(['Word','Word ID'])
    for wi,w in enumerate(words):
      writer.writerow([w,wi])</pre></div><p class="calibre10">然后，我们需要告诉TensorFlow在哪里可以找到我们保存到磁盘的嵌入数据的元数据。为此，我们需要创建一个<code class="literal">ProjectorConfig</code>对象，它维护关于我们想要显示的嵌入的各种配置细节。存储在<code class="literal">ProjectorConfig</code>文件夹中的详细信息将被保存到<code class="literal">models</code>目录下的一个名为<code class="literal">projector_config.pbtxt</code>的文件中:</p><div><pre class="programlisting">config = projector.ProjectorConfig()</pre></div><p class="calibre10">这里，我们将填充我们创建的<code class="literal">ProjectorConfig</code>对象的必填字段。首先，我们将告诉它我们想要可视化的变量的名字。接下来，我们将告诉它在哪里可以找到对应于该变量的<a id="id950"/>元数据:</p><div><pre class="programlisting">embedding_config = config.embeddings.add()
embedding_config.tensor_name = embeddings.name
embedding_config.metadata_path = 'metadata.tsv'</pre></div><p class="calibre10">我们现在将使用一个<a id="id951"/>摘要编写器把它写到<code class="literal">projector_config.pbtxt</code>文件中。TensorBoard将在启动时读取该文件:</p><div><pre class="programlisting">summary_writer = tf.summary.FileWriter(log_dir)
projector.visualize_embeddings(summary_writer, config)</pre></div><p class="calibre10">现在，如果您加载TensorBoard，您应该会看到类似于<em class="calibre15">图A.3 </em>的内容:</p><div><img alt="Saving word embeddings and visualizing via TensorBoard" src="img/B08681_12_98.jpg" class="calibre12"/><div><p class="calibre10">图A.3:嵌入的张量板视图</p></div></div><p class="calibre10">当您悬停在显示的点云上时，它将显示您当前悬停的单词的标签，因为我们在<code class="literal">metadata.tsv</code>文件中提供了该信息。此外，你<a id="id952"/>有几个选择。第一个选项(用虚线显示并标记为<strong class="calibre11"> 1 </strong>)将允许您选择完整嵌入空间的子集。你可以在你感兴趣的嵌入空间的区域上画一个包围盒，它看起来如图<em class="calibre15">图A.4 </em>所示。我选择了右下角的嵌入:</p><div><img alt="Saving word embeddings and visualizing via TensorBoard" src="img/B08681_12_99.jpg" class="calibre12"/><div><p class="calibre10">图A.4:选择嵌入空间的子集</p></div></div><p class="calibre10">你的另一个选择是查看单词本身，而不是点。您可以通过选择<em class="calibre15">图A.3 </em>中的第二个选项来完成此操作(显示在实心框内并标记为<strong class="calibre11"> 2 </strong>)。这将如图<em class="calibre15">图A.5 </em>所示。此外，您可以根据自己的喜好平移/缩放/旋转视图。如果您点击帮助按钮(显示在实心框内，在<em class="calibre15">图A.5 </em>中标记为<strong class="calibre11"> 1 </strong>)，它将向您显示控制视图的指南:</p><div><img alt="Saving word embeddings and visualizing via TensorBoard" src="img/B08681_12_100.jpg" class="calibre12"/><div><p class="calibre10">图A.5:嵌入向量显示为单词而不是点</p></div></div><p class="calibre10">最后，您<a id="id954"/>可以从左侧面板<a id="id955"/>中更改可视化算法(在<em class="calibre15">图A.3 </em>中用虚线显示并标有<strong class="calibre11"> 3 </strong>)。</p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec99" class="calibre7"/>总结</h1></div></div></div><p class="calibre10">在这里，我们讨论了一些数学背景知识以及一些我们在其他章节中没有涉及到的实现。首先，我们讨论了标量、向量、矩阵和张量的数学符号。然后我们讨论了在这些数据结构上执行的各种操作，比如矩阵乘法和求逆。接下来，我们讨论了有助于理解概率机器学习的各种术语，如概率密度函数、联合概率、边际概率和贝叶斯规则。之后，我们将讨论转移到了其他章节中没有涉及到的各种实现。我们学会了如何使用Keras实现CNN的高级TensorFlow库。然后，我们讨论了如何使用TensorFlow中的seq2seq库高效地实现神经机器翻译器，并与我们在第10章、<em class="calibre15">序列到序列学习-神经机器翻译</em>中讨论的实现进行了比较。最后，我们用一个指导来结束这一节，这个指导教你使用张量板来可视化单词嵌入；TensorFlow自带的可视化平台。</p></div></body></html>
</body></html>