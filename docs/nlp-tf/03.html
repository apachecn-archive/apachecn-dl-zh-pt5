<html><head/><body>
<html>
  <head>
    <title>Chapter 3. Word2vec – Learning Word Embeddings</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title"><a id="ch03" class="calibre7"/>第三章。Word 2 vec–学习单词嵌入</h1></div></div></div><p class="calibre10">在这一章中，我们将讨论NLP中最重要的一个主题——word 2 vec，这是一种学习单词嵌入或单词的分布式数字特征表示(即向量)的技术。学习单词表示是许多NLP任务的基础，因为许多NLP任务依赖于单词的良好特征表示，这些特征表示保留了它们的语义以及它们在语言中的上下文。例如，单词<em class="calibre15">森林</em>的特征表示应该与<em class="calibre15">烤箱</em>非常不同，因为这些单词很少在类似的上下文中使用，而<em class="calibre15">森林</em>和<em class="calibre15">丛林</em>的表示应该非常相似。</p><div><div><h3 class="title4"><a id="note15" class="calibre7"/>注意</h3><p class="calibre16">Word2vec被称为<em class="calibre15">分布式表示</em>，因为单词的语义是由完整表示向量的激活<a id="id141"/>模式捕获的，而不是表示向量的单个元素(例如，将向量中的单个元素设置为1，将单个单词的rest设置为0)。</p></div></div><p class="calibre10">我们将一步一步地从解决这个问题的经典方法到基于现代神经网络的方法，这些方法在寻找好的单词表示方面提供了最先进的性能。在图3.1 中，我们(使用t-SNE，一种用于高维数据的可视化技术)可视化了2D画布上一组单词的这种学习到的单词嵌入。如果你仔细观察，你会发现相似的东西被放置得彼此靠近(例如，中间的簇中的数字):</p><div><img alt="Word2vec – Learning Word Embeddings" src="img/B08681_03_01.jpg" class="calibre12"/><div><p class="calibre10">图3.1:使用t-SNE的习得单词嵌入的可视化示例</p></div></div><div><div><h3 class="title4"><a id="note16" class="calibre7"/>注意</h3><p class="calibre16"><strong class="calibre11"> t分布随机邻居嵌入(t-SNE) </strong></p><p class="calibre16">这是一种将高维数据投影到二维空间的降维技术。这使我们能够想象高维数据是如何在空间中分布的，这是非常有用的，因为我们无法轻松地将三维以外的数据可视化。你将在下一章更详细地了解SNE霸王龙。</p></div></div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec22" class="calibre7"/>什么是词的表征或意义？</h1></div></div></div><p class="calibre10"><em class="calibre15">这个词表示</em>是什么意思？这与其说是一个技术问题，不如说是一个哲学问题。所以，我们不会试图辨别这个问题最合适的答案，而是接受一个更谦虚的答案，那就是，<em class="calibre15">意为</em>是一个词所传达的思想或表示。因为自然语言处理的主要目标是在语言任务中实现类似人类的表现，所以<a id="id143"/>探索为机器表示单词的原则方法是明智的。为了<a id="id144"/>实现这一点，我们将使用能够分析给定文本语料库并得出单词的良好数字表示(即单词嵌入)的算法，这样，与不相关的单词(例如<em class="calibre15"> cat </em>和<em class="calibre15"> volcano </em>)相比，属于相似上下文的单词(例如<em class="calibre15"> one </em>和<em class="calibre15"> two </em>、<em class="calibre15"> I </em>和<em class="calibre15"> we </em>)将具有相似的数字表示。</p><p class="calibre10">首先，我们将讨论实现这一目标的一些经典方法，然后继续了解使用神经网络来学习这种特征表示并提供最先进性能的更复杂的最新方法。</p></div></div></body></html>


<html>
  <head>
    <title>Classical approaches to learning word representation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec23" class="calibre7"/>学习单词表征的经典方法</h1></div></div></div><p class="calibre10">在这一部分，我们将讨论一些用数字表示单词的经典方法。这些方法主要可以分为两类:使用外部资源来表示单词的方法和不使用外部资源的方法。首先，我们将讨论WordNet——最流行的基于外部资源的单词表示方法之一。然后我们再<a id="id146"/>进行更多的<a id="id147"/>本地化方法(即不依赖外部资源的方法)，比如<strong class="calibre11"> one-hot编码</strong>和<strong class="calibre11">词频-逆文档频</strong> ( <strong class="calibre11"> TF-IDF </strong>)。</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec25" class="calibre7"/>WordNet——使用外部词汇知识库学习单词表达</h2></div></div></div><p class="calibre10">WordNet是处理单词表示的最流行的经典<a id="id148"/>方法或统计NLP之一。它依赖于一个外部的词汇知识库，该知识库对给定单词的定义、同义词、祖先、后代等信息进行编码。WordNet允许用户推断给定单词的各种信息，例如前面句子中讨论的单词的各个方面以及两个单词之间的相似性。</p><div><div><div><div><h3 class="title5"><a id="ch03lvl3sec09" class="calibre7"/>WordNet之旅</h3></div></div></div><p class="calibre10">如前所述，WordNet是一个词汇数据库，编码单词之间的词性标记关系，包括名词、动词、形容词和副词。WordNet由美国普林斯顿大学心理学系首创，目前由普林斯顿大学计算机科学系托管。WordNet考虑单词之间的同义词来评估单词之间的关系。英语WordNet目前拥有超过150，000个单词和超过100，000个同义词组(即同义词集)。另外，WordNet不仅仅局限于英语。自成立以来，大量不同的wordnets已经建立，可以在http://globalwordnet.org/wordnets-in-the-world/的<a href="http://globalwordnet.org/wordnets-in-the-world/">查看。</a></p><p class="calibre10">为了理解如何利用WordNet，为WordNet中使用的术语<a id="id151"/>打下坚实的基础是很重要的。首先，WordNet使用术语<strong class="calibre11"> synset </strong>来表示一组或一组同义词。接下来，每个synset都有一个<strong class="calibre11">定义</strong>，解释synset代表什么。synset中包含的同义词<a id="id152"/>称为<strong class="calibre11">引理</strong>。</p><p class="calibre10">在WordNet中，单词表示是分层建模的，这在给定的同义词集和与另一个同义词集的关联之间形成了一个复杂的图形。这些关联可以属于两个不同的类别:一个<em class="calibre15">是一个</em>关系或者一个<em class="calibre15">是由</em>构成的关系。首先，我们将讨论<em class="calibre15">是一个</em>协会。</p><p class="calibre10">对于一个给定的同素集，存在两类关系:上义词和下义词。<strong class="calibre11">同素集的上位词</strong>是承载所考虑的同素集的一般(高级)含义的同素集。例如，<em class="calibre15">车</em>是同系列<em class="calibre15">车</em>的上位词。接下来，<strong class="calibre11">下位词</strong>是比对应的同义词集更具体的<a id="id154"/>同义词集。比如<em class="calibre15">丰田车</em>是synset <em class="calibre15">车</em>的下称。</p><p class="calibre10">现在让我们讨论一个synset的<em class="calibre15">是由</em>组成的关系。<strong class="calibre11">同素集的全名</strong>是一组<a id="id155"/>同素集，代表所考虑的同素集的整个实体。例如，<em class="calibre15">轮胎</em>的全称是<em class="calibre15">汽车</em> synset。<strong class="calibre11">部分名</strong>是一个<em class="calibre15"> is-made-of </em>范畴<a id="id156"/>，代表全名的反义词，其中部分名是组成相应同义词集的部分或物质同义词集。我们可以在<em class="calibre15">图3.2 </em>中看到这一点:</p><div><img alt="Tour of WordNet" src="img/B08681_03_02.jpg" class="calibre12"/><div><p class="calibre10">图3.2:存在于一个同义词集中的各种关联</p></div></div><p class="calibre10">NLTK <a id="id157"/>库是一个Python自然语言处理库，可以用来理解WordNet及其机制。位于<code class="literal">ch3</code>文件夹中的<code class="literal">ch3_wordnet.ipynb</code>文件中提供了完整的示例作为练习。</p><div><div><h3 class="title4"><a id="note17" class="calibre7"/>注意</h3><p class="calibre16"><strong class="calibre11">安装NLTK库</strong></p><p class="calibre16">要将NLTK库安装到Python中，可以使用下面的Python <code class="literal">pip</code>命令:</p><div><pre class="programlisting">
<strong class="calibre11">pip install nltk</strong>
</pre></div><p class="calibre16">或者，可以使用一个IDE(比如PyCharm)通过<strong class="calibre11">图形用户界面</strong> ( <strong class="calibre11"> GUI </strong>)来安装这个库。你可以在<a href="http://www.nltk.org/install.html">http://www.nltk.org/install.html</a>找到更详细的说明。</p><p class="calibre16">要将NLTK导入Python并下载WordNet语料库，首先导入<code class="literal">nltk</code>库:</p><div><pre class="programlisting">import nltk</pre></div><p class="calibre16">然后，您可以通过运行以下命令下载WordNet语料库:</p><div><pre class="programlisting">nltk.download('wordnet')</pre></div></div></div><p class="calibre10">在安装并导入了<code class="literal">nltk</code>库之后，我们需要用这个命令导入WordNet语料库:</p><div><pre class="programlisting">from nltk.corpus import wordnet as wn</pre></div><p class="calibre10">然后我们可以如下查询WordNet语料库:</p><div><pre class="programlisting"># retrieves all the available synsets
word = 'car'
car_syns = wn.synsets(word)

# The definition of each synset of car synsets
syns_defs = [car_syns[i].definition() for i in range(len(car_syns))]

# Get the lemmas for the first Synset
car_lemmas = car_syns[0].lemmas()[:3]

# Let's get hypernyms for a Synset (general superclass)
syn = car_syns[0]
print('\t',syn.hypernyms()[0].name(),'\n')

# Let's get hyponyms for a Synset (specific subclass)
syn = car_syns[0]
print('\t',[hypo.name() for hypo in syn.hyponyms()[:3]],'\n')

# Let's get part-holonyms for the third "car"
# Synset (specific subclass)
syn = car_syns[2]
print('\t',[holo.name() for holo in syn.part_holonyms()],'\n')

# Let's get meronyms for a Synset (specific subclass)
syn = car_syns[0]
print('\t',[mero.name() for mero in syn.part_meronyms()[:3]],'\n')</pre></div><p class="calibre10">运行<a id="id159"/>示例后，结果将如下所示:</p><div><pre class="programlisting">All the available Synsets for car
[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]

Example definitions of available synsets:
car.n.01 :  a motor vehicle with four wheels; usually propelled by an internal combustion engine
car.n.02 :  a wheeled vehicle adapted to the rails of railroad
car.n.03 :  the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant

Example lemmas for the Synset  car.n.03
['car', 'auto', 'automobile']

Hypernyms of the Synset  car.n.01
motor_vehicle.n.01
Hyponyms of the Synset  car.n.01
['ambulance.n.01', 'beach_wagon.n.01', 'bus.n.04']

Holonyms (Part) of the Synset  car.n.03
['airship.n.01']

Meronyms (Part) of the Synset  car.n.01
['accelerator.n.01', 'air_bag.n.01', 'auto_accessory.n.01']</pre></div><p class="calibre10">我们也可以用下面的方法得到两个同素集之间的相似性。NLTK中实现了几个<a id="id160"/>不同的相似性度量，你可以在官方网站上看到它们的运行(<a href="http://www.nltk.org/howto/wordnet.html">www.nltk.org/howto/wordnet.html</a>)。这里，我们使用Wu-Palmer相似性，它基于两个同义词集在同义词集层次结构中的深度来度量它们之间的<a id="id161"/>相似性:</p><div><pre class="programlisting">sim = wn.wup_similarity(w1_syns[0], w2_syns[0])</pre></div></div><div><div><div><div><h3 class="title5"><a id="ch03lvl3sec10" class="calibre7"/>WordNet的问题</h3></div></div></div><p class="calibre10">尽管WordNet是一个令人惊奇的资源，任何人都可以用它来学习自然语言处理任务中单词的意思，但是在这方面使用WordNet有很多缺点。它们如下:</p><div><ul class="itemizedlist"><li class="listitem">遗漏细微差别是WordNet的一个关键问题。这对于WordNet来说是不可行的，这既有理论上的原因，也有实践上的原因。从理论的角度来看，对两个实体之间的细微差异的定义进行建模是不适定的或不直接的。实事求是地说，定义细微差别是主观的。比如<em class="calibre15">想要</em>和<em class="calibre15">需要</em>这两个词意思差不多，但其中一个(<em class="calibre15">需要</em>)更有主见。这被认为是一个细微的差别。</li><li class="listitem">其次，WordNet本身是主观的，因为WordNet是由一个相对较小的社区设计的。因此，根据你要解决的问题，WordNet可能是合适的，或者你可以用一个宽松的单词定义来做得更好。</li><li class="listitem">还存在维护WordNet的问题，这是劳动密集型的。维护和添加新的synsets、定义、引理等可能会非常昂贵。这对WordNet的可伸缩性产生了不利影响，因为要让WordNet保持最新状态，人力是必不可少的。</li><li class="listitem">为其他语言开发WordNet的成本很高。此外，还有一些为其他语言构建的WordNet，以及将它与英语WordNet链接为T2(MWN T4)多字网(T1)的努力，但这些努力尚未完成。</li></ul></div><p class="calibre10">接下来，我们将讨论几种不依赖外部资源的单词表示技术。</p></div></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec26" class="calibre7"/>一热编码表示</h2></div></div></div><p class="calibre10">表示<a id="id165"/>单词的一种<a id="id164"/>更简单的方式是使用热门的编码表示。这意味着，如果我们拥有一个<em class="calibre15"> V </em>大小的词汇，对于每个<em class="calibre15">I<sup class="calibre26">th</sup>T13】单词<em class="calibre15">w<sub class="calibre17">I</sub></em>，我们将代表<em class="calibre15">w<sub class="calibre17">I</sub>T21【用一个<em class="calibre15"> V </em>长向量<em class="calibre15">0，0，0，0，…，0，1，0，…，0，0，0] </em>其中举个例子，想想这句话:</em></em></p><p class="calibre10"><em class="calibre15">鲍勃和玛丽是好朋友。</em></p><p class="calibre10">每个单词的热门编码表示可能如下所示:</p><p class="calibre10"><em class="calibre15"> Bob: [1，0，0，0，0，0] </em></p><p class="calibre10"><em class="calibre15">和:[0，1，0，0，0，0] </em></p><p class="calibre10"><em class="calibre15"> Mary: [0，0，1，0，0，0] </em></p><p class="calibre10"><em class="calibre15">为:【0，0，0，1，0，0】</em></p><p class="calibre10"><em class="calibre15">好:[0，0，0，0，1，0] </em></p><p class="calibre10"><em class="calibre15">朋友:[0，0，0，0，0，1] </em></p><p class="calibre10">但是，正如您可能已经发现的那样，这种表示有许多缺点。</p><p class="calibre10">这种表示方式不会以任何方式对单词之间的相似性进行编码，并且完全<a id="id166"/>忽略了单词使用的上下文。让我们将单词向量之间的点积视为<a id="id167"/>相似性度量。两个向量越相似，这两个向量的点积越高。例如，单词<em class="calibre15"> car </em>和<em class="calibre15">car</em>的表示将具有相似距离0，其中<em class="calibre15"> car </em>和<em class="calibre15">铅笔</em>也将具有相同的值。</p><p class="calibre10">这种方法对于大型词汇会变得非常无效。此外，对于典型的自然语言处理任务，词汇量很容易超过50，000个单词。因此，对于50，000个单词的单词表示矩阵将产生非常稀疏的50，000 × 50，000矩阵。</p><p class="calibre10">然而，即使在最先进的单词嵌入学习算法中，一键编码也起着重要的作用。我们使用一键编码来用数字表示单词，并将其输入神经网络，以便神经网络可以更好地学习单词的更小的数字特征表示。</p><div><div><h3 class="title4"><a id="note20" class="calibre7"/>注意</h3><p class="calibre16">一键编码也称为局部化表示(与分布式表示相反)，因为特征表示由向量中的单个元素的激活决定。</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec27" class="calibre7"/>TF-IDF法</h2></div></div></div><p class="calibre10">TF-IDF是<a id="id168"/>一种基于频率的方法，它考虑了<a id="id169"/>一个单词在语料库中出现的频率。这是一个单词表示，它表示给定文档中特定单词的重要性。直觉上，这个词出现的频率越高，它在文档中就越重要。例如，在一个关于猫的文档中，<em class="calibre15">猫</em>这个词会出现得更多。然而，仅仅计算频率是行不通的，因为像<em class="calibre15"> this </em>和<em class="calibre15"> is </em>这样的<a id="id170"/>单词出现的频率很高，但是并没有携带那么多信息。TF-IDF考虑到了这一点，对这样的常用词给了0值。</p><p class="calibre10">同样，<em class="calibre15"> TF </em>代表术语频率，<em class="calibre15"> IDF </em>代表逆文档频率:</p><p class="calibre10"><em class="calibre15">TF(w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)= w</em><em class="calibre15">I</em><em class="calibre15">出现次数/总字数</em></p><p class="calibre10"><em class="calibre15">IDF(w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)= log(文档总数/其中有w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">的文档数)</em></p><p class="calibre10"><em class="calibre15">TF-IDF(w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)= TF(w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)x IDF(w</em><em class="calibre15">T56】I</em><em class="calibre15">)</em></p><p class="calibre10">我们来做一个<a id="id171"/>快速练习。考虑两个文档:</p><div><ul class="itemizedlist"><li class="listitem">文档1: <em class="calibre15">这是关于猫的。猫是很好的伙伴。</em></li><li class="listitem">文档二:<em class="calibre15">这是关于狗的。狗很忠诚。</em></li></ul></div><p class="calibre10">现在让我们来处理一些数字:</p><p class="calibre10"><em class="calibre15"> TF-IDF (cats，doc1) = (2/8) * log(2/1) = 0.075 </em></p><p class="calibre10"><em class="calibre15"> TF-IDF (this，doc2) = (1/8) * log(2/2) = 0.0 </em></p><p class="calibre10">因此，单词<em class="calibre15"> cats </em>是信息性的，而<em class="calibre15">这个</em>不是。这是我们在衡量单词重要性时需要的预期行为。</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec28" class="calibre7"/>共生矩阵</h2></div></div></div><p class="calibre10">同现<a id="id172"/>矩阵不同于一热编码<a id="id173"/>表示，它对单词的上下文信息进行编码，但需要维护一个V × V矩阵。为了理解共现矩阵，我们来看两个例句:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">杰瑞和玛丽是朋友。</em></li><li class="listitem"><em class="calibre15">杰瑞给玛丽买花。</em></li></ul></div><p class="calibre10">共生矩阵看起来像下面的矩阵。因为矩阵是对称的，所以我们仅示出了矩阵的一个三角形:</p><div><table border="1" class="calibre18"><colgroup class="calibre19"><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/><col class="calibre20"/></colgroup><thead class="calibre21"><tr class="calibre22"><th valign="bottom" class="calibre23"> </th><th valign="bottom" class="calibre23">
<p class="calibre10">尿壶</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">和</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">玛丽</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">是</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">朋友</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">购买</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">花</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">为</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10">杰瑞</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">和</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">玛丽</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">是</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">朋友</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">购买</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">鲜花</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">一</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10"><strong class="calibre11">为</strong></p>
</td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25"> </td><td valign="top" class="calibre25">
<p class="calibre10">0</p>
</td></tr></tbody></table></div><p class="calibre10">然而，不难看出，维护这样的共现矩阵是有代价的，因为矩阵的大小随着词汇表的大小而多项式增长。此外，合并大于1的上下文窗口大小并不简单。一种选择是具有加权计数，其中上下文中的单词的权重随着与感兴趣的单词的距离而恶化。</p><p class="calibre10">所有这些缺点促使我们去研究更有原则、更健壮、更可扩展的学习和推断单词含义(即表示)的方法。</p><p class="calibre10">Word2vec <a id="id175"/>是一种最近推出的分布式单词表示学习技术，目前被用作许多NLP任务的特征工程技术(例如，机器翻译、聊天机器人和图像字幕生成器)。本质上，Word2vec通过查看使用单词的周围单词(即上下文)来学习单词表示。更具体地说，我们试图通过神经网络预测给定一些单词的上下文(反之亦然)，这导致神经网络被迫学习良好的单词嵌入。我们将在下一节详细讨论这种方法。Word2vec方法与前面描述的方法相比有许多优点。它们如下:</p><div><ul class="itemizedlist"><li class="listitem">Word2vec方法不像基于WordNet的方法那样受人类语言知识的影响。</li><li class="listitem">Word2vec表示向量大小与词汇大小无关，这不同于one-hot编码表示或单词共现矩阵。</li><li class="listitem">Word2vec是一种分布式表示。与表示依赖于表示向量的单个元素的激活(例如，一键编码)的局部化表示不同，分布式表示依赖于向量中所有元素的激活模式。这为Word2vec提供了比one-hot编码表示更强的表达能力。</li></ul></div><p class="calibre10">在接下来的部分中，我们将首先通过一个例子来开发一些关于学习单词嵌入<a id="id176"/>的直观感受。然后我们<a id="id177"/>将定义一个损失函数，这样我们就可以使用机器学习来学习单词嵌入。此外，我们将讨论两种Word2vec算法，即<strong class="calibre11">跳格</strong>和<strong class="calibre11">连续词包</strong> ( <strong class="calibre11"> CBOW </strong>)算法。</p></div></div></body></html>


<html>
  <head>
    <title>Word2vec – a neural network-based approach to learning word representation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec24" class="calibre7"/>word 2 vec——一种基于神经网络的单词表示学习方法</h1></div></div></div><div><blockquote class="blockquote"><p class="calibre10">"从一个人交的朋友，你就可以知道这个人说的话."</p><p class="calibre10">–<em class="calibre15">j . r . Firth</em></p></blockquote></div><p class="calibre10">J.R. Firth在1957年提出的这句话是Word2vec的基础，因为Word2vec <a id="id178"/>技术使用给定单词的上下文来学习其语义。Word2vec是一种突破性的方法，允许在没有任何人工干预的情况下学习单词的含义。此外，Word2vec通过查看给定单词周围的单词来学习单词的数字表示。</p><p class="calibre10">我们可以通过想象一个真实世界的场景来测试前面引述的正确性。想象一下，你正在参加一场考试，你在第一个问题中发现了这句话:“玛丽是个非常固执的孩子。她的淫荡本性总是给她带来麻烦。”现在，除非你非常聪明，否则你可能不知道<em class="calibre15">淫荡</em>是什么意思。在这种情况下，你会不由自主地被迫看感兴趣的单词周围的短语。在我们的例子中，<em class="calibre15">淫荡的</em>被<em class="calibre15">顽固的</em>、<em class="calibre15">天性的</em>和<em class="calibre15">麻烦的</em>所包围。看这三个字就足以确定pervicacious其实就是固执的一种状态。我认为这足以证明语境对于词义的重要性。</p><p class="calibre10">现在我们来讨论Word2vec的基础知识。如前所述，Word2vec通过查看上下文并用数字表示来学习给定单词的意思。通过<em class="calibre15">上下文</em>，我们指的是感兴趣的单词前面和后面的固定数量的单词。我们拿一个有<em class="calibre15"> N </em>个单词的假设语料来说。从数学上来说，这可以用一系列单词来表示，分别用<em class="calibre15"> w </em> <em class="calibre15"> <sub class="calibre17"> 0 </sub> </em>，<em class="calibre15"> w </em> <em class="calibre15"> <sub class="calibre17"> 1 </sub> </em>，…，<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>，<em class="calibre15">w</em><em class="calibre15">T45】N</em>表示，其中<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>是</p><p class="calibre10">接下来，如果我们想找到一个能够学习单词含义的好算法，给定一个单词，我们的算法应该能够正确预测上下文单词。这意味着，对于任何给定的单词<em class="calibre15">w<sub class="calibre17">I</sub>T3】，下面的概率应该是高的:</em></p><div><img alt="Word2vec – a neural network-based approach to learning word representation" src="img/B08681_03_03.jpg" class="calibre12"/></div><p class="calibre10">为了到达等式的右边，我们需要假设给定目标单词(<em class="calibre15">w<sub class="calibre17">I</sub>T7)，上下文单词彼此独立(例如，<em class="calibre15"> w </em> <em class="calibre15"> <sub class="calibre17"> i-2 </sub> </em>和<em class="calibre15">w</em><em class="calibre15"><sub class="calibre17">I-1</sub></em>是独立的)。虽然不完全正确，但是这种近似使得学习问题变得实际，并且在实践中运行良好。</em></p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec29" class="calibre7"/>练习:皇后=国王——他+她吗？</h2></div></div></div><p class="calibre10">在<a id="id179"/>继续之前，让我们做一个小练习来理解最大化前面提到的概率是如何导致找到单词的好的含义(或表示)的。考虑以下非常小的语料库:</p><p class="calibre10">有一个非常富有的国王。他有一个美丽的王后。她非常善良。</p><p class="calibre10">现在，让我们做一些手动预处理，删除标点符号和无信息的单词:</p><p class="calibre10">富有的国王有美丽的王后，她很善良</p><p class="calibre10">现在让我们按照格式(<em class="calibre15">目标词→上下文词1，上下文词2 </em>)为每个词及其上下文词组成一组元组。我们将假设任一侧的上下文窗口大小为1:</p><p class="calibre10"><em class="calibre15">曾→富</em></p><p class="calibre10"><strong class="calibre11">有钱→有势，称王</strong></p><p class="calibre10"><strong class="calibre11">国王→富人，他</strong></p><p class="calibre10"><em class="calibre15">何→王，有了</em></p><p class="calibre10"><em class="calibre15">有了→他，漂亮了</em></p><p class="calibre10"><em class="calibre15">漂亮→有了，女王</em></p><p class="calibre10"><strong class="calibre11">女王→美丽的她</strong></p><p class="calibre10"><strong class="calibre11">她→皇后，是</strong></p><p class="calibre10"><em class="calibre15">被→她，种</em></p><p class="calibre10"><em class="calibre15">善良→被</em></p><p class="calibre10">请记住，我们的目标是能够预测右边的单词，前提是给定左边的单词。要做到这一点，对于给定的单词，右侧上下文中的单词应该与左侧上下文中的单词具有高度的数字或几何相似性。换句话说，感兴趣的词应该由周围的词来传达。现在让我们假设实际的数字向量来理解这是如何工作的。为了简单起见，我们只考虑粗体突出显示的元组。让我们从假设单词<em class="calibre15"> rich </em>如下开始:</p><p class="calibre10"><em class="calibre15">富→【0，0】</em></p><p class="calibre10">为了能够正确预测<em class="calibre15">是</em>和<em class="calibre15">王</em>来自<em class="calibre15">富</em>，<em class="calibre15">是</em>和<em class="calibre15">王</em>应该与<em class="calibre15">富</em>这个词有很高的相似度。让我们假设向量之间的欧几里德距离为相似性乘积。</p><p class="calibre10">让我们为单词<em class="calibre15"> king </em>和<em class="calibre15"> rich </em>尝试以下值:</p><p class="calibre10"><em class="calibre15">国王→ [0，1] </em></p><p class="calibre10"><em class="calibre15">是→ [-1，0] </em></p><p class="calibre10">结果如下所示:</p><p class="calibre10"><em class="calibre15"> Dist(rich，king) = 1.0 </em></p><p class="calibre10"><em class="calibre15"> Dist(rich，mm2)= 1.0</em></p><p class="calibre10">这里，<em class="calibre15"> Dist </em>是两个词之间的欧几里德距离。如图3.3 所示:</p><div><img alt="Exercise: is queen = king – he + she?" src="img/B08681_03_04.jpg" class="calibre12"/><div><p class="calibre10">图3.3:单词“rich”、“was”和“king”的单词向量的位置</p></div></div><p class="calibre10">现在让我们考虑下面的元组:</p><p class="calibre10"><em class="calibre15">国王→富人，他</em></p><p class="calibre10">我们已经建立了<em class="calibre15">国王</em>和<em class="calibre15">富人</em>之间的关系。然而，这还没有完成；我们越看一段关系，这两个词应该越接近。所以，我们先调整一下<em class="calibre15">王</em>的矢量，让它更接近<em class="calibre15">富</em>:</p><p class="calibre10"><em class="calibre15">王者→【0，0.8】</em></p><p class="calibre10">接下来，我们将需要添加单词<em class="calibre15">他</em>到图片中。<em class="calibre15">何</em>这个词应该更接近<em class="calibre15">王</em>。这就是我们现在所知道的关于“他”这个词的所有信息:</p><p class="calibre10"><em class="calibre15">何→【0.5，0.8】</em></p><p class="calibre10">这时，带有文字的图形看起来像<em class="calibre15">图3.4 </em>:</p><div><img alt="Exercise: is queen = king – he + she?" src="img/B08681_03_05.jpg" class="calibre12"/><div><p class="calibre10">图3.4:单词“rich”、“was”、“king”和“he”的单词向量的位置</p></div></div><p class="calibre10">现在我们来进行下一个二元组:<em class="calibre15">女王→美丽的她</em>和<em class="calibre15">她→女王，曾是</em>。请注意，我已经交换了元组的顺序，因为这使我们更容易理解这个示例:</p><p class="calibre10"><em class="calibre15">她→女王，是</em></p><p class="calibre10">现在，我们将<a id="id181"/>不得不使用我们先前关于英语的知识继续前进。放置单词<em class="calibre15"> she </em>是一个合理的决定，它与<em class="calibre15"> he </em>距离单词<em class="calibre15"> was </em>的距离相同，因为它们在单词<em class="calibre15"> was </em>的上下文中的用法是等同的。因此，让我们用这个:</p><p class="calibre10"><em class="calibre15">她→【0.5，0.6】</em></p><p class="calibre10">接下来，我们将用<em class="calibre15">女王</em>这个词接近<em class="calibre15">她</em>这个词:</p><p class="calibre10"><em class="calibre15">皇后→ [0.0，0.6] </em></p><p class="calibre10">这在<em class="calibre15">图3.5 </em>中说明:</p><div><img alt="Exercise: is queen = king – he + she?" src="img/B08681_03_06.jpg" class="calibre12"/><div><p class="calibre10">图3.5:单词“富有”、“曾经”、“国王”、“他”、“她”和“女王”的单词向量的位置</p></div></div><p class="calibre10">接下来，我们只有以下元组:</p><p class="calibre10"><em class="calibre15">女王→美丽的她</em></p><p class="calibre10">在这里，<em class="calibre15">美丽的</em>这个词被发现了。它与<em class="calibre15">女王</em>和<em class="calibre15">她</em>这两个词的距离应该大致相同。让我们用下面的例子:</p><p class="calibre10"><em class="calibre15">漂亮→【0.25，0】</em></p><p class="calibre10">现在我们<a id="id182"/>有了下面这个描绘单词之间关系的图表。当我们观察<em class="calibre15">图3.6 </em>时，它似乎是词语含义的非常直观的表示:</p><div><img alt="Exercise: is queen = king – he + she?" src="img/B08681_03_07.jpg" class="calibre12"/><div><p class="calibre10">图3.6:单词“富有”、“曾经”、“国王”、“他”、“她”、“女王”和“美丽”的单词向量的位置</p></div></div><p class="calibre10">现在，让我们来看看这个从这个练习开始就一直潜伏在我们脑海中的问题。这个等式中的量是否等价:<em class="calibre15">皇后=国王–他+她</em>？好吧，我们已经得到了解开这个谜团所需的所有资源。让我们先试试等式的右边:</p><p class="calibre10"><em class="calibre15"> =国王-他+她</em></p><p class="calibre10"><em class="calibre15"> = [0，0.8]–[0.5，0.8] + [0.5，0.6] </em></p><p class="calibre10"><em class="calibre15"> = [0，0.6] </em></p><p class="calibre10">最后一切都解决了。如果你看看我们为单词<em class="calibre15">女王</em>获得的单词向量，你会发现这与我们之前推导出的答案完全相似。</p><p class="calibre10">请注意，这是一个显示如何学习单词嵌入的粗略工作，如果使用算法学习，这可能与单词嵌入的确切位置不同。</p><p class="calibre10">然而，请记住，这是一个不切实际的缩小现实世界语料库的练习。所以，你不能仅仅通过计算十几个数字就用手算出这些值。这就是像神经网络这样复杂的函数逼近器为我们工作的地方。但是，要使用神经网络，我们需要用一种数学上自信的方式来表述我们的问题。然而，这是一个很好的练习，实际上展示了单词向量的力量。</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec30" class="calibre7"/>设计用于学习单词嵌入的损失函数</h2></div></div></div><p class="calibre10">即使是一个简单的现实世界任务的词汇量也能轻易超过10，000个单词。因此，我们不能为大型文本语料库手动开发单词向量，而需要设计一种方法来使用一些机器学习算法(例如，神经网络)自动找到好的单词嵌入，以有效地执行这项费力的任务。此外，为了对任何类型的任务使用任何类型的机器学习算法，我们需要定义一个损失，因此完成任务就变成了最小化损失。让我们定义寻找好的单词嵌入向量的损失。</p><p class="calibre10">首先，让我们回忆一下我们在本节开始时讨论的等式:</p><div><img alt="Designing a loss function for learning word embeddings" src="img/B08681_03_08.jpg" class="calibre12"/></div><p class="calibre10">记住这个等式，我们可以定义神经网络的成本函数:</p><div><img alt="Designing a loss function for learning word embeddings" src="img/B08681_03_09.jpg" class="calibre12"/></div><p class="calibre10">记住，<img alt="Designing a loss function for learning word embeddings" src="img/B08681_03_30.jpg" class="calibre27"/>是损失(即成本)，不是回报。还有，我们要最大化<em class="calibre15">P(w<sub class="calibre17">j</sub></em><em class="calibre15">| w<sub class="calibre17">I</sub></em><em class="calibre15">)</em>。因此，我们需要在表达式前面加一个减号，将它转换成一个成本函数。</p><p class="calibre10">现在，我们不使用产品操作符，而是将它转换成日志空间。将方程转换到对数空间将引入一致性和数值稳定性。这给了我们以下等式:</p><div><img alt="Designing a loss function for learning word embeddings" src="img/B08681_03_10.jpg" class="calibre12"/></div><p class="calibre10">这个成本函数的公式被称为<strong class="calibre11">负对数似然</strong>。</p><p class="calibre10">现在，由于我们有一个公式化的成本函数，可以使用神经网络来优化<a id="id185"/>这个成本函数。这样做将迫使单词向量或单词嵌入根据它们的意思很好地组织它们自己。现在，是时候了解使用这个成本函数来找到好的单词嵌入的现有算法了。</p></div></div></body></html>


<html>
  <head>
    <title>The skip-gram algorithm</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec25" class="calibre7"/>跳格算法</h1></div></div></div><p class="calibre10">我们将要讨论的第一个<a id="id186"/>算法被称为<strong class="calibre11">跳格算法</strong>。Mikolov等人在2013年推出的skip-gram算法是一种利用书面文本的单词上下文来学习良好单词嵌入的算法。让我们一步一步来理解skip-gram算法。</p><p class="calibre10">首先，我们将讨论数据准备过程，然后介绍理解算法所需的符号。最后，我们将讨论算法本身。</p><p class="calibre10">正如我们在许多地方讨论的那样，这个词的意思可以从这个特定词周围的上下文中得到。然而，开发一个利用这一特性来学习词义的模型并不完全简单。</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec31" class="calibre7"/>从原始文本到结构化数据</h2></div></div></div><p class="calibre10">首先，我们<a id="id187"/>需要设计一个机制来提取一个数据集，这个数据集可以提供给我们的学习模型。这样的数据集应该是一组格式为<em class="calibre15">(输入，输出)</em>的元组。此外，这需要以无人监管的方式创建。也就是说，人类不应该有<a id="id189"/>来手动设计数据的标签。总之，数据准备过程应做到以下几点:</p><div><ul class="itemizedlist"><li class="listitem">捕捉给定单词的周围单词</li><li class="listitem">以无人监督的方式表演</li></ul></div><p class="calibre10">skip-gram模型使用以下方法来设计这样的数据集:</p><div><ol class="orderedlist"><li class="listitem1">对于给定的单词<em class="calibre15">w<sub class="calibre17">I</sub>T3】，假设一个上下文窗口大小<em class="calibre15"> m </em>。通过<em class="calibre15">上下文窗口大小</em>，我们指的是单侧被认为是上下文的单词的数量。因此，对于<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>，上下文窗口(包括目标单词<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>)的大小将为<em class="calibre15"> 2m+1 </em>，并且将如下所示:<em class="calibre15">【w</em><em class="calibre15"><sub class="calibre17">I-m</sub></em><em class="calibre15">，…，w</em><em class="calibre15">I-1</em><em class="calibre15">，w<em class="calibre15"/></em></em></li><li class="listitem1">Next, input-output tuples are formed as <em class="calibre15">[…, (w</em><em class="calibre15"><sub class="calibre17">i</sub></em><em class="calibre15">, w</em><em class="calibre15"><sub class="calibre17">i-m</sub></em><em class="calibre15">), …,(w</em><em class="calibre15"><sub class="calibre17">i</sub></em><em class="calibre15">,w</em><em class="calibre15"><sub class="calibre17">i-1</sub></em><em class="calibre15">), (w</em><em class="calibre15"><sub class="calibre17">i</sub></em><em class="calibre15">,w</em><em class="calibre15"><sub class="calibre17">i+1</sub></em><em class="calibre15">), …, (w</em><em class="calibre15"><sub class="calibre17">i</sub></em><em class="calibre15">,w</em><em class="calibre15"><sub class="calibre17">i+m</sub></em><em class="calibre15">), …]</em>; here, <img alt="From raw text to structured data" src="img/B08681_03_11.jpg" class="calibre27"/> and <em class="calibre15">N</em> is the number <a id="id190"/>of words in the text to get a practical <a id="id191"/>insight. Let's assume <a id="id192"/>the following sentence and context window size (<em class="calibre15">m</em>) of 1:
<em class="calibre15">The dog barked at the mailman.</em><p class="calibre10">对于此示例，数据集如下所示:</p><p class="calibre10"><em class="calibre15"> [(狗，那个)，(狗，吠叫)，(吠叫，狗)，(吠叫，在)，…，(那个，在)，(邮递员)] </em></p></li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec32" class="calibre7"/>用神经网络学习单词嵌入</h2></div></div></div><p class="calibre10">一旦<a id="id193"/>数据处于<em class="calibre15">(输入，输出)</em>格式，我们就可以使用神经网络来学习单词embeddings。首先，让我们确定学习单词嵌入所需的变量。为了存储<a id="id194"/>单词嵌入，我们需要一个V × D矩阵，其中<em class="calibre15"> V </em>是词汇量，而<em class="calibre15"> D </em>是单词嵌入的维度(即表示单个单词的向量中的元素数量)。<em class="calibre15"> D </em>是用户定义的超参数。D<em class="calibre15">D</em>越高，学习到的单词嵌入就越有表现力。该矩阵<a id="id195"/>将被称为<em class="calibre15">嵌入空间</em>或<em class="calibre15">嵌入层</em>。接下来，我们有一个softmax层，权重大小为D × V，偏移大小为<em class="calibre15"> V </em>。</p><p class="calibre10">每个单词将被表示为大小为<em class="calibre15"> V </em>的独热编码向量，其中一个元素为1，所有其他元素为0。因此，输入字和相应的输出字的大小都是<em class="calibre15"> V </em>。我们先把<em class="calibre15"> i </em> <sup class="calibre26"> th </sup>输入称为<em class="calibre15"> x </em> <em class="calibre15"> <sub class="calibre17"> i </sub> </em>对应的嵌入为<em class="calibre15">x</em><em class="calibre15"><sub class="calibre17">I</sub></em>z<em class="calibre15"/><em class="calibre15">I</em>，对应的输出为<em class="calibre15">y</em></p><p class="calibre10">至此，我们已经定义了必要的变量。接下来，对于每个输入<em class="calibre15">x</em><em class="calibre15">T3】IT5】，我们将从对应于该输入的嵌入层中查找嵌入向量。这个操作为我们提供了<em class="calibre15"> z </em> <em class="calibre15"> <sub class="calibre17"> i </sub> </em>，这是一个<em class="calibre15"> D </em>大小的向量(也就是一个<em class="calibre15"> D </em> -long嵌入向量)。随后，我们使用以下变换计算<em class="calibre15"> x </em> <em class="calibre15"> <sub class="calibre17"> i </sub> </em>的预测输出:</em></p><p class="calibre10"><em class="calibre15">logit(x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)= z</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">W+b</em></p><p class="calibre10"><em class="calibre15">ŷ</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">= softmax(logit(x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em></p><p class="calibre10">这里，<em class="calibre15">logit(x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>表示未规格化的分数(即logits)，<em class="calibre15"> ŷ </em> <em class="calibre15"> <sub class="calibre17"> i </sub> </em>是<em class="calibre15"> V </em>大小的预测输出(表示输出是来自<em class="calibre15"> V </em>大小的词汇的一个词的概率)，<em class="calibre15"> W 【T我们将可视化skip-gram模型的概念视图(<em class="calibre15">图3.7 </em>)和实现视图(<em class="calibre15">图3.8 </em>)。这里有一个符号的摘要<a id="id196"/>:</em></p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15"> V </em>:这是<a id="id197"/>词汇量的大小</li><li class="listitem"><em class="calibre15"> D </em>:这是嵌入层的维度</li><li class="listitem"><em class="calibre15"> x </em></li><li class="listitem"><em class="calibre15"> z </em> <em class="calibre15"> <sub class="calibre17"> i </sub> </em>:这是第<em class="calibre15"> i </em>个输入字对应的嵌入(即表示)向量</li><li class="listitem"><em class="calibre15">y</em><em class="calibre15"><sub class="calibre17">I</sub></em>:这是<a id="id198"/>对应于<em class="calibre15">x</em><em class="calibre15"><sub class="calibre17">I</sub></em>的一位热编码输出字</li><li class="listitem"><em class="calibre15">ŷ</em><em class="calibre15"><sub class="calibre17">I</sub></em>:这是<em class="calibre15">x</em><em class="calibre15"><sub class="calibre17">I</sub></em>的预测输出</li><li class="listitem"><em class="calibre15">logit(x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>:这是输入<em class="calibre15">x</em><em class="calibre15"><sub class="calibre17">I</sub></em><img alt="Learning the word embeddings with a neural network" src="img/B08681_03_31.jpg" class="calibre27"/>的非标准化分数</li><li class="listitem">:这是单词<em class="calibre15">w</em>T17】j的一个热编码表示</li><li class="listitem"><em class="calibre15"> W </em>:这是softmax权重矩阵</li><li class="listitem"><em class="calibre15"> b </em>:这是softmax <div> <img alt="Learning the word embeddings with a neural network" src="img/B08681_03_12.jpg" class="calibre12"/> <div> <p class="calibre10">的偏差图3.7:概念跳过图模型</p> </div> </div></li></ul></div><div><img alt="Learning the word embeddings with a neural network" src="img/B08681_03_13.jpg" class="calibre12"/><div><p class="calibre10">图3.8:跳格模型的实现</p></div></div><p class="calibre10">使用<a id="id199"/>现有的和派生的实体，我们<a id="id200"/>现在可以使用负对数似然损失函数来计算给定数据点<em class="calibre15">(x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">，y</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>的损失。如果你<a id="id201"/>想知道什么是<em class="calibre15">P(w</em><em class="calibre15"><sub class="calibre17">j</sub></em><em class="calibre15">| w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>的话，可以从已经定义好的实体中派生出来。接下来，让我们讨论如何从<em class="calibre15"/><em class="calibre15"><sub class="calibre17"/>| w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>ŷ<em class="calibre15"><sub class="calibre17">I</sub></em>中计算出<em class="calibre15">p(w</em><em class="calibre15"><sub class="calibre17">j</sub></em><em class="calibre15">j】j</em><em class="calibre15"><em class="calibre15">| w</em>I<em class="calibre15"/>并推导出正式定义。</em></p><div><div><h3 class="title4"><a id="note21" class="calibre7"/>注意</h3><p class="calibre16"><strong class="calibre11">原话嵌入纸为什么要用两个嵌入层？</strong></p><p class="calibre16">原始论文(Mikolov等人，2013年)使用两个不同的V × D嵌入空间来表示目标空间中的单词(用作目标时的单词)和上下文空间中的单词(用作上下文单词的单词)。这样做的一个动机是，同一个单词在其本身的上下文中不经常出现。所以，我们要把这种事情发生的概率降到最低。例如，对于目标词<em class="calibre15">狗</em>，极不可能在其上下文(<em class="calibre15"> P(狗|狗)~ 0 </em>)中也找到<em class="calibre15">狗</em>这个词。直观地说，如果我们把(<em class="calibre15">x</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">= dog</em>和<em class="calibre15">y</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">= dog</em>)数据点馈送给神经网络，如果神经网络预测<em class="calibre15"> dog </em>为<em class="calibre15"> dog </em>的上下文词，我们就是在要求神经网络给出更高的损失。换句话说，我们要求单词<em class="calibre15">狗</em>的单词嵌入与单词<em class="calibre15">狗</em>的单词嵌入有非常高的距离。这产生了强烈的矛盾，因为同一单词的嵌入之间的距离将是0。因此，如果我们只有一个单一的嵌入空间，我们无法实现这一点。然而，具有用于目标单词和上下文单词的两个单独的嵌入空间允许我们具有这个属性，因为这样我们对于相同的单词具有两个单独的嵌入向量。然而，在实践中，只要避免输入-输出元组，拥有相同的单词作为输入和输出允许我们使用单个嵌入空间，并消除对两个不同嵌入层的需要。</p></div></div><div><div><div><div><h3 class="title5">制定一个实用的损失函数</h3></div></div></div><p class="calibre10">让我们更仔细地检查我们的损失函数。我们得出的损失应该如下:</p><div><img alt="Formulating a practical loss function" src="img/B08681_03_14.jpg" class="calibre12"/></div><p class="calibre10">然而，从我们目前手头的实体中计算这个特定的损失并不完全简单。</p><p class="calibre10">先来了解一下<em class="calibre15">P(w</em><em class="calibre15"><sub class="calibre17">j</sub></em><em class="calibre15">| w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>实体代表什么。为此，我们将从单个单词符号转移到单个数据点符号。也就是我们会说<em class="calibre15">P(w</em><em class="calibre15"><sub class="calibre17">j</sub></em><em class="calibre15">，w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>是由<em class="calibre15">n</em><em class="calibre15"><sup class="calibre26">th</sup></em>数据点给出的，该数据点有<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>的一键编码向量作为输入(<em class="calibre15">这由以下等式给出:</em></p><div><img alt="Formulating a practical loss function" src="img/B08681_03_15.jpg" class="calibre12"/></div><p class="calibre10"><em class="calibre15"> logit(x </em>然后，我们相对于与整个词汇表中的所有单词相对应的所有logit值，对索引<em class="calibre15">w<sub class="calibre17">j</sub>T82】处的logit值进行归一化。这种特殊类型的归一化被称为softmax激活(或归一化)。现在，通过将其转换为对数空间，我们得到以下等式:</em></p><div><img alt="Formulating a practical loss function" src="img/B08681_03_17.jpg" class="calibre12"/></div><p class="calibre10">为了有效地计算<em class="calibre15"> logit </em>函数，我们可以摆弄变量，得出以下符号:</p><div><img alt="Formulating a practical loss function" src="img/B08681_03_18.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="Formulating a practical loss function" src="img/B08681_03_31.jpg" class="calibre27"/>是<em class="calibre15"> w <sub class="calibre17"> j </sub> </em>的一键编码向量。现在<em class="calibre15"> logit </em>运算已经简化为求和与乘积运算。因为<img alt="Formulating a practical loss function" src="img/B08681_03_31.jpg" class="calibre27"/>只有一个非零的<a id="id203"/>元素对应于单词<em class="calibre15">w<sub class="calibre17">j</sub>T12】，所以在计算中只使用向量的索引。这比通过扫描词汇表大小的向量来查找logit向量中与非零元素的索引相对应的值在计算上更有效。</em></p><p class="calibre10">现在，通过分配我们获得的logit计算，对于损失，我们得到以下结果:</p><div><img alt="Formulating a practical loss function" src="img/B08681_03_19.jpg" class="calibre12"/></div><p class="calibre10">让我们考虑一个例子来理解这个计算:</p><p class="calibre10"><em class="calibre15">我喜欢NLP </em></p><p class="calibre10">我们可以如下创建输入-输出元组:</p><p class="calibre10"><em class="calibre15">(喜欢，我)</em></p><p class="calibre10"><em class="calibre15">(喜欢，NLP) </em></p><p class="calibre10">现在让我们假设前面的单词有以下一个热编码表示:</p><p class="calibre10"><em class="calibre15">喜欢–1，0，0 </em></p><p class="calibre10"><em class="calibre15">I–0，1，0 </em></p><p class="calibre10"><em class="calibre15">NLP–0，0，1 </em></p><p class="calibre10">接下来我们来考虑输入输出元组<em class="calibre15"> (like，I) </em>。当我们通过skip-gram学习模型传播类似于的输入<em class="calibre15">时，让我们假设我们依次获得了类似于</em>、<em class="calibre15"> I </em>和<em class="calibre15"> NLP </em>的单词<em class="calibre15">的以下逻辑:</em></p><p class="calibre10">2,10,5</p><p class="calibre10">现在，词汇表中每个单词的softmax输出如下:</p><p class="calibre10"><em class="calibre15">P(like | like)= exp(2)/(exp(2)+exp(10)+exp(5))= 0.118</em></p><p class="calibre10"><em class="calibre15">P(I | like)= exp(10)/(exp(2)+exp(10)+exp(5))= 0.588</em></p><p class="calibre10"><em class="calibre15">P(NLP | like)= exp(5)/(exp(2)+exp(10)+exp(5))= 0.294</em></p><p class="calibre10">前面的损失函数说我们需要最大化<em class="calibre15"> P(I|like) </em>来最小化损失。现在让我们将我们的例子应用于这个损失函数:</p><p class="calibre10"><em class="calibre15"> =- ( [0，1，0] * ([2，10，5]) - log(exp([1，0，0]*[2，10，5]) + exp([0，1，0]*[2，10，5]))+exp([0，0，1]*[2，10，5])</em></p><p class="calibre10"><em class="calibre15">=-(10-log(exp(2)+exp(10)+exp(5)))= 0.007</em></p><p class="calibre10">有了这个<a id="id204"/>损失函数，对于负号前的项，对应单词<em class="calibre15"> I </em>的<em class="calibre15"> y </em>向量中只有一个非零元素。因此，我们将只考虑概率<em class="calibre15"> P(I|like) </em>，这正是我们想要的。</p><p class="calibre10">然而，这并不是我们想要的理想解决方案。这个损失函数的目的从实用的角度来看，我们希望在给定一个词的情况下最大化预测上下文词的概率，同时最小化给定一个词的“所有”非上下文词的概率。我们很快就会看到，在实践中，定义明确的损失函数并不能有效地解决我们的问题。我们将需要设计一个更巧妙的近似损失函数来在可行的持续时间内学习好的单词嵌入。</p></div><div><div><div><div><h3 class="title5"><a id="ch03lvl3sec12" class="calibre7"/>有效地逼近损失函数</h3></div></div></div><p class="calibre10">我们很幸运拥有一个在数学上和直觉上都很可靠的损失函数。然而，努力并没有就此结束。如果我们试图像前面讨论的那样计算封闭形式的损失函数，我们将不可避免地面临算法极其缓慢的问题。</p><p class="calibre10">这种缓慢是由于大量的词汇导致了性能瓶颈。让我们看看我们的成本函数:</p><div><img alt="Efficiently approximating the loss function" src="img/B08681_03_20.jpg" class="calibre12"/></div><p class="calibre10">您将看到，计算单个示例的损失需要计算词汇表中所有单词的logits。与计算机视觉问题不同，在计算机视觉问题中，数百个输出类足以解决大多数现存的现实世界问题，skip-gram <a id="id206"/>并不具备这些特性。因此，我们需要将注意力转向损失的有效近似值，而不丧失我们模型的有效性。</p><p class="calibre10">我们将讨论近似法的两种流行选择:</p><div><ul class="itemizedlist"><li class="listitem">负采样</li><li class="listitem">分级softmax</li></ul></div><div><div><div><div><h4 class="title6"><a id="ch03lvl4sec07"/>soft max层的负采样</h4></div></div></div><p class="calibre10">在这里，我们<a id="id207"/>将讨论我们的第一种方法:负采样软最大层。负采样是<strong class="calibre11">噪声对比估计</strong> ( <strong class="calibre11"> NCE </strong>)方法的近似。NCE说，一个好的模型应该通过逻辑回归来区分数据和噪音。</p><p class="calibre10">考虑到这一点，让我们重新表述我们学习单词嵌入的目标。我们不需要一个全概率模型，该模型对于一个给定的单词具有词汇表中所有单词的精确概率。我们需要的是高质量的词向量。因此，我们可以将我们的问题简化为区分实际数据(即输入输出对)和噪声(即K-多虚噪声输入输出对)。通过<em class="calibre15">噪声</em>，我们指的是使用不属于给定单词上下文的单词创建的错误输入-输出对。我们还将去掉softmax激活，代之以sigmoid激活(也称为逻辑函数)。这允许我们在保持输出在[0，1]之间的同时，消除成本函数对全部词汇的依赖性。我们可以在<em class="calibre15">图3.9 </em>中可视化负样本过程。</p><p class="calibre10">准确地说，我们的原始损失函数由以下等式给出:</p><div><img alt="Negative sampling of the softmax layer" src="img/B08681_03_32.jpg" class="calibre12"/></div><p class="calibre10">前面的公式变成了这样:</p><div><img alt="Negative sampling of the softmax layer" src="img/B08681_03_33.jpg" class="calibre12"/></div><p class="calibre10">这里σ表示sigmoid激活，其中<em class="calibre15"> σ(x)=1/(1+exp(-x)) </em>。注意，为了清晰起见，我在原来的损失函数中用一个<img alt="Negative sampling of the softmax layer" src="img/B08681_03_42.jpg" class="calibre27"/>代替了<em class="calibre15">logit(x</em><em class="calibre15">n</em><em class="calibre15">)</em><em class="calibre15">wj</em>。可以看到新的损失函数只依赖于词汇表中与<em class="calibre15"> k </em>项相关的计算。</p><p class="calibre10">经过一些简化后，我们得出以下等式:</p><div><img alt="Negative sampling of the softmax layer" src="img/B08681_03_36.jpg" class="calibre12"/></div><p class="calibre10">让我们花点时间来理解这个等式的含义。为了简化，让我们假设k=1。这给了我们以下等式:</p><div><img alt="Negative sampling of the softmax layer" src="img/B08681_03_37.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15">w<sub class="calibre17">j</sub>T18】表示<em class="calibre15">w<sub class="calibre17">I</sub>T22】的上下文词，<em class="calibre15"> w </em> <em class="calibre15"> <sub class="calibre17"> q </sub> </em>表示<em class="calibre15"> w <sub class="calibre17"> i </sub> </em>的非上下文词。这个等式本质上说的是，为了最小化<em class="calibre15"> J(θ) </em>，我们应该使<img alt="Negative sampling of the softmax layer" src="img/B08681_03_39.jpg" class="calibre27"/>，这意味着<img alt="Negative sampling of the softmax layer" src="img/B08681_03_41.jpg" class="calibre27"/>需要是一个大的正值。那么，<img alt="Negative sampling of the softmax layer" src="img/B08681_03_40.jpg" class="calibre27"/>意味着<img alt="Negative sampling of the softmax layer" src="img/B08681_03_43.jpg" class="calibre27"/>需要是一个大的负值。换句话说，对于表示真实目标词和上下文词的真实数据点，应该得到大的正值，而表示目标词和噪声的虚假数据点应该得到大的负值。这与softmax函数的行为相同，但计算效率更高。</em></em></p><div><img alt="Negative sampling of the softmax layer" src="img/B08681_03_23.jpg" class="calibre12"/><div><p class="calibre10">图3.9:负采样过程</p></div></div><p class="calibre10">这里，<img alt="Negative sampling of the softmax layer" src="img/B08681_03_24.jpg" class="calibre27"/>是<a id="id209"/>乙状结肠激活。直观上，我们在损失函数计算中执行以下两个步骤:</p><div><ul class="itemizedlist"><li class="listitem">计算<em class="calibre15">w<sub class="calibre17">j</sub>T44】非零列的损耗(推向正)</em></li><li class="listitem">计算K个噪声样本的损失(趋向负值)</li></ul></div></div><div><div><div><div><h4 class="title6"><a id="ch03lvl4sec08"/>分级softmax</h4></div></div></div><p class="calibre10">分级<a id="id210"/> softmax比负采样略复杂，但与负采样的目的相同；即，近似softmax，而不必计算所有训练样本的词汇表中所有单词的激活。但是，与负采样不同，分层softmax只使用实际数据，不需要噪声样本。我们可以在<em class="calibre15">图3.10 </em>中可视化层次化的softmax模型。</p><p class="calibre10">为了理解分层softmax，让我们考虑一个例子:</p><p class="calibre10">我喜欢自然语言处理。深度学习很神奇。</p><p class="calibre10">这方面的词汇如下:</p><p class="calibre10"><em class="calibre15">我，喜欢，NLP，深度，学习，是，惊艳</em></p><p class="calibre10">有了这个词汇表，我们将构建一个二叉树，词汇表中的所有单词都以叶节点<a id="id211"/>的形式出现。我们还将添加一个特殊的令牌<strong class="calibre11"> PAD </strong>来确保所有的树叶都有两个成员:</p><div><img alt="Hierarchical softmax" src="img/B08681_03_25.jpg" class="calibre12"/><div><p class="calibre10">图3.10:分层softmax</p></div></div><p class="calibre10">然后，我们的最后一个隐藏层将完全连接到层次结构中的所有节点(见<em class="calibre15">图3.11 </em>)。注意，与经典的softmax层相比，该模型具有相似的总重量；但是，对于给定的计算，它只使用其中的一个子集:</p><div><img alt="Hierarchical softmax" src="img/B08681_03_26.jpg" class="calibre12"/><div><p class="calibre10">图3.11:分级softmax如何连接到嵌入层</p></div></div><p class="calibre10">假设<a id="id212"/>我们需要推断<em class="calibre15"> P(NLP|like) </em>的概率，其中<em class="calibre15"> like </em>为输入词。那么我们只需要权重的子集来计算概率，如图<em class="calibre15">图3.12 </em>所示:</p><div><img alt="Hierarchical softmax" src="img/B08681_03_27.jpg" class="calibre12"/><div><p class="calibre10">图3.12:用分级softmax计算概率</p></div></div><p class="calibre10">具体来说，这里的<a id="id213"/>是如何计算概率的:</p><div><img alt="Hierarchical softmax" src="img/B08681_03_28.jpg" class="calibre12"/></div><p class="calibre10">既然现在我们知道了如何计算<em class="calibre15">P(w</em><em class="calibre15"><sub class="calibre17">j</sub></em><em class="calibre15">| w</em><em class="calibre15"><sub class="calibre17">I</sub></em><em class="calibre15">)</em>，我们就可以使用原来的损失函数。注意，该方法仅使用连接到路径中的节点的权重进行计算，导致高计算效率。</p></div><div><div><div><div><h4 class="title6"><a id="ch03lvl4sec09"/>学习层次结构</h4></div></div></div><p class="calibre10">虽然分层的softmax是有效的，但一个重要的问题仍然没有答案。我们如何确定树的分解？更准确地说，哪个单词将跟随哪个分支？有几个选项可以实现这一点:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre11">随机初始化层次</strong>:这种<a id="id215"/>方法确实有一些性能下降，因为随机布局不能保证在单词之间有最好的可能分支。</li><li class="listitem"><strong class="calibre11">使用WordNet确定层级</strong> : WordNet可用于确定<a id="id216"/>单词在树中的合适顺序。这种方法比随机初始化表现得好得多。</li></ul></div></div><div><div><div><div><h4 class="title6"><a id="ch03lvl4sec10"/>优化学习模式</h4></div></div></div><p class="calibre10">因为我们有一个公式化的损失函数，所以优化就是从TensorFlow库中调用正确的函数。将使用的优化流程是一个<a id="id217"/>随机优化流程，这意味着我们不会一次提供完整的数据集，而是针对许多步骤随机提供一批数据。</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec33" class="calibre7"/>用TensorFlow实现skip-gram</h2></div></div></div><p class="calibre10">我们现在将<a id="id218"/>浏览一个使用TensorFlow库的skip-gram算法的实现。在这里，我们将只讨论定义所需的张量流操作的基本部分，以学习嵌入，而不是运行操作。完整练习可在<code class="literal">ch3</code>练习目录的<code class="literal">ch3_word2vec.ipynb</code>中找到。</p><p class="calibre10">首先让我们定义模型的超参数。您可以随意更改这些超参数，以查看它们如何影响最终性能(例如，<code class="literal">batch_size = 16</code>或<code class="literal">batch_size = 256</code>)。然而，由于与更复杂的真实世界问题相比，这是一个简单的问题，所以您可能看不到任何显著的差异(除非您将它们变得极端，例如，<code class="literal">batch_size = 1</code>或<code class="literal">num_sampled = 1</code>):</p><div><pre class="programlisting">batch_size = 128
embedding_size = 128 # Dimension of the embedding vector.
window_size = 4 # How many words to consider left and right.
valid_size = 16 # Random set of words to evaluate similarity on.
# Only pick dev samples in the head of the distribution.
valid_window = 100 
valid_examples = get_common_and_rare_word_ids(valid_size//2,valid_size//2)
num_sampled = 32 # Number of negative examples to sample.</pre></div><p class="calibre10">接下来，为训练输入、标签和有效输入定义张量流占位符:</p><div><pre class="programlisting">train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</pre></div><p class="calibre10">然后，为嵌入层和softmax权重和偏差定义张量流变量:</p><div><pre class="programlisting">embeddings = tf.Variable(
  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
softmax_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
stddev=0.5 / math.sqrt(embedding_size)))
softmax_biases =
  tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))</pre></div><p class="calibre10">接下来，我们将定义一个嵌入查找操作，该操作收集一批给定训练输入的相应嵌入:</p><div><pre class="programlisting">embed = tf.nn.embedding_lookup(embeddings, train_dataset)</pre></div><p class="calibre10">之后，我们将使用负采样来定义softmax损失:</p><div><pre class="programlisting">loss = tf.reduce_mean(
  tf.nn.sampled_softmax_loss(weights=softmax_weights,
  biases=softmax_biases, inputs=embed,
  labels=train_labels, num_sampled=num_sampled,
  num_classes=vocabulary_size))</pre></div><p class="calibre10">这里我们定义了一个<a id="id219"/>优化器来优化(最小化)前面定义的<code class="literal">loss</code>函数。随意尝试https://www.tensorflow.org/api_guides/python/train列出的其他优化器:</p><div><pre class="programlisting">optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)</pre></div><p class="calibre10">计算验证输入示例和所有嵌入之间的相似性。使用余弦距离:</p><div><pre class="programlisting">norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))
normalized_embeddings = embeddings / norm
valid_embeddings = tf.nn.embedding_lookup(
  normalized_embeddings, valid_dataset)
similarity = tf.matmul(valid_embeddings,
  tf.transpose(normalized_embeddings))</pre></div><p class="calibre10">定义了所有的张量流变量和操作之后，我们现在可以继续执行操作来获得一些结果。这里我们将概述执行这些操作的基本过程。您可以参考练习文件以获得执行的完整视图。</p><div><ul class="itemizedlist"><li class="listitem">首先用<code class="literal">tf.global_variables_initializer().run()</code>初始化张量流变量</li><li class="listitem">对于每个步骤(对于预定义的总步骤数)，执行以下操作:<div> <ul class="itemizedlist2"> <li class="listitem">使用数据生成器</li> <li class="listitem">生成一批数据(<code class="literal">batch_data</code>–输入，<code class="literal">batch_labels</code>–输出)创建一个名为<code class="literal">feed_dict</code>的字典，将列车输入/输出占位符映射到由数据生成器生成的数据:<div> <pre class="programlisting">feed_dict = {train_dataset : batch_data, train_labels : batch_labels}</pre> </div> </li> <li class="listitem">执行一个优化步骤并获得损失值如下:<div> <pre class="programlisting">_, l = session.run([optimizer, loss], feed_dict=feed_dict)</pre> </div> </li> </ul> </div></li></ul></div><p class="calibre10">我们现在将讨论另一种流行的Word2vec算法，称为<strong class="calibre11">连续词袋</strong> ( <strong class="calibre11"> CBOW </strong>)模型。</p></div></div></body></html>


<html>
  <head>
    <title>The Continuous Bag-of-Words algorithm</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec26" class="calibre7"/>连续词袋算法</h1></div></div></div><p class="calibre10">CBOW模型的工作方式类似于skip-gram算法，但在问题表述方面有一个重大变化<a id="id220"/>。在跳格模型中，我们从目标词预测上下文词。然而，在CBOW模型中，我们将从上下文单词中预测目标。让我们通过前面的例句来比较skip-gram和CBOW的数据:</p><p class="calibre10">狗对着邮差吠叫。</p><p class="calibre10">对于skip-gram，数据元组— <em class="calibre15">(输入字，输出字)</em>—可能如下所示:</p><p class="calibre10"><em class="calibre15">(狗叫)</em>、<em class="calibre15">(狗叫)</em>、<em class="calibre15">(狗叫)</em>等等。</p><p class="calibre10">对于CBOW，数据元组如下所示:</p><p class="calibre10"><em class="calibre15">(【狗吠】、狗吠)</em>、<em class="calibre15">(【狗吠】、狗吠)</em>等等。</p><p class="calibre10">因此，CBOW的输入具有2 × m × D的维度，其中<em class="calibre15"> m </em>是上下文窗口大小，<em class="calibre15"> D </em>是嵌入的维度。CBOW的概念模型如图<em class="calibre15">图3.13 </em>所示:</p><div><img alt="The Continuous Bag-of-Words algorithm" src="img/B08681_03_29.jpg" class="calibre12"/><div><p class="calibre10">图3.13:CBOW模型</p></div></div><p class="calibre10">我们不会详细讨论CBOW的复杂性，因为它们与skip-gram非常相似。然而，我们将讨论算法实现(虽然不深入，因为它与skip-gram有许多相似之处)，以清楚地了解如何正确实现CBOW。CBOW的完整实现可在<code class="literal">ch3</code>练习文件夹中的<code class="literal">ch3_word2vec.ipynb</code>处获得。</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec34" class="calibre7"/>在TensorFlow中实施CBOW</h2></div></div></div><p class="calibre10">首先，我们<a id="id221"/>定义<a id="id222"/>变量；这与跳格模型的情况相同:</p><div><pre class="programlisting">embeddings = tf.Variable(tf.random_uniform([vocabulary_size,
  embedding_size], -1.0, 1.0, dtype=tf.float32))
softmax_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
  stddev=1.0 / math.sqrt(embedding_size),
  dtype=tf.float32))
softmax_biases =
  tf.Variable(tf.zeros([vocabulary_size],dtype=tf.float32))</pre></div><p class="calibre10">这里，我们正在创建一组堆叠的嵌入，代表上下文的每个位置。所以我们将有一个大小为<em class="calibre15">【批量大小，嵌入大小，2 *上下文窗口大小】</em>的矩阵。然后，我们将使用归约运算符，通过对最后一个轴上的堆叠嵌入求平均，将堆叠的<a id="id223"/>矩阵归约为大小为<em class="calibre15">【batch _ size，embedding size】</em>的<a id="id224"/>:</p><div><pre class="programlisting">stacked_embedings = None
for i in range(2*window_size):
  embedding_i = tf.nn.embedding_lookup(embeddings,
  train_dataset[:,i])
  x_size,y_size = embedding_i.get_shape().as_list()
  if stacked_embedings is None:
    stacked_embedings = tf.reshape(embedding_i,[x_size,y_size,1])
  else:
    stacked_embedings =
    tf.concat(axis=2,
      values=[stacked_embedings,
      tf.reshape(embedding_i,[x_size,y_size,1])]
    )

assert stacked_embedings.get_shape().as_list()[2]==2*window_size
mean_embeddings = tf.reduce_mean(stacked_embedings,2,keepdims=False)</pre></div><p class="calibre10">此后，<code class="literal">loss</code>和<code class="literal">optimizer</code>被定义为在跳格模型中:</p><div><pre class="programlisting">loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights,
        biases=softmax_biases,
        inputs=mean_embeddings,
        labels=train_labels, 
        num_sampled=num_sampled, 
        num_classes=vocabulary_size))
optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)</pre></div></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec27" class="calibre7"/>总结</h1></div></div></div><p class="calibre10">单词嵌入已经成为许多NLP任务的组成部分，并且广泛用于诸如机器翻译、聊天机器人、图像字幕生成和语言建模的任务。单词嵌入不仅起到了降维技术的作用(与一键编码相比),而且还提供了比其他现有技术更丰富的特征表示。在本章中，我们讨论了两种流行的基于神经网络的单词表示学习方法，即skip-gram模型和CBOW模型。</p><p class="calibre10">首先，我们讨论了经典的方法来理解单词表征在过去是如何习得的。我们讨论了各种方法，例如使用WordNet、构建单词的共现矩阵以及计算TF-IDF。稍后，我们讨论了这些方法的局限性。</p><p class="calibre10">这促使我们探索基于神经网络的单词表征学习方法。首先，我们手工设计了一个例子来理解如何计算单词嵌入或单词向量，以及一个单词向量的用例来学习使用单词向量可以做的有趣的事情。</p><p class="calibre10">接下来，我们讨论了第一个单词嵌入学习算法——跳格模型。然后，我们学习了如何准备用于学习的数据。后来，我们研究了如何设计一个损失函数，允许我们使用给定单词的上下文单词来使用单词嵌入。之后，我们讨论了我们开发的闭合形式损失函数的一个关键限制。损失函数对于大型词汇表是不可伸缩的。随后，我们分析了两种常见的闭合损失近似值，这两种近似值使我们能够高效且有效地计算损失——负采样和分层softmax。最后，我们讨论了如何使用TensorFlow实现skip-gram算法。</p><p class="calibre10">然后我们回顾了学习单词嵌入的下一个选择CBOW模型。我们还讨论了CBOW与跳格模型的不同之处。最后，我们还讨论了CBOW的TensorFlow实现。</p><p class="calibre10">在下一章中，我们将分析我们所学的Word2vec技术的性能，并学习几个显著提高其性能的扩展。此外，我们将学习另一种单词嵌入学习技术，称为全局向量或手套。</p></div></body></html>
</body></html>