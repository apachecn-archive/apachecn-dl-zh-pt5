<html><head/><body>
<html>
  <head>
    <title>Chapter 7. Long Short-Term Memory Networks</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title"><a id="ch07" class="calibre7"/>第七章。长短期记忆网络</h1></div></div></div><p class="calibre10">在这一章中，我们将讨论一种更高级的RNN变体，称为<strong class="calibre11">长短期记忆网络</strong>(<strong class="calibre11">lstm</strong>)。LSTMs <a id="id430"/>广泛用于许多顺序任务(包括股市预测、语言建模和机器翻译)，并已被证明比其他顺序模型(例如，标准rnn)执行得更好，尤其是在大量数据可用的情况下。LSTMs设计得很好，避免了我们在前一章讨论的消失梯度问题。</p><p class="calibre10">消失梯度造成的主要实际限制是它阻止模型学习长期依赖性。然而，通过避免消失梯度问题，LSTMs能够比普通rnn存储更长时间的记忆(数百个时间步长)。与那些仅保持单个隐藏状态的rnn相比，LSTMs具有更多的参数，并且可以更好地控制在给定的训练步骤中存储什么和丢弃什么。例如，rnn不能决定存储哪个存储器和丢弃哪个存储器，因为隐藏状态在每个训练步骤被强制更新。</p><p class="calibre10">具体来说，我们将在很高的层次上讨论什么是LSTM，以及lstm的功能如何允许它们存储长期依赖关系。然后，我们将进入管理LSTMs的实际基础数学框架，并讨论一个例子来强调为什么每个计算都很重要。我们还将比较lstm和普通rnn，并看到lstm具有更复杂的架构，允许它们在顺序任务中超过普通rnn。重温消失梯度的问题，并通过一个例子来说明它，将使我们理解LSTMs如何解决这个问题。</p><p class="calibre10">此后，我们将讨论几种已经引入的技术，以改进由标准LSTM产生的预测(例如，在文本生成任务中改进生成文本的质量/多样性)。例如，一次生成几个预测而不是逐个预测，有助于提高生成的预测的质量。我们还将研究<strong class="calibre11"> BiLSTMs </strong>或<strong class="calibre11">双向LSTMs </strong>，它们是标准LSTM的扩展，比标准LSTM具有更强的<a id="id431"/>能力来捕获序列中存在的模式。</p><p class="calibre10">最后，我们将讨论两个最近的LSTM变种。首先，我们将看看<strong class="calibre11">窥视孔连接</strong>，它<a id="id432"/>向LSTM门引入更多的参数和信息，使lstm能够更好地执行。接下来，我们将讨论<strong class="calibre11">门控循环单元</strong> ( <strong class="calibre11"> GRUs </strong>)，它们越来越受欢迎，因为与LSTMs相比，它们具有更简单的<a id="id433"/>结构，并且不会降低性能。</p><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec48" class="calibre7"/>理解长短期记忆网络</h1></div></div></div><p class="calibre10">在这一节中，我们<a id="id434"/>将首先解释LSTM细胞内发生的事情。我们将会看到，除了状态之外，还有一个控制细胞内信息流的门控机制。然后，我们将完成一个详细的示例，看看每个门和状态如何在示例的不同阶段帮助实现所需的行为，最终导致所需的输出。最后，我们将比较LSTM和标准的RNN，以了解LSTM和标准的RNN有什么不同。</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec95" class="calibre7"/>什么是LSTM？</h2></div></div></div><p class="calibre10">LSTMs可以被看作是rnn的一个更好的家族。LSTM主要由五种不同的东西组成:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre11">单元状态</strong>:这是<a id="id435"/>LSTM单元的内部单元状态(即存储器)</li><li class="listitem"><strong class="calibre11">隐藏状态</strong>:该<a id="id436"/>是用于计算预测的外部隐藏状态</li><li class="listitem"><strong class="calibre11">输入门</strong>:该<a id="id437"/>决定有多少电流输入被读入单元状态</li><li class="listitem"><strong class="calibre11">遗忘门</strong>:该<a id="id438"/>决定将多少先前的单元状态发送到当前单元状态</li><li class="listitem"><strong class="calibre11">输出门</strong>:该<a id="id439"/>决定有多少单元状态输出到隐藏状态</li></ul></div><p class="calibre10">我们可以将RNN包装成如下的单元架构。该单元将输出依赖于(具有非线性激活函数)先前单元状态和当前输入的某个状态。然而，在RNNs中，单元状态总是随着每个输入而改变。这导致rnn的小区状态总是变化。对于存储长期依赖关系来说，这种行为是非常不可取的。</p><p class="calibre10">LSTMs可以<a id="id440"/>决定何时替换、更新或忘记存储在细胞状态下每个神经元中的信息。换句话说，LSTMs配备了保持单元状态不变的机制(如果需要的话),使它们能够存储长期依赖关系。</p><p class="calibre10">这是通过引入门控机制来实现的。LSTMs拥有单元需要执行的每个操作的门。这些门在0和1之间是连续的(通常是sigmoid函数),其中0表示没有信息流过该门，1表示所有信息流过该门。LSTM为细胞中的每个神经元使用一个这样的门。如前所述，这些门控制以下内容:</p><div><ul class="itemizedlist"><li class="listitem">有多少电流输入被写入单元状态(输入门)</li><li class="listitem">从前一个单元状态中遗忘了多少信息(遗忘门)</li><li class="listitem">有多少信息从单元状态(输出门)输出到最终隐藏状态</li></ul></div><p class="calibre10"><em class="calibre15">图7.1 </em>说明了这一功能。每个门决定有多少不同的数据(例如，当前输入、先前的隐藏状态或先前的单元状态)流入状态(即，最终的隐藏状态或单元状态)。每条线的粗细表示有多少信息从该门流出或流入该门(在一些假设的场景中)。例如，在该图中，您可以看到输入门允许来自当前输入的比来自先前最终隐藏状态的多，其中遗忘门允许来自先前最终隐藏状态的比来自当前输入的多:</p><div><img alt="What is an LSTM?" src="img/B08681_07_01.jpg" class="calibre12"/><div><p class="calibre10">图7.1:LSTM中数据流的抽象视图</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec96" class="calibre7"/> LSTMs更详细</h2></div></div></div><p class="calibre10">在这里，我们将<a id="id441"/>带<a id="id442"/>了解LSTMs的实际机制。我们将首先简要讨论LSTM单元格的整体视图，然后<a id="id443"/>开始讨论发生在LSTM单元格内的每个操作，以及一个文本生成的例子。</p><p class="calibre10">如前所述，LSTMs主要由以下三个门组成:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre11">输入门</strong>:输出0(当前输入未写入单元状态)和1(当前输入完全写入单元状态)之间的值的门<a id="id444"/>。Sigmoid激活用于将输出压缩到0和1之间。</li><li class="listitem"><strong class="calibre11">遗忘门</strong>:s形门<a id="id445"/>，输出0(计算当前单元状态时完全遗忘前一单元状态)和1(计算当前单元状态时完全读入前一单元状态)之间的值。</li><li class="listitem"><strong class="calibre11">输出门</strong>:一个<a id="id446"/>s形门，输出0(计算最终状态时完全丢弃当前单元状态)和1(计算最终隐藏状态时完全使用当前单元状态)之间的值。</li></ul></div><p class="calibre10">这可以是图7.2 中的<em class="calibre15">所示的<a id="id447"/>。这是一个非常高级的图，为了避免混乱，隐藏了一些细节。为了提高理解，我们给出了带循环和不带循环的LSTMs。右侧的图描绘了具有环的LSTM，而左侧的图显示了具有扩展环的相同LSTM，因此模型中不存在环:</em></p><div><img alt="LSTMs in more detail" src="img/B08681_07_02.jpg" class="calibre12"/><div><p class="calibre10">图7.2:具有循环链路(即环路)的LSTM(右)具有扩展的循环链路的LSTM(左)</p></div></div><p class="calibre10">现在，为了更好地理解LSTMs，让我们考虑一个例子。我们将讨论实际的更新规则和公式，并通过一个例子来更好地理解LSTMs。</p><p class="calibre10">现在让我们考虑一个从下面的句子开始生成文本的例子:</p><p class="calibre10">约翰给了玛丽一只小狗。</p><p class="calibre10">我们输出的故事应该是关于<em class="calibre15">约翰</em>、<em class="calibre15">玛丽</em>和<em class="calibre15">小狗</em>的。让我们假设我们的LSTM在给定的句子后输出两个句子:</p><p class="calibre10">约翰给了玛丽一只小狗。____________________._____________________.</p><p class="calibre10">下面是我们LSTM给出的<a id="id448"/>输出:</p><p class="calibre10">约翰给了玛丽一只小狗。它叫得很大声。他们把它命名为露娜。</p><p class="calibre10">我们离输出这样真实的短语还很远。然而，LSTMs可以学习诸如名词和代词之间的关系。例如，<em class="calibre15">它</em>与<em class="calibre15">小狗</em>有关，而<em class="calibre15">它们</em>到<em class="calibre15">约翰</em>和<em class="calibre15">玛丽</em>。然后，它应该学习名词/代词和动词之间的关系。比如对于<em class="calibre15"> it </em>来说，动词的结尾要有一个<em class="calibre15"> s </em>。我们在<em class="calibre15">图7.3 </em>中说明了这些关系/依赖关系。两者我们都可以看到，长期的(例如<em class="calibre15">露娜</em> <em class="calibre15"> → </em> <em class="calibre15">小狗</em>)和短期的(例如<em class="calibre15">它</em> <em class="calibre15"> → </em> <em class="calibre15">吠叫</em>)依赖都存在于这个短语中。实线箭头表示名词和代词之间的联系，虚线箭头表示名词/代词和动词之间的联系:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_03.jpg" class="calibre12"/><div><p class="calibre10">图7.3:LSTM给出和预测的句子，单词之间的各种关系被突出显示</p></div></div><p class="calibre10">现在让我们考虑LSTMs如何使用它们的各种操作，对这样的关系和依赖关系建模，以输出有意义的文本，给定一个起始句子。</p><p class="calibre10">输入门(<em class="calibre15"> i <sub class="calibre17"> t </sub> </em>)将当前输入(<em class="calibre15"> x <sub class="calibre17"> t </sub> </em>)和之前的最终隐藏状态(<em class="calibre15"> h <sub class="calibre17"> t-1 </sub> </em>)作为输入，计算<em class="calibre15"> i <sub class="calibre17"> t </sub> </em>，如下:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_04.jpg" class="calibre12"/></div><p class="calibre10">输入门，<em class="calibre15"> i </em> <em class="calibre15"> t </em>可以理解为在具有s形激活的单隐藏层标准RNN的隐藏层执行的计算。请记住，我们计算标准RNN的隐藏状态如下:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_05.jpg" class="calibre12"/></div><p class="calibre10">因此，LSTM的<em class="calibre15">I<sub class="calibre17">t</sub>T23】的计算看起来非常类似于标准RNN的<em class="calibre15">h<sub class="calibre17">t</sub>T27】的计算，除了激活函数的改变和偏置的增加。</em></em></p><p class="calibre10">在<a id="id450"/>计算之后，<em class="calibre15"> i <sub class="calibre17"> t </sub> </em>的值0将意味着没有来自当前输入的信息将流向单元状态，其中值1意味着来自当前输入的所有信息将流向单元状态。</p><p class="calibre10">接下来，另一个值(称为<strong class="calibre11">候选值</strong>)被计算如下，该值被添加以稍后计算当前单元状态:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_06.jpg" class="calibre12"/></div><p class="calibre10">我们可以在<em class="calibre15">图7.4 </em>中看到这些计算:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_07.jpg" class="calibre12"/><div><p class="calibre10">图7.4。在LSTM中发生的所有计算(灰显)的上下文中，计算<em class="calibre15">I</em>t和<img alt="LSTMs in more detail" src="img/B08681_07_08.jpg" class="calibre12"/>(粗体)</p></div></div><p class="calibre10">在我们的例子中，在学习的<a id="id451"/>最开始，输入门需要被高度激活。LSTM输出的第一个字是<em class="calibre15"> it </em>。同样为了这样做，LSTM必须了解到<em class="calibre15">小狗</em>也被称为<em class="calibre15">它</em>。让我们假设我们的LSTM有五个神经元来存储状态。我们希望LSTM存储信息，即<em class="calibre15"> it </em>指的是<em class="calibre15">小狗</em>。我们希望LSTM(在不同的神经元中)学习的另一条信息是，当使用代词<em class="calibre15"> it </em>时，现在时态动词应该在动词末尾有一个<em class="calibre15"> s </em>。LSTM需要知道的另一件事是<em class="calibre15">小狗会大声叫</em>。<em class="calibre15">图7.5 </em>说明了如何将这些知识编码到<a id="id452"/> LSTM的单元状态中。每个圆圈代表单个神经元(即隐藏单元)的细胞状态:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_09.jpg" class="calibre12"/><div><p class="calibre10">图7.5:应该在单元格状态中编码以输出第一句话的知识</p></div></div><p class="calibre10">有了这些信息，我们可以输出第一个新句子:</p><p class="calibre10">约翰给了玛丽一只小狗。它叫得很大声。</p><p class="calibre10">接下来，遗忘门的计算如下:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_10.jpg" class="calibre12"/></div><p class="calibre10">遗忘门执行以下操作。遗忘门的值为0意味着没有来自<em class="calibre15">c<sub class="calibre17">t-1</sub>T5】的信息将被传递来计算<em class="calibre15">c<sub class="calibre17">t</sub>T9】，值为1意味着<em class="calibre15">c<sub class="calibre17">t-1</sub>T13】的所有信息将传播到<em class="calibre15">c<sub class="calibre17">t</sub>T17】的计算中。</em></em></em></em></p><p class="calibre10">现在我们来看看遗忘之门是如何帮助预测下一句话的:</p><p class="calibre10">他们把它命名为露娜。</p><p class="calibre10">现在你可以看到，我们正在看的新关系是在约翰和玛丽之间。因此，我们不再需要关于<em class="calibre15">它</em>和动词<em class="calibre15">吠叫</em>如何表现的信息，因为主语是<em class="calibre15">约翰</em>和<em class="calibre15">玛丽</em>。我们可以使用遗忘门结合当前主题<em class="calibre15">它们</em>和相应的名为的动词<em class="calibre15">来替换存储在<strong class="calibre11">当前主题</strong>和<strong class="calibre11">动词中的信息用于当前主题</strong>神经元(参见<em class="calibre15">图7.6 </em>):</em></p><div><img alt="LSTMs in more detail" src="img/B08681_07_11.jpg" class="calibre12"/><div><p class="calibre10">图7.6:左起第三个神经元中的知识(它<em class="calibre15"> → </em>叫)被新的信息(它们<em class="calibre15"> → </em>命名)取代。</p></div></div><p class="calibre10">根据权重的<a id="id453"/>值，我们在<em class="calibre15">图7.7 </em>中说明了这种转换。我们不改变神经元维持<em class="calibre15">it</em><em class="calibre15">→</em><em class="calibre15">puppy</em>关系的状态，因为<em class="calibre15"> puppy </em>在最后一句中是作为宾语出现的。这是通过设置连接<em class="calibre15">it</em><em class="calibre15">→</em><em class="calibre15">puppy</em>从<em class="calibre15"> c <sub class="calibre17"> t-1 </sub> </em>到<em class="calibre15"> c <sub class="calibre17"> t </sub> </em>到1的权重来实现的。然后我们会用新的主语和动词替换维护当前主语和当前动词信息的神经元。这是通过将该神经元的<em class="calibre15">f<sub class="calibre17">t</sub>t</em>的遗忘权重设置为0来实现的。然后我们将当前主语和动词连接到对应状态神经元的<em class="calibre15"> i <sub class="calibre17"> t </sub> </em>的权重设置为1。我们可以将<img alt="LSTMs in more detail" src="img/B08681_07_12.jpg" class="calibre27"/>视为包含哪些新信息(例如来自当前输入<em class="calibre15">x<sub class="calibre17">t</sub>T85】的新信息)应该被带入单元状态的实体:</em></p><div><img alt="LSTMs in more detail" src="img/B08681_07_13.jpg" class="calibre12"/><div><p class="calibre10">图7.7:如何用前一状态<em class="calibre15">c</em>T6】t-1和候选值<img alt="LSTMs in more detail" src="img/B08681_07_14.jpg" class="calibre12"/>计算单元格状态<em class="calibre15"> c </em> <em class="calibre15"> t </em></p></div></div><p class="calibre10">当前<a id="id454"/>单元状态将更新如下:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_15.jpg" class="calibre12"/></div><p class="calibre10">换句话说，当前状态是以下各项的组合:</p><div><ul class="itemizedlist"><li class="listitem">要忘记/记住来自先前细胞状态的什么信息</li><li class="listitem">向当前输入添加/丢弃什么信息</li></ul></div><p class="calibre10">接下来在<em class="calibre15">图7.8 </em>中，我们强调了到目前为止我们针对LSTM内部发生的所有计算所进行的计算:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_16.jpg" class="calibre12"/><div><p class="calibre10">图7.8:目前已覆盖的计算包括<em class="calibre15"> i </em> <em class="calibre15"> t </em>、<em class="calibre15"> f </em> <em class="calibre15"> t </em>、<img alt="LSTMs in more detail" src="img/B08681_07_08.jpg" class="calibre12"/>和<em class="calibre15"> c </em> <em class="calibre15"> t </em></p></div></div><p class="calibre10">学习完<a id="id455"/>完整状态后，它将看起来像<em class="calibre15">图7.9 </em>:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_18.jpg" class="calibre12"/><div><p class="calibre10">图7.9:输出两个句子后，完整的单元格状态看起来像这样</p></div></div><p class="calibre10">接下来，我们将看看<a id="id456"/>LSTM单元的最终状态(<em class="calibre15"> h <sub class="calibre17"> t </sub> </em>)是如何计算的:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_19.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_20.jpg" class="calibre12"/></div><p class="calibre10">在我们的示例中，我们希望输出下面的句子:</p><p class="calibre10">他们把它命名为露娜。</p><p class="calibre10">为此，我们不需要倒数第二个神经元来计算这句话，因为它包含关于小狗如何吠叫的信息，而这句话是关于小狗的名字。因此，我们可以在最后一句话的预测过程中忽略最后一个神经元(包含<em class="calibre15">吠- &gt;大声</em>的关系)。这正是<em class="calibre15">o<sub class="calibre17">t</sub>t</em>所做的；在计算LSTM单元的最终输出时，它将忽略不必要的内存，只从单元状态中检索相关的内存。同样，在<em class="calibre15">图7.10 </em>中，我们展示了一个LSTM细胞看起来的样子:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_21.jpg" class="calibre12"/><div><p class="calibre10">图7.10:完整的LSTM是什么样子</p></div></div><p class="calibre10">这里，我们总结了<a id="id457"/>所有与LSTM单元内发生的操作相关的方程式。</p><div><img alt="LSTMs in more detail" src="img/B08681_07_22.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_23.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_24.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_25.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_26.jpg" class="calibre12"/></div><div><img alt="LSTMs in more detail" src="img/B08681_07_27.jpg" class="calibre12"/></div><p class="calibre10">现在，在更大的画面中，对于一个<a id="id458"/>顺序学习问题，我们可以随着时间的推移展开LSTM细胞，以显示它们如何链接在一起，以便它们接收细胞的前一状态来计算下一状态，如图<em class="calibre15">图7.11 </em>所示:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_28.jpg" class="calibre12"/><div><p class="calibre10">图7.11:lstm将如何随着时间的推移而关联</p></div></div><p class="calibre10">然而，这不足以做一些有用的事情。正如你所看到的，即使我们可以创建一个很好的LSTMs链，实际上能够模拟一个序列，我们仍然没有输出或预测。但是如果我们想使用LSTM实际学到的东西，我们需要一种方法从LSTM中提取最终输出。因此，我们将在LSTM的顶部固定一个softmax层(权重为<em class="calibre15"> W </em> <em class="calibre15"> s </em>，偏移为<em class="calibre15"> b </em> <em class="calibre15"> s </em>)。最终输出通过下式获得:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_29.jpg" class="calibre12"/></div><p class="calibre10">现在，带有softmax图层的LSTM的最终图片<a id="id459"/>看起来像<em class="calibre15">图7.12 </em>:</p><div><img alt="LSTMs in more detail" src="img/B08681_07_30.jpg" class="calibre12"/><div><p class="calibre10">图7.12:具有softmax输出图层的LSTMs随时间的变化</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec97" class="calibre7"/>lstm与标准rnn有何不同</h2></div></div></div><p class="calibre10">现在让我们<a id="id460"/>研究LSTMs与标准rnn相比如何。与标准RNN相比，LSTM具有更复杂的结构。其中一个主要区别是LSTM有两种不同的状态:单元状态<em class="calibre15">c</em>T15】t和最终隐藏状态<em class="calibre15">h<sub class="calibre17">t</sub>T20】。然而，一个RNN只有一个隐藏状态<em class="calibre15">h<sub class="calibre17">t</sub>T24】。下一个主要区别是，由于LSTM有三个不同的门，所以在计算最终隐藏状态<em class="calibre15">h<sub class="calibre17">t</sub>T28】时，LSTM对如何处理当前输入和前一个单元状态有更多的控制。</em></em></em></p><p class="calibre10">拥有两种不同的状态是非常有利的。利用这种机制，即使当单元状态快速改变时，最终隐藏状态仍然会改变得更慢。因此，当单元状态正在学习短期和长期依赖性时，最终隐藏状态可以仅反映短期依赖性或者仅反映长期依赖性或者两者都反映。</p><p class="calibre10">接下来，门控机制由三个门组成:输入门、遗忘门和输出门:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">输入门</em>控制有多少电流输入被写入单元状态</li><li class="listitem"><em class="calibre15">遗忘门</em>控制有多少先前单元状态被带到当前单元状态</li><li class="listitem">最后，<em class="calibre15">输出门</em>控制有多少从单元状态传播到最终隐藏状态</li></ul></div><p class="calibre10">很明显，这是一种更有原则的方法(尤其是与标准RNNs相比)，它允许更好地控制当前输入和先前小区状态对当前小区状态的贡献。此外，输出门可以更好地控制单元状态对最终隐藏状态的贡献。在<em class="calibre15">图7.13 </em>中，我们比较了标准RNN和LSTM的示意图，以强调两种型号在功能上的差异。</p><p class="calibre10">总之，通过维护两种不同状态的设计，LSTM可以学习短期和长期依赖关系，这有助于解决消失梯度的问题，我们将在下一节中讨论。</p><div><img alt="How LSTMs differ from standard RNNs" src="img/B08681_07_31.jpg" class="calibre12"/><div><p class="calibre10">图7.13:标准RNN和LSTM比色皿的对比</p></div></div></div></div></div></body></html>


<html>
  <head>
    <title>How LSTMs solve the vanishing gradient problem</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1">【LSTMs如何解决消失梯度问题</h1></div></div></div><p class="calibre10">正如我们之前讨论的，尽管rnn理论上是合理的，但实际上它们有一个严重的缺点。也就是说，当使用通过时间(<strong class="calibre11">【BPTT】</strong>)的<strong class="calibre11">反向传播时，梯度迅速减小，这允许<a id="id463"/> us仅传播几个时间步长的信息。因此，我们只能存储很少时间步的信息，因此只拥有短期记忆。这反过来限制了rnn在现实世界顺序任务中的有用性。</strong></p><p class="calibre10">通常有用和有趣的顺序任务(如股票市场预测或语言建模)需要学习和存储长期依赖性的能力。想一想下面预测下一个单词的例子:</p><p class="calibre10">约翰是个有天赋的学生。他是优等生，打橄榄球和板球。所有其他学生都羡慕______。</p><p class="calibre10">对我们来说，这是一项非常容易的任务。答案是约翰。然而，对于一个RNN人来说，这是一项艰巨的任务。我们正试图预测一个答案，它就在这篇文章的开头。此外，为了解决这个任务，我们需要一种方法来存储RNN的长期依赖关系。这正是LSTMs要解决的任务类型。</p><p class="calibre10">在第6章、<em class="calibre15">递归神经网络</em>中，我们讨论了在没有任何非线性函数存在的情况下，消失/爆炸梯度是如何出现的。我们现在将看到，即使存在非线性项，它仍然可能发生。为此，我们将看到衍生项<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_32.jpg" class="calibre27"/>如何用于标准RNN和LSTM ( <img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_33.jpg" class="calibre27"/>用于LSTM)网络。这个<a id="id464"/>是导致渐变消失的关键术语，正如我们在前一章中了解到的。</p><p class="calibre10">让我们假设标准RNN的隐藏状态计算如下:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_34.jpg" class="calibre12"/></div><p class="calibre10">为了简化计算，我们可以忽略当前输入相关项，而专注于循环部分，这将给出以下等式:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_35.jpg" class="calibre12"/></div><p class="calibre10">如果我们为前面的等式计算<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_36.jpg" class="calibre27"/>，我们将得到以下结果:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_37.jpg" class="calibre12"/></div><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_38.jpg" class="calibre12"/></div><p class="calibre10">现在让我们看看当<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_39.jpg" class="calibre27"/>或<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_40.jpg" class="calibre27"/>时会发生什么(这将随着学习的继续而发生)。在这两种情况下，<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_41.jpg" class="calibre27"/>将开始接近0，导致渐变消失。甚至当<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_42.jpg" class="calibre27"/>时，对于乙状结肠激活，梯度最大(0.25)，当乘以许多时间步长时，总梯度变得相当小。此外，术语<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_43.jpg" class="calibre27"/>(可能是由于糟糕的初始化)也会导致渐变的爆炸或消失。然而，与由于<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_44.jpg" class="calibre27"/>或<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_45.jpg" class="calibre27"/>导致的梯度消失相比，由术语<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_46.jpg" class="calibre27"/>导致的梯度消失/爆炸相对容易解决(通过<a id="id465"/>权重和梯度剪裁的仔细初始化)。</p><p class="calibre10">现在让我们来看看LSTM细胞。更具体地说，我们将查看单元状态，由以下等式给出:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_47.jpg" class="calibre12"/></div><p class="calibre10">这是在LSTM发生的所有忘记门应用的产品。然而，如果您以类似的方式为LSTMs计算<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_48.jpg" class="calibre27"/>(即，忽略<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_49.jpg" class="calibre27"/>项和<em class="calibre15">b</em>f，因为它们是非递归的)，我们会得到以下结果:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_50.jpg" class="calibre12"/></div><p class="calibre10">在这种情况下，尽管如果<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_51.jpg" class="calibre27"/>梯度将消失，另一方面，如果<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_52.jpg" class="calibre27"/>，导数将比在标准RNN中下降得更慢。因此，我们有一个选择，梯度不会消失。此外，由于使用了挤压功能，梯度不会因为<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_53.jpg" class="calibre27"/>变大而爆炸(这在梯度爆炸过程中很可能发生)。此外，当<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_54.jpg" class="calibre27"/>时，我们得到接近1的最大梯度，这意味着梯度不会像我们在RNNs中看到的那样快速下降(当梯度最大时)。最后，在推导中没有诸如<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_55.jpg" class="calibre27"/>这样的术语。然而，对于<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_56.jpg" class="calibre27"/>，推导更加复杂。让我们看看这些术语是否出现在<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_57.jpg" class="calibre27"/>的推导中。如果你计算它的导数，你会得到如下形式的结果:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_58.jpg" class="calibre12"/></div><p class="calibre10">一旦你解决了这个问题，你会得到这样的东西:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_59.jpg" class="calibre12"/></div><p class="calibre10">我们并不关心<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_103.jpg" class="calibre27"/>或者<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_104.jpg" class="calibre27"/>里面的内容，因为无论什么值，都会被(0，1)或者(-1，1)所有界。如果我们进一步简化符号，将<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_60.jpg" class="calibre27"/>、<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_61.jpg" class="calibre27"/>、<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_62.jpg" class="calibre27"/>和<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_63.jpg" class="calibre27"/>项替换为一些常见符号，如<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_64.jpg" class="calibre27"/>，我们会得到以下形式的内容:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_65.jpg" class="calibre12"/></div><p class="calibre10">或者，我们<a id="id467"/>得到以下结果(假设外面的<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_66.jpg" class="calibre27"/>被方括号内的每个<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_67.jpg" class="calibre27"/>项吸收):</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_68.jpg" class="calibre12"/></div><p class="calibre10">这将给出以下内容:</p><div><img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_69.jpg" class="calibre12"/></div><p class="calibre10">这意味着尽管术语<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_70.jpg" class="calibre27"/>不受任何术语<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_71.jpg" class="calibre27"/>的影响，但<img alt="How LSTMs solve the vanishing gradient problem" src="img/B08681_07_72.jpg" class="calibre27"/>却不是。因此，我们必须小心初始化LSTM的权重，我们也应该使用梯度裁剪。</p><div><div><h3 class="title4"><a id="note28" class="calibre7"/>注意</h3><p class="calibre16">然而，LSTMs的<em class="calibre15"> h <sub class="calibre17"> t </sub> </em>不受消失梯度的影响并不像RNNs那样重要。因为<em class="calibre15"> c <sub class="calibre17"> t </sub></em></p></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec98" class="calibre7"/>改进LSTMs</h2></div></div></div><p class="calibre10">正如我们在学习rnn时已经看到的那样，拥有坚实的理论基础并不总是保证它们在实践中表现最佳。这是由于计算机数值精度的限制。对于LSTMs来说也是如此。拥有一个复杂的设计(允许对数据的长期相关性进行更好的建模)本身并不意味着LSTM会输出完美的现实预测。因此，已经开发了许多扩展来帮助LSTMs在预测阶段执行得更好。在这里，我们将讨论几个这样的改进:贪婪的<a id="id469"/>采样，波束搜索，使用单词向量代替单词的一个热编码表示，以及使用双向LSTMs。</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec99" class="calibre7"/>贪婪采样</h2></div></div></div><p class="calibre10">如果我们试图总是以最高的概率预测单词，LSTM将倾向于产生非常单调的结果。例如，在切换到另一个单词之前，它会多次重复单词<em class="calibre15">到</em>。</p><p class="calibre10">解决这个问题的一个方法是<a id="id471"/>使用<strong class="calibre11">贪婪采样</strong>，我们从那个集合中挑选预测最佳的<em class="calibre15"> n </em>并采样。这有助于打破预测的单调性质。</p><p class="calibre10">让我们考虑前一个例子的第一句话:</p><p class="calibre10">约翰给了玛丽一只小狗。</p><p class="calibre10">假设，我们从第一个单词开始，想要预测接下来的四个单词:</p><p class="calibre10"><em class="calibre15">约翰____ ____ _ _____。</em></p><p class="calibre10">如果我们试图确定性地选择样本，LSTM可能会输出如下内容:</p><p class="calibre10">约翰把玛丽送给了约翰。</p><p class="calibre10">但是，通过从词汇表中的单词子集(最有可能的单词)中抽取下一个单词，LSTM被迫改变预测，并可能输出以下内容:</p><p class="calibre10">约翰给了玛丽一只小狗。</p><p class="calibre10">或者，它会给出以下输出:</p><p class="calibre10">约翰给了小狗一只。</p><p class="calibre10">但是，即使贪婪采样有助于给生成的文本增加更多的变化，这种方法也不能保证输出总是真实的，尤其是在输出较长的文本序列时。现在，我们将看到一种更好的搜索技术，它实际上比预测提前了几个步骤。</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec100" class="calibre7"/>光束搜索</h2></div></div></div><p class="calibre10"><strong class="calibre11">波束搜索</strong>是<a id="id472"/>帮助提高由LSTM产生的预测质量的一种方式<a id="id473"/>。在这种情况下，通过解决搜索问题来找到预测。波束搜索的关键思想是一次产生<em class="calibre15"> b </em>输出(即<img alt="Beam search" src="img/B08681_07_73.jpg" class="calibre27"/>)，而不是单个输出<em class="calibre15"> y <sub class="calibre17"> t </sub> </em>。这里，<em class="calibre15"> b </em>称为光束的<strong class="calibre11">长度</strong>，产生的<em class="calibre15"> b </em>输出称为<strong class="calibre11">光束</strong>。更严格地说，我们<a id="id474"/>选择具有<a id="id475"/>最高联合概率<img alt="Beam search" src="img/B08681_07_74.jpg" class="calibre27"/>的波束，而不是选择最高概率<img alt="Beam search" src="img/B08681_07_105.jpg" class="calibre27"/>。在做出预测之前，我们对未来看得更远，这通常会带来更好的结果。</p><p class="calibre10">让我们通过前面的例子来理解波束搜索:</p><p class="calibre10">约翰给了玛丽一只小狗。</p><p class="calibre10">说，我们是一个字一个字的预测。最初我们有以下内容:</p><p class="calibre10"><em class="calibre15">约翰____ ____ _ _____。</em></p><p class="calibre10">让我们假设我们的LSTM使用波束搜索生成了这个例句。那么每个单词的概率可能会像我们在图7.13 中看到的那样。让我们假设波束长度<img alt="Beam search" src="img/B08681_07_75.jpg" class="calibre27"/>，我们将在搜索的每个阶段考虑<img alt="Beam search" src="img/B08681_07_77.jpg" class="calibre27"/>最佳候选。搜索树如下图所示:</p><div><img alt="Beam search" src="img/B08681_07_107.jpg" class="calibre12"/><div><p class="calibre10">图7.13:b = 2，n=3时波束搜索的搜索空间</p></div></div><p class="calibre10">我们从单词约翰开始，得到词汇表中所有单词的概率。在我们的例子中，作为<img alt="Beam search" src="img/B08681_07_76.jpg" class="calibre27"/>，我们为树的下一级挑选最好的三个候选人:<strong class="calibre11">give</strong>、<strong class="calibre11"> Mary </strong>和<strong class="calibre11"> puppy </strong>。(请注意，这些可能不是实际LSTM发现的候选，仅用作示例。)然后，从这些选择的候选者中，生长树的下一层。从那里，我们将挑选最好的三个候选，搜索将重复进行，直到我们到达树中的深度<em class="calibre15"> b </em>。</p><p class="calibre10">给予<a id="id476"/>最高联合概率的路径(即<img alt="Beam search" src="img/B08681_07_78.jpg" class="calibre27"/>)用粗箭头突出显示。此外，这是一个更好的预测机制，因为它会返回更高的概率，或奖励，例如约翰给玛丽的短语比约翰给玛丽的短语更高。</p><p class="calibre10">注意，在我们的例子中，贪婪采样和波束搜索产生的输出<a id="id477"/>是相同的，这是一个包含五个单词的简单句子。然而，当我们将此缩放以输出一篇小短文时，情况并非如此。那么由波束搜索产生的结果将比由贪婪采样产生的结果更加真实并且语法正确。</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec101" class="calibre7"/>使用词向量</h2></div></div></div><p class="calibre10">提高LSTM性能的另一种流行的<a id="id478"/>方法是使用字向量，而不是使用一位热编码向量作为LSTM的输入。我们通过一个例子来了解一下<a id="id479"/>这个方法的价值。假设我们想从某个随机单词开始生成文本。在我们的例子中，应该是这样的:</p><p class="calibre10"><em class="calibre15">约翰____ ____ _ _____。</em></p><p class="calibre10">我们已经用下列句子训练了我们的LSTM:</p><p class="calibre10">约翰给了玛丽一只小狗。玛丽送给鲍勃一只小猫。</p><p class="calibre10">我们还假设我们有如图<em class="calibre15">图7.15 </em>所示的单词向量:</p><div><img alt="Using word vectors" src="img/B08681_07_79.jpg" class="calibre12"/><div><p class="calibre10">图7.15:二维空间中假设的词向量拓扑</p></div></div><p class="calibre10">单词<a id="id480"/>这些单词的嵌入，以它们的数字形式，可能看起来像下面这样:</p><p class="calibre10"><em class="calibre15">小猫:[0.5，0.3，0.2] </em></p><p class="calibre10"><em class="calibre15">小狗:[0.49，0.31，0.25] </em></p><p class="calibre10"><em class="calibre15">给了:[0.1，0.8，0.9] </em></p><p class="calibre10">可以<a id="id481"/>看到那个<img alt="Using word vectors" src="img/B08681_07_80.jpg" class="calibre27"/>。但是，如果我们使用一键编码，它们将如下所示:</p><p class="calibre10"><em class="calibre15">小猫:[ 1，0，0，…] </em></p><p class="calibre10"><em class="calibre15">小狗:[0，1，0，…] </em></p><p class="calibre10"><em class="calibre15">给了:[0，0，1，…] </em></p><p class="calibre10">然后，<img alt="Using word vectors" src="img/B08681_07_81.jpg" class="calibre27"/>。正如我们已经看到的，一个热点编码的向量没有捕捉单词之间的正确关系，并且看到所有的单词彼此之间的距离相等。然而，词向量能够捕捉这种关系，并且更适合作为LSTM的特征。</p><p class="calibre10">使用单词向量，LSTM将学会更好地利用单词之间的关系。例如，通过单词向量，LSTM将学习以下内容:</p><p class="calibre10">约翰给了玛丽一只小猫。</p><p class="calibre10">这与以下内容非常接近:</p><p class="calibre10">约翰给了玛丽一只小狗。</p><p class="calibre10">此外，它与以下内容有很大不同:</p><p class="calibre10">约翰给了玛丽一个礼物。</p><p class="calibre10">然而，如果使用一位热编码矢量，情况就不是这样了。</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec102" class="calibre7"/>双向lstm(BiLSTM)</h2></div></div></div><p class="calibre10">使LSTM双向<a id="id482"/>是提高LSTM预测质量的另一种方式。我们的意思是用从<a id="id483"/>开始到结束以及从结束到开始读取的数据来训练LSTM。到目前为止，在LSTM的训练期间，我们将创建如下数据集:</p><p class="calibre10">考虑下面两句话:</p><p class="calibre10">约翰给了玛丽一个_____。它叫得很大声。</p><p class="calibre10">然而，在<a id="id484"/>这个阶段，在我们希望LSTM合理填充的一个句子中有数据丢失。</p><p class="calibre10">如果我们从头开始阅读<a id="id485"/>直到缺失的单词，将如下所示:</p><p class="calibre10">约翰给了玛丽一个_____。</p><p class="calibre10">这没有提供足够的关于丢失单词的上下文的信息来正确地填充该单词。但是，如果我们从两个方向阅读，结果将如下:</p><p class="calibre10">约翰给了玛丽一个_____。</p><p class="calibre10"><em class="calibre15"> _____。它叫得很大声。</em></p><p class="calibre10">如果我们用这两部分创建数据，就足以预测丢失的单词应该是类似于<em class="calibre15">狗</em>或<em class="calibre15">小狗</em>的东西。因此，某些问题可以从两边读取数据中受益匪浅。此外，这增加了神经网络可用的数据量，并提高了其性能。</p><p class="calibre10">BiLSTMs的另一个应用是神经机器翻译，我们将源语言的句子翻译成目标语言。由于一种语言到另一种语言的翻译之间没有特定的一致性，了解源语言的过去和未来可以极大地帮助更好地理解上下文，从而产生更好的翻译。例如，考虑将菲律宾语翻译成英语的翻译任务。在菲律宾语中，句子通常按照<em class="calibre15">动词-宾语-主语</em>的顺序书写，而在英语中，则是<em class="calibre15">主语-动词-宾语</em>。在这项翻译任务中，向前和向后阅读句子对翻译很有帮助。</p><p class="calibre10">BiLSTM实际上是两个独立的LSTM网络。一个网络从头到尾学习数据，另一个网络从头到尾学习数据。在<em class="calibre15">图7.16 </em>中，我们展示了一个BiLSTM网络的架构。</p><p class="calibre10">培训分两个阶段进行。首先，用从头到尾阅读文本所创建的数据来训练纯色网络。该网络代表用于标准LSTMs的正常培训程序。第二，用反向阅读文本产生的数据训练虚线网络。然后，在推断阶段，我们使用实线和虚线状态的信息(通过连接两种状态并创建向量)来预测丢失的单词:</p><div><img alt="Bidirectional LSTMs (BiLSTM)" src="img/B08681_07_82.jpg" class="calibre12"/><div><p class="calibre10">图7.16:bil STM的示意图</p></div></div></div></div></body></html>


<html>
  <head>
    <title>Other variants of LSTMs</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec50" class="calibre7"/>lstm的其他变体</h1></div></div></div><p class="calibre10">虽然我们主要关注标准LSTM架构，但是已经出现了许多变体，它们或者简化了标准lstm中的复杂架构，或者产生了更好的性能，或者两者兼而有之。我们将着眼于两个变种，介绍LSTM的细胞结构的结构修改:窥视孔连接和GRUs。</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec103" class="calibre7"/>窥视孔连接</h2></div></div></div><p class="calibre10"><strong class="calibre11">窥视孔连接</strong>允许<a id="id487"/>门不仅能看到<a id="id488"/>当前输入和先前的最终隐藏状态，还能看到先前的单元状态。这增加了LSTM池中的砝码数量。具有这样的连接已经显示出产生更好的结果。这些方程看起来像这样:</p><div><img alt="Peephole connections" src="img/B08681_07_83.jpg" class="calibre12"/></div><div><img alt="Peephole connections" src="img/B08681_07_84.jpg" class="calibre12"/></div><div><img alt="Peephole connections" src="img/B08681_07_86.jpg" class="calibre12"/></div><div><img alt="Peephole connections" src="img/B08681_07_87.jpg" class="calibre12"/></div><div><img alt="Peephole connections" src="img/B08681_07_88.jpg" class="calibre12"/></div><div><img alt="Peephole connections" src="img/B08681_07_89.jpg" class="calibre12"/></div><p class="calibre10">让我们简单地看一下这是如何帮助LSTM表现得更好的。到目前为止，门看到的是当前输入和最终隐藏状态，而不是单元状态。然而，在这种配置中，如果输出门接近零，即使当单元状态包含对于更好的性能至关重要的重要信息时，最终的隐藏状态将是接近零的<a id="id490"/>。因此，在计算过程中，门不会考虑隐藏状态。将单元状态直接包含在门计算公式中，可以更好地控制单元状态，即使在输出门接近零的情况下，它也能很好地工作。</p><p class="calibre10">我们在<em class="calibre15">图7.17 </em>中展示了带有窥视孔连接的LSTM的架构。我们将标准LSTM中的所有现有连接都显示为灰色，新添加的连接显示为黑色:</p><div><img alt="Peephole connections" src="img/B08681_07_90.jpg" class="calibre12"/><div><p class="calibre10">图7.17:带有窥视孔连接的LSTM(窥视孔连接显示为黑色，而其他连接显示为灰色)</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec104" class="calibre7"/>门控循环单元</h2></div></div></div><p class="calibre10">GRUs 可以被<a id="id491"/>视为标准LSTM架构的简化。正如我们已经看到的，LSTM有三个不同的门和两个不同的状态。仅这个<a id="id492"/>就需要大量的参数，即使对于小的状态大小。因此，科学家研究了减少参数数量的方法。gru就是这样一个努力的结果。</p><p class="calibre10">与LSTMs相比，gru有几个主要区别。</p><p class="calibre10">首先，GRUs将<a id="id493"/>两个状态，单元格状态和最终隐藏状态，合并成一个隐藏状态<em class="calibre15">h<sub class="calibre17">t</sub>T13】。现在，作为这个没有两个不同状态的简单修改的副作用，我们可以去掉输出门。记住，输出门仅仅是决定有多少单元状态被读入最终隐藏状态。该操作大大减少了单元中的参数数量。</em></p><p class="calibre10">接下来，GRUs <a id="id494"/>引入了一个重置门，当它接近1时，在计算当前状态时，会获取完整的先前状态信息。此外，当复位门接近0时，它在计算当前状态时会忽略先前的状态。</p><div><img alt="Gated Recurrent Units" src="img/B08681_07_91.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_92.jpg" class="calibre12"/></div><p class="calibre10">然后，GRUs将输入门和遗忘门合并成一个<em class="calibre15">更新门</em>。标准LSTM有两个门，称为输入门和遗忘门。输入门决定有多少当前输入被读入单元状态，而遗忘门决定有多少先前单元状态被读入当前单元状态。从数学上讲，这可以表示如下:</p><div><img alt="Gated Recurrent Units" src="img/B08681_07_93.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_94.jpg" class="calibre12"/></div><p class="calibre10">GRUs将这两个操作合并成一个门，称为更新门。如果更新门为0，则先前单元状态的完整状态信息被推入当前单元状态，其中没有当前输入被读入该状态。如果更新门为1，则所有的当前输入被读入当前单元状态，并且没有先前的单元状态被传播到当前单元状态。换句话说，输入门<em class="calibre15">I</em>T2【t】T3】变成了遗忘门的逆，也就是<img alt="Gated Recurrent Units" src="img/B08681_07_95.jpg" class="calibre27"/>:</p><div><img alt="Gated Recurrent Units" src="img/B08681_07_96.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_97.jpg" class="calibre12"/></div><p class="calibre10">现在让我们<a id="id495"/>把所有的方程放到一个地方。GRU <a id="id496"/>的计算看起来像这样:</p><div><img alt="Gated Recurrent Units" src="img/B08681_07_98.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_99.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_101.jpg" class="calibre12"/></div><div><img alt="Gated Recurrent Units" src="img/B08681_07_102.jpg" class="calibre12"/></div><p class="calibre10">这比LSTMs紧凑得多。在<em class="calibre15">图7.18 </em>中，我们可以看到GRU单元格(左)和LSTM单元格(右)并排在一起:</p><div><img alt="Gated Recurrent Units" src="img/B08681_07_106.jpg" class="calibre12"/><div><p class="calibre10">图7.18:GRU(左)和标准LSTM(右)的对比</p></div></div></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec51" class="calibre7"/>总结</h1></div></div></div><p class="calibre10">在本章中，您了解了LSTM网络。首先，我们讨论了什么是LSTM及其高级架构。我们还深入研究了LSTM中发生的详细计算，并通过一个示例讨论了计算。</p><p class="calibre10">我们看到LSTM主要由五种不同的东西组成:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre11">单元状态</strong>:LSTM单元的内部单元状态</li><li class="listitem"><strong class="calibre11">隐藏状态</strong>:用于计算预测的外部隐藏状态</li><li class="listitem"><strong class="calibre11">输入门</strong>:决定有多少电流输入被读入单元状态</li><li class="listitem"><strong class="calibre11">遗忘门</strong>:决定将多少先前的单元状态发送到当前单元状态</li><li class="listitem"><strong class="calibre11">输出门</strong>:决定有多少单元状态输出到隐藏状态</li></ul></div><p class="calibre10">拥有如此复杂的结构使得LSTMs能够很好地捕捉短期和长期的依赖关系。</p><p class="calibre10">我们比较了lstm和普通rnn，发现lstm实际上能够学习作为其结构固有部分的长期依赖性，而rnn不能学习长期依赖性。之后，我们讨论了LSTMs如何利用其复杂的结构求解消失梯度。</p><p class="calibre10">然后，我们讨论了几种改进LSTMs性能的扩展。首先，一个非常简单的技术，我们称之为贪婪采样，其中，我们不是总是输出最佳候选，而是从一组最佳候选中随机采样一个预测。我们看到这提高了生成文本的多样性。接下来，我们研究了一种更复杂的搜索技术，称为波束搜索。这样，我们不再预测未来的单个时间步，而是预测未来的几个时间步，并挑选出产生最佳联合概率的候选项。另一项改进是了解词向量如何帮助提高LSTM的预测质量。使用单词向量，LSTM可以更有效地学习在预测期间替换语义相似的单词(例如，LSTM可能会输出<em class="calibre15">猫</em>，而不是输出<em class="calibre15">狗</em>)，从而导致生成的文本更加真实和正确。我们考虑的最后一个扩展是BiLSTMs或双向LSTMs。BiLSTMs的一个流行应用是填充短语中缺失的单词。比尔斯特姆从两个方向阅读文本，从开头到结尾，从结尾到开头。这提供了更多的背景信息，因为我们在预测之前既要看过去又要看未来。</p><p class="calibre10">最后，我们讨论了普通LSTMs的两种变体:窥视孔连接和GRUs。Vanillan LSTMs在计算门时，只查看当前输入和隐藏状态。使用窥视孔连接，我们使门计算依赖于所有:当前输入、隐藏和单元状态。</p><p class="calibre10">gru是普通lstm的一个更加优雅的变体，它简化了lstm而不影响性能。gru只有两个门和一个状态，而vanilla LSTMs有三个门和两个状态。</p><p class="calibre10">在下一章中，我们将看到所有这些不同的架构和它们的实现，并看看它们在文本生成任务中的表现。</p></div></body></html>
</body></html>