<title>Preface</title>   

# 前言

在我们生活的数字信息时代，数据量呈指数级增长，在我们阅读本文时，数据量正以前所未有的速度增长。这些数据大多是与语言相关的数据(文本或口头)，如电子邮件、社交媒体帖子、电话和网络文章。自然语言处理(NLP)有效地利用这些数据来帮助人类处理业务或日常任务。NLP 已经彻底改变了我们使用数据来改善业务和生活的方式，并将在未来继续这样做。

NLP 最普遍的用例之一是虚拟助手(VAs)，如苹果的 Siri、谷歌助手和亚马逊 Alexa。每当您向您的虚拟助理询问“瑞士最便宜的酒店价格”时，就会触发一系列复杂的 NLP 任务。首先，您的虚拟设备需要理解(解析)您的请求(例如，了解它需要检索酒店价格，而不是狗公园)。VA 需要做的另一个决定是“什么是便宜的？”。接下来，VA 需要对瑞士的城市进行排名(可能基于您过去的旅行历史)。然后，VA 可能会抓取 Booking.com 和 Agoda.com 等网站来获取瑞士的酒店价格，并通过分析每家酒店的价格和评论来对其进行排名。如您所见，您在几秒钟内看到的结果是一系列非常复杂的 NLP 任务的结果。

那么，是什么让这样的 NLP 任务对我们的日常任务如此通用和准确呢？支撑元素是“深度学习”算法。深度学习算法本质上是复杂的神经网络，可以将原始数据映射到所需的输出，而不需要任何类型的特定任务特征工程。这意味着，你可以提供一个客户的酒店评论，该算法可以回答问题“客户对这家酒店的积极程度如何？”，直接。此外，深度学习已经在各种 NLP 任务(例如，语音识别和机器翻译)中达到甚至超过了人类水平的性能。

通过阅读这本书，你将学会如何使用深度学习解决许多有趣的 NLP 问题。所以，如果你想成为改变世界的影响者，学习 NLP 很关键。这些任务包括从学习单词的语义，到产生新的故事，到仅仅通过查看双语句子对来执行语言翻译。所有的技术章节都附有练习，包括为读者实现这些系统的逐步指导。对于书中的所有练习，我们将使用 Python 和 tensor flow——一个流行的分布式计算库，它使得实现深度神经网络非常方便。

<title>Preface</title>   

# 这本书是给谁的

这本书是为那些寻求通过利用语言数据来改变世界的有抱负的初学者而写的。这本书将为你解决 NLP 任务提供坚实的实践基础。在本书中，我们将涵盖 NLP 的各个方面，更多地关注实际实现而不是理论基础。当学习这些方法的更高级的理论方面时，拥有解决各种 NLP 任务的可靠的实践知识将帮助你有一个更平滑的过渡。此外，在对算法执行更多特定领域的调优时，扎实的实践理解将有助于从特定领域中获得最大收益。

<title>What this book covers</title>   

# 这本书涵盖了什么

[第 1 章](ch01.html "Chapter 1. Introduction to Natural Language Processing")，*自然语言处理介绍*，用一个对 NLP 的温和介绍开始我们的旅程。在这一章中，我们将首先看看我们需要 NLP 的原因。接下来，我们将讨论 NLP 中的一些常见子任务。此后，我们将讨论 NLP 的两个主要时代——传统时代和深度学习时代。我们将通过研究如何用传统算法解决语言建模任务来了解传统时代的特征。然后，我们将讨论深度学习时代，深度学习算法被大量用于 NLP。我们还将讨论深度学习算法的主要家族。然后，我们将讨论最基本的深度学习算法之一的基本原理——完全连接的神经网络。我们将以一个路线图来结束这一章，该路线图简要介绍了接下来的章节。

[第 2 章](ch02.html "Chapter 2. Understanding TensorFlow")，*了解 TensorFlow* ，向您介绍 Python TensorFlow 库——我们将在其上实现我们的解决方案的主要平台。我们将从编写代码来执行 TensorFlow 中的简单计算开始。然后我们将讨论事情是如何执行的，从运行代码到获得结果。因此，我们将详细了解张量流的底层组件。我们将用一个餐厅的生动比喻来进一步加强对 TensorFlow 的理解，并看看订单是如何执行的。后面我们会讨论 TensorFlow 更多的技术细节，比如 TensorFlow 中定义的数据结构和运算(大多与神经网络有关)。最后，我们将实现一个完全连接的神经网络来识别手写数字。这将有助于我们理解如何使用 TensorFlow 实现端到端解决方案。

[第 3 章](ch03.html "Chapter 3. Word2vec – Learning Word Embeddings")，*Word 2 vec——学习单词嵌入*，从讨论如何用 TensorFlow 解决 NLP 任务开始。在这一章中，我们将看到神经网络如何被用来学习单词向量或单词表示。单词向量也称为单词嵌入。单词向量是对相似单词具有相似值而对不同单词具有不同值的单词的数字表示。首先，我们将讨论实现这一点的几种传统方法，其中包括使用一个被称为 WordNet 的大型人工知识库。然后，我们将讨论基于现代神经网络的方法，称为 Word2vec，它在没有任何人工干预的情况下学习单词向量。我们将首先通过一个实际例子来理解 Word2vec 的机制。然后，我们将讨论实现这一点的两种算法变体——跳格和连续词袋(CBOW)模型。我们将讨论算法的概念细节，以及如何在 TensorFlow 中实现它们。

[第 4 章](ch04.html "Chapter 4. Advanced Word2vec")，*推进 Word2vec* ，带我们进入与单词向量相关的更高级的话题。首先，我们将比较 skip-gram 和 CBOW，看看是否存在赢家。接下来，我们将讨论几种可以用来提高 Word2vec 算法性能的改进。然后，我们将讨论一种更新更强大的单词嵌入学习算法 GloVe(全局向量)算法。最后，我们将在一个文档分类任务中研究单词向量的作用。在该练习中，我们将看到单词向量足够强大，可以表示文档所属的主题(例如，娱乐和体育)。

[第 5 章](ch05.html "Chapter 5. Sentence Classification with Convolutional Neural Networks")，*用卷积神经网络进行句子分类*，讨论卷积神经网络(CNN)——一种擅长处理图像或句子等空间数据的神经网络家族。首先，我们将通过讨论 CNN 如何处理数据以及涉及哪种操作，对其有一个坚实的高级理解。接下来，我们将深入 CNN 计算中涉及的每个操作，以理解 CNN 的基础数学。最后，我们将完成两个练习。首先，我们将使用 CNN 对手写数字图像进行分类。我们将看到，对于这项任务，细胞神经网络能够很快达到很高的精度。接下来，我们将探索如何使用 CNN 对句子进行分类。特别是，我们将要求 CNN 预测一个句子是否是关于一个物体、人、地点等等。

[第 6 章](ch06.html "Chapter 6. Recurrent Neural Networks")，*递归神经网络*，讲述了一个强大的神经网络家族，可以对数据序列进行建模，称为递归神经网络(RNNs)。我们将首先讨论 RNNs 背后的数学原理以及在学习过程中用于随时间更新 RNNs 的更新规则。然后，我们将讨论 RNNs 的不同变体及其应用(例如，一对一 RNNs 和一对多 RNNs)。最后，我们将进行一个练习，在这个练习中，使用 rnn 来完成文本生成任务。在这个节目中，我们将训练 RNN 人讲民间故事，并请 RNN 人编一个新故事。我们会看到 rnn 在保持长期记忆方面很差。最后，我们将讨论 RNNs 的一个更高级的变体，我们称之为 RNN-CF，它能够更长时间地保持记忆。

[第七章](ch07.html "Chapter 7. Long Short-Term Memory Networks")，*长短期记忆网络*，让我们能够探索更强大的技术，能够记忆更长的时间，已经发现 rnn 在保持长期记忆方面很差。我们将在本章讨论一种这样的技术——长短期记忆网络。LSTMs 更强大，并且在许多时序任务中表现出优于其他时序模型。我们将首先研究基础数学并更新 LSTM 的规则，同时用一个丰富多彩的例子来说明为什么每个计算都很重要。然后，我们将看看 LSTMs 如何能够更长时间地保持记忆。接下来，我们将讨论如何进一步提高 LSTMs 预测能力。最后，我们将讨论具有更复杂结构的 LSTMs 的几种变体(具有窥视孔连接的 LSTMs)，以及试图简化 LSTMs 门控循环单元(GRUs)的方法。

[第 8 章](ch08.html "Chapter 8. Applications of LSTM – Generating Text")，*LSTM 的应用——生成文本*，全面评估 LSTM 在文本生成任务中的表现。我们将定性和定量地衡量 LSTMs 生成的文本有多好。我们还将对 lstm、带窥视孔连接的 lstm 和 gru 进行比较。最后，我们将看到如何将单词嵌入引入到模型中来改进由 LSTMs 生成的文本。

[第 9 章](ch09.html "Chapter 9. Applications of LSTM – Image Caption Generation")，*LSTM 的应用——图像字幕生成*，在处理完文本数据后，将我们带到多模态数据(即图像和文本)上。在这一章中，我们将研究如何为给定的图像自动生成描述。这涉及以形成端到端机器学习流水线的方式将前馈模型(即 CNN)与单词嵌入层和顺序模型(即 LSTM)相结合。

[第十章](ch10.html "Chapter 10. Sequence-to-Sequence Learning – Neural Machine Translation")，*序列到序列学习**–**神经机器翻译*，讲的是实现神经机器翻译(NMT)的模型。机器翻译是将一个句子/短语从源语言翻译成目标语言。我们先简单讨论一下什么是机器翻译。接下来是关于机器翻译历史的部分。然后，我们将详细讨论现代神经机器翻译模型的架构，包括训练和推理过程。接下来，我们将看看如何从头实现一个 NMT 系统。最后，我们将探索改进标准 NMT 系统的方法。

[第 11 章](ch11.html "Chapter 11. Current Trends and the Future of Natural Language Processing")，*自然语言处理的当前趋势和未来*，最后一章，重点介绍了 NLP 的当前和未来趋势。我们将讨论与前几章讨论的系统和任务相关的最新发现。本章将涵盖大多数令人兴奋的新颖创新，以及给你深入的直觉，以实现一些技术。

[附录](apa.html "Appendix A. Mathematical Foundations and Advanced TensorFlow")、*数学基础和高级张量流*，将向读者介绍各种数学数据结构(例如矩阵)和运算(例如矩阵求逆)。我们还将讨论概率中的几个重要概念。然后我们将介绍 Keras——一个在底层使用 TensorFlow 的高级库。Keras 通过隐藏 TensorFlow 中的一些细节，简化了神经网络的实现，有些人可能会觉得这很有挑战性。具体来说，我们将看到如何用 Keras 实现 CNN，并对如何使用 Keras 有所了解。接下来，我们将讨论如何使用 TensorFlow 中的 seq2seq 库来实现一个神经机器翻译系统，使用的代码比我们在第 11 章“自然语言处理的当前趋势和未来”中使用的代码少得多。最后，我们将带您浏览一个旨在教您使用张量板来可视化单词嵌入的指南。TensorBoard 是 TensorFlow 附带的一个方便的可视化工具。这可用于可视化和监控 TensorFlow 客户端中的各种变量。

<title>To get the most out of this book</title>   

# 充分利用这本书

为了从本书中获得最大收益，我们向读者提出以下假设:

*   坚定的意志和学习 NLP 的现代方法的雄心
*   熟悉基本的 Python 语法和数据结构(例如，列表和字典)
*   对基础数学有很好的理解(例如，矩阵/向量乘法)
*   (可选)提高数学知识(例如，导数计算)以理解一些小节，这些小节涵盖了某些学习模型如何克服培训期间面临的潜在实际问题的细节
*   (可选)阅读研究论文，参考系统中的进展/细节，而不仅仅是书所涵盖的内容

## 下载示例代码文件

你可以从你在[http://www.packtpub.com](http://www.packtpub.com)的账户下载本书的示例代码文件。如果你在别处购买了这本书，你可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，让文件通过电子邮件直接发送给你。

您可以按照以下步骤下载代码文件:

1.  在[http://www.packtpub.com](http://www.packtpub.com)登录或注册。
2.  选择**支架**标签。
3.  点击**代码下载&勘误表**。
4.  在**搜索**框中输入图书名称，并按照屏幕上的说明进行操作。

下载文件后，请确保使用以下软件的最新版本解压或解压缩文件夹:

*   WinRAR / 7-Zip for Windows
*   适用于 macOS 的 Zipeg / iZip / UnRarX
*   用于 Linux 的 7-Zip / PeaZip

该书的代码包也托管在 GitHub 的[https://GitHub . com/packt publishing/Natural-Language-Processing-with-tensor flow](https://github.com/PacktPublishing/Natural-Language-Processing-with-TensorFlow)上。在 https://github.com/PacktPublishing/[网站](https://github.com/PacktPublishing/)上，我们也有来自我们丰富的书籍和视频目录的其他代码包。看看他们！

## 下载彩色图片

我们还提供了一个 PDF 文件，其中有本书中使用的截图/图表的彩色图像。可以在这里下载:[https://www . packtpub . com/sites/default/files/downloads/naturalglanguageprocessingwithtensorflow _ color images . pdf](https://www.packtpub.com/sites/default/files/downloads/NaturalLanguageProcessingwithTensorFlow_ColorImages.pdf)。

## 使用的约定

本书通篇使用了许多文本约定。

`CodeInText`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。比如说；"将下载的`WebStorm-10*.dmg`磁盘镜像文件作为另一个磁盘挂载到你的系统中."

代码块设置如下:

```
graph = tf.Graph() # Creates a graph
session = tf.InteractiveSession(graph=graph) # Creates a session
```

当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:

```
graph = tf.Graph() # Creates a graph
session = tf.InteractiveSession(graph=graph) # Creates a session

```

任何命令行输入或输出都按如下方式编写:

```
conda --version

```

**粗体**:表示一个新的术语、一个重要的单词，或者您在屏幕上看到的单词，例如，在菜单或对话框中，也像这样出现在文本中。例如:“从**管理**面板中选择**系统信息**”

*参考文献:*在[第 11 章](ch11.html "Chapter 11. Current Trends and the Future of Natural Language Processing")、*自然语言处理的当前趋势和未来*中，文本内参考文献包括一个带括号的编号(例如，[1])，该编号与该章末尾的*参考文献*部分中的编号相关。

### 注

警告或重要提示如下所示。

### 提示

提示和技巧是这样出现的。

<title>Get in touch</title>   

# 取得联系

我们随时欢迎读者的反馈。

总体反馈:给`feedback@packtpub.com`发电子邮件，并在邮件主题中提及书名。如果您对本书的任何方面有任何疑问，请通过`questions@packtpub.com`发邮件给我们。

**勘误表**:虽然我们已经尽一切努力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问，[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的图书，点击勘误表提交表单链接，并输入详细信息。

**盗版**:如果您在互联网上遇到我们作品的任何形式的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过`copyright@packtpub.com`联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有一个你擅长的主题，并且你有兴趣写一本书或者为一本书投稿，请访问[http://authors.packtpub.com](http://authors.packtpub.com)。

## 评论

请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！

有关 Packt 的更多信息，请访问[packtpub.com](http://packtpub.com)。