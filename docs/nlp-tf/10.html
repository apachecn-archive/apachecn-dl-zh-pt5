<html><head/><body>

  
    <title>Chapter 10. Sequence-to-Sequence Learning – Neural Machine Translation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title"><a id="ch10" class="calibre7"/>第十章。序列对序列学习–神经机器翻译</h1></div></div></div><p class="calibre10">序列到序列学习是用于需要将任意长度的序列映射到另一个任意长度的序列的任务的术语。这是涉及学习多对多映射的最复杂的任务之一。这项任务的例子包括<strong class="calibre11">神经机器翻译</strong> ( <strong class="calibre11"> NMT </strong>)和创建聊天机器人。NMT是我们将一个<a id="id661"/>句子从一种语言(源语言)翻译成另一种语言(目标语言)的地方。谷歌翻译是NMT系统的一个例子。聊天机器人(即可以与人交流/回答的软件)能够以逼真的方式与人交谈。这对于各种服务提供商来说尤其有用，因为聊天机器人可以用来找到客户可能有的容易解决的问题的答案，而不是将它们重定向到人工操作员。</p><p class="calibre10">在本章中，我们将学习如何实现一个NMT系统。然而，在直接进入这些最新进展之前，我们<a id="id662"/>将首先简要地访问一些<strong class="calibre11">统计机器翻译</strong> ( <strong class="calibre11"> SMT </strong>)方法，这些方法在NMT之前是最先进的系统，直到NMT赶上。接下来，我们将介绍构建NMT所需的步骤。最后，我们将学习如何实现一个真正的NMT系统，一步一步地从德语翻译成英语。</p><div><div><div><div><h1 class="title1"><a id="ch10lvl1sec71" class="calibre7"/>机器翻译</h1></div></div></div><p class="calibre10">与其他交流方式(例如，手势)相比，人类通常通过语言进行交流。目前，全世界有5000多种语言在使用。此外，将一门语言学习到一个母语为该语言的人容易理解的水平是一项很难掌握的任务。然而，交流对于分享知识、社交和拓展你的人际网络是必不可少的。因此，语言是与世界各地沟通的障碍。这就是<strong class="calibre11">机器翻译</strong> ( <strong class="calibre11"> MT </strong>)发挥作用的地方。机器翻译系统<a id="id664"/>允许用户用自己的语言(即源语言)输入句子，并以所需的目标语言输出句子。</p><p class="calibre10">MT的问题可以表述如下。比方说，给我们一个属于源语言<em class="calibre15"> S </em>的句子(或单词序列)，定义如下:</p><div><img alt="Machine translation" src="img/B08681_10_01.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="Machine translation" src="img/B08681_10_02.jpg" class="calibre27"/>。</p><p class="calibre10">源语言将被翻译成句子<img alt="Machine translation" src="img/B08681_10_03.jpg" class="calibre27"/>，其中<em class="calibre15"> T </em>是目标语言，由下式给出:</p><div><img alt="Machine translation" src="img/B08681_10_04.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="Machine translation" src="img/B08681_10_05.jpg" class="calibre27"/>。</p><p class="calibre10"><img alt="Machine translation" src="img/B08681_10_06.jpg" class="calibre27"/>通过MT系统获得，其输出如下:</p><div><img alt="Machine translation" src="img/B08681_10_07.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="Machine translation" src="img/B08681_10_08.jpg" class="calibre27"/>是算法为源句子找到的可能翻译候选的池。此外，候选人池中的最佳候选人由以下等式给出:</p><div><img alt="Machine translation" src="img/B08681_10_09.jpg" class="calibre12"/></div><p class="calibre10">这里，θ是模型参数。在训练期间，我们使用优化模型，以最大化一些已知目标翻译对于一组相应源翻译(即训练数据)的概率。</p><p class="calibre10">到目前为止，我们<a id="id665"/>讨论了我们有兴趣解决的语言翻译问题的形式设置。接下来，我们将回顾一下机器翻译的历史，感受一下早期人们是如何解决这个问题的。</p></div></div>



  
    <title>A brief historical tour of machine translation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1">机器翻译的简短历史之旅</h1></div></div></div><p class="calibre10">这里我们将<a id="id666"/>讨论机器翻译的历史。机器翻译的起源涉及基于规则的系统。然后，出现了统计上更合理的机器翻译系统。统计机器翻译使用一种语言的各种统计方法来翻译成另一种语言。然后是NMT时代。与其他方法相比，NMT目前在大多数机器学习任务中保持着最先进的性能。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec146" class="calibre7"/>基于规则翻译</h2></div></div></div><p class="calibre10">NMT是在统计机器学习之后很久才出现的，而统计机器学习已经存在了半个多世纪。SMT方法的出现可以追溯到1950-1960年，当时在第一个有记录的项目之一，乔治敦-IBM实验中，超过60个俄语句子被翻译成了英语。</p><p class="calibre10">机器翻译最初的<a id="id669"/>技术之一是基于单词的机器翻译。该系统使用双语词典进行单词到单词的翻译。但是，可以想象，这种方法有严重的局限性。显而易见的限制是，单词到单词的翻译不是不同语言之间的一对一映射。此外，逐字翻译可能会导致不正确的结果，因为它不考虑给定单词的上下文。给定单词在源语言中的翻译可以根据其使用的上下文而改变。为了用一个具体的例子来理解这一点，我们来看一下<em class="calibre15">图10.1 </em>中从英语到法语的翻译例子。你可以看到在给出的两个英语句子中，有一个单词发生了变化。然而，这给翻译带来了巨大的变化:</p><div><img alt="Rule-based translation" src="img/B08681_10_10.jpg" class="calibre12"/><div><p class="calibre10">图10.1:语言之间的翻译(英语到法语)不是单词之间的一对一映射</p></div></div><p class="calibre10">20世纪60年代，<strong class="calibre11">自动语言处理咨询委员会</strong> ( <strong class="calibre11"> ALPAC </strong>)发布了一份关于MT前景的报告，<em class="calibre15">语言与机器:翻译与语言学中的计算机</em>，<em class="calibre15">美国国家科学院(1966) </em>。结论是这样的:</p><div><blockquote class="blockquote"><p class="calibre10">有用的机器翻译没有立即或可预测的前景。</p></blockquote></div><p class="calibre10">这个<a id="id670"/>是因为当时机器翻译比人工翻译速度慢，准确性差，而且成本高。这对MT的进步是一个巨大的打击，差不多十年就这样无声无息地过去了。</p><p class="calibre10">接下来是基于语料库的机器翻译，其中<a id="id671"/>使用源句子的元组训练算法，并通过平行语料库获得相应的目标句子，即平行语料库将具有格式，(<em class="calibre15"> [( &lt;源_句子_1 &gt;，&lt;目标_句子_1 &gt;)，(&lt;源_句子_2 &gt;，&lt;目标_句子_2 &gt;)，…] </em>)。平行语料库是由元组形成的大型文本语料库，由来自源语言的<a id="id672"/>文本和该文本的相应翻译组成。表10.2 中显示了这方面的说明。应当注意，建立平行语料库比建立双语词典容易得多，并且更准确，因为训练数据比词到词训练数据更丰富。此外，可以使用平行语料库来构建两种语言的双语词典(即转换模型),而不是直接依赖于手动创建的双语词典。给定当前源单词/短语，转换模型显示目标单词/短语成为正确翻译的可能性。除了学习转换模型，基于语料库的机器翻译还学习单词对齐模型。单词对齐模型可以表示来自源语言的短语中的单词如何对应于该短语的翻译。在<em class="calibre15">图10.2 </em>中描述了一个平行语料库和单词对齐模型的示例。</p><p class="calibre10">在<em class="calibre15">表10.2 </em>中显示了一个平行语料库的示例:</p><div><table border="1" class="calibre18"><colgroup class="calibre19"><col class="calibre20"/><col class="calibre20"/></colgroup><thead class="calibre21"><tr class="calibre22"><th valign="bottom" class="calibre23">
<p class="calibre10">源语言句子(英语)</p>
</th><th valign="bottom" class="calibre23">
<p class="calibre10">目标语言句子(法语)</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10">我回家了</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">我喜欢这个房子</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10">约翰喜欢弹吉他</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">约翰·艾梅·吉他手</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10">他来自英国</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">这是安格特尔</p>
</td></tr><tr class="calibre22"><td valign="top" class="calibre25">
<p class="calibre10">…</p>
</td><td valign="top" class="calibre25">
<p class="calibre10">….</p>
</td></tr></tbody></table></div><div><img alt="Rule-based translation" src="img/B08681_10_11.jpg" class="calibre12"/><div><p class="calibre10">图10.2:两种不同语言之间的单词对齐</p></div></div><p class="calibre10">另一种流行的方法是语际机器翻译，它包括将源句子翻译成一种语言中立的语际语言(即一种元语言)，然后从语际语言中生成翻译的句子。更具体地说，语际机器翻译系统由两个重要的组件组成，一个分析器和一个合成器。分析器将获取源句子并识别施事(例如，名词)、动作(例如，动词)等等，以及它们如何相互作用。接下来，这些被识别的元素通过语际词典来表示。语际词典的一个例子是WordNet中可用的同义词集(即，具有相同含义的一组同义词)。然后，从这个语际表达，合成器将创建翻译。由于合成器通过语际表示知道名词、动词等，所以它可以通过结合语言特定的语法规则来生成目标语言的翻译。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec147" class="calibre7"/>统计机器翻译(SMT)</h2></div></div></div><p class="calibre10">接下来，更具统计学意义的<a id="id675"/>音响系统开始出现。这个时代的先驱模型之一是基于单词翻译的IBM模型1-5。然而，正如我们前面讨论的<a id="id676"/>，单词翻译并不是从源语言到目标语言的一对一的(例如，复合词和词法)。最终，研究人员开始试验基于短语的翻译系统，该系统在机器翻译方面取得了一些显著的进步。</p><p class="calibre10">基于短语的翻译与基于单词的翻译的工作方式类似，只是它使用语言的短语而不是单个单词作为翻译的原子单位。这是一种更明智的方法，因为它使得单词之间的一对多、多对一或多对多关系的建模更加容易。基于短语的翻译的主要目标是学习一个<em class="calibre15">短语翻译模型</em>，该模型包含给定源短语的不同<a id="id677"/>候选目标短语的概率分布。可以想象，这种方法需要维护两种语言中各种短语的巨大数据库。因为在一种语言的句子和另一种语言的句子之间没有单词的单调排序，所以也执行对短语的重新排序步骤。图10.2 中显示了一个这样的例子。如果语言之间的单词是单调排序的，那么单词映射之间不应该有交叉。</p><p class="calibre10">这种方法的局限性之一是解码过程(为给定的源短语找到最佳目标短语)代价很高。这是由于短语数据库的大小以及经常包含多个目标语言短语的源短语。为了减轻负担，基于语法的翻译应运而生。</p><p class="calibre10">在基于句法的翻译中，源句子由句法树表示。在<em class="calibre15">图10.3 </em>中，<strong class="calibre11"> NP </strong>代表<a id="id679"/>一个名词短语，<strong class="calibre11"> VP </strong>一个动词短语，<strong class="calibre11"> S </strong>一个句子。然后<strong class="calibre11">重新排序阶段</strong>发生，根据目标语言，树节点被重新排序以改变主语、动词和宾语的顺序。这是因为句子结构可以根据语言的不同而变化(例如，在英语中是<em class="calibre15">主语-动词-宾语</em>，而在日语中是<em class="calibre15">主语-宾语-动词</em>)。根据被称为<strong class="calibre11"> r表</strong>的东西<a id="id680"/>来决定重新排序。r表包含树节点被改变到某个其他顺序的可能性概率:</p><div><img alt="Statistical Machine Translation (SMT)" src="img/B08681_10_12.jpg" class="calibre12"/><div><p class="calibre10">图10.3。句子的语法树</p></div></div><p class="calibre10">出现<strong class="calibre11">插入阶段</strong>然后<a id="id681"/>。在插入阶段，我们向树的每个节点随机插入一个单词。这是由于假设存在一个不可见的空单词，并且它在树的随机位置生成目标单词。同样，插入一个单词的概率是由一个叫做<strong class="calibre11"> n表</strong>的东西决定的，这个表包含了将一个特定单词<a id="id682"/>插入到树中的概率。</p><p class="calibre10">接下来<strong class="calibre11">翻译阶段</strong>发生，其中<a id="id683"/>每个叶节点以逐字的方式被翻译成目标单词。最后，从语法树中读出<a id="id684"/>翻译句子，以构建<a id="id685"/>目标句子。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec148" class="calibre7"/>神经机器翻译(NMT)</h2></div></div></div><p class="calibre10">最后，大约在【2014年，NMT系统被引入。NMT是一个端到端的系统，它将一个完整的<a id="id687"/>句子作为输入，执行某些转换，然后输出对应源句子的翻译句子。因此，NMT消除了机器翻译所需的功能工程，如构建短语翻译模型和构建语法树，这是NLP社区的一大胜利。此外，NMT在很短的时间内，仅仅两到三年，就胜过了所有其他流行的机器翻译技术。在<em class="calibre15">图10.4 </em>中，我们描述了机器翻译文献中报道的各种机器翻译系统的结果。例如，2016年的结果是从Sennrich等人的论文中获得的，<em class="calibre15">爱丁堡WMT神经机器翻译系统16 </em>，<em class="calibre15">计算语言学协会</em>，<em class="calibre15">第一届机器翻译会议论文集</em>，<em class="calibre15">2016年8月:371-376 </em>，以及从Williams等人的论文中获得的，<em class="calibre15">爱丁堡WMT 16统计机器翻译系统</em>，<em class="calibre15">计算语言学协会<em class="calibre15">所有机器翻译系统都用BLEU评分进行评估。正如我们在<a href="ch09.html" title="Chapter 9. Applications of LSTM – Image Caption Generation">第9章</a>、<em class="calibre15">LSTM的应用-图像字幕生成</em>中所讨论的，BLEU分数表示候选翻译中与参考翻译相匹配的n元语法(例如，一元语法和二元语法)的数量。所以BLEU分数越高，MT系统越好。我们将在本章后面详细讨论BLEU度量。没有必要强调NMT是一个明显的赢家:</em></em></p><div><img alt="Neural Machine Translation (NMT)" src="img/B08681_10_13.jpg" class="calibre12"/><div><p class="calibre10">图10.4。统计机器翻译系统与NMT系统的比较。里科·森里奇的好意。</p></div></div><p class="calibre10">评估NMT系统潜力的案例研究<a id="id688"/>见<em class="calibre15">神经机器翻译准备好部署了吗？30个翻译方向的案例研究</em>、<em class="calibre15"> Junczys-Dowmunt </em>、<em class="calibre15"> Hoang </em>和<em class="calibre15"> Dwojak </em>、<em class="calibre15">第九届口语翻译国际研讨会论文集</em>、<em class="calibre15">西雅图(2016) </em>。该研究着眼于不同系统在不同语言(英语、阿拉伯语、法语、俄语和汉语)之间的几项<a id="id689"/>翻译任务中的表现。结果也支持NMT系统(NMT 120万和NMT 240万)比SMT系统(PB-SMT和Hiero)表现更好。</p><p class="calibre10"><em class="calibre15">图10.5 </em>显示了2017年当前最先进的机器翻译机的几个统计数据。这是来自一个演示文稿，<em class="calibre15">机器翻译的状态</em>，<em class="calibre15"> Intento，Inc </em>，<em class="calibre15"> 2017 </em>，由Intento的联合创始人兼首席执行官Konstantin Savenkov制作。我们可以看到，DeepL(<a href="https://www.deepl.com">https://www.deepl.com</a>)生产的MT的性能似乎正在与包括谷歌在内的其他MT巨头展开激烈竞争。比较对象包括DeepL (NMT)、Google (NMT)、Yandex (NMT-s MT混合)、微软(同时拥有SMT和NMT)、IBM (SMT)、Prompt(基于规则)、SYSTRAN(基于规则/SMT混合)等MT系统。图表<a id="id690"/>清楚地显示，NMT系统公司引领着当前的机器翻译发展。LEPOR分数<a id="id691"/>用于评估不同的系统。LEPOR是比BLEU更高级的度量标准，它试图解决<em class="calibre15">语言偏见问题</em>。语言偏差问题是指某些评价指标(如BLEU)对某些语言表现良好，而对另一些语言表现不佳的现象。</p><p class="calibre10">然而，还应该注意，由于在比较中使用的平均机制，结果确实包含一些偏差。例如，Google Translator已经在更大的语言集(包括困难的翻译任务)上进行了平均，而DeepL已经在更小且相对更容易的语言子集上进行了平均。所以不应该得出DeepL MT系统比Google MT系统好的结论。然而，总体结果提供了当前NMT和SMT系统的性能的一般比较:</p><div><img alt="Neural Machine Translation (NMT)" src="img/B08681_10_14.jpg" class="calibre12"/><div><p class="calibre10">图10.5:各种机器翻译系统的性能。Intento公司提供。</p></div></div><p class="calibre10">我们已经看到，NMT在短短几年内已经超越了SMT系统，这是当前的技术水平。我们现在将继续讨论NMT系统的细节和架构。最后，我们将从头开始实施NMT系统。</p></div></div>



  
    <title>Understanding Neural Machine Translation</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec73" class="calibre7"/>理解神经机器翻译</h1></div></div></div><p class="calibre10">既然我们已经对机器翻译如何随着时间的推移而演变有了一个<a id="id693"/>的认识，那么让我们试着去理解最先进的NMT是如何工作的。首先，我们将看看神经机器翻译器使用的模型架构，然后继续理解实际的训练算法。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec149" class="calibre7"/>NMT背后的直觉</h2></div></div></div><p class="calibre10">首先，让我们理解NMT系统设计背后的<a id="id694"/>直觉。假设你是一个英语和德语流利的人，要求你将下面的句子翻译成英语:</p><p class="calibre10"><em class="calibre15">我去了那家餐厅</em></p><p class="calibre10">这句话翻译过来就是:</p><p class="calibre10"><em class="calibre15">我回家了</em></p><p class="calibre10">虽然对于一个流利的人来说，翻译这个可能不需要超过几秒钟，但翻译中涉及到一定的过程。首先，你读德语句子，然后你创造一个关于这个句子代表或暗示什么的想法或概念。最后，你把这个句子翻译成英语。同样的想法也用于构建NMT系统(见<em class="calibre15">图10.6 </em>)。编码器读取源句子(即类似于你读取德语句子)。然后编码器输出一个上下文向量(上下文向量对应的是你看完句子后想象的思想/概念)。最后，解码器接收上下文向量并输出英语翻译:</p><div><img alt="Intuition behind NMT" src="img/B08681_10_15.jpg" class="calibre12"/><div><p class="calibre10">图10.6。NMT系统的概念架构</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec150" class="calibre7"/> NMT建筑</h2></div></div></div><p class="calibre10">现在我们将<a id="id695"/>更详细地看看这个架构。这里讨论的序列到序列方法是由Sutskever，Vinyals和Le在他们的论文中提出的，<em class="calibre15">序列到序列学习与神经网络</em>，<em class="calibre15">第27届国际神经信息处理系统会议录-第2卷:3104-3112 </em>。从<em class="calibre15">图10.6 </em>中的图表，我们可以看到<a id="id696"/>NMT架构中有两个主要组件。这些被称为编码器和解码器。换句话说，NMT可以被视为一个编码器-解码器架构。<strong class="calibre11">编码器</strong>将一个句子从给定的源语言转换成<em class="calibre15">思维</em>，而<strong class="calibre11">解码器</strong> <a id="id698"/>将<em class="calibre15">思维</em>解码或翻译成目标语言。如您所见，这与我们简单讨论过的语际机器翻译方法有一些共同的特征。这在<em class="calibre15">图10.7 </em>中有所说明。上下文向量的左侧表示编码器(它逐字逐句地获取源句子来训练时间序列模型)。右侧的<a id="id699"/>表示解码器，它通过<a id="id700"/>单词输出单词(同时使用前一个单词作为当前输入)源句子的相应翻译。我们还将使用嵌入层(针对源语言和目标语言)来提供单词向量作为模型的输入:</p><div><img alt="NMT architecture" src="img/B08681_10_16.jpg" class="calibre12"/><div><p class="calibre10">图10.7:随着时间的推移展开源句子和目标句子</p></div></div><p class="calibre10">有了对NMT的基本了解，让我们正式定义NMT的目标。给定源句子<em class="calibre15">x<sub class="calibre17">s</sub>T28】及其对应的<em class="calibre15">y<sub class="calibre17">t</sub>T32】，NMT系统的最终目标是最大化对数似然，即最大化以下内容:</em></em></p><div><img alt="NMT architecture" src="img/B08681_10_17.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> N </em>是指我们作为训练数据的源和目标句子元组的数量。</p><p class="calibre10">然后，在推理过程中，对于一个给定的源句子，<img alt="NMT architecture" src="img/B08681_10_18.jpg" class="calibre27"/>，我们将使用下面的语句找到<img alt="NMT architecture" src="img/B08681_10_19.jpg" class="calibre27"/>的翻译:</p><div><img alt="NMT architecture" src="img/B08681_10_20.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="NMT architecture" src="img/B08681_10_21.jpg" class="calibre27"/>是可能的候选句子的集合。</p><p class="calibre10">在我们<a id="id701"/>检查NMT架构的每个部分之前，让我们<a id="id702"/>定义数学符号来更具体地理解这个系统。</p><p class="calibre10">让我们将编码器LSTM定义为<img alt="NMT architecture" src="img/B08681_10_22.jpg" class="calibre27"/>，解码器LSTM定义为<img alt="NMT architecture" src="img/B08681_10_23.jpg" class="calibre27"/>。在时间步<em class="calibre15"> t </em>处，让我们将LSTM的单元状态定义为<em class="calibre15">c<sub class="calibre17">t</sub>T14】，将外部隐藏状态定义为<em class="calibre15">h<sub class="calibre17">t</sub>T18】。因此，输入<em class="calibre15">x<sub class="calibre17">t</sub>T22】进给到LSTM产生<em class="calibre15">c<sub class="calibre17">t</sub>T26】和<em class="calibre15">h<sub class="calibre17">t</sub>T30:</em></em></em></em></em></p><div><img alt="NMT architecture" src="img/B08681_10_24.jpg" class="calibre12"/></div><p class="calibre10">现在，我们将讨论嵌入层、编码器、上下文向量，最后是解码器。</p><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec47" class="calibre7"/>嵌入层</h3></div></div></div><p class="calibre10">在<a href="ch08.html" title="Chapter 8. Applications of LSTM – Generating Text">第8章</a>、<em class="calibre15">LSTM的应用-生成文本</em>和<a href="ch09.html" title="Chapter 9. Applications of LSTM – Image Caption Generation">第9章</a>、<em class="calibre15">LSTM的应用-图像标题生成</em>中，我们详细讨论了使用单词嵌入<a id="id703"/>而不是单词的一次热编码表示的好处，尤其是当词汇量很大时。这里，我们也使用了两个单词的嵌入层，<img alt="The embedding layer" src="img/B08681_10_25.jpg" class="calibre27"/>用于源语言，<img alt="The embedding layer" src="img/B08681_10_26.jpg" class="calibre27"/>用于目标语言。因此，我们将得到<img alt="The embedding layer" src="img/B08681_10_27.jpg" class="calibre27"/>，而不是直接将<em class="calibre15">x<sub class="calibre17">t</sub>t</em>进给到<em class="calibre15"> LSTM </em>中。然而，为了避免不必要的增加符号，我们将假设<img alt="The embedding layer" src="img/B08681_10_28.jpg" class="calibre27"/>。</p></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec48" class="calibre7"/>编码器</h3></div></div></div><p class="calibre10">正如前面提到的<a id="id704"/>，编码器负责生成一个<em class="calibre15">思维向量</em>或者一个代表源语言含义的上下文向量。为此，我们将使用LSTM网络(见<em class="calibre15">图10.8 </em>):</p><div><img alt="The encoder" src="img/B08681_10_29.jpg" class="calibre12"/><div><p class="calibre10">图10.8:一个LSTM单元</p></div></div><p class="calibre10">编码器以c <sub class="calibre17"> 0 </sub>和h <sub class="calibre17"> 0 </sub>作为零向量进行初始化。编码器将单词序列<img alt="The encoder" src="img/B08681_10_32.jpg" class="calibre27"/>作为输入，并计算上下文向量<img alt="The encoder" src="img/B08681_10_33.jpg" class="calibre27"/>，其中<em class="calibre15"> v </em> <em class="calibre15"> c </em>是最终的单元状态，并且<em class="calibre15"> v <sub class="calibre17"> h </sub> </em>是在处理序列的最后一个元素<img alt="The encoder" src="img/B08681_10_34.jpg" class="calibre27"/><em class="calibre15">x<sub class="calibre17">T</sub></em>之后获得的最终外部隐藏状态。我们将此描述如下:</p><div><img alt="The encoder" src="img/B08681_10_35.jpg" class="calibre12"/></div><div><img alt="The encoder" src="img/B08681_10_36.jpg" class="calibre12"/></div><div><img alt="The encoder" src="img/B08681_10_37.jpg" class="calibre12"/></div></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec49" class="calibre7"/>上下文向量</h3></div></div></div><p class="calibre10"><a id="id705"/>上下文向量(<em class="calibre15"> v </em>)的思想是简明地表示源语言的句子。此外，与编码器状态的初始化方式相反(即，它们用零初始化)，上下文向量成为解码器LSTM的初始状态。换句话说，解码器LSTM不是以零的初始状态开始，而是以上下文向量作为其初始状态。我们接下来会更详细地讨论这一点。</p></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec50" class="calibre7"/>解码器</h3></div></div></div><p class="calibre10">解码器<a id="id706"/>负责将上下文向量解码成期望的翻译。我们的解码器也是LSTM网络。虽然编码器和解码器可以共享同一组权重，但通常最好是编码器和解码器使用两个不同的网络。这增加了我们模型中的参数数量，允许我们更有效地学习翻译。</p><p class="calibre10">首先，用上下文向量<img alt="The decoder" src="img/B08681_10_38.jpg" class="calibre27"/>初始化解码器的状态，如下所示:<img alt="The decoder" src="img/B08681_10_39.jpg" class="calibre27"/> <img alt="The decoder" src="img/B08681_10_40.jpg" class="calibre27"/></p><p class="calibre10">在这里，<img alt="The decoder" src="img/B08681_10_41.jpg" class="calibre27"/>。</p><p class="calibre10">这(<em class="calibre15"> v </em>)是<a id="id707"/>连接编码器和解码器形成端到端计算链的关键环节(参见<em class="calibre15">图10.6 </em>编码器和解码器唯一共享的是<em class="calibre15"> v </em>)。此外，这是解码器可以获得的关于源句子的唯一信息。</p><p class="calibre10">然后，我们将使用以下公式计算翻译句子的第<em class="calibre15"> m </em> <em class="calibre15"> th </em>预测:</p><div><img alt="The decoder" src="img/B08681_10_42.jpg" class="calibre12"/></div><div><img alt="The decoder" src="img/B08681_10_43.jpg" class="calibre12"/></div><p class="calibre10">完整的NMT系统以及编码器中的LSTM单元如何连接到解码器中的LSTM单元以及softmax层如何用于输出预测的细节如图<em class="calibre15">图10.9 </em>所示:</p><div><img alt="The decoder" src="img/B08681_10_44.jpg" class="calibre12"/><div><p class="calibre10">图10.9:带LSTMs的编码器-解码器架构</p></div></div></div></div></div>



  
    <title>Preparing data for the NMT system</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec74" class="calibre7"/>为NMT系统准备数据</h1></div></div></div><p class="calibre10">在本节中，我们<a id="id708"/>将讨论从NMT系统中为训练和预测准备数据的确切过程。首先，我们将讨论如何准备<a id="id709"/>训练数据(即源句子和目标句子对)来训练NMT系统，然后输入给定的源句子来产生源句子的翻译。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec151" class="calibre7"/>训练时</h2></div></div></div><p class="calibre10">训练数据<a id="id710"/>由成对的源句子和相应的目标语言翻译组成。一个例子可能是这样的:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">(我回家了)</em></li><li class="listitem"><em class="calibre15">(她在学校的帽子，她在学校等着)</em></li></ul></div><p class="calibre10">我们的数据集中有<em class="calibre15"> N </em>个这样的对。如果我们要实现一个相当好的翻译器，<em class="calibre15"> N </em>需要在百万级别。同样，训练数据的增加也意味着训练时间的延长。</p><p class="calibre10">接下来介绍两个特殊令牌:<em class="calibre15"> &lt; s &gt; </em>和<em class="calibre15"> &lt; /s &gt; </em>。<em class="calibre15"> &lt; s &gt; </em>记号表示一个句子的开始，而<em class="calibre15"> &lt; /s &gt; </em>表示一个句子的结束。现在，数据看起来像这样:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">(&lt;&gt;我去了家&lt;/我回家了&lt;/我回家了&gt;/我回家了&lt;/我回家了&gt; ) </em></li><li class="listitem">她在学校里等着呢</li></ul></div><p class="calibre10">此后，我们将使用<em class="calibre15"> &lt; /s &gt; </em>标记填充句子，使得源句子具有固定长度<em class="calibre15"> L </em>，目标句子具有固定长度<em class="calibre15"> M </em>。需要注意的是<em class="calibre15"> L </em>和<em class="calibre15"> M </em>不需要相等。该步骤会导致以下结果:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">(&lt;s&gt;I ching nach Hause&lt;/s&gt;&lt;/s&gt;&lt;/s&gt;，&lt; s &gt;我回家了&lt;/s&gt;&lt;/s&gt;&lt;/s&gt;)</em></li><li class="listitem">她在学校里等着呢</li></ul></div><p class="calibre10">如果一个句子的长度大于<em class="calibre15"> L </em>或<em class="calibre15"> M </em>，它将被截断以适应长度。然后将句子通过分词器，得到分词后的单词。这里我忽略了第二个元组(即一对句子)，因为两者的处理方式相似:</p><p class="calibre10"><em class="calibre15"> ([' &lt; s &gt;'，' Ich '，' ging '，' nach '，' Hause '，'&lt; /s &gt;'，'&lt; /s &gt;'，&lt; s &gt;'，'我'，'去了'，'家'，'&lt; /s &gt;'，'&lt; /s &gt;'，'&lt; /s &gt;</em></p><p class="calibre10">应该注意的是，将句子定长并不是必须的，因为LSTMs能够处理动态序列大小的<a id="id711"/>。然而，将它们固定长度有助于我们成批处理句子，而不是逐个处理。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec152" class="calibre7"/>颠倒源句</h2></div></div></div><p class="calibre10">接下来我们将<a id="id712"/>在源句子上表演一个特殊的戏法。比方说，我们在源语言中有一个句子<em class="calibre15"> ABC </em>，我们想把它翻译成目标语言中的<img alt="Reversing the source sentence" src="img/B08681_10_45.jpg" class="calibre27"/>。我们将首先颠倒源句子，这样句子<em class="calibre15"> ABC </em>将读作<em class="calibre15"> CBA </em>。这意味着为了将<em class="calibre15"> ABC </em>翻译成<img alt="Reversing the source sentence" src="img/B08681_10_46.jpg" class="calibre27"/>，我们需要输入<em class="calibre15"> CBA </em>。这大大提高了我们的模型的性能，尤其是当源语言和目标语言共享相同的句子结构(例如，主语-动词-宾语)时。</p><p class="calibre10">让我们试着理解为什么这有帮助。主要是，它有助于在编码器和解码器之间建立良好的<em class="calibre15">通信</em>。让我们从前面的例子开始。我们将连接源句子和目标句子:</p><div><img alt="Reversing the source sentence" src="img/B08681_10_47.jpg" class="calibre12"/></div><p class="calibre10">如果你计算一下从<em class="calibre15"> A </em>到<img alt="Reversing the source sentence" src="img/B08681_10_48.jpg" class="calibre27"/>或者<em class="calibre15"> B </em>到<img alt="Reversing the source sentence" src="img/B08681_10_49.jpg" class="calibre27"/>的距离(也就是分隔两个单词的字数)，它们会是一样的。但是，当您颠倒源句子时，请考虑这一点，如下所示:</p><div><img alt="Reversing the source sentence" src="img/B08681_10_50.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> A </em>和<img alt="Reversing the source sentence" src="img/B08681_10_51.jpg" class="calibre27"/>非常接近，以此类推。此外，要做出好的翻译，从一开始就建立良好的沟通是很重要的。这可能有助于NMT系统通过这个简单的技巧来提高其性能。</p><p class="calibre10">现在，我们的数据集变成了这样:</p><p class="calibre10"><em class="calibre15"> ([' &lt; /s &gt;'，'&lt; /s &gt;'，'&lt; /s &gt;'，'豪斯'，' nach '，' ging '，' Ich '，'&lt; s &gt;')，&lt; s &gt;'，'我'，'去了'，'家'，'&lt; /s &gt;'，'&lt; /s &gt;'，'&lt;</em></p><p class="calibre10">接下来，使用<a id="id713"/>所学习的嵌入、<img alt="Reversing the source sentence" src="img/B08681_10_52.jpg" class="calibre27"/>和<img alt="Reversing the source sentence" src="img/B08681_10_53.jpg" class="calibre27"/>，我们用相应的嵌入向量替换每个单词。</p><p class="calibre10">另一个好消息是，我们的源句子以一个<em class="calibre15"> &lt; s &gt; </em>标记结束，而目标句子以一个<em class="calibre15"> &lt; s &gt; </em>标记开始，因此在训练期间，我们不必做任何特殊的处理来建立源句子的结尾和目标句子的开头之间的链接。</p><div><div><h3 class="title4"><a id="note36" class="calibre7"/>注</h3><p class="calibre16">注意，源句子反转步骤是主观预处理步骤。对于某些翻译任务，这可能不是必需的。例如，如果你的翻译任务是从日语(也就是经常写的主语-宾语-动词格式)翻译成菲律宾语(经常写的动词-主语-宾语)，那么颠倒源句实际上可能会造成伤害而不是帮助。这是因为通过反转日语中的文本，您正在增加目标句子的起始元素(即动词(日语))和对应的源语言实体(即动词(菲律宾语))之间的距离。</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec153" class="calibre7"/>测试时</h2></div></div></div><p class="calibre10">在测试的时候，我们<a id="id714"/>只有源句子，没有目标句子。此外，我们准备我们的源数据，就像我们在训练阶段所做的那样。接下来，我们通过将解码器的最后一个预测单词作为下一个输入，来得到逐字翻译的输出。预测过程首先通过首先向解码器输入一个<em class="calibre15"> &lt; s &gt; </em>令牌来触发。</p><p class="calibre10">我们将讨论给定源句子的精确训练过程和预测过程。</p></div></div>



  
    <title>Training the NMT</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec75" class="calibre7"/>训练NMT</h1></div></div></div><p class="calibre10">既然我们已经<a id="id715"/>定义了NMT架构并预处理了训练数据，那么训练模型就相当简单了。这里我们将定义和说明(参见<em class="calibre15">图10.10 </em>)用于培训的确切流程:</p><div><ol class="orderedlist"><li class="listitem1">Preprocess <div><img alt="Training the NMT" src="img/B08681_10_54.jpg" class="calibre12"/></div><p class="calibre10">如前所述</p></li><li class="listitem1">将<em class="calibre15">x<sub class="calibre17">s</sub>T16】送入<img alt="Training the NMT" src="img/B08681_10_55.jpg" class="calibre27"/>并根据<em class="calibre15">x<sub class="calibre17">s</sub>T23】计算<em class="calibre15"> v </em></em></em></li><li class="listitem1">用<em class="calibre15"> v </em>初始化<img alt="Training the NMT" src="img/B08681_10_56.jpg" class="calibre27"/></li><li class="listitem1">从<img alt="Training the NMT" src="img/B08681_10_58.jpg" class="calibre27"/>预测对应输入句子<em class="calibre15"> x <sub class="calibre17"> s </sub> </em>的<img alt="Training the NMT" src="img/B08681_10_57.jpg" class="calibre27"/>，其中<em class="calibre15"> m <sub class="calibre17"> th </sub> </em>预测，出目标词汇<em class="calibre15"> V </em>计算如下:<div> <img alt="Training the NMT" src="img/B08681_10_59.jpg" class="calibre12"/> </div> <div> <img alt="Training the NMT" src="img/B08681_10_111.jpg" class="calibre12"/> </div>这里wTm表示第m个位置的最佳目标词。</li><li class="listitem1">计算损失:预测单词<img alt="Training the NMT" src="img/B08681_10_60.jpg" class="calibre27"/>和在<img alt="Training the NMT" src="img/B08681_10_61.jpg" class="calibre27"/>位置的实际单词<img alt="Training the NMT" src="img/B08681_10_62.jpg" class="calibre27"/>之间的分类交叉熵</li><li class="listitem1">相对于损失<div> <img alt="Training the NMT" src="img/B08681_10_65.jpg" class="calibre12"/> <div> <p class="calibre10">优化<img alt="Training the NMT" src="img/B08681_10_63.jpg" class="calibre27"/>、<img alt="Training the NMT" src="img/B08681_10_64.jpg" class="calibre27"/>和<em class="calibre15"> softmax </em>层图10.10:NMT</p></div></div>的训练过程</li></ol></div></div>



  
    <title>Inference with NMT</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec76" class="calibre7"/>推论与NMT</h1></div></div></div><p class="calibre10">推理<a id="id716"/>与NMT的训练过程略有不同(<em class="calibre15">图10.11 </em>)。由于我们在推理时没有目标句子，我们需要一种方法在编码阶段结束时触发解码器。这与我们在<a href="ch09.html" title="Chapter 9. Applications of LSTM – Image Caption Generation">第9章</a>、<em class="calibre15">LSTM的应用——图像字幕生成</em>中所做的图像字幕练习有相似之处。在那个练习中，我们将<em class="calibre15"> &lt; SOS &gt; </em>标记附加到字幕的开头来表示字幕的开始，将<em class="calibre15"> &lt; EOS &gt; </em>标记附加到字幕的结尾。</p><p class="calibre10">我们可以简单地做到这一点，将<em class="calibre15"> &lt; s &gt; </em>作为解码器的第一个输入，然后将预测作为输出，并将最后一个预测作为下一个输入提供给NMT:</p><div><ol class="orderedlist"><li class="listitem1">如前所述预处理<em class="calibre15">x<sub class="calibre17">s</sub>T31】</em></li><li class="listitem1">将<em class="calibre15">x<sub class="calibre17">s</sub>T35】送入<img alt="Inference with NMT" src="img/B08681_10_66.jpg" class="calibre27"/>并根据<em class="calibre15">x<sub class="calibre17">s</sub>T42】计算<em class="calibre15"> v </em></em></em></li><li class="listitem1">用<em class="calibre15"> v </em>初始化<img alt="Inference with NMT" src="img/B08681_10_67.jpg" class="calibre27"/></li><li class="listitem1">对于初始预测步骤，通过调节<img alt="Inference with NMT" src="img/B08681_10_69.jpg" class="calibre27"/>和<em class="calibre15"> v </em>上的预测来预测<img alt="Inference with NMT" src="img/B08681_10_68.jpg" class="calibre27"/></li><li class="listitem1">对于随后的时间步骤，当<img alt="Inference with NMT" src="img/B08681_10_70.jpg" class="calibre27"/>时，通过调节对<img alt="Inference with NMT" src="img/B08681_10_72.jpg" class="calibre27"/>和<em class="calibre15"> v </em> <div> <img alt="Inference with NMT" src="img/B08681_10_73.jpg" class="calibre12"/> <div> <p class="calibre10">的预测来预测<img alt="Inference with NMT" src="img/B08681_10_71.jpg" class="calibre27"/>图10.11:从NMT </p> </div> </div>推断</li></ol></div></div>



  
    <title>The BLEU score – evaluating the machine translation systems</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec77" class="calibre7"/>BLEU评分——评估机器翻译系统</h1></div></div></div><p class="calibre10"><strong class="calibre11"> BLEU </strong>代表<strong class="calibre11">双语评估候补</strong>，是自动评估<a id="id717"/>机器翻译系统的一种方式。这种<a id="id718"/>度量首次在论文中介绍，<em class="calibre15"> BLEU:一种自动评估机器翻译的方法</em>，<em class="calibre15"> Papineni等</em>，<em class="calibre15">计算语言学协会(ACL)第40届年会论文集</em>，<em class="calibre15">费城</em>，<em class="calibre15">2002年7月:311-318 </em>。我们将实现BLEU分数计算算法，并在<code class="literal">bleu_score_example.ipynb</code>中作为练习提供。我们来了解一下这个是怎么算出来的。</p><p class="calibre10">让我们考虑一个例子来学习BLEU分数的计算。比方说，对于某个给定的源句子，我们有两个候选句子(即由我们的机器翻译系统预测的句子)和一个参考句子(即相应的实际翻译):</p><div><ul class="itemizedlist"><li class="listitem">参考文献1:猫坐在垫子上</li><li class="listitem">候选人1:猫在垫子上</li></ul></div><p class="calibre10">要看翻译的有多好，我们可以用一个衡量标准，<em class="calibre15">精度</em>。Precision是对候选词中有多少单词实际出现在引用中的度量。一般而言，如果考虑具有两个类别(由负数和正数表示)的分类问题，精度由以下公式给出:</p><div><img alt="The BLEU score – evaluating the machine translation systems" src="img/B08681_10_74.jpg" class="calibre12"/></div><p class="calibre10">现在让我们计算候选1的精度:</p><p class="calibre10"><em class="calibre15">精度=候选词的每个词在参考文献中出现的次数/候选词的字数</em></p><p class="calibre10">在数学上，这可以由以下公式给出:</p><div><img alt="The BLEU score – evaluating the machine translation systems" src="img/B08681_10_75.jpg" class="calibre12"/></div><p class="calibre10"><em class="calibre15">候选人1的精度= 5/6 </em></p><p class="calibre10">这也被称为1克精度，因为我们一次只考虑一个单词。</p><p class="calibre10">现在让我们<a id="id721"/>介绍一位新的候选人:</p><p class="calibre10">候选人2:那只猫猫猫</p><p class="calibre10">人类不难看出候选人1比候选人2好得多。让我们计算一下精度:</p><p class="calibre10"><em class="calibre15">候选2的精度= 6/6 = 1 </em></p><p class="calibre10">我们可以看到，精度分数与我们做出的判断不一致。因此，不能仅仅依靠精确度来衡量翻译的质量。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec154" class="calibre7"/>修改精度</h2></div></div></div><p class="calibre10">为了解决<a id="id723"/>的精度限制，我们可以使用修正的1克精度。修改后的精度会根据候选词中每个唯一词在引用中出现的次数来裁剪该词的出现次数:</p><div><img alt="Modified precision" src="img/B08681_10_76.jpg" class="calibre12"/></div><p class="calibre10">因此，对于候选项1和2，修改后的精度如下:</p><p class="calibre10"><em class="calibre15"> Mod-1-gram-Precision候选1 = (1 + 1 + 1 + 1 + 1)/ 6 = 5/6 </em></p><p class="calibre10"><em class="calibre15"> Mod-1-gram-Precision候选2= (2 + 1) / 6 = 3/6 </em></p><p class="calibre10">我们已经可以看到这是一个很好的修改，因为候选2的精度降低了。通过一次考虑<em class="calibre15"> n </em>个单词而不是单个单词，这可以扩展到任何n元语法。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec155" class="calibre7"/>简洁处罚</h2></div></div></div><p class="calibre10">精准自然<a id="id725"/>偏爱小句。这在评估中提出了一个问题，因为机器翻译系统可能为较长的引用生成小句，但仍然具有较高的精度。因此，引入了<em class="calibre15">简洁惩罚</em>来避免这种情况。简洁损失的计算方法如下:</p><div><img alt="Brevity penalty" src="img/B08681_10_77.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> c </em>是候选句子长度，<em class="calibre15"> r </em>是参考句子长度。在我们的示例中，我们的计算如下所示:</p><p class="calibre10">候选人1的BP =<img alt="Brevity penalty" src="img/B08681_10_78.jpg" class="calibre27"/></p><p class="calibre10">候选人2的BP =<img alt="Brevity penalty" src="img/B08681_10_79.jpg" class="calibre27"/></p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec156" class="calibre7"/>最终BLEU分数</h2></div></div></div><p class="calibre10">接下来，为了<a id="id726"/>计算BLEU得分，我们首先为一组不同的<img alt="The final BLEU score" src="img/B08681_10_80.jpg" class="calibre27"/>值计算几个不同的修正n元语法精度。然后，我们将计算n元语法精度的加权几何平均值:</p><div><img alt="The final BLEU score" src="img/B08681_10_109.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15">w<sub class="calibre17">n</sub>T25】为修正后的n元精度<em class="calibre15">p<sub class="calibre17">n</sub>T29】的权重。默认情况下，所有n-gram值使用相同的权重。总之，BLEU计算一个修改的n-gram精度，并用一个简洁惩罚来惩罚修改的n-gram精度。修改后的n-gram精度避免了给予无意义句子(例如，候选2)的潜在高精度值。</em></em></p></div></div>



  
    <title>Implementing an NMT from scratch – a German to English translator</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec78" class="calibre7"/>从零开始实现NMT——德语到英语的翻译器</h1></div></div></div><p class="calibre10">现在我们将<a id="id727"/>实现一个实际的神经机器翻译器。我们将使用原始张量流操作变量实现NMT。该练习可在<code class="literal">ch10/neural_machine_translation.ipynb</code>中找到。不过TensorFlow中有一个子库，叫做<code class="literal">seq2seq</code>库。你可以阅读更多关于<code class="literal">seq2seq</code>的信息，也可以学习用<a href="apa.html" title="Appendix A. Mathematical Foundations and Advanced TensorFlow">附录</a>、<em class="calibre15">数学基础和高级张量流</em>中的<code class="literal">seq2seq</code>实现一个NMT。</p><p class="calibre10">我们之所以使用raw TensorFlow是因为，一旦你在不使用任何辅助函数的情况下从头开始学习实现一个机器翻译器，你将能够快速学会使用<code class="literal">seq2seq</code>库。此外，学习使用原始张量流实现序列到序列模型的在线资源非常匮乏。然而，有许多关于如何使用<code class="literal">seq2seq</code>库进行机器翻译的资源/教程。</p><div><div><h3 class="title4"><a id="note37" class="calibre7"/>注意</h3><p class="calibre16">TensorFlow在<a href="https://www.tensorflow.org/tutorials/seq2seq">https://www.tensorflow.org/tutorials/seq2seq</a>针对NMT提供了非常翔实的序列对序列学习教程。</p></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec157" class="calibre7"/>数据简介</h2></div></div></div><p class="calibre10">我们使用在https://nlp.stanford.edu/projects/nmt/可用的英德句子对。大约有450万对可用的句子。然而，由于计算的可行性，我们将只使用250，000个句子对。词汇表由50，000个最常见的英语单词和50，000个最常见的德语单词组成，词汇表中没有的单词将被替换为一个特殊的标记<code class="literal">&lt;unk&gt;</code>。这里，我们将列出在数据集中找到的例句:</p><div><pre class="programlisting">DE:  Das Großunternehmen sieht sich einfach die Produkte des kleinen Unternehmens an und unterstellt so viele Patentverletzungen , wie es nur geht .

EN:  The large corporation will look at the products of the small company and bring up as many patent infringement assertions as possible .

DE:  In der ordentlichen Sitzung am 22. September 2008 befasste sich der Aufsichtsrat mit strategischen Themen aus den einzelnen Geschäftsbereichen wie der Positionierung des Kassamarktes im Wettbewerb mit außerbörslichen Handelsplattformen , den Innovationen im Derivatesegment und verschiedenen Aktivitäten im Nachhandelsbereich .

EN:  At the regular meeting on 22 September 2008 , the Supervisory Board dealt with strategic issues from the various business areas , such as the positioning of the cash market in competition with OTC trading platforms , innovation in the derivatives segment and various post ##AT##-##AT## trading activities .</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec158" class="calibre7"/>预处理数据</h2></div></div></div><p class="calibre10">按照练习文件中的说明下载训练数据(<code class="literal">train.en</code>和<code class="literal">train.de)</code>)后，让我们看看这些文件中有什么。<code class="literal">train.en</code>文件包含英语句子，而<code class="literal">train.de</code>包含相应的德语句子。接下来，我们将从作为数据的大型语料库中选择250，000个句子对。我们还将从训练数据中收集100个句子作为测试数据。最后，两种语言的词汇表可以在<code class="literal">vocab.50K.en.txt</code>和<code class="literal">vocab.50K.de.txt</code>中找到。</p><p class="calibre10">然后我们将按照本章前面的解释对这些数据进行预处理。反转句子对于单词嵌入学习是可选的(如果单独执行)，因为反转句子不会改变给定单词的上下文。我们将使用下面的简单分词算法将句子分词。本质上，我们在各种标点符号前引入空格，这样它们就可以标记为单个元素。然后对于任何在词汇表中找不到的单词，我们会用一个特殊的<code class="literal">&lt;unk&gt;</code>标记来替换它。<code class="literal">is_source</code>参数告诉我们是处理源句子(<code class="literal">is_source = True</code>)还是目标句子(<code class="literal">is_source = False</code>):</p><div><pre class="programlisting">def split_to_tokens(sent,is_source):
    '''
    This function takes in a sentence (source or target)
    and preprocess the sentency with various steps
    (e.g. removing punctuation)
    '''

    global src_unk_count, tgt_unk_count

    # Remove punctuation and new-line chars
    sent = sent.replace(',',' ,')
    sent = sent.replace('.',' .')
    sent = sent.replace('\n',' ') 
    
    sent_toks = sent.split(' ')
    for t_i, tok in enumerate(sent_toks):
        if is_source:
            # src_dictionary contain the word -&gt; 
            # word ID mapping for source vocabulary
            if tok not in src_dictionary.keys():
                if not len(tok.strip())==0:
                    sent_toks[t_i] = '&lt;unk&gt;'
                    src_unk_count += 1
        else:
            # tgt_dictionary contain the word -&gt; 
            # word ID mapping for target vocabulary
            if tok not in tgt_dictionary.keys():
                if not len(tok.strip())==0:
                    sent_toks[t_i] = '&lt;unk&gt;'
                    # print(tok)
                    tgt_unk_count += 1
    return sent_toks</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec159" class="calibre7"/>学习单词嵌入</h2></div></div></div><p class="calibre10">接下来，我们将<a id="id728"/>继续学习单词嵌入。为了学习单词嵌入，我们将使用<strong class="calibre11">连续单词袋</strong> ( <strong class="calibre11"> CBOW </strong>)模型。但是，欢迎你尝试其他单词嵌入学习方法，比如GloVe。我们不会浏览代码(在<code class="literal">word2vec.py</code>文件中找到)，但是分享一些学到的单词嵌入:</p><p class="calibre10"><em class="calibre15">德语单词嵌入</em></p><div><pre class="programlisting">Nearest to In: in, Aus, An, Neben, Bei, Mit, Trotz, Auf,
Nearest to war: ist, hat, scheint, wäre, hatte, bin, waren, kam,
Nearest to so: verbreitet, eigentlich, ausserdem, ziemlich, Rad-, zweierlei, wollten, ebenso,
Nearest to Schritte: Meter, Minuten, Gehminuten, Autominuten, km, Kilometer, Fahrminuten, Steinwurf,
Nearest to Sicht: Aussicht, Ausblick, Blick, Kombination, Milde, Erscheinung, Terroranschläge, Ebenen,</pre></div><p class="calibre10"><em class="calibre15">英语单词嵌入</em></p><div><pre class="programlisting">Nearest to more: cheaper, less, easier, better, further, greater, bigger, More,
Nearest to States: Kingdom, Nations, accross, attrition, Efex, Republic, authoritative, Sorbonne,
Nearest to Italy: Spain, Poland, France, Switzerland, Madrid, Portugal, Fuengirola, 51,
Nearest to island: shores, Principality, outskirts, islands, skyline, ear, continuation, capital,
Nearest to 2004: 2005, 2001, 2003, 2007, 1996, 2006, 1999, 1995,</pre></div><p class="calibre10">可以在训练机器翻译<a id="id729"/>系统的同时学习嵌入。另一种选择是使用预训练的单词嵌入。我们将在本章的后面讨论如何做到这一点。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec160" class="calibre7"/>定义编码器和解码器</h2></div></div></div><p class="calibre10">我们将使用两个<a id="id730"/>独立的LSTMs作为编码器，使用<a id="id731"/>作为解码器。</p><p class="calibre10">首先，我们将定义超参数:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">batch_size</code>:你<a id="id732"/>在设置批量的时候要非常小心。我们的NMT在运行时会占用大量内存。</li><li class="listitem">这是LSTM隐藏单位的数量。大的<code class="literal">num_nodes</code>超参数将导致更好的性能和更高的计算成本。</li><li class="listitem"><code class="literal">enc_num_unrollings</code>:我们将它设置为源句子中的字数。我们将在一次计算中展开整个句子的LSTM。<code class="literal">enc_num_unrollings</code>越高，你的模型表现越好。但是，这会降低算法的速度。</li><li class="listitem"><code class="literal">dec_num_unrollings</code>:设置为目标句子的字数。更高的<code class="literal">dec_num_unrollings</code>也将<a id="id734"/>导致更好的性能，但是计算成本较大。</li><li class="listitem"><code class="literal">embedding_size</code>:这个<a id="id735"/>就是我们学习的向量的维度。100-300的嵌入大小对于大多数使用单词向量的实际问题来说是足够的。</li></ul></div><p class="calibre10">这里我们将定义超参数:</p><div><pre class="programlisting"># We set the input size by loading the saved word embeddings
# and getting the column size
tgt_emb_mat = np.load('en-embeddings.npy')
input_size = tgt_emb_mat.shape[1]

num_nodes = 128
batch_size = 10

# We unroll the full length at one go
# both source and target sentences
enc_num_unrollings = 40
dec_num_unrollings = 60</pre></div><div><div><h3 class="title4"><a id="note38" class="calibre7"/>注意</h3><p class="calibre16">如果批量较大(在标准笔记本电脑上超过20个)，可能会遇到如下问题:</p><div><pre class="programlisting">Resource exhausted: OOM when allocating tensor with ...</pre></div><p class="calibre16">在这种情况下，您应该减少批处理大小并重新运行代码。</p></div></div><p class="calibre10">接下来，我们将定义LSTMs和softmax图层的权重和偏差。我们将使用编码器和解码器变量作用域，使变量的命名更加直观。这个<a id="id736"/>是标准的LSTM电池，我们不再<a id="id737"/>重复重量定义。</p><p class="calibre10">然后，我们将为培训定义四个张量流占位符:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">enc_train_inputs</code>:这是一个<code class="literal">enc_num_unrollings</code>占位符的列表，其中每个<a id="id738"/>占位符的大小为<code class="literal">[batch_size, input_size]</code>。这用于将一批源语言句子馈送给编码器。</li><li class="listitem"><code class="literal">dec_train_inputs</code>:这是<code class="literal">dec_num_unrollings</code>占位符的列表，其中<a id="id739"/>每个占位符都是<code class="literal">[batch_size, input_size]</code>大小。这用于馈送目标语言句子的相应批次。</li><li class="listitem"><code class="literal">dec_train_labels</code>: This is a list of the <code class="literal">dec_num_unrollings</code> placeholders, where each<a id="id740"/> placeholder is of the <code class="literal">[batch_size, vocabulary_size]</code> size. This contains words of the <code class="literal">dec_train_inputs</code> offset by 1. So that two placeholders from <code class="literal">dec_train_inputs</code> and <code class="literal">dec_train_labels</code> with the same index in the list would have the <em class="calibre15">i<sup class="calibre26">th</sup></em> word and the <div><img alt="Defining the encoder and the decoder" src="img/B08681_10_81.jpg" class="calibre12"/></div><p class="calibre10">词。</p></li><li class="listitem"><code class="literal">dec_train_masks</code>:与<code class="literal">dec_train_inputs</code>大小相同，屏蔽掉损失计算中任何带有<code class="literal">&lt;/s&gt;</code>标签的<a id="id741"/>元素。这很重要，因为有许多数据点带有<code class="literal">&lt;/s&gt;</code>标记，因为它用于将句子填充到固定长度:<div> <pre class="programlisting">for ui in range(dec_num_unrollings):     dec_train_inputs.append(tf.placeholder(tf.float32,         shape=[batch_size,input_size],         name='dec_train_inputs_%d'%ui))     dec_train_labels.append(tf.placeholder(tf.float32,         shape=[batch_size,vocabulary_size],         name = 'dec_train_labels_%d'%ui))     dec_train_masks.append(tf.placeholder(tf.float32,         shape=[batch_size,1],         name='dec_train_masks_%d'%ui))  for ui in range(enc_num_unrollings):     enc_train_inputs.append(tf.placeholder(tf.float32,         shape=[batch_size,input_size],         name='train_inputs_%d'%ui))</pre> </div></li></ul></div><div><div><h3 class="title4"><a id="note40" class="calibre7"/>注</h3><p class="calibre16">为了初始化LSTM细胞和softmax层的<a id="id742"/>权重，我们将使用<strong class="calibre11"> Xavier初始化</strong>，这是Glorot和Bengio在2010年他们的论文<em class="calibre15">中介绍的，理解训练深度前馈神经网络的困难</em>，<em class="calibre15">第13届国际人工智能和统计会议(2010) </em>。这是一种原则性的初始化技术，旨在缓解<a id="id743"/>超深网络中的消失梯度问题。这可以通过TensorFlow中提供的<code class="literal">tf.contrib.layers.xavier_initializer()</code>变量初始化器来实现。具体来说，在Xavier初始化中，神经网络<a id="id744"/>的第<em class="calibre15"> j <sup class="calibre26"> th </sup> </em>层的权重按照均匀分布<em class="calibre15">U【a，b】</em>进行初始化，其中<em class="calibre15"> a </em>为最小值，<em class="calibre15"> b </em>为最大值:</p><div><img alt="Defining the encoder and the decoder" src="img/B08681_10_82.jpg" class="calibre12"/></div><p class="calibre16">这里，<em class="calibre15">n<sub class="calibre17">j</sub>T27】是第<em class="calibre15">j<sup class="calibre26">th</sup>T31】层的大小。</em></em></p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec161" class="calibre7"/>定义端到端输出计算</h2></div></div></div><p class="calibre10">这里，定义了变量和输入/输出<a id="id745"/>占位符后，我们将继续定义从编码器到解码器的输出计算以及损失函数。</p><p class="calibre10">对于输出，我们将首先计算给定的一批句子中所有单词的LSTM单元状态和隐藏状态。这是通过运行一个<code class="literal">for</code>循环来实现的，其中在第<em class="calibre15"> i <sup class="calibre26"> th </sup> </em>迭代中，我们在<code class="literal">enc_train_inputs</code>中输入第<em class="calibre15"> i <sup class="calibre26"> th </sup> </em>占位符，以及来自<img alt="Defining the end-to-end output calculation" src="img/B08681_10_84.jpg" class="calibre27"/>迭代的单元格状态和输出隐藏状态。<code class="literal">enc_lstm_cell</code>函数的工作方式类似于我们在<a href="ch08.html" title="Chapter 8. Applications of LSTM – Generating Text">第8章</a>、<em class="calibre15">LSTM的应用-生成文本</em>和<a href="ch09.html" title="Chapter 9. Applications of LSTM – Image Caption Generation">第9章</a>、<em class="calibre15">LSTM的应用-图像字幕生成</em>中看到的<code class="literal">lstm_cell</code>函数:</p><div><pre class="programlisting"># Update the output and state of the encoder iteratively
for i in enc_train_inputs:
    output, state = enc_lstm_cell(i, output,state)</pre></div><pre>v to initialize the decoder states with. This is achieved with the <code class="literal">tf.control_dependencies(...)</code> statement. So the nested commands within the <code class="literal">with</code> statement will only execute after the encoder output is fully calculated:</pre><div><pre class="programlisting"># With the computations of the enc_lstm_cell done,
# calculate the output and state of the decoder
with tf.control_dependencies([saved_output.assign(output),
                             saved_state.assign(state)]):
    # Calculate the decoder state and output iteratively
    for i in dec_train_inputs:
        output, state = dec_lstm_cell(i, output, state)
        outputs.append(output)</pre></div><p class="calibre10">然后，在计算解码器输出之后，我们将使用LSTM的隐藏状态作为层的输入来计算softmax层的logits:</p><div><pre class="programlisting"># Calculate the logits of the decoder for all unrolled steps
logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b</pre></div><p class="calibre10">现在，计算出逻辑值后，我们就可以计算损失了。请注意，我们使用mask来屏蔽掉不应该造成损失的元素(即，我们添加的用于构成固定长度句子的<code class="literal">&lt;/s&gt;</code>元素):</p><div><pre class="programlisting">loss_batch = tf.concat(axis=0,values=dec_train_masks)*
             tf.nn.softmax_cross_entropy_with_logits_v2(
                 logits=logits, labels=tf.concat(axis=0,
                 values=dec_train_labels))
loss = tf.reduce_mean(loss_batch)</pre></div><p class="calibre10">此后，与前几章不同，我们将使用两个优化器:Adam和标准随机梯度下降。这是因为长期使用Adam会产生不希望的结果(例如，BLEU分数突然大幅波动)。我们也使用渐变剪辑来避免任何渐变爆炸。</p><div><pre class="programlisting"># We use two optimizers: Adam and naive SGD
# using Adam in the long run produced undesirable results 
# (e.g.) sudden fluctuations in BLEU
# Therefore we use Adam to get a good starting point for optimizing
# and then switch to SGD from that point onwards
with tf.variable_scope('Adam'):
    optimizer = tf.train.AdamOptimizer(learning_rate)
with tf.variable_scope('SGD'):
    sgd_optimizer = tf.train.GradientDescentOptimizer(sgd_learning_rate)

# Calculates gradients with clipping for Adam
gradients, v = zip(*optimizer.compute_gradients(loss))
gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
optimize = optimizer.apply_gradients(zip(gradients, v))

# Calculates gradients with clipping for SGD
sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))
sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 5.0)
sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))</pre></div><p class="calibre10">我们将使用以下语句，通过确保所有可训练变量都存在梯度，来确保梯度正确地从解码器流向编码器:</p><div><pre class="programlisting">for (g_i,v_i) in zip(gradients,v):
    assert g_i is not None, 'Gradient none for %s'%(v_i.name)</pre></div><p class="calibre10">请注意，运行NMT将比之前的练习慢得多，在单个GPU上完全运行可能需要<a id="id747"/>12个多小时。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec162" class="calibre7"/>一些翻译结果</h2></div></div></div><p class="calibre10">这些是我们经过10，000步后得到的结果:</p><div><pre class="programlisting">
<strong class="calibre11">DE:  &amp;#124; Ferienwohnungen 1 Zi &amp;#124; Ferienhäuser &amp;#124; Landhäuser &amp;#124; Autovermietung &amp;#124; Last Minute Angebote ! !</strong>

EN (TRUE):&amp;#124; 1 Bedroom Apts &amp;#124; Holiday houses &amp;#124; Rural Homes &amp;#124; Car Rental &amp;#124; Last Minute Offers !

EN (Predicted): Casino Tropez &amp;#124; Club &amp;#124; Club &amp;#124; Aparthotels Hotels &amp;#124; Club &amp;#124; Last Minute Offers &amp;#124; Last Minute Offers &amp;#124; Last Minute Offers &amp;#124; Last Minute Offers &amp;#124; Last Minute Offers ! &lt;/s&gt;

<strong class="calibre11">DE: Wie hilfreich finden Sie die Demo ##AT##-##AT## CD ?</strong>

EN (TRUE): How helpful do you find the demo CD ##AT##-##AT## ROM ?

EN (Predicted): How to install the new version of XLSTAT ? &lt;/s&gt;

<strong class="calibre11">DE:  Das „ Ladino di Fassa “ ist jedoch mehr als ein Dialekt – es ist eine richtige Sprache .</strong>

EN (TRUE):This is Ladin from Fassa which is more than a dialect : it is a language in its own right .

EN (Predicted): The &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;

<strong class="calibre11">DE: In der Hotelbeschreibung im Internet müßte die Zufahrt beschrieben</strong>
<strong class="calibre11">werden .</strong>
EN (TRUE): There are no adverse comments about this hotel at all .

EN (Predicted): The &lt;unk&gt; &lt;unk&gt; is a bit of the &lt;unk&gt; &lt;unk&gt; . &lt;/s&gt;</pre></div><p class="calibre10">我们可以看到第一句话是相当认可的。然而，第二句翻译得很差。</p><p class="calibre10">此外，以下是100，000步后获得的结果:</p><div><pre class="programlisting">
<strong class="calibre11">DE: Das Hotel Opera befindet sich in der Nähe des Royal Theatre ,</strong>
<strong class="calibre11">Kongens Nytorv , &amp;apos; Stroget &amp;apos; und Nyhavn .</strong>

EN (TRUE): Hotel Opera is situated near The Royal Theatre , Kongens
Nytorv , &amp;quot; Strøget &amp;quot; and fascinating Nyhavn .

EN (Predicted): Best Western Hotel &lt;unk&gt; &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,
&lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt;
, &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,
&lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,

<strong class="calibre11">DE:  Alle älteren Kinder oder Erwachsene zahlen EUR 32,00 pro Übernachtung und Person für Zustellbetten .</strong>

EN (TRUE):All older children or adults are charged EUR 32.00 per night and person for extra beds .

EN (Predicted): All older children or adults are charged EUR 15 &lt;unk&gt; per night and person for extra beds . &lt;/s&gt;

<strong class="calibre11">DE:  Im Allgemeinen basieren sie auf Datenbanken , Templates und Skripts .</strong>

EN (TRUE):In general they are based on databases , template and scripts .

EN (Predicted): The user is the most important software of the software . &lt;/s&gt;

<strong class="calibre11">DE: Tux Racer wird Ihnen helfen , die Zeit totzuschlagen und sie</strong>
<strong class="calibre11">können OpenOffice zum Arbeiten verwenden .</strong>

EN (TRUE): Tux Racer will help you pass the time while you wait ,
and you can use OpenOffice for work .

EN (Predicted): &lt;unk&gt; .com we have a very friendly and helpful
staff . &lt;/s&gt;</pre></div><p class="calibre10">我们可以看到，即使<a id="id749"/>的翻译并不完美，但大多数时候还是抓住了源句子的上下文，我们的NMT非常擅长生成语法正确的句子。</p><p class="calibre10"><em class="calibre15">图10.12 </em>描绘了一段时间内NMT的BLEU分数。随着时间的推移，训练和测试数据集的BLEU分数都有明显增加:</p><div><img alt="Some translation results" src="img/B08681_10_85.jpg" class="calibre12"/><div><p class="calibre10">图10.12:一段时间内NMT的BLEU分数</p></div></div></div></div>



  
    <title>Training an NMT jointly with word embeddings</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec79" class="calibre7"/>用词嵌入联合训练NMT</h1></div></div></div><p class="calibre10">在这里，我们将<a id="id750"/>讨论如何通过单词嵌入来训练NMT。在本节中，我们将讨论两个概念:</p><div><ul class="itemizedlist"><li class="listitem">与单词嵌入层联合训练NMT</li><li class="listitem">使用预先训练的嵌入，而不是随机初始化嵌入层</li></ul></div><p class="calibre10">有几个可用的多语言单词嵌入库:</p><div><ul class="itemizedlist"><li class="listitem">脸书的fast text:<a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">https://github . com/Facebook research/fast text/blob/master/pre trained-vectors . MD</a></li><li class="listitem">CMU多语种嵌入:<a href="http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html">http://www . cs . CMU . edu/~ AFM/projects/multilingual _ embeddings . html</a></li></ul></div><p class="calibre10">由此，我们<a id="id751"/>将使用CMU嵌入(~200 MB)，因为它比fastText (~5 GB)小得多。我们首先需要下载德语(<code class="literal">multilingual_embeddings.de</code>)和英语(<code class="literal">multilingual_embeddings.en</code>)嵌入。这可以在<code class="literal">ch10</code>文件夹的<code class="literal">nmt_with_pretrained_wordvecs.ipynb</code>中作为练习。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec163" class="calibre7"/>最大化数据集词汇和预训练嵌入之间的匹配</h2></div></div></div><p class="calibre10">我们将首先<a id="id752"/>必须获得与我们感兴趣解决的问题相关的预训练单词嵌入的子集。这很重要，因为预训练单词嵌入的词汇表可能很大，并且可能包含许多在数据集词汇表中找不到的单词。预训练的单词嵌入是一组行，其中行是单词和单词向量，由空格分隔。预训练嵌入的示例行可能如下所示:</p><div><pre class="programlisting">door 0.283259492301 0.198089365764 0.335635845187 -0.385702777914 0.491404970211 …</pre></div><p class="calibre10">实现这一点的一个显而易见且简单的方法是逐行遍历预训练的数据集词汇表，如果当前行中的单词与数据集词汇表中的任何单词匹配，我们将保存该单词嵌入以供将来使用。然而，这将是非常低效的，因为通常词汇表倾向于偏向于由创作者做出的各种设计决策。例如，有些人可能认为<em class="calibre15">猫的</em>、<em class="calibre15">猫的</em>和<em class="calibre15">猫的</em>是同一个单词，而其他人可能认为它们是不同的单词。如果我们天真地匹配预训练的单词嵌入词汇表和数据集词汇表，我们可能会错过许多单词。因此，我们将使用下面的逻辑来确保我们从预训练的单词向量中获得最多。</p><p class="calibre10">首先，我们将定义两个NumPy数组来保存源语言和目标语言的相关单词嵌入:</p><div><pre class="programlisting">de_embeddings = np.random.uniform(size=(vocabulary_size, embeddings_size),low=-1.0, high=1.0)
en_embeddings = np.random.uniform(size=(vocabulary_size, embeddings_size),low=-1.0, high=1.0)</pre></div><p class="calibre10">然后，我们将打开包含单词向量的文本文件，如下所示。<code class="literal">filename</code>参数是德语的<code class="literal">multilingual_embeddings.de</code>和英语的<code class="literal">miltilingual_embeddings.en</code>:</p><div><pre class="programlisting">with open(filename,'r',encoding='utf-8') as f:</pre></div><p class="calibre10">接下来，我们将通过用空格分割行来分离单词和单词向量:</p><div><pre class="programlisting">        line_tokens = line.split(' ')
        lword = line_tokens[0]
        vector = [float(v) for v in line_tokens[1:]]</pre></div><p class="calibre10">我们还将<a id="id753"/>忽略单词是否为空(即只有空格、制表符或换行符):</p><div><pre class="programlisting">        if len(lword.strip())==0:
            continue</pre></div><p class="calibre10">我们还将去掉单词中的任何重音符号(尤其是德语单词),以确保我们最有可能找到匹配的单词:</p><div><pre class="programlisting">        lword = unidecode.unidecode(lword)</pre></div><p class="calibre10">此后，我们将使用以下逻辑来检查匹配。我们将编写一组级联条件来检查源语言和目标语言的匹配:</p><div><ol class="orderedlist"><li class="listitem1">首先检查来自预训练嵌入(<code class="literal">lword</code>)的单词是否在数据集词汇表中</li><li class="listitem1">如果没有，检查首字母是否大写(即<em class="calibre15"> cat </em>变成<em class="calibre15"> Cat </em>)，如果在数据集词汇表中找到的话</li><li class="listitem1">如果不是，通过从数据集词汇单词中移除特殊字符(例如，重音)，检查来自预训练嵌入(<code class="literal">lword</code>)的单词是否与任何单词结果相似</li></ol></div><p class="calibre10">如果满足其中一个条件，我们将获得单词嵌入向量，并将其分配给由该单词的ID索引的行(<em class="calibre15">单词</em> <em class="calibre15"> → </em> <em class="calibre15"> ID </em>)映射存储在两种语言的<code class="literal">src_dictionary</code>和<code class="literal">tgt_dictionary</code>中。我们将对两种语言都这样做:</p><div><pre class="programlisting">            # Update the randomly initialized
            # matrix for the embeddings
            # Update the number of words
            # matched with pretrained embeddings
            try:
                dword = dictionary[lword]
                words_found_ids.append(dictionary[lword])
                embeddings[dictionary[lword],:] = vector
                words_found += 1
            
            # If a given word is not found in our vocabulary,
            except KeyError:
                try:
                    # First try to match the same
                    # with first letter capitalized
                    # capitalized
                    if len(lword)&gt;0:
                        firt_letter_cap = lword[0].upper()+lword[1:]

                    else:
                        continue
                        
                    # Update the word embeddings matrix
                    dword = dictionary[firt_letter_cap]
                    words_found_ids.append(dictionary[firt_letter_cap])
                    embeddings[dictionary[firt_letter_cap],:] = vector
                    words_found += 1
                
                except KeyError:
                    # If not found try to match the word with
                    # the unaccented word
                    try:
                        dword = unaccented_dict[lword]
                        words_found_ids.append(dictionary[lword])
                        embeddings[dictionary[lword],:] = vector
                        words_found += 1
                    except KeyError:

                        continue</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec164" class="calibre7"/>将嵌入层定义为张量流变量</h2></div></div></div><p class="calibre10">我们将<a id="id754"/>为嵌入层定义两个可训练的张量流变量(即<code class="literal">tgt_word_embeddings</code>和<code class="literal">src_word_embeddings</code>，如下所示:</p><div><pre class="programlisting">tgt_word_embeddings = tf.get_variable(
    'target_embeddings',shape=[vocabulary_size,
        embeddings_size],
    dtype=tf.float32, initializer = tf.constant_initializer(
        en_embeddings)
)
src_word_embeddings = tf.get_variable(
    'source_embeddings',shape=[vocabulary_size,
        embeddings_size], 
    dtype=tf.float32, initializer = tf.constant_initializer(
        de_embeddings)
)</pre></div><p class="calibre10">然后我们首先将<code class="literal">dec_train_inputs</code>和<code class="literal">enc_train_inputs</code>中占位符的维度改为<code class="literal">[batch_size]</code>，数据类型改为<code class="literal">tf.int32</code>。这样我们可以使用它们为每个展开的输入执行嵌入查找(<code class="literal">tf.nn.embedding_lookup(...)</code>)，如下所示:</p><div><pre class="programlisting"># Defining unrolled training inputs as well as embedding lookup (Encoder)
for ui in range(enc_num_unrollings):
    enc_train_inputs.append(tf.placeholder(tf.int32,
                            shape=[batch_size],
                            name='train_inputs_%d'%ui))
    enc_train_input_embeds.append(tf.nn.embedding_lookup(
                                  src_word_embeddings,
                                  enc_train_inputs[ui]))

# Defining unrolled training inputs, embeddings,
# outputs, and masks (Decoder)
for ui in range(dec_num_unrollings):     dec_train_inputs.append(tf.placeholder(tf.int32,
                            shape=[batch_size],
                            name='dec_train_inputs_%d'%ui))
    dec_train_input_embeds.append(tf.nn.embedding_lookup(
                                  tgt_word_embeddings,
                                  dec_train_inputs[ui]))
    dec_train_labels.append(tf.placeholder(tf.float32,
                            shape=[batch_size,vocabulary_size],
                            name = 'dec_train_labels_%d'%ui))
    dec_train_masks.append(tf.placeholder(tf.float32,
                           shape=[batch_size,1],
                           name='dec_train_masks_%d'%ui))</pre></div><p class="calibre10">然后，编码器和解码器的LSTM单元<a id="id755"/>计算发生变化，如本部分所示，我们首先使用源语句输入计算编码器LSTM单元输出。接下来，通过使用来自编码器的最终状态信息作为解码器的初始化状态(即使用<code class="literal">tf.control_dependencies(...)</code>)，我们计算解码器输出以及softmax逻辑和预测:</p><div><pre class="programlisting"># Update the output and state of the encoder iteratively
for i in enc_train_inputs:
    output, state = enc_lstm_cell(i, output,state)


print('Calculating Decoder Output')
# With the computations of the enc_lstm_cell done,
# calculate the output and state of the decoder
with tf.control_dependencies([saved_output.assign(output),
                             saved_state.assign(state)]):
    # Calculate the decoder state and output iteratively
    for i in dec_train_inputs:
        output, state = dec_lstm_cell(i, output, state)
        outputs.append(output)</pre></div><p class="calibre10">请注意，练习文件的输出计算与这里显示的略有不同。我们没有将前面的预测作为输入，而是将真实的单词作为输入。这往往比之前预测中的feeding提供更好的性能，将在下一节中详细讨论。然而，总的想法是一样的。</p><p class="calibre10">最后的步骤包括，计算解码器的损耗，并定义一个优化器来优化模型参数，如我们之前所见。</p><p class="calibre10">最后，我们概述了实现我们的NMT的计算图。在这里，我们将模型的计算图可视化。</p><div><img alt="Defining the embeddings layer as a TensorFlow variable" src="img/B08681_10_112.jpg" class="calibre12"/><div><p class="calibre10">图10.13:具有预训练嵌入的NMT系统的计算图</p></div></div></div></div>



  
    <title>Improving NMTs</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec80" class="calibre7"/>改进NMTs</h1></div></div></div><p class="calibre10">从前面的结果可以看出，我们的翻译模型表现得并不理想。这些结果是通过在单个NVIDIA 1080 Ti GPU上运行优化超过12小时获得的。还要注意，这甚至不是完整的数据集，我们只使用了250，000个句子对进行训练。然而，如果你在使用<strong class="calibre11">谷歌神经机器翻译</strong> ( <strong class="calibre11"> GNMT </strong>)系统的<a id="id757"/>谷歌翻译中输入一些东西，翻译几乎总是看起来非常逼真，只有一些小错误。因此，了解我们如何改进模型以使其产生更好的结果是很重要的。在这一节中，我们将讨论改善高考的几种方法，如教师强制、深层LSTMs和注意机制。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec165" class="calibre7"/>老师逼死</h2></div></div></div><p class="calibre10">正如我们在<em class="calibre15">训练NMT </em>一节中所讨论的，我们通过以下方式来训练NMT:</p><div><ul class="itemizedlist"><li class="listitem">首先，我们输入完整的编码器语句，以获得编码器的最终状态输出</li><li class="listitem">然后，我们将编码器的最终状态设置为解码器的初始状态</li><li class="listitem">我们还要求解码器在没有任何附加信息的情况下预测完整的目标句子，除了编码器的最后状态输出</li></ul></div><p class="calibre10">对于模型来说，这可能是一项非常困难的任务。我们可以这样理解这个现象。比方说，一位老师让一名幼儿园学生在给出第一个单词的情况下完成下面的句子:</p><p class="calibre10"><em class="calibre15">我_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</em></p><p class="calibre10">这意味着<a id="id758"/>孩子需要选择一个主题；动词；和一个对象，知道语言的语法，了解语言的语法规则等等。因此，孩子产生不正确句子的倾向很高。</p><p class="calibre10">然而，如果我们让孩子一个字一个字地说出来，他们可能会在造句方面做得更好。换句话说，我们要求孩子说出下面的下一个单词:</p><p class="calibre10"><em class="calibre15">我____ </em></p><p class="calibre10">然后，我们要求他们填写给定的空白:</p><p class="calibre10"><em class="calibre15">我喜欢____ </em></p><p class="calibre10">并以同样的方式继续:</p><p class="calibre10">我喜欢__，我喜欢飞，我喜欢放风筝</p><p class="calibre10">这样，<a id="id759"/>孩子就能更好地写出正确而有意义的句子。这种现象被称为<strong class="calibre11">老师逼</strong>。我们可以采用同样的方法来减轻翻译任务的难度，如图<em class="calibre15">图10.13 </em>所示:</p><div><img alt="Teacher forcing" src="img/B08681_10_86.jpg" class="calibre12"/><div><p class="calibre10">图10.14:教师强制机制。输入中较暗的箭头表示新引入的解码器输入连接。右边的图显示了解码器LSTM单元是如何变化的。</p></div></div><p class="calibre10">如图中的粗体所示，解码器的输入已经被训练数据中的实际目标单词替换。因此，NMT解码器不再需要承担在给定源句子的情况下预测整个目标句子的负担。相反，给定前一个字，解码器只需正确预测当前字。值得注意的是，在之前的讨论中，我们讨论了培训程序，但没有任何关于教师强制的细节。然而，我们实际上在本章的所有练习中使用了教师强制。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec166" class="calibre7"/>深层LSTMs</h2></div></div></div><p class="calibre10">我们可以做的一个明显的<a id="id760"/>改进是通过堆叠LSTM来增加层数，从而创建一个<em class="calibre15">深LSTM </em>(参见<em class="calibre15">图10.14 </em>)。例如，谷歌系统使用八个相互堆叠的层(<em class="calibre15">谷歌的神经机器翻译系统:弥合人类与机器翻译之间的鸿沟</em>、<em class="calibre15">吴等人</em>、<em class="calibre15">技术报告(2016) </em>)。虽然这妨碍了计算效率，但拥有更多层可以极大地提高神经网络学习两种语言的语法和其他语言特征的能力。</p><div><img alt="Deep LSTMs" src="img/B08681_10_87.jpg" class="calibre12"/><div><p class="calibre10">图10.15:深LSTM的图示</p></div></div></div></div>



  
    <title>Attention</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec81" class="calibre7"/>注意</h1></div></div></div><p class="calibre10">注意力是机器翻译的关键突破之一，它带来了更好的NMT系统。注意允许解码器访问编码器的完整状态历史，导致在翻译时创建源句子的更丰富的表示。在深入研究注意力机制的细节之前，让我们先了解一下当前NMT系统中的一个关键瓶颈，以及注意力在处理这个问题时的好处。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec167" class="calibre7"/>打破语境向量瓶颈</h2></div></div></div><p class="calibre10">正如你可能已经猜到的那样，瓶颈是位于编码器和解码器之间的上下文向量或思想向量(参见图10.15)。</p><div><img alt="Breaking the context vector bottleneck" src="img/B08681_10_88.jpg" class="calibre12"/><div><p class="calibre10">图10.16:编码器-解码器架构</p></div></div><p class="calibre10">为了理解为什么这是一个瓶颈，让我们想象翻译下面的英语句子:</p><p class="calibre10">我去花市买了些花</p><p class="calibre10">这转化为以下内容:</p><p class="calibre10">我想买一个，嗯，我想买一个</p><p class="calibre10">如果我们要将它压缩成一个固定长度的向量，得到的向量需要包含以下内容:</p><div><ul class="itemizedlist"><li class="listitem">关于主题的信息(<em class="calibre15"> I </em></li><li class="listitem">关于动词的信息(<em class="calibre15">买</em>和<em class="calibre15">去</em></li><li class="listitem">对象信息(<em class="calibre15">花卉</em>和<em class="calibre15">花卉市场</em>)</li><li class="listitem">句子中主语、动词和宾语的相互作用</li></ul></div><p class="calibre10">通常，上下文向量具有128或256个元素的大小。这对于系统来说是非常不切实际且极其困难的要求。因此，在大多数情况下，语境向量无法提供进行良好翻译所需的完整信息。这导致解码器性能不佳，无法最佳翻译句子。</p><p class="calibre10">此外，在解码期间，仅在开始时观察上下文向量。此后，解码器LSTM必须记住上下文向量，直到翻译结束。尽管LSTMs擅长长时记忆，但实际上它们是有限的。这将严重影响结果，尤其是长句子。</p><p class="calibre10">这就是注意力派上用场的地方。利用注意机制，解码器将能够访问每个解码时间步长的编码器的完整状态历史。这使得解码器能够访问源句子的非常丰富的表示。此外，<a id="id763"/>注意机制引入了一个softmax层，允许解码器计算过去观察到的编码器状态的加权平均值，该平均值将用作解码器的上下文向量。这允许解码器在不同的解码步骤中对不同的单词投入不同的注意力。</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec168" class="calibre7"/>详细的注意机制</h2></div></div></div><p class="calibre10">现在让我们详细调查一下<a id="id764"/>实际实施的注意机制。我们将使用论文中详述的注意力机制，<em class="calibre15">神经机器翻译通过学习来联合对齐和翻译</em>、<em class="calibre15">巴赫达瑙</em>、<em class="calibre15">乔</em>、<em class="calibre15">本吉奥</em>、<em class="calibre15"> arXiv:1409.0473 (2014) </em>。为了与本文保持一致，我们将使用以下符号:</p><div><ul class="itemizedlist"><li class="listitem">编码器的隐藏状态:<em class="calibre15">h<sub class="calibre17">I</sub>T16】</em></li><li class="listitem">目标句单词:<em class="calibre15">y<sub class="calibre17">I</sub>T20】</em></li><li class="listitem">解码器的隐藏状态:<em class="calibre15">s<sub class="calibre17">I</sub>T24】</em></li><li class="listitem">上下文向量:<em class="calibre15"> c <sub class="calibre17"> i </sub> i </em></li></ul></div><p class="calibre10">到目前为止，我们的解码器LSTM由一个输入<em class="calibre15"> y <sub class="calibre17"> i </sub>和一个隐藏状态<img alt="The attention mechanism in detail" src="img/B08681_10_89.jpg" class="calibre27"/>组成。我们将忽略单元状态，因为这是LSTM的内部部分。这可以表示如下:</em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_90.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> f </em>表示用于计算<em class="calibre15">y<sub class="calibre17">I+1</sub>T5】和<em class="calibre15">s<sub class="calibre17">I</sub>T9】的实际更新规则。利用注意机制，我们为第<em class="calibre15">I<sup class="calibre26">th</sup>T17】解码步骤引入了新的时间相关上下文向量<em class="calibre15">c<sub class="calibre17">I</sub>T13】。<em class="calibre15">c<sub class="calibre17">I</sub>T21矢量是所有展开的编码器步骤的隐藏状态的加权平均值。如果第<em class="calibre15">j<sup class="calibre26">th</sup>th</em>字对于翻译目标语言中的第<em class="calibre15">I<sup class="calibre26">th</sup>th</em>字更重要，则将给予编码器的第th</em>隐藏状态更高的权重。现在解码器LSTM变成了这样:</em></em></em></em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_91.jpg" class="calibre12"/></div><p class="calibre10">从概念上讲，注意力机制可以被认为是一个独立的层，如图<em class="calibre15">图10.16 </em>所示。如图所示，注意力作为一个层起作用。关注层负责为解码过程的第<em class="calibre15"> i <sup class="calibre26"> th </sup> </em>时间步产生<em class="calibre15">c<sub class="calibre17">I</sub>T39】；</em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_92.jpg" class="calibre12"/><div><p class="calibre10">图10.17:NMT的概念性注意机制</p></div></div><p class="calibre10">现在让我们看看如何计算<em class="calibre15">c<sub class="calibre17">I</sub>T47】:</em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_93.jpg" class="calibre12"/></div><p class="calibre10">这里，<em class="calibre15"> L </em>是源句子中的字数，<img alt="The attention mechanism in detail" src="img/B08681_10_94.jpg" class="calibre27"/>是代表用于计算第<em class="calibre15"> i <sup class="calibre26">第</sup>T59】解码器预测的第<em class="calibre15"> j <sup class="calibre26">第</sup>T55】编码器隐藏状态的重要性的<a id="id765"/>归一化权重。这是使用softmax层计算的。<em class="calibre15"> L </em>是编码器句子的长度:</em></em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_95.jpg" class="calibre12"/></div><p class="calibre10">这里，<img alt="The attention mechanism in detail" src="img/B08681_10_96.jpg" class="calibre27"/>是<em class="calibre15">能量</em>或<em class="calibre15">重要性</em>衡量编码器的<em class="calibre15">j<sup class="calibre26">th</sup>T70】隐藏状态和前一个解码器状态<img alt="The attention mechanism in detail" src="img/B08681_10_97.jpg" class="calibre27"/>对计算<em class="calibre15">s<sub class="calibre17">I</sub>T75】的贡献:</em></em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_98.jpg" class="calibre12"/></div><p class="calibre10">这实质上意味着<img alt="The attention mechanism in detail" src="img/B08681_10_99.jpg" class="calibre27"/>是用多层感知器计算的，其权重为<em class="calibre15">v<sub class="calibre17">a</sub>T13】、<em class="calibre15">W<sub class="calibre17">a</sub>T17】、<em class="calibre15">U<sub class="calibre17">a</sub>T21，<img alt="The attention mechanism in detail" src="img/B08681_10_100.jpg" class="calibre27"/>和<em class="calibre15">h<sub class="calibre17">j</sub>T26】是网络的输入。注意机构如图<em class="calibre15">图10.17 </em>所示:</em></em></em></em></p><div><img alt="The attention mechanism in detail" src="img/B08681_10_101.jpg" class="calibre12"/><div><p class="calibre10">图10.18:注意机制</p></div></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec51" class="calibre7"/>实施注意机制</h3></div></div></div><p class="calibre10">这里我们将讨论如何实现注意力机制。该系统将经历的两个主要变化如下:</p><div><ul class="itemizedlist"><li class="listitem">将引入更多的参数(即权重)(用于计算注意力并将注意力用作解码器LSTM单元的输入)</li><li class="listitem">将引入一个新的注意力相关计算函数(即<code class="literal">attn_layer</code>)</li><li class="listitem">更改解码器LSTM单元计算，将所有编码器LSTM单元输出的注意力加权和作为输入</li></ul></div><p class="calibre10">我们将只讨论与标准NMT模型相比引入的额外内容。你可以在<code class="literal">neural_machine_translation_attention.ipynb</code>中找到NMT的完整练习。</p></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec52" class="calibre7"/>定义权重</h3></div></div></div><p class="calibre10">将引入三组新的<a id="id767"/>权重来实现注意机制。所有这些权重都用于计算我们之前讨论的<em class="calibre15">能量</em>项(即<img alt="Defining weights" src="img/B08681_10_102.jpg" class="calibre27"/>):</p><div><pre class="programlisting">    W_a = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],
        stddev=0.05),name='W_a')
    U_a = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],
        stddev=0.05),name='U_a')
    v_a = tf.Variable(tf.truncated_normal([num_nodes,1],
        stddev=0.05),name='v_a')</pre></div><p class="calibre10">此外，我们将定义一组新的权重，用于将<em class="calibre15">c</em>T38】I作为解码器展开的第<em class="calibre15"> i <sup class="calibre26">个</sup> </em>步骤的输入:</p><div><pre class="programlisting">    dec_ic = tf.get_variable('ic',shape=[num_nodes, num_nodes],
        initializer = tf.contrib.layers.xavier_initializer())
    dec_fc = tf.get_variable('fc',shape=[num_nodes, num_nodes],
        initializer = tf.contrib.layers.xavier_initializer())
    dec_cc = tf.get_variable('cc',shape=[num_nodes, num_nodes],
        initializer = tf.contrib.layers.xavier_initializer())
    dec_oc = tf.get_variable('oc',shape=[num_nodes, num_nodes],
        initializer = tf.contrib.layers.xavier_initializer())</pre></div></div><div><div><div><div><h3 class="title5"><a id="ch10lvl3sec53" class="calibre7"/>计算注意力</h3></div></div></div><p class="calibre10">为了计算编码器和解码器的每个位置的<a id="id768"/>注意值，我们将定义一个函数来为我们做这些，<code class="literal">attn_layer(...)</code>。该方法为解码器的单个展开步骤计算编码器的所有位置(即<code class="literal">num_enc_unrollings</code>)的注意力。<code class="literal">attn_layer(...)</code>方法将两个参数作为函数的参数:</p><div><pre class="programlisting">attn_layer(h_j_unrolled, s_i_minus_1)</pre></div><p class="calibre10">这些参数如下:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">h_i_unrolled</code>:这些是<code class="literal">num_enc_unrolling</code>编码器LSTM单元输出，我们在将源句子输入编码器时计算了这些输出。这将是一个<code class="literal">num_enc_unrolling</code>张量的列表，其中每个张量的大小都是<code class="literal">[batch_size, num_nodes]</code>的。</li><li class="listitem"><code class="literal">s_i_minus_1</code>:前一个解码器的LSTM单元输出。这将是一个<code class="literal">[batch_size, num_nodes]</code>大小的张量。</li></ul></div><p class="calibre10">首先我们将<a id="id769"/>用<code class="literal">[num_enc_unrollings * batch_size, num_nodes]</code>大小的展开编码器输出列表创建一个张量:</p><div><pre class="programlisting">    enc_logits = tf.concat(axis=0,values=h_j_unrolled)</pre></div><p class="calibre10">然后我们用下面的运算来计算<img alt="Computing attention" src="img/B08681_10_103.jpg" class="calibre27"/>:</p><div><pre class="programlisting">    # of size [enc_num_unroll x batch_size, num_nodes]
    w_a_mul_s_i_minus_1 = tf.matmul(enc_outputs,W_a)</pre></div><p class="calibre10">接下来我们就来算一算<img alt="Computing attention" src="img/B08681_10_104.jpg" class="calibre27"/>:</p><div><pre class="programlisting">    # of size [enc_num_unroll x batch_size, num_nodes]
    u_a_mul_h_j = tf.matmul(tf.tile(s_i_minus_1,[enc_num_unrollings,1]), U_a)</pre></div><p class="calibre10">现在我们将<em class="calibre15">能量</em>计算为<img alt="Computing attention" src="img/B08681_10_105.jpg" class="calibre27"/>。这是一个<code class="literal">[enc_num_unroll * batch_size ,1]</code>大小的张量:</p><div><pre class="programlisting">    e_j = tf.matmul(tf.nn.tanh(w_a_mul_s_i_minus_1 +
        u_a_mul_h_j),v_a)</pre></div><p class="calibre10">我们现在可以首先将大的<code class="literal">e_j</code>分解为带有<code class="literal">tf.split(...)</code>的<code class="literal">enc_num_unrolling</code>长张量列表，其中每个张量的大小为<code class="literal">[batch_size, 1]</code>。此后，我们沿着轴1连接这个列表以产生一个<code class="literal">[batch_size, enc_num_unrollings]</code>大小的张量(即<code class="literal">reshaped_e_j</code>)。因此，单行<code class="literal">reshaped_e_j</code>将对应于编码器展开的时间步长的所有位置的注意值:</p><div><pre class="programlisting">    # list of enc_num_unroll elements, each 
    # element [batch_size, 1]
    batched_e_j = tf.split(axis=0,
        num_or_size_splits=enc_num_unrollings,value=e_j) 
    # of size [batch_size, enc_num_unroll]
    reshaped_e_j = tf.concat(axis=1,values=batched_e_j) </pre></div><p class="calibre10">我们现在可以很容易地计算出<code class="literal">reshaped_e_j</code>的标准化关注值。这些值将在展开的时间步长内标准化(第1轴<code class="literal">reshaped_e_j</code>):</p><div><pre class="programlisting">    # of size [batch_size, enc_num_unroll]
    alpha_i = tf.nn.softmax(reshaped_e_j) </pre></div><p class="calibre10">接下来是将<code class="literal">alpha_i</code>分解成一个<code class="literal">enc_num_unroll</code>张量列表，每个<code class="literal">[batch_size,1]</code>大小为:</p><div><pre class="programlisting">    alpha_i_list = tf.unstack(alpha_i,axis=1)</pre></div><p class="calibre10">之后，我们<a id="id770"/>将计算每个编码器输出(即<code class="literal">h_j_unrolled</code>)的加权和，并将其分配给<code class="literal">c_i</code>，这将被用作解码器LSTM单元的第<em class="calibre15"> i <sup class="calibre26"> th </sup> </em>展开时间步长的输入:</p><div><pre class="programlisting">    c_i_list =  [tf.reshape(alpha_i_list[e_i],
        [-1,1])*h_j_unrolled[e_i] for e_i in range(enc_num_unrollings)]
    c_i = tf.add_n(c_i_list) # of size [batch_size, num_nodes]</pre></div><p class="calibre10">然后，将<code class="literal">c_i</code>作为展开解码器LSTM单元的第<em class="calibre15"> i <sup class="calibre26">个</sup>T33】步骤的输入，解码器LSTM单元计算变化如下:</em></p><div><pre class="programlisting"># Definition of the cell computation (Decoder)
def dec_lstm_cell(i, o, state, c):
    """Create a LSTM cell"""
    input_gate = tf.sigmoid(tf.matmul(i, dec_ix) + tf.matmul(o, dec_im) +
                 tf.matmul(c, dec_ic) + dec_ib)
    forget_gate = tf.sigmoid(tf.matmul(i, dec_fx) + tf.matmul(o, dec_fm) +
                  tf.matmul(c, dec_fc) + dec_fb)
    update = tf.matmul(i, dec_cx) + tf.matmul(o, dec_cm) +
             tf.matmul(c, dec_cc) +dec_cb 
    state = forget_gate * state + input_gate * tf.tanh(update)
    output_gate = tf.sigmoid(tf.matmul(i, dec_ox) + tf.matmul(o, dec_om) +
                  tf.matmul(o, dec_oc) + dec_ob)
    return output_gate * tf.tanh(state), state</pre></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec169" class="calibre7"/>一些翻译结果——NMT关注着</h2></div></div></div><p class="calibre10">以下是我们经过10，000步后获得的结果<a id="id771"/>:</p><div><pre class="programlisting">
<strong class="calibre11">DE:  &amp;#124; Ferienwohnungen 1 Zi &amp;#124; Ferienhäuser &amp;#124; Landhäuser &amp;#124; Autovermietung &amp;#124; Last Minute Angebote ! !</strong>

EN (TRUE):&amp;#124; 1 Bedroom Apts &amp;#124; Holiday houses &amp;#124; Rural Homes &amp;#124; Car Rental &amp;#124; Last Minute Offers !

EN (Predicted): &amp;#124; Apartments &amp;#124; Hostels &amp;#124; Hostels &amp;#124; Last Minute Offers ! &lt;/s&gt;

<strong class="calibre11">DE: Wie hilfreich finden Sie die Demo ##AT##-##AT## CD ?</strong>

EN (TRUE): How helpful do you find the demo CD ##AT##-##AT## ROM ?

EN (Predicted): How can you find the XLSTAT ##AT##-##AT## MX ? &lt;/s&gt;

<strong class="calibre11">DE:  Das „ Ladino di Fassa “ ist jedoch mehr als ein Dialekt – es ist eine richtige Sprache .</strong>

EN (TRUE):This is Ladin from Fassa which is more than a dialect : it is a language in its own right .

EN (Predicted): The &lt;unk&gt; &amp;quot; is a very important role in the world . &lt;/s&gt;

<strong class="calibre11">DE: In der Hotelbeschreibung im Internet müßte die Zufahrt</strong>
<strong class="calibre11">beschrieben werden .</strong>

EN (TRUE): There are no adverse comments about this hotel at all .

EN (Predicted): The &lt;unk&gt; &lt;unk&gt; is the &lt;unk&gt; of the Internet . &lt;/s&gt;</pre></div><p class="calibre10">与我们之前观察到的类似，注意力集中的NMT擅长翻译一些句子，但不擅长翻译其他句子。</p><p class="calibre10">此外，这些是<a id="id772"/>100，000步后获得的结果:</p><div><pre class="programlisting">
<strong class="calibre11">DE: Das Hotel Opera befindet sich in der Nähe des Royal Theatre , Kongens Nytorv , &amp;apos; Stroget &amp;apos; und Nyhavn .</strong>

EN (TRUE): Hotel Opera is situated near The Royal Theatre , Kongens Nytorv , &amp;quot; Strøget &amp;quot; and fascinating Nyhavn .

EN (Predicted): Best Western Hotel &lt;unk&gt; &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,
&lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt;
, &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,
&lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; , &lt;unk&gt; ,

<strong class="calibre11">DE:  Alle älteren Kinder oder Erwachsene zahlen EUR 32,00 pro Übernachtung und Person für Zustellbetten .</strong>

EN (TRUE):All older children or adults are charged EUR 32.00 per night and person for extra beds .

EN (Predicted): All older children or adults are charged EUR 15 &lt;unk&gt; per night and person for extra beds . &lt;/s&gt;

<strong class="calibre11">DE:  Im Allgemeinen basieren sie auf Datenbanken , Templates und Skripts .</strong>

EN (TRUE):In general they are based on databases , template and scripts .

EN (Predicted): The user is the most important software of the software . &lt;/s&gt;

<strong class="calibre11">DE: Tux Racer wird Ihnen helfen , die Zeit totzuschlagen und sie</strong>
<strong class="calibre11">können OpenOffice zum Arbeiten verwenden .</strong>

EN (TRUE): Tux Racer will help you pass the time while you wait ,
and you can use OpenOffice for work .

EN (Predicted): &lt;unk&gt; .com we have a very friendly and helpful
staff . &lt;/s&gt;</pre></div><p class="calibre10">为了便于比较，我们使用了与评估标准NMT相同的测试句子。我们可以看到，与标准NMT相比，注意力NMT模型提供了更好的翻译。但是仍然有可能得到一些错误的翻译，因为我们使用的数据量有限。</p><p class="calibre10"><em class="calibre15">图10.18 </em>描绘了NMT和NMT的BLEU分数随时间的变化情况，并将其并排显示。我们可以清楚地看到，注意力集中的NMT在训练和测试数据中都给出了更好的BLEU分数:</p><div><img alt="Some translation results – NMT with attention" src="img/B08681_10_106.jpg" class="calibre12"/><div><p class="calibre10">图10.19:NMT和NMT+注意力的BLEU分数随时间的变化</p></div></div><div><div><h3 class="title4"><a id="note42" class="calibre7"/>注意</h3><p class="calibre16">根据2017年的结果，德语到英语翻译的当前技术水平BLEU得分为35.1 ( <em class="calibre15">爱丁堡大学的神经机器翻译系统WMT17，由Rico Sennrich等人编写的arXiv预印本arXiv:1708.00726 (2017) </em>)</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec170" class="calibre7"/>视觉化对源句子和目标句子的注意力</h2></div></div></div><p class="calibre10">在<em class="calibre15">图10.19 </em>中，我们可以<a id="id773"/>可视化对于几个源到目标翻译对，注意力值如何寻找给定目标词的不同源词。如果你记得，当计算注意力时，我们有解码器给定位置的<code class="literal">enc_num_unrollings</code>注意力值。因此，如果你将解码器中所有位置的所有注意力向量连接起来，你可以创建<a id="id774"/>一个<strong class="calibre11">注意力矩阵</strong>。</p><p class="calibre10">在注意力矩阵中，我们将目标词作为行，将源词作为列。一些行和列的较高(较亮)的值指示当预测在该行中找到的目标单词时，解码器主要关注由该列给出的源单词。例如，您可以看到目标句子中的<code class="literal">Hotel</code>与源句子中的<code class="literal">Hotel</code>高度相关:</p><div><img alt="Visualizing attention for source and target sentences" src="img/B08681_10_107.jpg" class="calibre12"/><div><p class="calibre10">图10.20:几种不同源-目标翻译对的注意矩阵</p></div></div><p class="calibre10">这让我们结束了对NMT的讨论。我们讨论了NMT使用的基本编码器-解码器架构，以及如何评估NMT系统。然后，我们讨论了改善NMT系统的几种方法，如教师强制、使用深度LSTMs和注意机制。</p><p class="calibre10">理解NMT在现实世界中有各种各样的用例是很重要的。一个明显的用例是在许多国家都有分支机构的国际企业。在这样的企业中，来自不同国家的员工需要有更快的沟通方式，而不要让语言成为障碍。因此，自动将电子邮件从一种语言翻译成另一种语言对于这样的公司来说非常有用。其次，在制造业中，机器翻译可以用来制作多语言产品描述/产品用户手册。然后，专家可以进行简单的后期处理，以确保翻译的准确性。最后，对于日常任务，例如多语言翻译，机器翻译可以派上用场。比如说，用户的母语不是英语，他需要搜索一些他们不知道如何用英语完全描述的东西。在这种情况下，用户可以编写多语言搜索查询。然后，MT系统可以将查询翻译成不同的语言，并在互联网上搜索与用户的搜索请求相匹配的资源。</p></div></div>



  
    <title>Other applications of Seq2Seq models – chatbots</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec82" class="calibre7"/>seq 2 seq模型的其他应用–聊天机器人</h1></div></div></div><p class="calibre10">序列对序列模型的另一个流行的<a id="id777"/>应用是创建聊天机器人。聊天机器人是一种能够与人进行真实对话的计算机程序。这类应用对拥有庞大客户群的公司非常有用。对客户提出的答案显而易见的基本问题做出回应，在客户支持请求中占了很大一部分。当聊天机器人能够找到答案时，它可以为客户提供基本的服务。此外，如果聊天机器人不能回答问题，请求会被重定向到人工操作员。聊天机器人可以节省人工操作员回答基本问题的大量时间，让他们专注于更困难的任务。</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec171" class="calibre7"/>训练聊天机器人</h2></div></div></div><p class="calibre10">那么，我们如何<a id="id779"/>使用序列对序列模型来训练聊天机器人呢？答案很简单，因为我们已经了解了机器翻译模型。唯一的区别是源句子和目标句子是如何形成的。</p><p class="calibre10">在NMT系统中，句子对由源句子和该句子在目标语言中的相应翻译组成。然而，在训练聊天机器人时，数据是从两个人的对话中提取的。源句子将是人A说出的句子/短语，目标句子将是人b对人A的回复。这里有一个这样的例子。这些数据由人与人之间的电影对话组成，可在https://www . cs . Cornell . edu/~ cristian/Cornell _ Movie-Dialogs _ corpus . html找到。</p><div><blockquote class="blockquote"><p class="calibre10">比安卡:他们没有！</p><p class="calibre10">卡梅伦:他们会的！</p><p class="calibre10">比安卡:我希望如此。</p><p class="calibre10">她还好吗？</p><p class="calibre10">比安卡:我们走吧。</p><p class="calibre10">卡梅伦:哇</p><p class="calibre10">比安卡:好吧——你需要学习如何撒谎。</p><p class="calibre10">卡梅伦:不</p><p class="calibre10">比安卡:我开玩笑的。你知道有时候你会变成一个“角色”吗？你不知道如何戒掉？</p><p class="calibre10">比安卡:就像我害怕穿彩色蜡笔一样？</p><p class="calibre10">卡梅隆:“真实的你”。</p></blockquote></div><p class="calibre10">这里有一些链接<a id="id780"/>指向其他几个用于训练对话聊天机器人的数据集:</p><div><ul class="itemizedlist"><li class="listitem">Reddit评论数据集:<a href="https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/">https://www . Reddit . com/r/datasets/comments/3bxlg 7/I _ have _ every _ public _ available _ Reddit _ comment/</a></li><li class="listitem">马鲁巴对话数据集:<a href="https://datasets.maluuba.com/Frames">https://datasets.maluuba.com/Frames</a></li><li class="listitem">Ubuntu对话文集:<a href="http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/">http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/</a></li><li class="listitem">NIPS对话智能挑战:<a href="http://convai.io/">http://convai.io/</a></li><li class="listitem">微软研究院社交媒体文本语料库:<a href="https://tinyurl.com/y7ha9rc5">https://tinyurl.com/y7ha9rc5</a></li></ul></div><p class="calibre10">图10.20显示了聊天机器人系统和NMT系统的相似性。例如，我们用由两个人之间的对话组成的数据集来训练聊天机器人。编码器接受一个人所说的句子/短语，解码器被训练来预测另一个人的反应。经过这样的训练后，我们可以使用聊天机器人来回答一个给定的问题:</p><div><img alt="Training a chatbot" src="img/B08681_10_108.jpg" class="calibre12"/><div><p class="calibre10">图10.21:聊天机器人的图示</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec172" class="calibre7"/>评估聊天机器人–图灵测试</h2></div></div></div><p class="calibre10">图灵测试<a id="id781"/>是艾伦·图灵在20世纪50年代发明的，作为一种测量机器智能的方法。实验设置非常适合评估聊天机器人。实验的设置如下。</p><p class="calibre10">这里涉及到三个<a id="id782"/>方:一个评价者(也就是一个人)(<strong class="calibre11"> A </strong>)、另一个人(<strong class="calibre11"> B </strong>)和一台机器(<strong class="calibre11"> C </strong>)。他们三个坐在三个不同的房间里，这样他们谁也看不见谁。唯一的交流媒介是文本，由一方输入计算机，接收者在他们这边的计算机上看到文本。评估器同时与人和机器通信。在对话的最后，评估者要区分机器和人类。如果评估者不能做出区分，就说机器通过了图灵测试。该设置如图<em class="calibre15">图10.21 </em>所示:</p><div><img alt="Evaluating chatbots – Turing test" src="img/B08681_10_110.jpg" class="calibre12"/><div><p class="calibre10">图10.22:图灵测试</p></div></div></div></div>



  
    <title>Summary</title>
    <meta content="DocBook XSL Stylesheets V1.75.2" name="generator"/>
    <meta content="urn:uuid:7a74de9d-8dca-491a-886e-bcc2b2120efe" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><div><div><div><h1 class="title1"><a id="ch10lvl1sec83" class="calibre7"/>总结</h1></div></div></div><p class="calibre10">在这一章中，我们详细讨论了NMT系统。机器翻译是将给定的文本语料库从源语言翻译成目标语言的任务。首先，我们简要地讨论了机器翻译的历史，以建立一种对机器翻译的欣赏意识，从而成为今天的机器翻译。我们看到今天最高性能的机器翻译系统实际上是NMT系统。接下来，我们讨论了这些系统的基本概念，并将模型分解为嵌入层、编码器、上下文向量和解码器。我们首先确定了拥有嵌入层的好处，因为与一次性编码的向量相比，它给出了单词的语义表示。然后我们理解了编码器的目标，即学习一个好的固定维向量来表示源句子。接下来，一旦学习了固定维度的上下文向量，我们就用它来初始化解码器。解码器负责产生源句子的实际翻译。然后我们讨论了训练和推理在NMT系统中是如何工作的。</p><p class="calibre10">然后我们看了一个NMT系统的实际实现，该系统将句子从德语翻译成英语，以理解NMT系统的内部机制。在这里，我们查看了一个使用基本TensorFlow操作实现的NMT系统，与使用TensorFlow中的seq2seq等现成库相比，这让我们对系统的逐步执行有了深入的了解。然后我们了解到，上下文向量会导致系统出现瓶颈，因为系统被迫将源句子中的所有知识嵌入到一个固定维度(相对较小)的向量中。由于系统表现不佳的任务的难度，我们继续学习避免这个瓶颈的技术:注意力机制。注意力机制允许解码器在每个解码步骤观察编码器的完整状态历史，从而允许解码器形成丰富的上下文向量，而不是仅仅依赖于固定维度的向量来学习翻译。我们看到这种技术使得NMT系统性能更好。</p><p class="calibre10">最后，我们讨论了序列对序列学习的另一个流行应用:聊天机器人。聊天机器人是机器学习应用程序，能够与人进行真实的对话，甚至回答问题。我们看到NMT系统和聊天机器人的工作方式相似，只有训练数据不同。我们还讨论了图灵测试，这是一种定性测试，可用于评估聊天机器人。</p><p class="calibre10">在下一章，我们将讨论自然语言处理的各种未来趋势。</p></div>
</body></html>