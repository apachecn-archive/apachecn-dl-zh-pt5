# 六、大规模运行超参数调优

**超参数调整**或**超参数优化** ( **HPO** )是在合理的计算资源限制和时间框架内，找到最佳可能的深度神经网络结构、预训练模型类型和模型训练过程的过程。在这里，超参数是指在 ML 训练过程中不能改变或学习的参数，如深度神经网络内部的层数，预训练语言模型的选择，或训练过程的学习速率，批量大小和优化器。在本章中，我们将使用 HPO 作为超参数调整和优化过程的简称。HPO 是生产高性能 ML/DL 模型的关键步骤。鉴于超参数的搜索空间非常大，有效地大规模运行 HPO 是一个重大挑战。与经典的 ML 模型相比，评估 DL 模型的复杂性和高成本进一步增加了挑战。因此，我们需要学习最先进的 HPO 方法和实现框架，实现日益复杂和可扩展的 HPO 方法，并使用 MLflow 对其进行跟踪，以确保可再现的调整过程。学习完本章后，您将能够熟练地为 DL 模型管道实现可伸缩的 HPO。

在这一章中，首先，我们将给出一个不同的自动 HPO 框架和 DL 模型调优应用的概述。此外，我们将了解优化什么以及何时选择使用什么框架。我们将比较三个流行的 HPO 框架: **HyperOpt** 、 **Optuna** 和 **Ray Tune** 。我们将展示哪一个是大规模运行 HPO 的最佳选择。然后，我们将重点学习如何创建可以使用 Ray Tune 和 MLflow 的 HPO 就绪 DL 模型代码。接下来，我们将以 Optuna 为主要示例，展示如何轻松切换到使用不同的 HPO 算法。

在本章中，我们将讨论以下主题:

*   了解 DL 管线的自动 HPO
*   使用光线调节和 MLflow 创建 HPO 就绪的 DL 模型
*   使用 MLflow 运行第一个射线调节 HPO 实验
*   使用 Optuna 和 HyperBand 运行 Ray Tune HPO

# 技术要求

要理解本章中的示例，需要以下关键技术要求:

*   Ray Tune 1.9.2:这是一个灵活而强大的超参数调优框架([https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html))。
*   Optuna 2.10.0:这是一个命令式的由运行定义的超参数调优 Python 包([https://optuna.org/](https://optuna.org/))。
*   本章的代码可以在下面的 GitHub URL 中找到，其中还包含了包含前面的密钥包和其他依赖项的`requirements.txt`文件:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 06](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter06)。

# 了解 DL 管线的自动 HPO

自从 1995 年第一篇关于该主题的已知论文发表以来，自动 HPO 已经被研究了二十多年([https://www . science direct . com/science/article/pii/b 9781558603776500451](https://www.sciencedirect.com/science/article/pii/B9781558603776500451))。众所周知，调整 ML 模型的超参数可以提高模型的性能——有时，效果非常显著。近年来，DL 模型的兴起引发了新一轮的创新和新框架的开发，以解决 DL 管道的 HPO 问题。这是因为 DL 模型流水线强加了许多新的和大规模的优化挑战，这些挑战不能由先前的 HPO 方法容易地解决。注意，与可以在模型训练过程中学习的模型参数相反，必须在训练之前设置一组超参数。

HPO 与迁移学习微调的区别

在本书中，我们一直专注于一种成功的 DL 方法，称为**迁移学习**(请参考 [*第 1 章*](B18120_01_ePub.xhtml#_idTextAnchor015) 、*深度学习生命周期和 MLOps 挑战*，以获得完整的讨论)。迁移学习过程的关键步骤是用一些特定于任务和领域的标记数据来微调预训练的模型，以获得良好的特定于任务的 DL 模型。然而，微调步骤只是一种特殊的模型训练步骤，也有许多超参数需要优化。这就是 HPO 发挥作用的地方。

## 超参数的类型及其挑战

有几种类型的超参数可用于 DL 管道:

*   **DL 模型类型和架构**:在迁移学习的情况下，选择使用哪些预训练模型是一个可能的超参数。例如，**抱脸**模型库([https://huggingface.co/models](https://huggingface.co/models))中有超过 27000 个经过预训练的模型，包括**伯特**、**罗伯塔**等等。对于一个特定的预测任务，我们可能想要尝试其中的一些来决定哪一个是最好的。
*   **学习和训练相关参数**:包括不同类型的优化器，如**随机梯度下降** ( **SGD** )和 **Adam** (您可以在[https://machine Learning knowledge . ai/PyTorch-optimizer-complete-guide-for-初学者/](https://machinelearningknowledge.ai/pytorch-optimizers-complete-guide-for-beginner/) 查看 py torch 优化器列表)。它还包括相关的参数，如学习率和批量大小。建议在适用的情况下，首先按照神经网络模型的重要性顺序调整以下参数:学习速率、动量、小批量、隐藏层数、学习速率衰减和正则化([https://arxiv.org/abs/2003.05689](https://arxiv.org/abs/2003.05689))。
*   **数据和管道配置**:DL 管道可以包括可能影响模型训练的数据处理和转换步骤。例如，如果我们想要比较具有或不具有签名文本主体的电子邮件消息的分类模型的性能，则需要用于是否包括电子邮件签名的超参数。另一个例子是当我们没有足够的数据或数据的变化时；我们可以尝试使用各种数据扩充技术，这将为模型训练带来不同的输入集([https://neptune.ai/blog/data-augmentation-nlp](https://neptune.ai/blog/data-augmentation-nlp))。

提醒一下，并不是所有的超参数都是可调的或者需要调整。例如，不需要调整 DL 模型中的**历元数**。这是因为当精度指标停止提高或没有任何希望比其他超参数配置做得更好时，训练应该停止。这被称为早期停止或修剪，是支撑最近一些最先进的 HPO 算法的关键技术之一(有关早期停止的更多讨论，请参考[https://databricks . com/blog/2019/08/15/how-not-to-scale-deep-learning-in-6-easy-steps . html](https://databricks.com/blog/2019/08/15/how-not-to-scale-deep-learning-in-6-easy-steps.html))。

注意这三类超参数都可以混合搭配，整个超参数空间的配置可以非常大。例如，如果我们想要选择我们想要用作超参数的预训练模型的类型(例如，选择可以是 **BERT** 或 **RoBERTa** )，两个学习相关的参数(例如学习速率和批量大小)，以及两个用于 NLP 文本的不同数据扩充技术(例如随机插入和同义词替换)，那么我们有五个超参数要优化。请注意，每个超参数可以有相当多不同的候选值可供选择，如果每个超参数有 5 个不同的值，那么我们将有总共 55 = 3125 个超参数组合可供尝试。在实践中，有几十个超参数可供尝试是很常见的，每个超参数可能有几十个选择或分布可供采样。这很快就导致了维数灾难问题([https://insaid . medium . com/automated-hyperparameter-tuning-988 b5 aeb 7 F2 a](https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a))。DL 模型昂贵的训练和评估成本加剧了这种高维搜索空间的挑战；我们知道，即使是一个微小的 BERT 的 1 个历元，我们在前面的章节中尝试过，使用一个微小的训练和验证数据集也需要 1-2 分钟。现在想象一个带有 HPO 的现实生产级 DL 模型，如果执行效率不高，可能需要几个小时、几天甚至几周的时间。总体而言，以下列出了需要大规模应用高性能 HPO 的主要挑战:

*   超参数的高维搜索空间
*   越来越大的 DL 模型的模型训练和评估时间的高成本
*   Time-to-production and deployment for DL models used in production

    同时进行模型训练和 HPO

    在训练过程中可以动态地改变超参数。这是一种同时进行模型训练和 HPO 的混合方法，例如**基于群体的训练**(**PBT**；[https://deep mind . com/blog/article/population-based-training-neural-networks](https://deepmind.com/blog/article/population-based-training-neural-networks))。然而，这并不能改变这样一个事实，即当开始一个新的训练时期时，需要预定义一组超参数。这种 PBT 是试图降低搜索高维超参数空间的成本和 DL 模型的训练成本的创新之一。感兴趣的读者应该查阅*延伸阅读*部分来深入了解这个主题。

既然我们已经了解了要优化的超参数的一般挑战和类别，那么让我们看看 HPO 是如何工作的，以及如何为我们的使用选择一个框架。

## HPO 是如何运作的，选择哪一个

有不同的方式来理解 HPO 是如何运作的。经典的 HPO 方法包括网格搜索和随机搜索，其中用一系列候选值选择一组超参数。每一个都独立运行直到完成，然后我们从我们运行的一组试验中选择最佳的超参数配置，给出我们发现的最佳模型性能度量。虽然这种类型的搜索易于实现，甚至可能不需要复杂的框架来支持它，但它本质上是低效的，并且由于 HPO 的非凸性质，甚至可能找不到超参数的最佳配置。术语非凸意味着存在多个局部最小或最大点，并且优化方法可能无法找到全局最优(即，最小或最大)。简而言之，现代 HPO 需要做两件事:

*   超参数的自适应采样(也称为**配置选择**或 **CS** ):这意味着它需要利用先验知识找到要尝试的超参数集。这主要是关于使用贝叶斯优化的不同变体，以一种顺序的方式基于先前的试验来自适应地识别新的配置。这已被证明优于传统的网格搜索和随机搜索方法。
*   对一组超参数的性能进行自适应评估(也称为**配置评估**或 **CE** ):这些方法侧重于自适应地为有前途的超参数配置分配更多资源，同时快速修剪较差的配置。资源可以是不同的形式，例如训练数据集的大小(例如，仅使用训练数据集的一小部分)或迭代次数(例如，仅使用几次迭代来决定终止哪些迭代而不运行到收敛)。有一个方法家族叫做多臂 bandit 算法，比如**异步连续减半算法** ( **ASHA** )。在这里，所有的试验都从最初的预算开始，然后去掉最差的一半，为剩下的部分调整预算，如此重复，直到只剩下一个试验。

在实践中，我们希望使用以下五个标准选择一个合适的 HPO 框架:

*   与 MLflow 的回调集成
*   GPU 集群的可扩展性和支持
*   易用性和灵活的 API
*   与先进的 HPO 算法( **CS** 和 **CE** )集成
*   DL 框架的支持

在本书中，对三个框架进行了比较，结果总结在*图 6.1* 中:

![Figure 6.1: Comparison of Ray Tune, Optuna, and HyperOpt
](img/B18120_06_01.jpg)

图 6.1:光线调节、Optuna 和远视的比较

从*图 6.1* 中可以看出，与**Optuna**([https://optuna.org/](https://optuna.org/))和**远视**([https://hyperopt.github.io/hyperopt/](https://hyperopt.github.io/hyperopt/))相比，胜出者是**雷调**([https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html))。让我们解释一下这五个标准，如下所示:

*   **与 MLflow 的回调集成** : Optuna 对 MLflow 回调的支持仍然是一个实验性的特性，而 HyperOpt 根本不支持回调，这给用户留下了额外的工作来管理每次试运行的 MLflow 跟踪。

只有 Ray Tune 支持 Python mixin 装饰器和与 MLflow 的回调集成。Python mixin 是一种模式，允许在任何需要的时候混合独立的函数。在这种情况下，MLflow 功能在模型训练期间通过`mlflow_mixin`装饰器自动混合。这可以将任何训练功能转换为射线调节可训练功能，自动配置 MLflow 并在每次调节试验的相同过程中创建运行。然后，您可以在训练函数中使用 MLflow API，它会自动报告正确的运行。此外，它支持 MLflow 的自动记录，这意味着所有 MLflow 跟踪信息将被记录到正确的试验中。例如，下面的代码片段显示了我们之前的 DL 微调函数可以变成一个`mlflow_mixin`光线调整函数，如下所示:

```
@mlflow_mixin
def train_dl_model():
    mlflow.pytorch.autolog()
    trainer = flash.Trainer(
        max_epochs=num_epochs,
        callbacks=[TuneReportCallback(
            metrics, on='validation_end')])
    trainer.finetune()
```

请注意，当我们定义训练器时，我们可以添加`TuneReportCallback`作为一个回调函数，它会将指标传递回给 Ray Tune，而 MLflow 自动日志记录会同时记录所有跟踪结果。在下一节中，我们将向您展示如何将前一章的微调 DL 模型的示例变成可训练的光线调整。

*   **GPU 集群的可扩展性和支持**:虽然 Optuna 和 HyperOpt 支持并行化，但是它们都依赖于一些外部数据库(关系数据库或 MongoDB)或 SparkTrials。只有 Ray Tune 通过 Ray distributed 框架原生支持并行和分布式 HPO，也是这三个框架中唯一支持在 GPU 集群上运行的。
*   **API 的易用性和灵活性**:在所有三个框架中，只有 Optuna 支持**define-by-run**API，允许你以 Pythonic 式的编程风格动态定义超参数，包括循环和分支([https://Optuna . readthedocs . io/en/stable/tutorial/10 _ key _ features/002 _ configurations . html](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html))。这与 Ray Tune 和 HyperOpt 都支持的**定义并运行**API 形成对比，后者在评估目标函数之前由预定义的字典定义搜索空间。这两个术语，**定义-运行**和**定义-运行**，实际上是由 DL 框架的开发社区创造的。在早期，TensorFlow 1.0 最初发布的时候，需要先定义一个神经网络，然后在后面懒洋洋地执行，这就是所谓的定义-运行。这两个阶段，1)构建神经网络阶段和 2)评估阶段，是顺序执行的，并且在构建阶段之后不能改变神经网络结构。较新的 DL 框架，如 TensorFlow 2.0(或 TensorFlow 的快速执行版本)和 PyTorch，支持**运行定义**神经网络计算。构建和评估神经网络没有两个独立的阶段。用户可以在进行计算时直接操纵神经网络。虽然 Optuna 提供的 **define-by-run** API 可以用来直接动态定义超参数搜索空间，但是它确实有一些缺点。主要问题是直到运行时才知道参数并发，这会使优化方法的实现变得复杂。这是因为许多采样方法都支持预先知道参数并发。因此，在本书中，我们更喜欢使用**定义并运行**API。另外，请注意，Ray Tune 可以通过与 Optuna 的集成来支持 **define-by-run** API(您可以在 Ray Tune 的 GitHub 资源库中的[https://GitHub . com/Ray-project/Ray/blob/master/python/Ray/Tune/examples/Optuna _ define _ by _ run _ example . py # L35](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/optuna_define_by_run_example.py#L35)中看到一个示例)。
*   **与前沿 HPO 算法的集成** ( **CS 和 CE** ):在 **CS** 方面，在这三个框架中，HyperOpt 支持或集成最新前沿 HPO 采样和搜索方法的开发最少。它的主要搜索方法是**树形结构 Parzen 估计器** ( **TPE** )，这是一种贝叶斯优化变体，对混合分类和条件超参数搜索空间特别有效。同样，Optuna 的主要取样方法是 TPE。相反，Ray Tune 支持所有先进的搜索方法，包括:
    *   DragonFly([https://dragonfly-opt.readthedocs.io/en/master/](https://dragonfly-opt.readthedocs.io/en/master/))，这是一个高度可扩展的贝叶斯优化框架
    *   微软研究院的 blend search([https://Microsoft . github . io/FLAML/docs/Use-Cases/Tune-User-Defined-Function/# hyperparameter-optimization-algorithm](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function/#hyperparameter-optimization-algorithm)

此外，Ray Tune 还通过与 Optuna 和 HyperOpt 的集成支持 TPE。

在 **CE** 端，HyperOpt 不支持任何修剪或调度程序来停止非承诺的超参数配置。Optuna 和 Ray Tune 都支持相当多的修剪程序(在 Optuna 中)或调度程序(在 Ray Tune 中)。但是，只有光线调节支持 PBT。鉴于 Ray tune 开发的活跃的开发社区和灵活的 API，Ray Tune 有可能继续及时地集成和支持任何新兴的调度程序或修剪程序。

*   **对 DL 框架的支持** : HyperOpt 不是专门设计的，也不与任何 DL 框架集成。这并不意味着您不能使用 HyperOpt 来调优 DL 模型。然而，HyperOpt 不提供任何修剪或调度程序支持来为不看好的超参数配置执行早期停止，这是 HyperOpt 用于 DL 模型调优的一个主要缺点。Ray Tune 和 Optuna 都集成了流行的 DL 框架，如 PyTorch Lightning 和 TensorFlow/Keras。

除了我们刚刚讨论的主要标准之外，Ray Tune 还拥有最好的文档、丰富的代码示例和充满活力的开源开发人员社区，这就是为什么我们在本章的学习中更喜欢使用 Ray Tune。在下面的章节中，我们将学习如何使用 Ray Tune 和 MLflow 创建 HPO 就绪的 DL 模型。

# 使用 Ray Tune 和 MLflow 创建 HPO 就绪的 DL 模型

为了将射线调整与 HPO 的 MLflow 一起使用，让我们使用第 5 章 、*在不同环境中运行 DL 管道*中的 DL 管道示例中的微调步骤，来查看需要设置什么以及需要进行哪些代码更改。在我们开始之前，首先让我们回顾一些与我们使用射线调节特别相关的关键概念:

*   **目标函数**:目标函数可以是最小化或最大化给定超参数配置的一些度量值。例如，在 DL 模型训练和微调场景中，我们希望最大化 NLP 文本分类器准确度的 F1 值。这个目标函数需要包装成一个可训练的函数，其中射线调优可以做 HPO。在下一节中，我们将说明如何包装我们的 NLP 文本情感模型。
*   `tune.report`用于报告模型指标([https://docs . ray . io/en/latest/tune/API _ docs/trainiable . html # function-API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#function-api))。一个基于类的 API 需要模型训练函数(可训练的)是`tune.Trainable` ([https://docs . ray . io/en/latest/tune/API _ docs/trainiable . html # trainiable-class-API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-class-api))的子类。基于类的 API 提供了对光线调节如何控制模型训练处理的更多控制。如果您开始为神经网络模型编写一个新的架构，这可能会非常有帮助。然而，当使用预训练的基础模型进行微调时，使用基于函数的 API 要容易得多，因为我们可以利用 PyTorch Lightning Flash 等包来进行 HPO。
*   `tune.run`，雷调将在那里编排 HPO 进程。
*   `tune.loguniform`)或者来自一些范畴变量(例如，`tune.choice(['a', 'b' ,'c'])`可以让你统一选择这三个选项)。通常，这个搜索空间被定义为一个名为`config`的 Python 字典变量。
*   `tune.suggest`API([https://docs . ray . io/en/latest/tune/API _ docs/suggestion . html # tune-search-alg](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg))。
*   `tune.suggest` API 提供了搜索的优化算法，它不提供早期停止或修剪功能来在几次迭代后停止明显没有希望的试验。由于早期停止或修剪可以显著加快 HPO 进程，强烈建议您将调度程序与搜索器结合使用。Ray Tune 通过它的调度器 API ( `tune.schedulers`)提供了很多流行的调度器，比如 ASHA、HyperBand 等等。(请访问[https://docs . ray . io/en/latest/tune/API _ docs/schedulers . html # trial-schedulers-tune-schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#trial-schedulers-tune-schedulers)。)

回顾了射线调节的基本概念和 API 之后，在下一节中，我们将设置射线调节和 MLflow 来运行 HPO 实验。

## 设置射线调节和 MLflow

现在我们已经了解了基本的概念和 Ray Tune 的 API，让我们看看如何设置 Ray Tune 来为我们之前的 NLP 情感分类器的微调步骤执行 HPO。你可能想下载这一章的代码([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 06/](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/))来跟随这些指令:

1.  通过在您的 conda 虚拟环境中键入以下命令来安装 Ray Tune，`dl_model_hpo` :

    ```
    pip install ray[tune]==1.9.2
    ```

2.  这将在虚拟环境中安装 Ray Tune，您将在虚拟环境中启动 HPO 运行，以便对 DL 模型进行微调。请注意，我们还在本章的 GitHub 存储库中提供了完整的`requirements.txt`文件([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 06/requirements . txt](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/requirements.txt))，在这里您应该能够运行以下安装命令:

    ```
    pip install -r requirements.txt
    ```

3.  如果你需要知道如何设置一个合适的虚拟环境，位于同一文件夹中的`README.md`文件中的完整说明会给你更多的指导。
4.  对于 MLflow 设置，假设您已经设置了一个完整的 MLflow 跟踪服务器，您唯一需要注意的是确保您已经正确设置了访问 MLflow 跟踪服务器的环境变量。在您的 shell 中运行以下命令来设置它们。或者，您可以通过在 Python 代码中调用`os.environ["environmental_name"]=value`来覆盖您的环境变量。提醒一下，我们已经展示了可以在每个终端会话的命令行中设置的以下环境变量:

    ```
    export MLFLOW_TRACKING_URI=http://localhost export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000 export AWS_ACCESS_KEY_ID="minio" export AWS_SECRET_ACCESS_KEY="minio123"
    ```

5.  运行`download_data`步骤，将原始数据下载到本地`chapter06`父文件夹下的文件夹:

    ```
    mlflow run . -P pipeline_steps='download_data' --experiment-name dl_model_chapter06
    ```

当前面的执行完成后，您应该能够在 **chapter06/data/** 文件夹下找到 IMDB 数据。

现在我们准备创建一个 HPO 步骤来微调我们之前构建的 NLP 情绪模型。

## 为 DL 模型创建可训练的光线调节

我们需要做多处改变来让光线调整到运行 HPO 来微调我们在前面章节中开发的 DL 模型。让我们按照如下步骤进行操作:

1.  首先，让我们在前面的微调代码中确定可能的超参数(可调和不可调)列表。回想一下，我们的微调代码看起来与下面类似(这里只显示了代码的关键行；完整的代码可以在 GitHub 资源库的`chapter05`中找到，该资源库位于[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 05/pipeline/fine _ tuning _ model . py # L19](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter05/pipeline/fine_tuning_model.py#L19):

    ```
    datamodule = TextClassificationData.from_csv(     input_fields="review",     target_fields="sentiment",     train_file=f"{data_path}/imdb/train.csv",     val_file=f"{data_path}/imdb/valid.csv",     test_file=f"{data_path}/imdb/test.csv") classifier_model = TextClassifier(     backbone= "prajjwal1/bert-tiny",     num_classes=datamodule.num_classes,      metrics=torchmetrics.F1(datamodule.num_classes)) trainer = flash.Trainer(max_epochs=3) trainer.finetune(classifier_model,      datamodule=datamodule, strategy="freeze") 
    ```

前面的代码有四个主要部分:

*   `datamodule`变量:它定义了训练、验证和测试的数据源。有一个默认值为`1`的`batch_size`参数，这里没有显示，但它是需要优化的最重要的超参数之一。更多详情请参见`lightning-flash`代码文档中的解释([https://github . com/PyTorchLightning/lightning-flash/blob/450902d 713980 e 0 edefcfd 2d 2 a 35 EB 875072d 7/flash/core/data/data _ module . py # L64](https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/core/data/data_module.py#L64))。
*   `classifier_model`:通过`lightning-flash`的`TextClassifier` API 定义了一个暴露参数的分类器。输入参数中有多个可以优化的超参数，包括`learning_rate`、`backbone`基础模型、`optimizer`等等。你可以在`TextClassifier` API 的`lightning-flash`代码文档中看到输入参数的完整列表([https://github . com/PyTorchLightning/lightning-flash/blob/450902d 713980 e 0 edefcfd 2d 2 a 35 EB 875072d 7/flash/text/classification/model . py # L44](https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/text/classification/model.py#L44))。
*   `trainer`:这定义了一个可用于微调的训练器变量。这里有几个超参数需要设置，但不一定要调优，比如前面讨论过的`num_epochs`。
*   `trainer.finetune`:这是实际的微调(迁移学习)。请注意，还有一个可能的超参数**策略**可以调整。

出于学习的目的，我们将选择`learning_rate`和`batch_size`作为两个要优化的超参数，因为这两个是要为 DL 模型优化的最重要的超参数。一旦你完成了这一章，你应该能够很容易地添加额外的超参数到优化的候选列表中。

1.  光线调节需要将一个可训练的函数传递给`tune.run`。这意味着我们需要创建一个可训练的函数。默认情况下，一个可训练函数只接受一个必需的输入参数`config`，它包含一个超参数的键值对字典和其他用于标识执行环境(如 MLflow 跟踪 URL)的参数。然而，Ray Tune 提供了一个名为`tune.with_parameters`的包装函数，它允许您传递附加的任意参数和对象([https://docs . Ray . io/en/latest/Tune/tutorials/overview . html # how-can-I-pass-further-parameter-values-to-my-training able](https://docs.ray.io/en/latest/tune/tutorials/overview.html#how-can-i-pass-further-parameter-values-to-my-trainable))。首先，让我们使用一个`mlflow_mixin`装饰器，创建一个名为`finetuning_dl_model`的函数来封装我们刚刚检查过的关于微调步骤的逻辑。这允许在调用此函数时自动初始化 ml flow:

    ```
    @mlflow_mixin def finetuning_dl_model(config, data_dir=None,                         num_epochs=3, num_gpus=0):
    ```

该函数将一个`config`字典作为输入，其中可以传递一个超参数列表和 MLflow 配置。此外，我们向函数签名添加了三个额外的参数:`data_dir`表示目录的位置，`num_epochs`表示每个试验运行的最大时期数，`num_gpus`表示每个试验使用的 GPU 数量(如果有的话)。

1.  在这个`mlflow_mixin`修饰函数中，如果有必要，我们可以使用所有的 MLflow 跟踪 API，但是从 MLflow 版本 1.22.0 开始，由于 MLflow 的自动日志支持不再是一个实验性的特性，而是一个成熟的生产质量特性([https://github.com/mlflow/mlflow/releases/tag/v1.22.0](https://github.com/mlflow/mlflow/releases/tag/v1.22.0))，我们应该在我们的代码中使用自动日志，如下:

    ```
    mlflow.pytorch.autolog()
    ```

这是有效的，并且不需要改变。然而，`batch_size`超参数不会被自动记录自动捕获，因此我们需要在微调完成后再添加一条记录语句，如下所示:

```
mlflow.log_param('batch_size',config['batch_size'])
```

1.  在`finetuning_dl_model` 函数的其余实现体中，代码的大部分与之前相同。有一些变化。在`datamodule`变量赋值语句中，我们添加了`batch_size=config['batch_size']`以允许训练数据的小批量可调，如下所示:

    ```
    datamodule = TextClassificationData.from_csv(     input_fields="review",     target_fields="sentiment",     train_file=f"{data_dir}/imdb/train.csv",     val_file=f"{data_dir}/imdb/valid.csv",     test_file=f"{data_dir}/imdb/test.csv",     batch_size=config['batch_size'])
    ```

2.  当定义`classifier_model`变量时，不使用超参数集的默认值，现在我们需要传入`config`字典来分配这些值:

    ```
    classifier_model = TextClassifier(     backbone=config['foundation_model'],     learning_rate=config['lr'],     optimizer=config['optimizer_type'],     num_classes=datamodule.num_classes,     metrics=torchmetrics.F1(datamodule.num_classes))
    ```

3.  Next, we need to modify the trainer assignment code. Here, we need to do two things: first, we need to define a metrics key-value dictionary to pass from PyTorch Lightning to Ray Tune. The key in this metrics dictionary is the name to be referenced in the Ray Tune trial run, while the value of the key in this dictionary is the corresponding metric name reported by PyTorch Lightning.

    PyTorch Lightning 验证步骤中的度量名称

    将指标传递给 Ray Tune 时，首先，我们需要知道 PyTorch Lightning 在验证步骤中使用的指标名称，因为 HPO 只使用验证数据进行评估，而不使用保留测试数据集。事实证明 PyTorch Lightning 有一个硬编码的约定，用相应的培训、验证和测试步骤名称和下划线作为所有指标的前缀。名为`f1`的指标将在 PyTorch Lightning 中报告为:在训练步骤中为`train_f1`，在验证步骤中为`val_f1`，在测试步骤中为`test_f1`。(可以在[https://github . com/PyTorchLightning/Lightning-flash/blob/8b 244d 785 c 5569 e 9 aa 7 D2 b 878 a5f 94 af 976d 3 f 55/flash/core/model . py # L462](https://github.com/PyTorchLightning/lightning-flash/blob/8b244d785c5569e9aa7d2b878a5f94af976d3f55/flash/core/model.py#L462)查看 PyTorch Lightning 代码逻辑)。在我们的示例中，我们可以在验证步骤中选择`cross_entropy`和`f1`作为度量，它们被命名为`val_cross_entropy`和`val_f1`，分别作为`loss`和`f1`传递回 Ray Tune。这意味着，在 Ray Tune 的试运行中，我们将这两个指标简单地称为`loss`和`f1`。

因此，我们在这里定义了两个指标，我们希望从 PyTorch Lightning 验证步骤`val_cross_entropy`和`val_f1`传递到 Ray Tune，分别为`loss`和`f1`:

```
metrics = {"loss":"val_cross_entropy", "f1":"val_f1"}
```

现在，我们可以将这个指标字典传递给培训师任务，如下所示:

```
trainer = flash.Trainer(max_epochs=num_epochs,
    gpus=num_gpus,
    progress_bar_refresh_rate=0,
    callbacks=[TuneReportCallback(metrics, 
        on='validation_end')])
```

注意，当`validation_end`事件发生时，度量字典通过`TuneReportCallBack`传递。这意味着当 PyTorch Lightning 中的验证步骤完成时，它将自动触发 Ray Tune 报告功能，以将指标列表报告回 Ray Tune 进行评估。供`TuneReportCallback`使用的有效事件的支持列表可以在 Ray Tune 与 PyTorch Lightning 源代码的集成中找到([https://github . com/Ray-project/Ray/blob/FB 0 D6 e 6 b 0 b 48 b 0 a 68171943691405 b 96 fbea 104/python/Ray/Tune/integration/py torch _ Lightning . py # L170](https://github.com/ray-project/ray/blob/fb0d6e6b0b48b0a681719433691405b96fbea104/python/ray/tune/integration/pytorch_lightning.py#L170))。

1.  最后，我们可以调用`trainer.finetune`来执行微调步骤。这里，我们可以将`finetuning_strategies`作为可调超参数之一传递给参数列表:

    ```
    trainer.finetune(classifier_model,     datamodule=datamodule,     strategy=config['finetuning_strategies'])
    ```

2.  这就完成了对微调 DL 模型的原始功能的更改。现在我们有了一个新的`finetuning_dl_model`函数，它准备好被包装在`tune.with_parameters`中，成为一个光线调节可训练函数。应该这样叫:

    ```
    trainable = tune.with_parameters(finetuning_dl_model, data_dir, num_epochs, num_gpus)
    ```

3.  注意，不需要传递`config`参数，因为它隐含地假设它是`finetuning_dl_model`的第一个参数。其他三个参数需要传递给`tune.with_parameters`包装器。此外，确保创建光线调节可训练对象的语句位于`finetuning_dl_model`函数的之外。

在下一节中，它将被放置在 Ray Tune 的名为`run_hpo_dl_model`的 HPO 运行函数中。

## 创建射线调 HPO 运行功能

现在，让我们创建一个雷调 HPO 运行函数来做以下五件事:

*   定义 MLflow 运行时配置参数，包括跟踪 URI 和实验名称。
*   使用 Ray Tune 的随机分布 API([https://docs . Ray . io/en/latest/Tune/API _ docs/search _ space . html # random-distributions-API](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#random-distributions-api))定义超参数搜索空间，以对我们之前确定的超参数列表进行采样。
*   使用`tune.with_parameters`定义一个光线调节可训练对象，如前一小节末尾所示。
*   调用`tune.run`。这将执行 HPO 运行，并在完成后返回 Ray Tune 的实验分析对象。
*   当整个 HPO 运行完成时，记录最佳配置参数。

让我们浏览一下实现，看看这个函数是如何实现的:

1.  首先，让我们定义超参数的`config`字典，如下:

    ```
    mlflow.set_tracking_uri(tracking_uri) mlflow.set_experiment(experiment_name)
    ```

这将把 MLflow 的`tracking_uri`和`experiment_name`作为输入参数，并正确设置它们。如果这是你第一次运行这个，MLflow 也会创建这个实验。

1.  然后，我们可以定义`config`字典，它可以包括可调参数和不可调参数，以及 MLflow 配置参数。正如上一节所讨论的，我们将调优`learning_rate`和`batch_size`，但也将包括其他超参数，用于簿记和未来的调优目的:

    ```
    config = {         "lr": tune.loguniform(1e-4, 1e-1),         "batch_size": tune.choice([32, 64, 128]),         "foundation_model": "prajjwal1/bert-tiny",         "finetuning_strategies": "freeze",         "optimizer_type": "Adam",         "mlflow": {             "experiment_name": experiment_name,             "tracking_uri": mlflow.get_tracking_uri()         },     }
    ```

正如您在`config`字典中看到的，我们调用`tune.loguniform`对`1e-4`和`1e-1`之间的对数均匀分布进行采样，以选择学习率。对于批量大小，我们调用`tune.choice`从三个不同的值中统一选择一个。对于其余的键-值对，它们是不可调的，因为它们不使用任何采样方法，但却是运行试验所必需的。

1.  使用`tune.with_parameters`和除`config`参数之外的所有额外参数定义可训练对象:

    ```
    trainable = tune.with_parameters(     finetuning_dl_model,     data_dir=data_dir,     num_epochs=num_epochs,     num_gpus=gpus_per_trial)
    ```

在下一条语句中，这将被称为`tune.run`函数。

1.  现在我们准备通过调用`tune.run`来运行 HPO，如下:

    ```
    analysis = tune.run(     trainable,     resources_per_trial={         "cpu": 1,         "gpu": gpus_per_trial     },     metric="f1",     mode="max",     config=config,     num_samples=num_samples,     name="hpo_tuning_dl_model")
    ```

这里，目标是找到在所有试验中最大化 F1 分数的一组超参数，因此模式是`max`并且度量是`f1`。请注意，这个度量名称`f1`来自我们在前面的`finetuning_dl_model`函数中定义的`metrics`字典，在这里我们将 PyTorch Lightning 的`val_f1`映射到`f1`。此`f1`值为，然后在每个试验的验证步骤结束时传递给 Ray Tune。`trainable`对象作为第一个参数传递给`tune.run`，在`num_samples`的参数允许的情况下会执行多次。接下来，`resources_per_trial`定义了要使用的 CPU 和 GPU。注意，在前面的例子中，我们没有指定任何搜索算法。这意味着它将默认使用`tune.suggest.basic_variant`，这是一种网格搜索算法。也没有定义调度器，因此，默认情况下，没有提前停止，所有试验都将与执行机器上允许的最大数量的 CPU 并行运行。当运行结束时，返回一个`analysis`变量，它包含找到的最佳超参数，以及和其他信息。

1.  记录找到的最佳超参数配置。这可以通过使用从`tune.run`返回的`analysis`变量来完成，如下:

    ```
    logger.info("Best hyperparameters found were: %s", analysis.best_config)
    ```

就是这样。现在我们可以试一试了。如果你从本章的 GitHub 库下载了完整的代码，你应该可以在`pipeline`文件夹下找到`hpo_finetuning_model.py`文件。

有了前面的更改，现在我们准备运行我们的第一个 HPO 实验。

# 用 MLflow 运行首个射线调 HPO 实验

既然我们已经设置了射线调优、MLflow，并创建了 HPO 运行函数，我们可以尝试运行我们的第一个射线调优 HPO 实验，如下所示:

```
python pipeline/hpo_finetuning_model.py
```

几秒钟后，您将看到以下屏幕，*图 6.2* ，显示所有 10 个试验(即我们为`num_samples`设置的值)同时运行:

![Figure 6.2 – Ray Tune running 10 trials in parallel on a local multi-core laptop
](img/B18120_06_02.jpg)

图 6.2–在本地多核笔记本电脑上并行运行 10 次试验的 Ray Tune

大约 12-14 分钟后，您将看到所有试验都已完成，最佳超参数将打印在屏幕上，如下图所示(由于随机性、样本数量有限以及使用网格搜索，您的结果可能会有所不同，但网格搜索并不能保证全局最优):

```
Best hyperparameters found were: {'lr': 0.025639008922511797, 'batch_size': 64, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}
```

您可以在结果日志目录下找到每次试验的结果，默认情况下，该目录位于当前用户的`ray_results`文件夹中。从*图 6.2* 中，我们可以看到结果在`/Users/yongliu/ray_results/hpo_tuning_dl_model`中。

您将在屏幕上看到最佳超参数的最终输出，这意味着您已经完成了第一次 HPO 实验！您可以看到所有 10 次试验都记录在 MLflow 跟踪服务器中，并且您可以使用 MLflow 跟踪服务器提供的平行坐标图来可视化和比较所有 10 次运行。您可以通过转到 MLflow 实验页面并选择您刚刚完成的 10 个试验，然后单击页面顶部附近的**比较**按钮来生成这样的图(参见*图 6.3* )。这将带您进入并排比较页面，绘图选项显示在页面底部:

![Figure 6.3 – Clicking Compare to compare all 10 trial runs on the MLflow experiment page
](img/B18120_06_03.jpg)

图 6.3–单击“比较”来比较 MLflow 实验页面上的所有 10 次试运行

您可以点击**平行坐标绘图**菜单项，这允许您选择要绘图的参数和度量。这里，我们选择 **lr** 和 **batch_size** 作为参数，选择 **val_f1** 和 **val_cross_entropy** 作为度量。该图见*图 6.4* :

![Figure 6.4 –Parallel Coordinates Plot for comparing the HPO trial results
](img/B18120_06_04.jpg)

图 6.4-比较 HPO 试验结果的平行坐标图

在*图 6.4* 中可以看到，很容易看出 128 的 **batch_size** 和 0.02874 的 **lr** 产生了最好的 **val_f1** 得分 0.6544 和 **val_cross_entropy** (损失值)0.62222。如前所述，这次 HPO 运行没有使用任何高级搜索算法和调度程序，所以让我们看看是否可以通过使用早期停止和修剪的更多实验在下面的部分做得更好。

# 使用 Optuna 和 HyperBand 运行带有光线调节的 HPO

现在，让我们用不同的搜索算法和调度程序做一些实验。考虑到 Optuna 是一个非常棒的基于 TPE 的搜索算法，而 ASHA 是一个优秀的调度器，可以进行异步并行试验，并提前终止那些没有希望的试验，那么看看我们需要做多少改变才能完成这项工作将会非常有趣。

事实证明，根据我们在上一节中所做的工作，变化非常小。在这里，我们将说明四个主要变化:

1.  安装 **Optuna** 组件。这可以通过运行以下命令来完成:

    ```
    pip install optuna==2.10.0
    ```

这将在我们之前拥有的相同虚拟环境中安装 Optuna。如果您已经运行了`pip install -r requirements.text`，那么 Optuna 已经安装好了，您可以跳过这一步。

1.  导入与 Optuna 和 ASHA 调度器集成的相关光线调整模块(这里我们使用 ASHA 的 HyperBand 实现)如下:

    ```
    from ray.tune.suggest import ConcurrencyLimiter from ray.tune.schedulers import AsyncHyperBandScheduler from ray.tune.suggest.optuna import OptunaSearch
    ```

2.  现在我们准备好添加搜索算法变量和调度程序变量到 HPO 执行函数`run_hpo_dl_model`，如下:

    ```
    searcher = OptunaSearch() searcher = ConcurrencyLimiter(searcher, max_concurrent=4) scheduler = AsyncHyperBandScheduler()
    ```

请注意，`searcher`变量现在正在使用 Optuna，我们将这个`searcher`变量的最大并发运行次数设置为`4`，以便在 HPO 搜索过程中的任何给定时间进行尝试。使用 HyperBand 调度程序初始化调度程序。

1.  将搜索器和调度器分配给`tune.run`调用的相应参数，如下所示:

    ```
    analysis = tune.run(     trainable,     resources_per_trial={         "cpu": 1,         "gpu": gpus_per_trial     },     metric="f1",     mode="max",     config=config,     num_samples=num_samples,     search_alg=searcher,     scheduler=scheduler,     name="hpo_tuning_dl_model")
    ```

注意`searcher`被分配给`search_alg`参数，`scheduler`被分配给`scheduler`参数。就是这样。现在，我们已经准备好在统一的 Ray Tune 框架下使用 Optuna 运行 HPO，并集成了 Ray Tune 已经提供的所有 MLflow。

我们已经在`pipeline`文件夹下的`hpo_finetuning_model_optuna.py`文件中提供了完整的 Python 代码。让我们按照下面的来运行这个 HPO 实验:

```
python pipeline/hpo_finetuning_model_optuna.py
```

您将立即在控制台输出中注意到以下内容:

```
[I 2022-02-06 21:01:27,609] A new study created in memory with name: optuna
```

这意味着我们现在使用 Optuna 作为搜索算法。此外，您会注意到屏幕上显示的状态输出中有四个并发试验。随着时间的推移，一些试验会在完成前经过一两次迭代(epochs)后终止。这意味着 ASHA 在工作，他已经取消了那些没有希望的尝试，以节省计算资源和加快搜索过程。*图 6.5* 显示了运行期间的一个输出，其中三次试验仅用一次迭代就终止了。你可以在状态输出中找到`num_stopped=3`(图 6.5 中*的第三行)，那里写着`Using AsynHyerBand: num_stopped=3`。这意味着`AsyncHyperBand`在这三项审判完成之前就终止了它们:*

![Figure 6.5 – Running HPO with Ray Tune using Optuna and AsyncHyperBand 
](img/B18120_06_05.jpg)

图 6.5–使用 Optuna 和异步带运行带有 Ray Tune 的 HPO

运行结束时，您将看到以下结果:

```
2022-02-06 21:11:59,695    INFO tune.py:626 -- Total run time: 632.10 seconds (631.91 seconds for the tuning loop).
2022-02-06 21:11:59,728 Best hyperparameters found were: {'lr': 0.0009599443695046438, 'batch_size': 128, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}
```

请注意，总运行时间只有 10 分钟。与之前使用网格搜索而没有提前停止的部分相比，这节省了 2-4 分钟。现在，这可能看起来很简短，但请记住，我们在这里只使用了一个只有 3 个时期的微型 BERT 模型。在生产 HPO 运行中，使用具有 20 个历元的大型预训练基础模型并不少见，并且使用与调度程序(如异步 HyperBand 调度程序)相结合的良好搜索算法，搜索速度将会非常显著。Ray Tune 提供的 MLflow 集成是免费的，因为我们现在可以在单个框架下切换到不同的搜索算法和/或调度程序。

虽然本节只向您展示了如何在 Ray Tune 和 MLflow 框架中使用 Optuna，但是用 HyperOpt 替换 Optuna 只是一个简单的插入式更改。不用`OptunaSearch`初始化一个搜索器，我们可以用`HyperOptSearch`(可以在[https://github . com/ray-project/ray/blob/d 6 b 0 B9 a 209 E3 f 693 AFA 6441 EB 284 e 48 c 02 b 10a 80/python/ray/tune/examples/hyperopt _ conditional _ search _ space _ example . py # L80](https://github.com/ray-project/ray/blob/d6b0b9a209e3f693afa6441eb284e48c02b10a80/python/ray/tune/examples/hyperopt_conditional_search_space_example.py#L80)看到一个例子)，其余的代码都是一样的。我们把这个作为一个练习留给你去探索。

使用不同的搜索算法和调度器进行光线调节

请注意，不是所有的搜索算法都可以与任何调度程序一起工作。选择什么样的搜索算法和调度器取决于模型复杂度和评估成本。对于 DL 模型，由于运行一个历元的成本通常很高，因此非常需要使用现代搜索算法，如 TPE、蜻蜓和 BlendSearch，以及 ASHA 类型的调度程序，如我们使用的 HyperBand 调度程序。有关使用哪些搜索算法和调度程序的更多详细指导，请参考 Ray Tune 网站上的以下文档:[https://docs . Ray . io/en/latest/Tune/tutorials/overview . html # which-search-algorithm-scheduler-should-I-choose](https://docs.ray.io/en/latest/tune/tutorials/overview.html#which-search-algorithm-scheduler-should-i-choose)。

现在，我们已经了解了如何使用 Ray Tune 和 MLflow 对 DL 模型进行高度并行和高效的 HPO，这为我们在未来进行更高级的大规模 HPO 实验奠定了基础。

# 总结

在本章中，我们介绍了 HPO 的基础和挑战，为什么它对 DL 模型管道如此重要，以及现代 HPO 框架应该支持什么。我们比较了三种流行的框架——Ray Tune、Optuna 和 HyperOpt——并选择 Ray Tune 作为大规模运行最先进的 HPO 的赢家。我们看到了如何使用 Ray Tune 和 MLflow 创建 HPO 就绪的 DL 模型代码，并使用 Ray Tune 和 MLflow 运行了我们的第一个 HPO 实验。此外，我们以 Optuna 和 HyperBand 调度器为例，介绍了一旦建立了 HPO 代码框架，如何切换到其他搜索和调度算法。本章的学习将帮助您在现实生产环境中胜任地进行大规模 HPO 实验，从而以经济高效的方式生产出高性能的 DL 模型。我们还在本章末尾的*进一步阅读*部分提供了许多参考资料，以鼓励你进一步学习。

在我们的下一章中，我们将继续学习如何使用 MLflow 为模型推理管道构建预处理和后处理步骤，这是一个真实生产环境中的典型场景，在此之前，已经为生产准备好了 HPO 调优的 DL 模型。

# 进一步阅读

*   *模型调优和超参数优化的最佳工具*:[https://Neptune . ai/blog/Best-Tools-for-Model-Tuning-and-Hyperparameter-Optimization](https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization%20)
*   Optuna 和远视的对比:[https://neptune.ai/blog/optuna-vs-hyperopt](https://neptune.ai/blog/optuna-vs-hyperopt)
*   *如何(不)用 Hyperopt 调优你的模型*:[https://data bricks . com/blog/2021/04/15/How-Not-to-Tune-Your-Model-with-Hyperopt . html](https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html%20)
*   *为什么超参数调整对您的模型很重要？*:[https://medium . com/analytics-vid hya/why-hyper-parameter-tuning-is-important-for-your-model-1 ff 4c 8 f 145d 3](https://medium.com/analytics-vidhya/why-hyper-parameter-tuning-is-important-for-your-model-1ff4c8f145d3)
*   *深度神经网络超参数整定的艺术举例*:[https://towardsdatascience . com/The-Art-of-Hyperparameter-Tuning-in-Deep-Neural-Nets-by-Example-685 CB 5429 a38](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38)
*   *自动超参数调优*:[https://in said . medium . com/Automated-Hyperparameter-tuning-988 b5 aeb 7 F2 a](https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a)
*   *更好地利用闪电和光线调整构建 PyTorch 模型*:[https://towardsdatascience . com/Get-better-at-building-py torch-models-with-Lightning and-Ray-Tune-9fc 39 b 84 e 602](https://towardsdatascience.com/get-better-at-building-pytorch-models-with-lightning-and-ray-tune-9fc39b84e602)
*   *Ray & MLflow:将分布式机器学习应用用于生产*:[https://medium . com/Distributed-computing-with Ray/Ray-ml flow-Taking Distributed-Machine-Learning-Applications-to-Production-103 f 5505 cb88](https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88)
*   *大规模超参数优化新手指南*:[https://wood-b . github . io/post/A-novices-Guide-to-Hyperparameter-Optimization-at-Scale/](https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/)
*   在 Databricks 集群上运行 Ray Tune 和 MLflow 的 Databricks 笔记本:[https://data bricks-prod-cloudfront . cloud . data bricks . com/public/4027 EC 902 e 239 c 93 eaaa 8714 f 173 bcfc/6762389964551879/1089858099311442/7376217192554178/latest . html](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6762389964551879/1089858099311442/7376217192554178/latest.html)
*   *光线分布对象简介，光线调优，与 Parsl 的小对比*:[https://cloud 4 scieng . org/2021/04/08/A-Brief-Introduction-to-Ray-Distributed-Objects-Ray-Tune-and-A-Small-Comparison-to-Parsl/](https://cloud4scieng.org/2021/04/08/a-brief-introduction-to-ray-distributed-objects-ray-tune-and-a-small-comparison-to-parsl/)