<html><head/><body>




<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style>
<div><div><h1 id="_idParaDest-128"><em class="italic"> <a id="_idTextAnchor127"/>第十章</em>:用MLflow实现DL可解释性</h1>
			<p>正如我们在前一章所了解到的，深度学习 ( <strong class="bold"> DL </strong>)可解释性的重要性现在已经被很好地确立了。为了在现实世界的项目中实现DL可解释性，最好将解释者和解释作为工件记录下来，就像MLflow服务器中的其他模型工件一样，这样我们就可以轻松地跟踪和再现解释。DL可解释性工具的集成，如SHAP(<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>)与MLflow可以支持不同的实现机制，理解这些集成如何用于我们的DL可解释性场景是很重要的。在本章中，我们将通过使用不同的MLflow功能来探索将SHAP解释集成到MLflow中的几种方法。由于可解释性工具和DL模型都在快速发展，我们也将强调使用MLflow实现DL可解释性时当前的局限性和解决方法。在本章结束时，你将会对使用MLflow APIs实现可扩展模型解释能力的SHAP解释器和解释器感到舒适。</p>
			<p>在本章中，我们将讨论以下主要话题:</p>
			<ul>
				<li>了解当前MLflow可解释性集成</li>
				<li>使用MLflow工件日志记录API实现SHAP解释</li>
				<li>使用MLflow pyfunc API实现SHAP解释器</li>
			</ul>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor128"/>技术要求</h1>
			<p>完成本章需要满足以下要求:</p>
			<ul>
				<li>MLflow成熟的本地服务器:这是我们从<a href="B18120_03_ePub.xhtml#_idTextAnchor040"> <em class="italic">第3章</em> </a>、<em class="italic">跟踪模型、参数和指标</em>以来一直使用的服务器。</li>
				<li>https://github.com/slundberg/shap SHAP Python库:<a href="https://github.com/slundberg/shap"/>。</li>
				<li>Spark 3.2.1和PySpark 3.2.1:详见本章GitHub库的<code>README.md</code>文件。</li>
				<li>本章代码来自GitHub知识库:<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 10</a>。</li>
			</ul>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/>了解当前MLflow可解释集成</h1>
			<p>MLflow有几种方法来支持可解释性集成。当实现可解释性时，我们提到两种类型的工件:解释者<a id="_idIndexMarker615"/>和解释:</p>
			<ul>
				<li>解释器<a id="_idIndexMarker616"/>是<a id="_idIndexMarker617"/>可解释性模型，常见的<a id="_idIndexMarker618"/>是SHAP模型，可以是不同种类的SHAP解释器，如<strong class="bold"> TreeExplainer </strong>、<strong class="bold"> KernelExplainer </strong>和<strong class="bold">partition explainer</strong>(<a href="https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html">https://shap . readthedocs . io/en/latest/generated/shap . explainers . html</a>)。为了计算效率，我们通常为DL模型选择<strong class="bold"> PartitionExplainer </strong>。</li>
				<li>解释是展示解释者某种形式的输出的工件，可以是文本、数值或图表。解释可能发生在离线培训或测试中，也可能发生在在线生产中。因此，如果我们想知道为什么模型提供某些预测，我们应该能够为离线评估提供一个解释器，或者为在线查询提供一个解释器端点。</li>
			</ul>
			<p>在此，我们简要概述了从MLflow版本1.25.1开始的当前能力(【https://pypi.org/project/mlflow/1.25.1/】T21)。使用MLflow进行解释有四种不同的方式，如下所示:</p>
			<ul>
				<li>使用<a id="_idIndexMarker621"/>的<code>mlflow.log_artifact</code>API(<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact">https://www . ml flow . org/docs/latest/python _ API/ml flow . html # ml flow . log _ artifact</a>)来记录相关的解释工件，例如条形图和Shapley值数组。这为日志解释提供了最大的灵活性。这既可以作为批处理离线使用，也可以在我们为某个预测自动记录SHAP条形图时在线使用。请注意，在在线生产场景中记录每个预测的解释是非常昂贵的，因此我们应该为按需查询提供单独的解释API。</li>
				<li>使用<a id="_idIndexMarker622"/>的<code>mlflow.pyfunc.PythonModel</code>API(<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">https://www . mlflow . org/docs/latest/python _ API/ml flow . py func . html # ml flow . py func . python model</a>)创建一个解释器，可以使用ml flow的<code>pyfunc</code>方法、<code>mlflow.pyfunc.log_model</code>进行日志记录，使用<code>mlflow.pyfunc.load_model</code>或<code>mlflow.pyfunc.spark_udf</code>加载一个解释器。这给了我们最大的灵活性来创建定制的解释器作为MLflow通用<code>pyfunc</code>模型，并且可以用于离线批处理<a id="_idIndexMarker623"/>解释<a id="_idIndexMarker624"/>或者在线作为<strong class="bold">解释作为服务</strong> ( <strong class="bold"> EaaS </strong>)。</li>
				<li>使用<a id="_idIndexMarker625"/>的<code>mlflow.shap</code>API(<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html">https://www . ml flow . org/docs/latest/python _ API/ml flow . shap . html</a>)。这有一定的局限性。例如，<code>mlflow.shap.log_explainer</code>方法只支持scikit-learn和PyTorch模型。<code>mlflow.shap.log_explanation</code>方法只支持<code>shap.KernelExplainer</code>(<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html">https://shap-lrjball . readthedocs . io/en/latest/generated/shap。KernelExplainer.html</a>)。这是非常计算密集型的，因为计算时间相对于特征的数量呈指数增长；因此，即使是中等大小的数据集，也不可能计算出解释(见GitHub发布的一期<a href="https://github.com/mlflow/mlflow/issues/4071">https://github.com/mlflow/mlflow/issues/4071</a>)。MLflow提供的现有示例针对scikit-learn包中的经典ML模型，如线性回归或随机森林，没有DL模型可解释性示例(<a href="https://github.com/mlflow/mlflow/tree/master/examples/shap">https://github.com/mlflow/mlflow/tree/master/examples/shap</a>)。我们将在本章的后面部分展示这个API目前不支持基于变形金刚的SHAP解释器和解说，因此我们不会在本章使用这个API。我们将在本章的例子中强调一些问题。</li>
				<li>使用<a id="_idIndexMarker626"/>的<code>mlflow.evaluate</code>API(<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate">https://www . ml flow . org/docs/latest/python _ API/ml flow . html # ml flow . evaluate</a>)。这可以在模型已经被训练和测试之后用于评估。这是一个实验性的特性，将来可能会改变。它支持MLflow <code>pyfunc</code>型号。但是，它有一些限制，评估数据集标签值必须是数字或布尔值，所有特征值必须是数字，每个特征列必须只包含标量值(<a href="https://www.mlflow.org/docs/latest/models.html#model-evaluation">https://www . ml flow . org/docs/latest/models . html # model-evaluation</a>)。还是那句话，MLflow提供的现有例子只针对scikit-learn包中的经典ML模型(<a href="https://github.com/mlflow/mlflow/tree/master/examples/evaluation">https://github . com/ML flow/ML flow/tree/master/examples/evaluation</a>)。我们可以使用这个API来记录NLP情感模型的分类器指标，但是这个API会自动跳过解释部分，因为它需要一个包含标量值的特征列(NLP模型输入是一个文本输入)。因此，这不适用于我们需要的DL模型可解释性。所以，我们不会在本章中使用这个API。</li>
			</ul>
			<p>考虑到这些API中的一些仍然是实验性的，并且仍然在发展中，用户应该意识到使用MLflow成功实现可解释性的局限性和变通方法。对于<a id="_idIndexMarker627"/> DL模型的可解释性，正如我们将在本章中了解到的，使用MLflow实现相当具有挑战性，因为从MLflow版本1.25.1起，MLflow与SHAP的集成仍在进行中。在接下来的部分中，我们将学习何时以及如何使用这些不同的API来实现解释，并为DL模型记录和加载解释器。</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>使用MLflow工件记录API实现SHAP解释</h1>
			<p>MLflow <a id="_idIndexMarker628"/>有一个通用的<a id="_idIndexMarker629"/>跟踪API，可以记录任何工件:<code>mlflow.log_artifact</code>。然而，MLflow文档中给出的示例通常使用scikit-learn和表格数字数据进行训练、测试和解释。在这里，我们想要展示如何为NLP感性DL模型使用<code>mlflow.log_artifact</code>来记录相关的工件，比如Shapley值数组和Shapley值条形图。您可以在本章的GitHub资源库(<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ log _ artifact . py</a>)中查看Python VS Code笔记本<code>shap_mlflow_log_artifact.py</code>，并按照以下步骤进行操作:</p>
			<ol>
				<li>确保您已经准备好了先决条件，包括本地成熟的MLflow服务器和conda虚拟环境。按照<a href="B18120_10_ePub.xhtml#_idTextAnchor127"> <em class="italic">第10章</em> </a>文件夹中<code>README.md</code>(<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md">https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/Chapter 10/readme . MD</a>)文件中的说明做好这些准备。</li>
				<li>在开始运行本章中的任何代码之前，确保您激活了如下的<code>chapter10-dl-explain</code>虚拟环境:<pre><strong class="bold">conda activate chapter10-dl-explain</strong></pre></li>
				<li>导入笔记本开头的相关库如下:<pre>import os import matplotlib.pyplot as plt import mlflow from mlflow.tracking import MlflowClient from mlflow.utils.file_utils import TempDir import shap import transformers from shap.plots import * import numpy as np</pre></li>
				<li>下一步是设置一些环境变量。前三个环境变量用于本地MLflow URIs，第四个用于禁用由于已知的拥抱脸令牌化问题而产生的拥抱脸警告:<pre>os.environ["AWS_ACCESS_KEY_ID"] = "minio" os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123" os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://localhost:9000" os.environ["TOKENIZERS_PARALLELISM"] = "False"</pre></li>
				<li>我们<a id="_idIndexMarker630"/>还需要<a id="_idIndexMarker631"/>设置MLflow实验，并在屏幕上显示MLflow实验ID作为输出:<pre>EXPERIMENT_NAME = "dl_explain_chapter10" mlflow.set_tracking_uri('http://localhost') mlflow.set_experiment(EXPERIMENT_NAME) experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME) print("experiment_id:", experiment.experiment_id)</pre></li>
			</ol>
			<p>如果您一直在运行笔记本，您应该会看到如下输出:</p>
			<pre><strong class="bold">experiment_id: 14</strong></pre>
			<p>这意味着实验名<code>dl_explain_chapter10</code>的MLflow实验ID是<code>14</code>。请注意，您也可以将MLflow追踪URI设置为环境变量，如下所示:</p>
			<pre><strong class="bold">export MLFLOW_TRACKING_URI=http://localhost</strong></pre>
			<p>这里，我们使用MLflow的<code>mlflow.set_tracking_uri</code> API来定义URI的位置。哪种方式都可以。</p>
			<ol>
				<li value="6">现在，我们可以使用Hugging Face的transformer pipeline API创建一个DL模型，将一个句子分为积极情绪或消极情绪。因为这已经被微调了，我们将关注于如何获得<a id="_idIndexMarker632"/>和<a id="_idIndexMarker633"/>模型的解释者和说明，而不是关注于如何训练或微调一个模型:<pre>dl_model = transformers.pipeline('sentiment-analysis', return_all_scores=False) explainer = shap.Explainer(dl_model) shap_values = explainer(["Not a good movie to spend time on.", "This is a great movie."])</pre></li>
			</ol>
			<p>代码片段创建了一个情感分析模型<code>dl_model</code>，然后为这个模型创建了一个SHAP <code>explainer</code>。然后我们为这个解释者提供一个两句话的列表来获取<code>shap_values</code>对象。这将用于登录MLflow。</p>
			<p>给定<code>shap_values</code>对象，我们现在可以开始新的MLflow运行，并记录我们在上一章中看到的Shapley值和条形图(<a href="B18120_09_ePub.xhtml#_idTextAnchor112"> <em class="italic">第9章</em> </a> <em class="italic">，深度学习可解释性的基础</em>)。第一行代码确保所有活动的MLflow运行都已结束。如果我们希望以交互方式多次重新运行这段代码，这将非常有用:</p>
			<pre>mlflow.end_run()</pre>
			<p>然后我们定义两个常数。一个是<code>artifact_root_path</code>，用于MLflow工件存储中的根路径，它将用于存储所有SHAP解释对象。另一个是<code>shap_bar_plot</code>，用于工件文件名，将用于条形图:</p>
			<pre>artifact_root_path = "model_explanations_shap"
artifact_file_name = 'shap_bar_plot'</pre>
			<ol>
				<li value="7">然后，我们开始一个新的MLflow运行，在这个运行下，我们将生成三个SHAP文件并将其记录到MLflow工件存储中的路径<code>model_explanations_shap</code> : <pre>with mlflow.start_run() as run:    with TempDir() as temp_dir:         temp_dir_path = temp_dir.path()         print("temp directory for artifacts: {}".format(temp_dir_path))</pre>下</li>
			</ol>
			<p>我们还需要<a id="_idIndexMarker634"/>到<a id="_idIndexMarker635"/>有一个临时的本地目录，如前面的代码片段所示，首先保存SHAP文件，然后将这些文件记录到MLflow服务器。如果到目前为止您已经运行了笔记本，您应该会在输出中看到一个临时目录，如下所示:</p>
			<pre><strong class="bold">temp directory for artifacts: /var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpgw520wu1</strong></pre>
			<ol>
				<li value="8">现在我们准备生成SHAP文件并保存它们。第一个是条形图，保存和记录有点棘手。让我们通过下面的代码来理解我们是如何做到这一点的:<pre>try:      plt.clf()      plt.subplots_adjust(bottom=0.2, left=0.4)      shap.plots.bar(shap_values[0, :, "NEGATIVE"],                     show=False)      plt.savefig(f"{temp_dir_path}/{artifact_file_name}") finally:      plt.close(plt.gcf()) mlflow.log_artifact(f"{temp_dir_path}/{artifact_file_name}.png", artifact_root_path)</pre></li>
			</ol>
			<p>请注意，我们使用的是作为<code>plt</code>导入的<code>matplotlib.pyplot</code>，首先使用<code>plt.clf()</code>清除图形，然后创建一个带有一些调整的子情节。在这里，我们定义了<code>bottom=0.2</code>，这意味着支线剧情底边的位置在人物高度的20%。同样，我们调整支线剧情的左边缘。然后，我们使用<code>shap.plots.bar</code> SHAP API来绘制第一句对预测的特征贡献的条形<a id="_idIndexMarker636"/>图<a id="_idIndexMarker637"/>，但是将<code>show</code>参数设为<code>False</code>。这意味着，我们在交互运行中看不到图形，但是图形存储在pyplot <code>plt</code>变量中，然后可以使用<code>plt.savefig</code>保存到文件名前缀为<code>shap_bar_plot</code>的本地临时目录中。<code>pyplot</code>会在文件保存后自动添加文件扩展名<code>.png</code>。因此，这将在临时文件夹中保存一个名为<code>shap_bar_plot.png</code>的本地图像文件。最后一条语句调用MLflow的<code>mlflow.log_artifact</code>将这个PNG文件上传到MLflow跟踪服务器的工件存储在根文件夹<code>model_explanations_shap</code>中。我们还需要确保通过调用<code>plt.close(plt.gcf())</code>来结束当前的数字。</p>
			<ol>
				<li value="9">除了将<code>shap_bar_plot.png</code>记录到MLflow服务器，我们还希望将Shapley <code>base_values</code>数组和<code>shap_values</code>数组作为NumPy数组记录到MLflow track服务器。这可以通过下面的语句来完成:<pre>np.save(f"{temp_dir_path}/shap_values",          shap_values.values) np.save(f"{temp_dir_path}/base_values",          shap_values.base_values)         mlflow.log_artifact(             f"{temp_dir_path}/shap_values.npy",              artifact_root_path)         mlflow.log_artifact(             f"{temp_dir_path}/base_values.npy",              artifact_root_path)      </pre></li>
			</ol>
			<p>这将首先<a id="_idIndexMarker638"/>在本地临时文件夹中保存<code>shap_values.npy</code>和<code>base_values.npy</code>的本地副本<a id="_idIndexMarker639"/>，然后将其上传到MLflow跟踪服务器的工件存储。</p>
			<ol>
				<li value="10">如果您跟踪笔记本直到这里，您应该能够在本地MLflow服务器中验证这些工件是否被成功存储。转到localhost–<code>http://localhost/</code>的MLflow UI，然后找到实验<code>dl_explain_chapter10</code>。然后，您应该能够找到刚刚运行的实验。它看起来应该类似于<em class="italic">图10.1 </em>，在这里你可以在<code>model_explanations_shap</code>文件夹中找到三个文件:<code>base_values.npy</code>、<code>shap_bar_plot.png</code>和<code>shap_values.npy</code>。<em class="italic">图10.1 </em>显示了不同的表征或词对句子的预测结果的特征贡献的柱状图——T12。这个实验页面的URL如下所示:<pre>http://localhost/#/experiments/14/runs/10f0655189f740aeb813a015f1f6e115</pre></li>
			</ol>
			<div><div><img src="img/B18120_10_001.jpg" alt="Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image &#13;&#10;in the MLflow tracking server&#13;&#10;" width="1391" height="859"/>
				</div>
			</div>
			<p class="figure-caption">图10.1–MLflow log _ artifact API将SHAP条形图保存为ml flow跟踪服务器中的图像</p>
			<p>或者，您也可以使用代码以编程方式下载这些存储在<a id="_idIndexMarker640"/> MLflow <a id="_idIndexMarker641"/>跟踪服务器中的文件，并在本地检查它们。我们在笔记本的最后一个单元格中提供了这样的代码。</p>
			<ol>
				<li value="11">如果您运行笔记本代码的最后一个单元块，即从我们刚刚保存的MLflow服务器下载三个文件并将其打印出来，您应该能够看到以下输出，如图<em class="italic">图10.2 </em>所示。从MLflow跟踪服务器下载工件的机制是使用<code>MlflowClient().download_artifacts</code> API，其中您提供MLflow运行ID(在我们的例子中是<code>10f0655189f740aeb813a015f1f6e115</code>)和工件根路径<code>model_explanations_shap</code>作为API的参数:<pre>downloaded_local_path = MlflowClient().download_artifacts(run.info.run_id, artifact_root_path)</pre></li>
			</ol>
			<p>这将把MLflow tracking服务器上<code>model_explanations_shap</code>中的所有文件下载到<a id="_idIndexMarker642"/>一个<a id="_idIndexMarker643"/>本地路径，这是返回变量<code>downloaded_local_path</code>:</p>
			<div><div><img src="img/B18120_10_002.jpg" alt="Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow tracking server to a local path and display them&#13;&#10;" width="842" height="603"/>
				</div>
			</div>
			<p class="figure-caption">图10.2–将SHAP基值和形状值数组从MLflow追踪服务器下载到本地路径并显示它们</p>
			<p>为了显示这两个NumPy数组，我们需要调用NumPy的<code>load</code> API来加载它们，然后打印它们:</p>
			<pre>base_values = np.load(os.path.join(downloaded_local_path, "base_values.npy"), allow_pickle=True)
shap_values = np.load(os.path.join(downloaded_local_path, "shap_values.npy"), allow_pickle=True)</pre>
			<p>注意，在调用<code>np.load</code> API时，我们需要将<code>allow_pickle</code>参数设置为<code>True</code>，以便NumPy可以正确地将这些文件加载回内存。</p>
			<p>虽然您可以在VS代码环境中以交互方式运行这个笔记本，但是您也可以在命令行中运行它，如下所示:</p>
			<pre><strong class="bold">python shap_mlflow_log_artifact.py</strong></pre>
			<p>这将在控制台中生成所有输出，并将所有工件记录到MLflow服务器中，正如我们在笔记本的交互式运行中看到的那样。</p>
			<p>如果到目前为止您已经运行了<a id="_idIndexMarker644"/>代码<a id="_idIndexMarker645"/>，那么恭喜您使用MLflow的<code>mlflow.log_artifact</code> API成功实现了对MLflow跟踪服务器的日志记录SHAP解释！</p>
			<p>尽管记录所有解释的过程似乎有点长，但是这种方法确实具有不依赖于所使用的解释器类型的优点，因为解释器是在MLflow工件记录API之外定义的。</p>
			<p>在下一节中，我们将看到如何使用内置的<code>mlflow.pyfunc.PythonModel</code> API将一个SHAP解释器记录为一个MLflow模型，然后部署为一个端点或者以批处理模式使用它，就像它是一个通用的MLflow <code>pyfunc</code>模型一样。</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>使用MLflow pyfunc API实现SHAP解释器</h1>
			<p>正如我们从上一节<a id="_idIndexMarker647"/>中了解到的那样，通过使用SHAP API创建一个解释器的新实例，SHAP解释器可以在任何需要的时候离线使用。然而，由于底层的DL模型经常被记录到MLflow服务器中，所以最好也将相应的解释器记录到MLflow服务器中，这样我们不仅可以跟踪DL模型，还可以跟踪它们的解释器。此外，我们可以为解释器使用通用的MLflow pyfunc模型日志和加载API，从而统一对DL模型及其解释器的访问。</p>
			<p>在本节中，我们将逐步学习如何将SHAP解释器实现为通用MLflow pyfunc模型，以及如何使用它进行离线和在线解释。我们将把这个过程分成三个部分:</p>
			<ul>
				<li>创建和记录MLflow pyfunc解释器</li>
				<li>为EaaS部署MLflow pyfunc解释器</li>
				<li>使用MLflow pyfunc解释器进行批处理解释</li>
			</ul>
			<p>让我们从创建和记录MLflow pyfunc解释器的第一小节开始。</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>创建和记录MLflow pyfunc解释器</h2>
			<p>为了让<a id="_idIndexMarker648"/>了解本节内容，请查看GitHub知识库中的<code>nlp_sentiment_classifier_explainer.py</code>(<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/pipeline/NLP _ sensation _ classifier _ explainer . py</a>):</p>
			<ol>
				<li value="1">首先，通过子类化<code>mlflow.pyfunc.PythonModel</code>，我们可以创建一个定制的MLflow模型来封装一个SHAP解释器。所以，让我们将这个类声明如下:<pre>class SentimentAnalysisExplainer(mlflow.pyfunc.PythonModel):</pre></li>
				<li>接下来，我们需要实例化一个解释器。我们将使用<code>load_context</code>方法为拥抱脸NLP情感分析分类器加载一个SHAP解释器，而不是在这个类的<code>init</code>方法中创建一个解释器，如下:<pre>def load_context(self, context):   from transformers import pipeline   import shap   self.explainer = shap.Explainer(pipeline('sentiment-analysis', return_all_scores=True))</pre></li>
			</ol>
			<p>每当这个<code>SentimentAnalysisExplainer</code>类被执行时，这将创建一个SHAP解释器。请注意，情感分类器<a id="_idIndexMarker650"/>是一个拥抱面部管道<a id="_idIndexMarker651"/>对象，其<code>return_all_scores</code>参数设置为<code>True</code>。这意味着这将返回每个输入文本的正面和负面情绪的标签和概率分数。</p>
			<p class="callout-heading">为SHAP解释者避免运行时错误</p>
			<p class="callout">如果我们<a id="_idIndexMarker652"/>在这个类的<code>init</code>方法中实现<code>self.explainer</code>，我们会遇到一个与SHAP包的<code>_masked_model.py</code>文件相关的运行时错误，该文件抱怨<code>init</code>方法会被MLflow序列化，所以很明显这个运行时错误来自于MLflow的序列化。然而，在<code>load_context</code>函数中实现<code>self.explainer</code>避免了MLflow的序列化，并且在运行时调用这个解释器时工作正常。</p>
			<ol>
				<li value="3">然后我们将实现<code>sentiment_classifier_explanation</code>方法，该方法接受一个pandas DataFrame行的输入，并生成一个pickled <code>shap_values</code>输出，作为单行文本输入的解释:<pre>def sentiment_classifier_explanation(self, row):   shap_values = self.explainer([row['text']])   return [pickle.dumps(shap_values)]</pre></li>
			</ol>
			<p>注意，我们需要用一对方括号将<code>row['text'] </code>值括起来，这样它就成为一个列表，而不仅仅是一个值。这是因为这个SHAP解释器需要一个文本列表，而不仅仅是一个字符串。如果我们不将值括在方括号内，那么解释器将一个字符一个字符地拆分整个字符串，将每个字符视为一个单词，这不是我们想要的。一旦我们从解释器得到Shapley值作为输出作为<code>shap_values</code>，我们就需要在返回给调用者之前使用<code>pickle.dumps</code>序列化它们。MLflow pyfunc模型输入和输出签名不支持无序列化的复杂对象，因此该酸洗步骤<a id="_idIndexMarker653"/>确保模型输出签名符合MLflow。我们将很快在<em class="italic">步骤5 </em>中看到这个MLflow pyfunc解释器的输入和输出签名的定义。</p>
			<ol>
				<li value="4">接下来，我们需要为这个类实现所需的<code>predict</code>方法。这将把<code>sentiment_classifier_explanation</code>方法应用于整个输入熊猫数据帧，如下:<pre>def predict(self, context, model_input):   model_input[['shap_values']] = model_input.apply(     self.sentiment_classifier_explanation, axis=1,      result_type='expand')   model_input.drop(['text'], axis=1, inplace=True)   return model_input</pre></li>
			</ol>
			<p>这将在<code>text</code>列中为输入pandas数据帧的每一行生成一个名为<code>shap_values</code>的新列。然后我们删除<code>text</code>列并返回一个单列的<code>shap_values</code>数据帧作为最终的预测结果:在本例中，解释结果是一个数据帧。</p>
			<ol>
				<li value="5">现在我们已经有了<code>SentimentAnalysisExplainer</code>类实现，我们可以使用标准的MLflow pyfunc模型日志API将这个模型记录到MLflow跟踪服务器中。在进行MLflow日志记录之前，让我们确保声明了这个解释器的模型签名，如下:<pre>input = json.dumps([{'name': 'text', 'type': 'string'}]) output = json.dumps([{'name': 'shap_values', 'type': 'string'}]) signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})</pre></li>
			</ol>
			<p>这些<a id="_idIndexMarker655"/>语句<a id="_idIndexMarker656"/>声明输入是具有单个<code>string</code>类型<code>text</code>列的数据帧，输出是具有单个<code>string</code>类型<code>shap_values</code>列的数据帧。回想一下，这个<code>shap_values</code>列是一个经过酸洗的序列化字节字符串，它包含Shapley值对象。</p>
			<ol>
				<li value="6">最后，我们可以在任务方法中使用<code>mlflow.pyfunc.log_model</code>方法来实现解释者日志记录步骤，如下:<pre>with mlflow.start_run() as mlrun:             mlflow.pyfunc.log_model(     artifact_path=MODEL_ARTIFACT_PATH,      conda_env=CONDA_ENV,                                python_model=SentimentAnalysisExplainer(),      signature=signature)</pre></li>
			</ol>
			<p>我们使用的<code>log_model</code>方法中有四个参数。<code>MODEL_ARTIFACT_PATH</code>是MLflow跟踪服务器中存储解释器的文件夹的名称。这里，该值在您签出的Python文件中被定义为<code>nlp_sentiment_classifier_explainer</code>。<code>CONDA_ENV</code>是本章根文件夹中的<code>conda.yaml</code>文件。<code>python_model</code>参数是我们刚刚实现的<code>SentimentAnalysisExplainer</code>类，<code>signature</code>是我们定义的解释器输入和输出签名。</p>
			<ol>
				<li value="7">现在我们准备在命令行中运行整个文件，如下所示:<pre><strong class="bold">python nlp_sentiment_classifier_explainer.py</strong></pre></li>
			</ol>
			<p>假设您已经按照GitHub存储库中本章的<code>README.md</code>文件正确设置了本地MLflow跟踪服务器和环境变量，这个<a id="_idIndexMarker657"/>将在控制台输出中产生下面两行<a id="_idIndexMarker658"/>:</p>
			<pre><strong class="bold">2022-05-11 17:49:32,181 Found credentials in environment variables.</strong>
<strong class="bold">2022-05-11 17:49:32,384 finished logging nlp sentiment classifier explainer run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9</strong></pre>
			<p>这意味着我们已经成功地在本地MLflow跟踪服务器中记录了explainer。</p>
			<ol>
				<li value="8">在网络浏览器中进入位于<code>http://localhost/</code>的MLflow网络界面，点击<code>dl_explain_chapter10</code>实验文件夹。你应该可以在<code>nlp_sentiment_classifier_explainer</code>下的<code>Artifacts</code>文件夹中找到这次运行和记录的解释者，应该如图<em class="italic">图10.3 </em>所示:</li>
			</ol>
			<div><div><img src="img/B18120_10_003.jpg" alt="Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model &#13;&#10;" width="1224" height="558"/>
				</div>
			</div>
			<p class="figure-caption">图10.3-SHAP解释器被记录为MLflow pyfunc模型</p>
			<p>请注意，<em class="italic">图10.3 </em>中显示的<code>MLmodel</code>元数据与我们之前记录为MLflow pyfunc <a id="_idIndexMarker659"/>模型<a id="_idIndexMarker660"/>的普通DL推理管道没有太大区别，除了<code>artifact_path</code>名称和<code>signature</code>。这就是使用这种方法的优势，因为现在我们可以使用通用的MLflow pyfunc模型方法来加载这个解释器或将其部署为服务。</p>
			<p class="callout-heading">mlflow.shap.log_explainer API的问题</p>
			<p class="callout">正如我们前面提到的<a id="_idIndexMarker661"/>，MLflow有一个<code>mlflow.shap.log_explainer</code> API，它提供了一个记录解释器的方法。然而，这个API不支持我们的NLP情感分类器解释器，因为我们的NLP管道不是MLflow当前支持的已知模型风格。因此，即使<code>log_explainer</code>可以将这个解释器对象写入跟踪服务器，当使用<code>mlflow.shap.load_explainer</code> API将解释器加载回内存时，它也会失败，并显示以下错误消息:本书中的<code>mlflow.shap.log_explainer</code> API。</p>
			<p>现在我们有了一个已记录的解释器，我们可以以两种方式使用它:将它部署到web服务中，这样我们就可以创建一个端点来建立EaaS，或者使用MLflow <code>run_id</code>通过<a id="_idIndexMarker662"/> MLflow <a id="_idIndexMarker663"/> pyfunc <code>load_model</code>或<code>spark_udf</code>方法直接加载解释器。让我们通过设置本地web服务来开始web服务部署。</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>为EaaS部署MLflow pyfunc解释器</h2>
			<p>我们可以用标准的MLflow方式设置本地EaaS，因为现在SHAP解释器就像一个通用的MLflow pyfunc模型。执行以下步骤，了解如何在本地实现这一点:</p>
			<ol>
				<li value="1">运行下面的MLflow命令，为我们刚刚记录的解释器设置一个本地web服务。本例中的<code>run_id</code>为<code>ad1edb09e5ea4d8ca0332b8bc2f5f6c9</code> : <pre><strong class="bold">mlflow models serve -m runs:/ ad1edb09e5ea4d8ca0332b8bc2f5f6c9/nlp_sentiment_classifier_explainer</strong></pre></li>
			</ol>
			<p>这将产生以下控制台输出:</p>
			<div><div><img src="img/B18120_10_004.jpg" alt=" Figure 10.4 – SHAP EaaS console output&#13;&#10;" width="1465" height="249"/>
				</div>
			</div>
			<p class="figure-caption">图10.4–SHAP EaaS控制台输出</p>
			<p>注意在<em class="italic">图10.4 </em>中，默认的底层预训练语言模型是在<code>gunicore</code> HTTP服务器启动并运行后加载的。这是因为我们的解释器实现在<code>load_context</code>方法中，这正是我们所期望的:在web服务启动并运行后立即加载解释器。</p>
			<ol>
				<li value="2">在另一个终端窗口中，键入以下命令，在本地主机的端口<code>5000</code>调用explainer web服务，输入两个示例文本:<pre><strong class="bold">curl -X POST -H "Content-Type:application/json; format=pandas-split" --data '{"columns":["text"],"data":[["This is meh weather"], ["This is great weather"]]}' http://127.0.0.1:5000/invocations</strong></pre></li>
			</ol>
			<p>这将使<a id="_idIndexMarker666"/>产生<a id="_idIndexMarker667"/>以下输出:</p>
			<div><div><img src="img/B18120_10_005.jpg" alt="Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS&#13;&#10;" width="1594" height="549"/>
				</div>
			</div>
			<p class="figure-caption">图10.5–调用我们的SHAP EaaS后数据帧中的响应</p>
			<p>注意在<em class="italic">图10.5 </em>中，列名为<code>shap_values</code>，而值为酸洗字节十六进制数据。这些不是人类可读的，但是可以在调用者端使用<code>pickle.loads</code>方法转换回原始的<code>shap_values</code>。因此，如果您看到类似<em class="italic">图10.5 </em>的响应输出，那么恭喜您建立了本地EaaS！您可以像其他MLflow服务部署一样部署此解释器服务，如第8章  <em class="italic">中所述，在规模</em>上部署DL推理管道，因为此解释器现在可以像通用MLflow pyfunc模型服务一样调用。</p>
			<p>接下来，我们将看到如何使用MLflow pyfunc解释器进行批量解释。</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>使用MLflow pyfunc解释器进行批量解释</h2>
			<p>使用MLflow pyfunc解释器有两种方法实现<a id="_idIndexMarker669"/>离线批处理解释:</p>
			<ul>
				<li>将pyfunc解释器作为MLflow pyfunc模型加载，以解释给定的pandas DataFrame输入。</li>
				<li>将pyfunc解释器作为PySpark UDF加载，以解释给定的PySpark数据帧输入。</li>
			</ul>
			<p>让我们从加载解释器作为MLflow pyfunc模型开始。</p>
			<h3>将MLflow pyfunc解释器作为MLflow pyfunc模型加载</h3>
			<p>正如我们已经提到的<a id="_idIndexMarker670"/>，使用MLflow日志解释器的另一种方式<a id="_idIndexMarker671"/>是直接使用MLflow的pyfunc <code>load_model</code>方法在本地Python代码中加载解释器，而不是将其部署到web服务中。这非常简单，我们将向您展示如何做到这一点。您可以在GitHub存储库中查看<code>shap_mlflow_pyfunc_explainer.py</code>文件中的代码(<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ py func _ explainer . py</a>):</p>
			<ol>
				<li value="1">第一步是将记录的解释器加载回内存。下面的代码使用<code>mlflow.pyfunc.load_model</code>和解释器<code>run_id</code> URI: <pre>run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9" logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer' explainer = mlflow.pyfunc.load_model(logged_explainer)</pre>来完成这个任务</li>
			</ol>
			<p>这应该加载解释器，就好像它只是一个通用的MLflow pyfunc模型。我们可以通过运行以下代码打印出解释器的元数据:</p>
			<pre>explainer</pre>
			<p>这将显示以下输出:</p>
			<pre><strong class="bold">mlflow.pyfunc.loaded_model: artifact_path: nlp_sentiment_classifier_explainer flavor: mlflow.pyfunc.model run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9</strong></pre>
			<p>这意味着这是一个<code>mlflow.pyfunc.model</code>风格，这是一个好消息，因为我们可以使用相同的MLflow pyfunc API来使用这个解释器。</p>
			<ol>
				<li value="2">接下来，我们将获得一些示例数据来测试新加载的解释器:<pre>import datasets dataset = datasets.load_dataset("imdb", split="test") short_data = [v[:500] for v in dataset["text"][:20]] df_test = pd.DataFrame (short_data, columns = ['text'])</pre></li>
			</ol>
			<p>这将<a id="_idIndexMarker672"/>加载<a id="_idIndexMarker673"/>IMDb测试数据集，将每个评论文本截短为500个字符，并挑选前20行来制作一个pandas数据帧，用于下一步的解释。</p>
			<ol>
				<li value="3">现在，我们可以如下运行解释器:<pre>results = explainer.predict(df_test)</pre></li>
			</ol>
			<p>这将为输入数据帧<code>df_test</code>运行SHAP分区解释器。运行时，它将为数据帧的每一行显示以下输出:</p>
			<pre><strong class="bold">Partition explainer: 2it [00:38, 38.67s/it]</strong></pre>
			<p>结果将是一个只有一列的pandas数据框架，<code>shap_values</code>。这可能需要几分钟的时间，因为它需要标记每一行，执行解释器，并序列化输出。</p>
			<ol>
				<li value="4">一旦解释器执行完成，我们可以通过反序列化行内容来检查结果。下面是检查第一个输出的代码:<pre>results_deserialized = pickle.loads(results['shap_values'][0]) print(results_deserialized)</pre></li>
			</ol>
			<p>这将打印出第一行的<code>shap_values</code>。<em class="italic">图10.6 </em>显示了<code>shap_values</code>输出的部分截图:</p>
			<div><div><img src="img/B18120_10_006.jpg" alt="Figure 10.6 – Partial output of the deserialized shap_values from the explanation&#13;&#10;" width="377" height="219"/>
				</div>
			</div>
			<p class="figure-caption">图10.6–解释中反序列化的shap_values的部分输出</p>
			<p>正如我们可以在<em class="italic">图10.6 </em>中<a id="_idIndexMarker674"/>看到<a id="_idIndexMarker675"/>一样，<code>shap_values</code>的输出与我们在<a href="B18120_09_ePub.xhtml#_idTextAnchor112"> <em class="italic">第九章</em> </a> <em class="italic">、深度学习可解释性基础</em>中所学的没有什么不同，当时我们没有使用MLflow来记录和加载解释器。我们还可以生成Shapley文本图来突出文本对预测情感的贡献。</p>
			<ol>
				<li value="5">在笔记本中运行下面的语句，查看匀称的文本情节:<pre>shap.plots.text(results_deserialized[:,:,"POSITIVE"])</pre></li>
			</ol>
			<p>这将生成一个显示在<em class="italic">图10.7 </em>中的图:</p>
			<div><div><img src="img/B18120_10_007.jpg" alt="Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow logged explainer&#13;&#10;" width="1188" height="218"/>
				</div>
			</div>
			<p class="figure-caption">图10.7–使用来自MLflow日志解释器的反序列化shap_values的Shapley文本图</p>
			<p>从<em class="italic">图10.7 </em>中可以看出，该评论的情绪是积极的，对预测情绪有贡献的关键词或短语是<code>good</code>、<code>love</code>，以及其他一些用红色突出显示的短语。当你看到这个Shapley文本情节时，你应该给自己一点掌声，因为你已经学会了如何使用MLflow logged explainer来生成批处理解释。</p>
			<p>正如在这个批量解释的逐步实现过程中提到的，使用这个pyfunc模型方法做<a id="_idIndexMarker676"/>一个<a id="_idIndexMarker677"/>大批量解释有点慢。幸运的是，我们有另一种方法来使用PySpark UDF函数实现批处理解释，我们将在下一小节中解释。</p>
			<h3>将pyfunc解释器作为PySpark UDF加载</h3>
			<p>对于可伸缩的<a id="_idIndexMarker678"/>批处理<a id="_idIndexMarker679"/>解释，我们可以使用Spark的分布式计算能力，这是通过将pyfunc解释器加载为PySpark UDF来支持的。使用这个功能不需要额外的工作，因为MLflow pyfunc API已经通过<code>mlflow.pyfunc.spark_udf</code>方法提供了这个功能。我们将向您展示如何逐步实现这种大规模的解释:</p>
			<ol>
				<li value="1">首先，确保您已经完成了<code>README.md</code>文件(<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md">https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/readme . MD</a>)来安装Spark，创建并激活<code>chapter10-dl-pyspark-explain</code>虚拟环境，并在运行PySpark UDF代码进行大规模解释之前设置所有的环境变量。</li>
				<li>然后就可以开始运行VS代码笔记本了，<code>shap_mlflow_pyspark_explainer.py</code>，可以在GitHub的资源库中查看:<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ py spark _ explainer . py</a>。在<code>chapter10/notebooks/</code>运行以下命令:<pre><strong class="bold">python shap_mlflow_pyspark_explainer.py</strong></pre></li>
			</ol>
			<p>您将得到<a id="_idIndexMarker680"/>在<em class="italic">图10.8 </em>中显示的<a id="_idIndexMarker681"/>最终输出，在这最后几行之前的几行输出中:</p>
			<div><div><img src="img/B18120_10_008.jpg" alt="Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's shap_values along with their input texts&#13;&#10;" width="968" height="166"/>
				</div>
			</div>
			<p class="figure-caption">图10.8-PySpark UDF解释器输出的前两行文本的shap_values以及它们的输入文本</p>
			<p>从<em class="italic">图10.8 </em>中可以看出，PySpark UDF解释器的输出是一个包含两列的PySpark数据帧:<code>text</code>和<code>shap_values</code>。<code>text</code>列是原始输入文本，而<code>shap_values</code>列包含经过腌制的序列化Shapley值，就像我们在前一小节中看到的对pandas数据帧使用pyfunc解释器一样。</p>
			<p>现在让我们看看代码中发生了什么。我们将解释<code>shap_mlflow_pyspark_explainer.py</code>文件中的关键代码块。由于这是一个VS代码笔记本，您可以像我们刚才做的那样在命令行中运行它，也可以在VS代码ide窗口中交互运行它。</p>
			<ol>
				<li value="3">第一个关键代码块使用<code>mflow.pyfunc.spark_udf</code>方法加载解释器，如下:<pre>spark = SparkSession.builder.appName("Batch explanation with MLflow DL explainer").getOrCreate() run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9" logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer' explainer = mlflow.pyfunc.spark_udf(spark, model_uri=logged_explainer, result_type=StringType())</pre></li>
			</ol>
			<p>第一个<a id="_idIndexMarker682"/>语句<a id="_idIndexMarker683"/>是初始化一个<code>SparkSession</code>变量，然后使用<code>run_id</code>将记录的解释器加载到内存中。运行解释器来获取元数据，如下所示:</p>
			<pre>explainer</pre>
			<p>我们将得到以下结果:</p>
			<pre><strong class="bold">&lt;function mlflow.pyfunc.spark_udf.&lt;locals&gt;.udf(iterator: Iterator[Tuple[Union[pandas.core.series.Series, pandas.core.frame.DataFrame], ...]]) -&gt; Iterator[pandas.core.series.Series]&gt;</strong></pre>
			<p>这意味着我们现在有了一个包装成火花UDF函数的SHAP解释器。这允许我们在下一步中为输入PySpark数据帧直接应用SHAP解释器。</p>
			<ol>
				<li value="4">我们像以前一样加载IMDb测试数据集，得到一个<code>short_data</code>列表，然后为测试数据集的前20行创建一个PySpark数据帧，用于解释:<pre>df_pandas = pd.DataFrame (short_data, columns = ['text']) spark_df = spark.createDataFrame(df_pandas) spark_df = spark_df.withColumn('shap_values', explainer())</pre></li>
			</ol>
			<p>注意最后一条语句，它使用PySpark的<code>withColumn</code>函数向输入数据帧<code>spark_df</code>添加一个新的<code>shap_values</code>列，该数据帧最初只包含一列<code>text</code>。这是使用Spark并行和分布式计算能力的自然方式。如果您已经使用MLflow pyfunc <code>load_model</code>方法运行了以前的非Spark方法和当前的PySpark UDF方法，您会注意到Spark方法运行得更快，甚至在本地计算机上也是如此。这允许我们对输入文本的许多实例进行大规模的SHAP解释。</p>
			<ol>
				<li value="5">最后，为了验证结果，我们展示了<code>spark_df</code>数据帧的顶部两行，如图<em class="italic">图10.8 </em>所示。</li>
			</ol>
			<p>至此，有了MLflow的pyfunc Spark UDF包装的SHAP讲解器，我们可以理直气壮地做大规模<a id="_idIndexMarker684"/>批量<a id="_idIndexMarker685"/>讲解了。恭喜你！</p>
			<p>现在让我们在下一节总结一下我们在本章中学到的内容。</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>摘要</h1>
			<p>在本章中，我们首先回顾了MLflow APIs中可用于实现可解释性的现有方法。现有的两个ml flow API<code>mlflow.shap</code>和<code>mlflow.evaluate</code>有局限性，因此不能用于我们需要的复杂DL模型和管道可解释性场景。然后，我们重点介绍了在MLflow API框架中实现SHAP解释和解释器的两种主要方法:<code>mlflow.log_artifact</code>用于记录解释，而<code>mlflow.pyfunc.PythonModel</code>用于记录SHAP解释器。使用<code>log_artifact</code> API可以允许我们将Shapley值和解释图记录到MLflow跟踪服务器中。使用<code>mlflow.pyfunc.PythonModel</code>允许我们将SHAP解释器作为MLflow pyfunc模型登录，从而打开了将SHAP解释器作为web服务部署以创建EaaS端点的大门。它还打开了通过MLflow pyfunc <code>load_model</code>或<code>spark_udf</code> API使用SHAP解释器进行大规模离线批量解释的大门。这使我们能够自信地为DL模型大规模实现可解释性。</p>
			<p>随着可解释性领域的继续发展，MLflow与SHAP和其他可解释性工具箱的集成也将继续改进。鼓励有兴趣的读者通过延伸阅读部分提供的链接继续他们的学习之旅。快乐的不断学习和成长！</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor136"/>延伸阅读</h1>
			<ul>
				<li>比例下的Shapley值:<a href="https://neowaylabs.github.io/data-science/shapley-values-at-scale/">https://neowaylabs . github . io/data-science/Shapley-Values-at-Scale/</a></li>
				<li>使用PySpark和Pandas UDF缩放SHAP计算:<a href="https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html">https://databricks . com/blog/2022/02/02/scaling-shap-Calculations-With-py spark-and-Pandas-UDF . html</a></li>
				<li>使用分布式计算系统Ray加速Shapley值计算:<a href="https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/">https://www . telesens . co/2020/10/05/speeding-up-Shapley-value-computing-using-Ray-a-distributed-computing-system/</a></li>
				<li>用莱姆和SHAP解释NLP模型:<a href="mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4">https://medium . com/@ kalia _ 65609/interpreting-an-NLP-model-with-LIME-and-shap-834 CFA 124 E4</a></li>
				<li>MLflow中的模型评估:<a href="https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html">https://databricks . com/blog/2022/04/19/model-Evaluation-in-ml flow . html</a></li>
			</ul>
		</div>
	</div>
</body></html>