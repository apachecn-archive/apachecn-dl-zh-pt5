# *第 3 章*:跟踪模型、参数和度量

鉴于 MLflow 可以在 DL 模型的生命周期中支持多种场景，因此通常会增量使用 MLflow 的功能。通常，人们从 MLflow 跟踪开始，因为它易于使用，并且可以处理许多用于再现性、来源跟踪和审计目的的场景。此外，从摇篮到日落跟踪模型的历史不仅超出了数据科学实验管理领域，而且对于企业中的模型治理也很重要，因为在生产中使用模型需要管理业务和监管风险。虽然在生产中跟踪模型的精确的商业价值仍然在发展，但是跟踪模型的整个生命周期的需求是不容置疑的，并且还在增长。为了能够做到这一点，我们将在本章开始时设置一个成熟的本地 MLflow 跟踪服务器。

然后，我们将深入探讨如何使用 MLflow 的跟踪和注册 API 来跟踪模型及其参数和指标。在本章结束时，您应该能够自如地使用 MLflow 的跟踪和注册 API 进行各种重现和审计。

在本章中，我们将讨论以下主要话题:

*   设置成熟的本地 MLflow 跟踪服务器
*   跟踪模型来源
*   跟踪模型指标
*   跟踪模型参数

# 技术要求

以下是遵循本章中提供的说明所需的要求:

*   Docker 桌面:[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)。
*   PyTorch `lightning-flash` : 0.5.0。:[https://github . com/PyTorchLightning/lightning-flash/releases/tag/0 . 5 . 0](https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0)。
*   带有 Jupyter 笔记本扩展的 VS 代码:[https://github . com/Microsoft/VS Code-Jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks)。
*   本章代码如下 GitHub URL:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03)。
*   WSL2:如果你是微软 Windows 用户，建议安装 WSL2 运行本书提供的 Bash 脚本:[https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10)。

# 建立完善的本地 MLflow 跟踪服务器

在 [*第 2 章*](B18120_02_ePub.xhtml#_idTextAnchor027)*深度学习 MLflow 入门*中，我们获得了使用基于本地文件系统的 MLflow 跟踪服务器和检查 MLflow 实验的组件的实践经验。但是，默认的基于本地文件系统的 MLflow 服务器有一些限制，因为模型注册表功能不可用。拥有模型注册中心的好处是，我们可以注册模型，对模型进行版本控制，并为模型部署到产品中做准备。因此，这个模型注册表将在离线实验和在线部署生产场景之间架起一座桥梁。因此，我们需要一个完整的 MLflow 跟踪服务器，它具有以下商店来跟踪模型的完整生命周期:

*   **后端存储**:需要一个关系数据库后端来支持 MLflow 存储关于实验的元数据(指标、参数和许多其他信息)。这也允许使用实验的查询能力。我们将使用 MySQL 数据库作为本地后端存储。
*   **工件存储**:一个对象存储，可以存储任意类型的对象，比如序列化的模型、词汇文件、图形等等。在生产环境中，一个受欢迎的选择是 AWS S3 商店。我们将使用**MinIO**([https://min.io/](https://min.io/))，一个多云对象商店，作为一个本地工件商店，它与 AWS S3 商店 API 完全兼容，但可以在你的笔记本电脑上运行，无需你访问云。

为了使这个本地设置尽可能简单，我们将使用`docker-compose`([https://docs.docker.com/compose/](https://docs.docker.com/compose/))工具和一行命令来启动和停止本地成熟的 MLflow 跟踪服务器，如下面的步骤所述。注意，在执行这些步骤之前，必须在机器上安装并运行 Docker Desktop([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/))。Docker 帮助构建和共享容器化应用和微服务。以下步骤将在本地 Docker 容器中启动本地 MLflow 跟踪服务器:

1.  查看您本地开发环境的`chapter03`代码库:[https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03)。
2.  将目录转到`mlflow_docker_setup`子文件夹，可以在`chapter03`文件夹下找到。
3.  运行以下命令:

    ```
    bash start_mlflow.sh
    ```

如果该命令成功，您应该会在屏幕上看到类似以下内容的输出:

![Figure 3.1 – A local full-fledged MLflow tracking server is up and running
](img/B18120_03_001.jpg)

图 3.1-本地成熟的 MLflow 跟踪服务器启动并运行

1.  前往`http://localhost/`查看 MLflow UI 网页。然后，点击 UI 中的**型号**选项卡(*图 3.2* )。请注意，如果您只有一个本地文件系统作为 MLflow 跟踪服务器的后端存储，则此选项卡不起作用。因此，MLflow UI 的后端现在运行在您刚刚启动的 Docker 容器服务上，而不是本地文件系统。由于这是一台全新的服务器，因此还没有注册型号:

![Figure 3.2 – MLflow model registry UI
](img/B18120_03_002.jpg)

图 3.2–ml flow 模型注册界面

1.  转到`http://localhost:9000/`，应出现 MinIO 工件商店 web UI 的以下屏幕(*图 3.3* )。在`mlflow_docker_setup`文件夹下输入`.env`文件的`minio123`的`minio`:

![Figure 3.3 – MinIO Web UI login page and browser page after logging in
](img/B18120_03_003.jpg)

图 3.3–MinIO Web UI 登录页面和登录后的浏览器页面

此时您应该已经成功运行了一个成熟的本地 MLflow 跟踪服务器！如果您想停止服务器，只需键入以下命令:

```
bash stop_mlflow.sh
```

基于 Docker 的 MLflow 跟踪服务器将停止。我们现在准备使用这个本地 MLflow 服务器来跟踪模型出处、参数和度量。

# 跟踪模型出处

**出处**文献中对数字制品的跟踪已经研究了很久。例如，当您在生物医学行业使用一条患者诊断数据时，人们通常想知道它来自哪里，对数据进行了什么样的处理和清理，谁拥有数据，以及关于数据的其他历史和血统信息。生产中工业和商业场景的 ML/DL 模型的兴起使得来源跟踪成为一项必需的功能。来源跟踪的不同粒度不仅对数据科学离线实验的操作和管理至关重要，而且对模型在生产中部署之前/期间/之后的操作和管理也至关重要。那么，需要追踪哪些东西的出处呢？

## 了解开放出处跟踪框架

让我们看看一个通用的起源追踪框架，来理解为什么起源追踪是一项主要工作。下图是基于**开放源模型词汇表规范**([http://open-biomed.sourceforge.net/opmv/ns.html](http://open-biomed.sourceforge.net/opmv/ns.html))的:

![Figure 3.4 – An open provenance tracking framework
](img/B18120_03_004.jpg)

图 3.4–一个开放的出处跟踪框架

在上图中，有三个重要的项目:

*   **神器**:工艺( **A1** 和 **A2** )生产或使用的东西。
*   **过程**:使用或生产工件所执行的动作( **P1** 和 **P2** )。
*   **因果关系**:工件与工序之间的边缘或关系，如使用的*，*由*生成，*来源于上图中的*(**R1**， **R2** ， **R3** )。*

直观地说，这个**开放出处模型** ( **OPM** )框架允许我们提出以下 5W1H(五个 w 和一个 H)问题，具体如下:

![Figure 3.5 – Types of provenance questions
](img/B18120_03_005.jpg)

图 3.5-起源问题的类型

拥有一个系统的起源框架和一组问题将帮助我们学习如何跟踪模型起源，并为这些问题提供答案。这将激励我们在下一节中实现 MLflow 模型跟踪。

## 实现物流模型跟踪

如果我们为我们使用的 DL 模型实现了 MLflow 日志记录和注册，我们可以使用 MLflow 跟踪服务器来回答这些类型的起源问题。首先，让我们回顾一下 MLflow 在模型起源跟踪方面提供了什么。MLflow 为模型出处提供了两组 API:

*   **日志 API** :这个允许实验的每次运行或者模型管道将模型工件记录到工件存储中。
*   **Registry API**: This allows a centralized location to track the version of the model and the stages of the model's life cycle (**None**, **Archived**, **Staging**, or **Production**).

    模型日志记录和模型注册的区别

    尽管实验的每一次运行都需要被记录，并且模型需要被保存在工件库中，但是并不是模型的每一个实例都需要在模型注册表中注册。这是因为，对于许多早期的探索性模型实验来说，模型可能并不好。因此，不必注册来跟踪版本。只有当一个模型具有良好的离线性能，并成为推广到生产的候选时，我们才需要在模型注册表中注册它，以走模型推广流程。

    虽然 MLflow 的官方 API 文档将日志记录和注册表分成两个组件，但在本书中，我们将它们一起称为 MLflow 中的模型跟踪功能。

我们已经看到了我们在 [*第二章*](B18120_02_ePub.xhtml#_idTextAnchor027)*深度学习 MLflow 入门*中构建的 DL 模型的 MLflow 自动日志记录。尽管自动日志功能强大，但当前版本存在两个问题:

*   它不会自动将模型注册到模型注册中心。
*   如果您只是按照 MLflow 的建议使用`mlflow.pyfunc.load_model` API 来加载日志模型，那么日志模型直接处理原始输入数据(在我们的例子中，是一个英语句子)并不是现成的。这是一个限制，可能是由于 MLflow 中当前自动日志 API 的实验性质。

让我们通过一个示例来回顾 MLflow 的功能和自动日志记录的局限性，以及我们如何解决这些问题:

1.  在运行 MinIO 和基于 MySQL 的 Docker 组件的 Bash 终端中设置以下环境变量:

    ```
    export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000 export AWS_ACCESS_KEY_ID=minio export AWS_SECRET_ACCESS_KEY=minio123
    ```

注意，`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`使用的是在`mlflow_docker_setup`文件夹下的`.env`文件中定义的相同值。这样做是为了确保我们使用的是之前设置的 MLflow 服务器。由于这些环境变量是基于会话的，我们还可以在笔记本的代码中设置以下环境变量，如下所示:

```
os.environ["AWS_ACCESS_KEY_ID"] = "minio"
os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"
os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://localhost:9000"
```

前面的三行代码可以在本章的笔记本文件中找到，就在导入所需的 Python 包之后。在执行笔记本之前，请确保运行以下命令来初始化虚拟环境`dl_model`，它现在在`requirements.txt`文件中定义了额外的必需包:

```
conda create -n dl_model python==3.8.10
conda activate dl_model
pip install -r requirements.txt
```

如果您在前面的章节中设置了`dl_model`虚拟环境，那么您可以跳过创建一个名为`dl_model`的虚拟环境的第一行。然而，您仍然需要激活`dl_model`作为当前活动的虚拟环境，然后运行`pip install -r requirements.txt`来安装所有需要的 Python 包。一旦`dl_model`虚拟环境设置成功，您可以继续下一步。

1.  为了跟进这个模型跟踪实现，请通过访问本章的 GitHub 存储库查看 VS 代码中的`dl_model_tracking.ipynb`笔记本文件:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 03/dl _ model _ tracking . ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb)。

请注意，在`dl_model_tracking.ipynb`笔记本的第四个单元格中，我们需要将它指向我们刚刚在 Docker 中设置的正确的新 MLflow 跟踪 URI，并定义一个新实验，如下所示:

```
EXPERIMENT_NAME = "dl_model_chapter03"
mlflow.set_tracking_uri('http://localhost')
```

1.  我们仍将使用 MLflow 提供的自动记录功能，但是我们将为运行分配一个变量名`dl_model_tracking_run` :

    ```
    mlflow.pytorch.autolog() with mlflow.start_run(experiment_id=experiment.experiment_id, run_name="chapter03") as dl_model_tracking_run:     trainer.finetune(classifier_model, datamodule=datamodule, strategy="freeze")     trainer.test()
    ```

`dl_model_tracking_run`允许我们以编程的方式获得`run_id`参数和关于这次运行的其他元数据，我们将在下一步中看到。一旦执行了该代码单元，我们将在 MLflow 跟踪服务器中记录一个经过训练的模型，其中包含所有必需的参数和指标。然而，该模型尚未注册。我们可以在 MLflow web UI 中找到记录的实验，以及所有相关的参数和指标，位于 http://localhost/#/experiments/1/runs/37 a3 Fe 9 b 6 fa f 41d 89001 ECA 13 ad 6 ca 47。您可以在`http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/`中找到模型工件来查看存储 UI，如下图所示:

![Figure 3.6 – Model artifacts logged In the MinIO storage backend 
](img/B18120_03_006.jpg)

图 3.6–记录在 MinIO 存储后端的模型工件

文件夹的结构类似于我们在 [*第 2 章*](B18120_02_ePub.xhtml#_idTextAnchor027) 、*深度学习 MLflow 入门*中看到的，当时我们使用一个普通的文件系统来存储模型工件。然而，在这里，我们使用一个**迷你**桶来存放这些模型文物。

1.  从`dl_model_tracking_run`中检索`run_id`参数，以及其他元数据，如下:

    ```
    run_id = dl_model_tracking_run.info.run_id print("run_id: {}; lifecycle_stage: {}".format(run_id,     mlflow.get_run(run_id).info.lifecycle_stage))
    ```

这将打印出如下内容:

```
run_id: 37a3fe9b6faf41d89001eca13ad6ca47; lifecycle_stage: active
```

1.  通过定义记录的模型 URI 检索记录的模型。这将允许我们在这个特定的位置重新加载记录的模型:

    ```
    logged_model = f'runs:/{run_id}/model'
    ```

2.  使用`mlflow.pytorch.load_model`和下面的`logged_model` URI 将模型加载回内存，并对给定的输入句子进行新的预测，如下:

    ```
    model = mlflow.pytorch.load_model(logged_model) model.predict({'This is great news'})
    ```

这将输出一个模型预测标签，如下所示:

```
['positive']
```

mlflow.pytorch.load_model 与 mlflow.pyfunc.load_model

默认情况下，在 MLflow 实验跟踪页面的工件部分，如果您有一个已记录的模型，MLflow 将建议使用`mlflow.pyfunc.load_model`加载一个已记录的模型用于预测。然而，这只适用于像 pandas DataFrame、NumPy 数组或 tensor 这样的输入；这不适用于 NLP 文本输入。因为 PyTorch lightning 的自动记录使用了`mlflow.pytorch.log_model`来保存模型，所以正确的方法是使用`mlflow.pytorch.load_model`，正如我们在这里所展示的。这是因为 MLflow 的默认设计是使用标准化的`mlflow.pyfunc.load_model`，并且有一个已知的限制，即只能接受数字形式的输入格式。对于文本和图像数据，需要将标记化步骤作为预处理步骤。然而，由于我们在这里保存的 PyTorch 模型已经作为序列化模型的一部分执行了标记化，我们可以使用原生的`mlflow.pytorch.load_model`来直接加载接受文本作为输入的模型。

至此，我们已经成功地记录了模型并加载回模型以进行预测。如果我们认为这个模型表现得足够好，那么我们可以注册它。

1.  让我们通过使用`mlflow.register_model` API:

    ```
    model_registry_version = mlflow.register_model(logged_model, 'nlp_dl_model') print(f'Model Name: {model_registry_version.name}') print(f'Model Version: {model_registry_version.version}')
    ```

    来注册模型

此将产生以下输出:

![Figure 3.7 – Model registration success message 
](img/B18120_03_007.jpg)

图 3.7–型号注册成功消息

这表明该模型已经成功地在模型注册中心注册为版本 1，名称为`nlp_dl_model`。

我们也可以通过点击`http://localhost/#/models/nlp_dl_model/versions/1`在 MLflow web UI 中找到这个注册的模型:

![Figure 3.8 – MLflow tracking server web UI showing the newly registered model
](img/B18120_03_008.jpg)

图 3.8–显示新注册模型的 MLflow tracking server web 用户界面

默认情况下，新注册的模型的 stage 是 **None** ，如前面的截图所示。

通过使用版本号和阶段标签注册模型，我们为部署到阶段(也称为预生产)和生产奠定了基础。我们将在本书的后面讨论如何基于注册的模型执行模型部署。

至此，我们已经解决了本节开始时提出的关于自动日志限制的两个问题:

*   如何使用`mlflow.pytorch.load_model` API 而不是`mlflow.pyfunc.load_model` API 加载一个记录的 DL PyTorch 模型
*   How to register a logged DL PyTorch model using the `mlflow.register_model` API

    MLflow DL 模型日志记录 API 的选择

    对于 DL 模型，PyTorch 的自动记录只对`mlflow.pyfunc.log_model`记录模型有效，特别是当我们需要多步 DL 模型管道的时候。我们将在本书的后面实现这种定制的 MLflow 模型风格。如果你不想为 PyTorch 使用自动日志，那么你可以直接使用`mlflow.pytorch.log_model`。PyTorch 的自动日志在其实现内部使用了`mlflow.pytorch.log_model`(见官方 MLflow 开源实现此处:[https://github . com/ml flow/ml flow/blob/290 BF 3d 54d 1 e 5 ce 61944455 CB 302 a5d 6390107 f 0/ml flow/py torch/_ py torch _ autolog . py # L314](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314))。

如果我们不想使用自动日志，那么我们可以直接使用 MLflow 的模型日志 API。这也为我们提供了一种在一次调用中同时注册模型的替代方法。您可以使用下面的代码行来记录和注册训练模型:

```
mlflow.pytorch.log_model(pytorch_model=trainer.model, artifact_path='dl_model', registered_model_name='nlp_dl_model')
```

请注意，这一行代码没有记录模型的任何参数或指标。

这样，我们不仅在跟踪服务器中记录了许多实验和模型用于离线实验，而且还注册了高性能模型用于将来的产品部署，以及版本控制和来源跟踪。我们现在可以回答我们在本章开始时提出的一些出处问题了:

![Figure 3.9 – Answers to model provenance questions
](img/B18120_03_009.jpg)

图 3.9-模型起源问题的答案

“为什么”和“出处在哪里”的问题还没有完全回答，但将在本书的后面部分回答。这是因为生产模型的“为什么起源”问题只能在模型准备好部署时被跟踪和记录，我们需要添加注释和理由来证明模型的部署。当我们有一个多步骤的模型管道时，哪里起源的问题可以得到完全的回答。然而，在这里，我们只有一个单步流水线，这是最简单的情况。多步骤管道包含明确分离的模块化代码，以指定哪个步骤执行什么功能，以便我们可以轻松地更改任何步骤的详细实现，而无需更改管道的流程。在接下来的两个部分中，我们将研究如何在不使用自动日志记录的情况下跟踪度量和模型的参数。

# 跟踪模型指标

PyTorch `lightning-flash`包中文本分类模型的默认度量是**准确性**。如果我们想要将度量标准更改为 **F1 得分**(精确度和召回率的调和平均值)，这是一个非常常见的用于测量分类器性能的度量标准，那么我们需要在开始模型训练过程之前更改分类器模型的配置。让我们了解如何进行这一更改，然后使用 MLflow 的非自动日志 API 来记录指标:

1.  在定义分类器变量时，我们将传递一个名为`torchmetrics.F1`的度量函数作为变量，而不是使用默认的度量，如下:

    ```
    classifier_model = TextClassifier(backbone="prajjwal1/bert-tiny", num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))
    ```

这使用了`torchmetrics`、`F1`模块的内置度量函数，以及我们需要作为参数分类的数据中的类的数量。这确保了使用这个新的度量来训练和测试模型。您将看到类似如下的输出:

```
{'test_cross_entropy': 0.785443127155304, 'test_f1': 0.5343999862670898}
```

这表明模型训练和测试使用 F1 分数作为度量，而不是默认的准确性度量。有关如何使用`torchmetrics`定制指标的更多信息，请参考其文档网站:[https://torchmetrics.readthedocs.io/en/latest/](https://torchmetrics.readthedocs.io/en/latest/)。

1.  现在，如果我们想要将所有指标记录到 MLflow 跟踪服务器，包括培训、验证和测试指标，我们需要通过调用培训师的回调函数来获取所有当前指标，如下所示:

    ```
        cur_metrics = trainer.callback_metrics
    ```

然后，我们需要将所有指标值转换为`float`，以确保它们与 MLflow `log_metrics` API 兼容:

```
    metrics = dict(map(lambda x: (x[0], float(x[1])), cur_metrics.items()))
```

1.  现在，我们可以调用 MLflow 的`log_metrics`来记录跟踪服务器中的所有指标:

    ```
        mlflow.log_metrics(metrics)
    ```

在使用 F1 分数作为分类器的指标后，您将看到以下指标，这些指标将记录在 MLflow 的跟踪服务器中:

```
{'train_f1': 0.5838666558265686, 
'train_f1_step': 0.75, 
'train_cross_entropy': 0.7465656399726868, 
'train_cross_entropy_step': 0.30964696407318115, 
'val_f1': 0.5203999876976013, 
'val_cross_entropy': 0.8168156743049622, 
'train_f1_epoch': 0.5838666558265686, 
'train_cross_entropy_epoch': 0.7465656399726868, 
'test_f1': 0.5343999862670898, 
'test_cross_entropy': 0.785443127155304}
```

使用 MLflow 的`log_metrics` API 可以通过额外的代码行给我们更多的控制，但是如果我们对它的自动日志功能感到满意，那么我们唯一需要改变的就是我们想要为模型训练和测试过程使用什么度量。在这种情况下，我们只需要在声明新的 DL 模型时定义一个新的度量标准(也就是说，使用 F1 分数而不是默认的准确性度量标准)。

1.  如果您想要同时跟踪多个模型指标，比如 F1 分数、准确度、精确度和召回率，那么您唯一需要做的事情就是定义一个您想要计算和跟踪的指标的 Python 列表，如下:

    ```
    list_of_metrics = [torchmetrics.Accuracy(),    torchmetrics.F1(num_classes=datamodule.num_classes),    torchmetrics.Precision(num_classes=datamodule.num_classes),    torchmetrics.Recall(num_classes=datamodule.num_classes)]
    ```

然后，在模型初始化语句中，不需要将单个指标传递给`metrics`参数，您可以传递我们刚刚定义的`list_of_metrics` Python 列表，在`metrics`参数之上，如下所示:

```
classifier_model = TextClassifier(backbone="prajjwal1/bert-tiny", num_classes=datamodule.num_classes, metrics=list_of_metrics)
```

不需要对代码的其余部分进行更多的修改。所以，在`dl_model-non-auto-tracking.ipynb`笔记本(https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 03/dl _ model-non-auto-tracking . ipynb)中，你会注意到前面一行是默认注释掉的。但是，您可以取消对它的注释，然后注释掉前一个:

```
classifier_model = TextClassifier(backbone="prajjwal1/bert-tiny", num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))
```

然后，当您运行笔记本的其余部分时，您将在笔记本的输出中获得模型测试报告，以及以下指标:

```
{'test_accuracy': 0.6424000263214111, 'test_cross_entropy': 0.6315688490867615, 'test_f1': 0.6424000263214111, 'test_precision': 0.6424000263214111, 'test_recall': 0.6424000263214111}
```

您可能会注意到，准确度、F1、精确度和召回率的数字是相同的。这是因为，默认情况下，`torchmetrics`使用的`torchmetrics`不支持`none`方法，该方法计算每个类的度量并返回每个类的度量，即使是二进制的分类模型。所以，这不会产生一个单一的标量数。但是，您可以调用 scikit-learn 的 metrics API，通过传递两个值列表来计算 F1 分数或基于二进制平均法的其他指标。这里可以使用`y_true`和`y_predict`，其中`y_true`是地面真实标签值列表，`y_predict`是模型预测标签值列表。这对你来说是一个很好的尝试，因为这是所有 ML 模型的普遍做法，而不是对 DL 模型的特殊处理。

# 跟踪模型参数

正如我们已经看到的，在 MLflow 中使用自动记录有很多好处，但是如果我们想要跟踪额外的模型参数，我们可以使用 MLflow 在自动记录记录的基础上记录额外的参数，或者直接使用 MLflow 记录我们想要的所有参数，而根本不使用自动记录。

让我们在不使用 MLflow 自动日志记录的情况下浏览一下笔记本。如果我们想要完全控制 MLflow 将记录哪些参数，我们可以使用两个 API。第一个日志记录了一对键值参数，而第二个日志记录了键值参数的整个字典。那么，我们有兴趣跟踪什么样的参数呢？以下回答了这个问题:

*   `log_params` API 在实验中记录它们。
*   **模型参数**:这些参数是在模型训练过程中学习到的。对于 DL 模型，这些通常是指在训练期间学习的神经网络权重。我们不需要单独记录这些重量参数，因为它们已经在记录的 DL 模型中。

让我们使用 MLflow 的`log_params` API 记录这些超参数，如下所示:

```
    params = {"epochs": trainer.max_epochs}
```

```
    if hasattr(trainer, "optimizers"):
```

```
        optimizer = trainer.optimizers[0]
```

```
        params["optimizer_name"] = optimizer.__class__.__name__
```

```
    if hasattr(optimizer, "defaults"):
```

```
        params.update(optimizer.defaults)
```

```
    params.update(classifier_model.hparams)
```

```
    mlflow.log_params(params)
```

请注意，在这里，我们记录了最大的历元数、训练器的第一个优化器的名称、优化器的默认参数以及整个分类器的超参数(`classifier_model.hparams`)。一行代码`mlflow.log_params(params)`将`params`字典中的所有键值参数记录到 MLflow 跟踪服务器。如果您在 MLflow 跟踪服务器中看到以下超参数，则表示它工作正常！

![Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters 
](img/B18120_03_010.jpg)

图 3.10–ml flow tracking server web 用户界面显示记录的模型超参数

注意，这个参数列表比自动记录器记录的要多，因为我们在实验中添加了额外的超参数来记录。如果您想要记录任何其他定制的参数，您可以在您的实验中遵循相同的模式。完整的笔记本，不使用自动日志，可以在本章的 GitHub 知识库中查看，网址为[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 03/dl _ model-non-auto-tracking . ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb)。

如果您已经达到本章的这一点，那么您已经成功地实现了 MLflow 跟踪模型及其指标和参数！

# 总结

在本章中，我们使用 MySQL 和 MinIO 对象存储建立了一个完全支持后端存储和工件存储的本地 MLflow 开发环境。这将对我们在本书中开发支持 MLflow 的 DL 模型非常有用。我们首先介绍了开放式起源跟踪框架，并询问了感兴趣的模型起源跟踪问题。我们致力于解决自动记录的问题，并通过使用`mlflow.pytorch.load_model` API 从 MLflow 中的已记录模型加载一个训练模型来进行预测，从而成功注册了一个训练模型。我们还试验了如何直接使用 MLflow 的`log_metrics`、`log_params`和`log_model`API 而不使用自动日志记录，这让我们在如何记录额外的或定制的指标和参数方面有了更多的控制和灵活性。通过执行模型起源跟踪，我们能够回答许多起源问题，以及通过提供一些需要进一步研究使用 MLflow 来跟踪多步模型管道及其部署的问题。

我们将在下一章继续我们的学习之旅，并学习如何使用 MLflow 执行代码和数据跟踪，这将为我们提供额外的能力来回答数据和代码相关的起源问题。

# 延伸阅读

要了解本章中涵盖的主题的更多信息，请查看以下资源:

*   MLflow Docker 设置参考:[https://github.com/sachua/mlflow-docker-compose](https://github.com/sachua/mlflow-docker-compose)
*   MLflow PyTorch 自动登录实现:[https://github . com/ml flow/ml flow/blob/master/ml flow/py torch/_ py torch _ autolog . py](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py)
*   MLflow PyTorch 模型日志记录、加载和注册表文档:[https://www . ml flow . org/docs/latest/python _ API/ml flow . py torch . html](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html)
*   MLflow 参数和度量记录文档:[https://www.mlflow.org/docs/latest/python_api/mlflow.html](https://www.mlflow.org/docs/latest/python_api/mlflow.html)
*   MLflow 模型注册文档:[https://www.mlflow.org/docs/latest/model-registry.html](https://www.mlflow.org/docs/latest/model-registry.html)
*   挖大种源(用铁锹):[https://queue.acm.org/detail.cfm?id=3476885](https://queue.acm.org/detail.cfm?id=3476885)
*   如何利用`torchmetrics`和`lightning-flash`:[https://www . exxactcorp . com/blog/Deep-Learning/advanced-py torch-lightning-using-torch metrics-and-lightning-flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)
*   为什么在多类问题中使用微平均时，精度、召回率和 F1 分数相等？[https://simonhessner . de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-a-a-a-multi-class-problem/](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)