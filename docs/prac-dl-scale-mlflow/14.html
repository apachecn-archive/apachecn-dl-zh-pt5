<html><head/><body>




<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style>
<div><div><h1 id="_idParaDest-113"><em class="italic"> <a id="_idTextAnchor112"/>第九章</em>:深度学习可解释性的基础</h1>
			<p>可解释性是为自动化系统提供的决策提供选择性的人类可理解的解释。在本书的上下文中，在<strong class="bold">深度学习</strong> ( <strong class="bold"> DL </strong>)开发的整个生命周期中，可解释性应该被强调为一流的工件，与其他三个支柱:数据、代码和模型一起。这是因为不同的利益相关者和监管者、模型开发人员和模型输出的最终消费者可能有不同的需求来了解如何使用数据以及为什么模型产生某些预测或分类。没有这样的理解，就很难获得模型输出的消费者的信任，或者当模型输出结果漂移时，很难诊断出哪里出了问题。这也意味着可解释性工具不仅应该用于解释生产中或离线实验期间部署的模型的预测结果，还应该用于理解离线模型训练中使用的数据集和在线模型操作中遇到的数据集之间的数据特征和差异。</p>
			<p>此外，在许多高度监管的行业，如自动驾驶、医疗诊断、银行和金融，也有一项法律要求，要求任何个人都有解释算法输出的<strong class="bold">权利</strong>(<a href="https://academic.oup.com/idpl/article/7/4/233/4762325">https://academic.oup.com/idpl/article/7/4/233/4762325</a>)。最后，最近的一项调查显示，超过82%的首席执行官认为，随着企业加快投资开发和部署基于人工智能的计划，基于人工智能的决策必须是可以解释和可信的(<a href="https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models">https://cloud . Google . com/blog/topics/developers-practices/big query-explable-AI-now-ga-help-you-interpret-your-machine-learning-models</a>)。因此，学习可解释性的基本原理和相关工具是很重要的，这样我们就知道什么时候为什么样的受众使用什么样的工具来提供相关的、准确的和一致的解释。</p>
			<p>在本章结束时，你将有信心知道什么是好的解释，以及有什么工具可以用于不同的可解释性目的，并将获得使用两个可解释性工具箱来解释DL情感分类模型的实践经验。</p>
			<p>在本章中，我们将讨论以下主要话题:</p>
			<ul>
				<li>理解可解释性的范畴和受众</li>
				<li>探索SHAP可解释性工具箱</li>
				<li>探索变形金刚解释工具箱</li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>技术要求</h1>
			<p>完成本章的学习需要满足以下要求:</p>
			<ul>
				<li>https://github.com/slundberg/shap SHAP Python库:<a href="https://github.com/slundberg/shap"/></li>
				<li>变形金刚解读Python库:【https://github.com/cdpierse/transformers-interpret T2】</li>
				<li>Captum Python库:【https://github.com/pytorch/captum T4】</li>
				<li>本章代码来自GitHub知识库:<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09">https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 09</a></li>
			</ul>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>理解可解释性的类别和受众</h1>
			<p>正如本章<a id="_idIndexMarker572"/>的<a id="_idIndexMarker573"/>开篇所暗示的，在高度监管的行业，如金融、法律、政府和医疗应用领域，DL系统的可解释性变得越来越重要，有时甚至是强制性的。部分由于缺乏ML可解释性的一个示例诉讼是<a id="_idIndexMarker574"/>案件<strong class="bold">B2C 2v Quoine</strong>(<a href="https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning">https://www . SCL . org/articles/12130-explaible-machine-learning-how-can-determine-what-a-party-know-or-intended-when-a-a-party-a-decision-made-by-machine-learning</a>)，其中自动人工智能交易算法错误地以250倍于比特币交易市场价的价格下单。最近DL模型在生产中的成功应用刺激了可解释领域的积极和丰富的研究和开发，因为需要理解DL模型为什么以及如何工作。你可能听说过术语<strong class="bold">可解释的人工智能</strong> ( <strong class="bold"> XAI </strong>)的<a id="_idIndexMarker575"/>，它是由<strong class="bold">美国国防高级研究计划局</strong> ( <strong class="bold"> DARPA </strong>)在2015年为其XAI计划启动的，目标是<a id="_idIndexMarker576"/>使最终用户能够更好地理解、信任和有效管理人工智能系统(<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61">https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61</a>)。然而，可解释性的概念可以追溯到20世纪80年代甚至更早的专家系统的早期(<a href="https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391">https://wires . online library . Wiley . com/doi/full/10.1002/widm . 1391</a>)，最近对可解释性主题的关注激增正好凸显了它的重要性。</p>
			<p>那么，怎么解释呢？事实证明，这仍然是ML/DL/AI社区中一个活跃的研究课题。从实际目的来看，解释的精确定义取决于在整个ML/DL/AI生命周期中的什么时间，谁为了什么目的想要解释(<a href="https://dl.acm.org/doi/abs/10.1145/3461778.3462131">https://dl.acm.org/doi/abs/10.1145/3461778.3462131</a>)。因此，可解释性可以被定义为<em class="italic">提供一种适合受众的、人类可以理解的解释的能力，解释为什么以及如何一个模型提供某些预测</em>。这可能还包括数据可解释性方面，数据在哪里以及如何通过来源跟踪<a id="_idIndexMarker578"/>被<a id="_idIndexMarker577"/>使用，数据特征是什么，或者它是否由于意外事件而改变。例如，由于意外的COVID爆发，销售和营销电子邮件发生了变化(<a href="https://www.validity.com/resource-center/disruption-in-email/">https://www . validity . com/resource-center/disruption-in-email/</a>)。这种数据变化将意外地改变模型预测结果的分布。当解释模型漂移时，我们需要考虑这样的数据变化。这意味着解释的复杂性需要根据受众的需求进行定制和选择，而不需要大量的信息。例如，包含许多技术术语(如<em class="italic">激活</em>)的复杂解释可能不如包含业务友好术语的简单文本摘要有效。这个<a id="_idIndexMarker579"/>进一步说明了可解释性也是一个<strong class="bold">人机界面/交互</strong> ( <strong class="bold"> HCI </strong>)话题。</p>
			<p>为了更全面地了解可解释性类别和相应的受众，我们考虑图9.1 所示的八个解释维度:</p>
			<div><div><img src="img/B18120_09_001.jpg" alt="Figure 9.1 – Eight dimensions to understand explainability &#13;&#10;" width="772" height="700"/>
				</div>
			</div>
			<p class="figure-caption">图9.1-理解可解释性的八个维度</p>
			<p>从<em class="italic">图9.1 </em>可以看出，可解释性的复杂性可以从八个维度来理解。这不一定是一个详尽的分类，而是一个从HCI、AI/ML/DL的完整生命周期和<a id="_idIndexMarker580"/>不同的<a id="_idIndexMarker581"/>技术方法理解不同观点的指南。在下面的讨论中，我们将强调与DL应用程序最相关的维度及其相互关系，因为本章的重点是DL可解释性。</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>观众:谁需要知道</h2>
			<p>正如最近一项研究(<a href="https://dl.acm.org/doi/abs/10.1145/3461778.3462131">https://dl.acm.org/doi/abs/10.1145/3461778.3462131</a>)所指出的，理解在人工智能项目生命周期的哪个阶段，谁需要知道什么样的解释是很重要的。这也将影响解释输出格式。一项早期的研究(【https://arxiv.org/pdf/1702.08608.pdf】<a href="https://arxiv.org/pdf/1702.08608.pdf"/>)也指出，根据领域专家是否参与实际的应用任务(例如，癌症诊断中的医生)，验证解释的成本也可能很高，因为它需要真实的人在真实的工作环境中。</p>
			<p>对于当前的实际DL项目，我们需要根据目标受众(如数据科学家、ML工程师、业务<a id="_idIndexMarker582"/>利益相关者、<strong class="bold">用户体验(UX) </strong>设计师或最终用户)来定制我们的方法和解释演示，因为没有放之四海而皆准的方法。</p>
			<h2 id="_idParaDest-117"><strong class="bold"> <a id="_idTextAnchor116"/>阶段:在DL生命周期中何时提供解释</strong></h2>
			<p>一个<strong class="bold">阶段</strong>通常指在模型开发生命周期中可以提供解释的时候。对于像决策树这样的模型，因为它是白盒模型，我们说我们可以<a id="_idIndexMarker583"/>提供<strong class="bold">事前</strong>可解释性。然而，目前，大多数DL模型大多被视为黑盒模型，即使自我解释的DL模型正在逐步发展，具有事前解释能力(<a href="https://arxiv.org/abs/2108.11761">https://arxiv.org/abs/2108.11761</a>)。因此，对于当前实际的DL <a id="_idIndexMarker584"/>应用，需要<strong class="bold">事后</strong>可解释性。此外，当模型开发阶段处于训练、验证或生产阶段时，可解释性范围可以是全局的、群组的或局部的，即使使用相同的事后可解释性工具(<a href="https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f">https://towards data science . com/a-look-into-global-grouton-and-local-model-explability-973 BD 449969 f</a>)。</p>
			<h2 id="_idParaDest-118">范围:哪个预测需要解释</h2>
			<p><strong class="bold">范围</strong>指<a id="_idIndexMarker585"/>到<a id="_idIndexMarker586"/>我们是否可以为所有预测、预测的子集或仅仅一个特定的预测提供解释，即使我们对黑盒DL模型使用相同的事后工具。最常见的全局可解释性是描述<strong class="bold">特征重要性</strong>，让用户知道哪个特征对整体模型性能最有影响。局部<a id="_idIndexMarker587"/>可解释性是关于特定预测实例的<strong class="bold">特征属性</strong>。特征归因和特征重要性的区别在于，特征归因不仅量化了特征影响的排名和大小，还量化了影响的方向(例如，一个特征对预测的影响是积极的还是消极的)。</p>
			<p>许多用于DL模型的后期工具都非常擅长本地解释。群组可解释性对于识别某些特定群体(如年龄或种族群体)的潜在模型偏差是有用的。对于一个DL模型，如果我们想要有一个全局的解释，我们经常需要使用一个代理模型比如决策树模型来模拟DL模型的行为(<a href="https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a">https://towardsdatascience . com/explaible-ai-xai-methods-part-5-global-surrogate-models-9c 228d 27 e 13 a</a>)。然而，这种方法并不总是工作得很好，因为很难知道代理模型是否足够好地逼近原始黑盒模型的预测。因此，在实践中，经常使用DL模型的本地<a id="_idIndexMarker588"/>可解释性工具，例如<strong class="bold">SHapley Additive explaining</strong>(<strong class="bold">SHAP</strong>)，我们将在方法维度对此进行解释。</p>
			<h2 id="_idParaDest-119"><strong class="bold"> <a id="_idTextAnchor118"/>输入数据格式:输入数据的格式是什么</strong></h2>
			<p><strong class="bold">输入数据格式</strong>是指我们在开发和使用模型时，要处理什么样的输入数据。虽然简单的模型可能只关注单一类型的输入数据格式，如文本，但许多复杂的模型可能需要混合使用结构化表格数据和非结构化数据，如图像或文本。此外，还需要单独了解输入数据的隐藏偏差(在模型训练和验证期间)或漂移(在生产期间)。因此，这是一个相当复杂的话题。数据解释也可用于监测生产过程中的数据异常值和漂移。这适用于所有类型的ML/DL模型。</p>
			<h2 id="_idParaDest-120"><strong class="bold"> <a id="_idTextAnchor119"/>输出数据格式:输出说明</strong>的格式是什么</h2>
			<p><strong class="bold">输出解释格式</strong>指我们如何向目标受众展示解释。通常，图像解释可能是显示前几个特征及其分数的特征重要性的条形图，或者是突出显示每个图像中特定类对图像相关的ML问题的空间支持的显著性图<a id="_idIndexMarker589"/>。对于文本输出，它可以是一个英语句子，说明由于<a id="_idIndexMarker591"/>申请人可以理解的几个因素，信贷申请被拒绝的原因。<strong class="bold">自然语言处理(NLP) </strong>模型的可解释性也可以通过使用显著性地图、注意力和其他丰富可视化的交互式探索来实现(参见谷歌的<strong class="bold">语言可解释性工具</strong>(<strong class="bold">LIT</strong>):<a href="https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html">https://ai . Google blog . com/2020/11/the-Language-inter ability-Tool-LIT . html</a>)中的<a id="_idIndexMarker592"/>示例。由于没有解释这些复杂输出格式的灵丹妙药，所以满足要求解释的观众的需求、体验和期望是至关重要的。</p>
			<h2 id="_idParaDest-121"><strong class="bold"> <a id="_idTextAnchor120"/>问题类型:什么是机器学习问题类型</strong></h2>
			<p><strong class="bold">问题类型</strong>泛指各种ML/AI问题，但出于实用目的，目前商业上成功的问题多围绕分类、回归、聚类展开。强化学习和推荐系统在行业中也越来越多地被成功采用。DL模型现在经常被用于所有这些类型的问题，或者至少被评估为一个潜在的候选模型。</p>
			<h2 id="_idParaDest-122"><strong class="bold"> <a id="_idTextAnchor121"/>目标类型:解释</strong>的动机或目标是什么</h2>
			<p><strong class="bold">目标类型</strong>指在AI/ML项目中使用可解释性的动机。有人认为，可解释性的首要目标是通过提供对人工智能系统行为的充分理解和揭示系统的漏洞、偏见和缺陷来获得信任。另一个动机是从输入和输出预测中推断因果关系。其他目标包括通过更好地理解AI/ML系统的内部工作来提高模型的准确性，以及当涉及潜在的<a id="_idIndexMarker593"/>严重后果时，通过透明的解释来证明模型行为和决策的合理性。甚至有可能<a id="_idIndexMarker594"/>揭示基于解释的未知见解和规律(https://www . tandfonline . com/doi/full/10.1080/10580530 . 2020 . 1849465)。总的来说，打破黑盒是非常可取的，这样当在真实的生产系统中使用时，可以放心地使用AI/ML模型和系统。</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>方法类型:使用的特定事后解释方法是什么</h2>
			<p><strong class="bold">方法类型(事后)</strong>指与DL模型非常相关的事后方法。有两大类事后方法:基于扰动和基于梯度。最近的工作已经开始统一这两种方法，尽管它尚未广泛适用于实际应用(<a href="https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based">https://team core . seas . Harvard . edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based</a>)。以下是对这两种方法的简要讨论:</p>
			<ul>
				<li>基于扰动的方法利用单个实例的扰动来构建可解释的局部近似，使用线性模型来解释<a id="_idIndexMarker595"/>预测。最受欢迎的基于扰动的方法包括<strong class="bold">局部可解释的模型不可知解释</strong> ( <strong class="bold">莱姆</strong>)、<a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a>)、SHAP，以及莱姆和SHAP的变体，如BayesLIME和BayesSHAP、TreeSHAP等等(<a href="https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df">https://towards data science . com/what-is-the-prevailing-explability-methods-3 BC 1a 44 f 94 df</a>)。LIME可用于表格、图像和文本输入数据，并且与模型无关。也就是说，LIME可以用于任何类型的分类器(基于树的或DL模型)，而不管所使用的算法。SHAP运用合作博弈论的原理来确定不同特征对预测的贡献，以便量化每个特征的影响。SHAP产生了一个所谓的shapely值，它是不同特征的所有可能联盟或组合的所有边际贡献的平均值。它适用于许多类型的模型，包括DL模型，尽管基于树的模型，如XGBoost或light GBM(<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>)的计算时间可能要快得多。</li>
				<li>基于梯度的<a id="_idIndexMarker596"/>方法，例如<a id="_idIndexMarker597"/>smooth grad(<a href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</a>)和集成梯度(<a href="https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf">https://towards data science . com/understanding-deep-learning-models-with-Integrated-Gradients-24 ddce 643 DBF</a>)，利用根据单个实例的输入维度计算的梯度来解释模型预测。它们可以应用于图像和文本输入数据，尽管有时，文本输入可能遭受操纵或对手攻击(<a href="https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269">https://towards data science . com/limits-of-integrated-gradients-for-feature-attribute-ca 2 a 50 e 7d 269</a>)，这将不期望地改变特征重要性。</li>
			</ul>
			<p>请注意，还有其他类型的方法，如反事实方法(https://christophm . github . io/interpretable-ml-book/counter factual . html)或基于原型的方法(<a href="https://christophm.github.io/interpretable-ml-book/proto.html">https://christophm . github . io/interpretable-ml-book/proto . html</a>)，我们不会在本书中介绍这些方法。</p>
			<p>在讨论了可解释性的许多维度之后，重要的是要知道XAI仍然是一个新兴领域(<a href="https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf">https://fairlyaccountable . org/aaai-2021-tutorial/doc/AAAI _ slides _ final . pdf</a>)，当应用于相同的数据集或模型时，有时甚至很难在不同的可解释性方法之间找到一致意见(参见最近一项关于《从实践者的角度看可解释的ML中的分歧问题的主题的研究:<a href="https://arxiv.org/abs/2202.01602">https://arxiv.org/abs/2202.01602</a>)。最后，它确实需要一些实验来找出哪种可解释性提供了符合现实世界中特定预测任务要求的人类验证的解释。</p>
			<p>在这一章的下两个部分，我们将集中使用一些流行的和新兴的工具包来提供一些动手实验，以学习如何做可解释性。</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>探索SHAP可解释性工具箱</h1>
			<p>为了我们的学习目的，让我们回顾一些流行的可解释性工具箱，同时用一些例子进行实验。根据GitHub stars的数量(截至2022年4月16，000，<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>)，SHAP是使用最广泛和集成最完整的开源模型可解释性工具箱。它也是与MLflow集成的基础解释工具。在这里，我们想运行一个小实验来获得一些关于这是如何工作的实践经验。让我们使用一个情感分析NLP模型来探索SHAP如何被用于解释模型行为:</p>
			<ol>
				<li>从GitHub查看本章代码后，在您的本地环境中设置虚拟环境。运行以下命令将创建一个名为<code>dl-explain</code> : <pre><strong class="bold">conda env create -f conda.yaml</strong></pre>的新虚拟环境</li>
			</ol>
			<p>这将在这个虚拟环境中安装SHAP及其相关的依赖项，如<code>matplotlib</code>。创建此虚拟环境后，通过运行以下命令激活此虚拟环境:</p>
			<pre><strong class="bold">conda activate dl-explain</strong></pre>
			<p>现在，我们准备对SHAP进行实验。</p>
			<ol>
				<li value="2">您可以查看<code>shap_explain.ipynb</code>笔记本，继续探索。这个笔记本的第一步是导入相关的Python库:<pre>import transformers import shap from shap.plots import *</pre></li>
			</ol>
			<p>这些导入<a id="_idIndexMarker601"/>将允许我们使用拥抱脸变形金刚管道API来获得预训练的NLP模型和SHAP函数。</p>
			<ol>
				<li value="3">然后，我们使用用于<code>sentiment_analysis</code>的transformers管道API创建<code>dl_model</code>。请注意，这是一个预训练的管道，因此我们可以使用它，而无需额外的微调。此管道中使用的默认转换器型号是<code>distilbert-base-uncased-finetuned-sst-2-english</code>(https://hugging face . co/distil Bert-base-un cased-fine tuned-SST-2-English):<pre>dl_model = transformers.pipeline(     'sentiment-analysis', return_all_scores=True)</pre></li>
			</ol>
			<p>这将产生一个准备好预测输入句子的正面或负面情绪的模型。</p>
			<ol>
				<li value="4">用两个输入句子试试这个<code>dl_model</code>，看看输出是否有意义:<pre>dl_model(     ["What a great movie! ...if you have no taste.",       "Not a good movie to spend time on."])</pre></li>
			</ol>
			<p>这将产生每个句子的标签和概率分数的输出，如下所示:</p>
			<pre>[[{'label': 'NEGATIVE', 'score': 0.00014734962314832956}, {'label': 'POSITIVE', 'score': 0.9998526573181152}], [{'label': 'NEGATIVE', 'score': 0.9997993111610413}, {'label': 'POSITIVE', 'score': 0.00020068213052581996}]]</pre>
			<p>好像第一句被大概率预测为<code>POSITIVE</code>，第二句被大概率预测为<code>NEGATIVE</code>。现在，如果我们深入研究第一句话，我们可能会认为模型预测是不正确的，因为在句子的第二部分(<code>no taste</code>)有一种微妙的负面情绪。所以，我们想知道为什么模型会做出这样的预测。这就是模型可解释性发挥作用的地方。</p>
			<ol>
				<li value="5">现在，让我们使用<a id="_idIndexMarker602"/>SHAP API<code>shap.Explainer</code>，来获得我们想要解释的两个句子的Shapley值:<pre>explainer = shap.Explainer(dl_model)  shap_values = explainer(["What a great movie! ...if you have no taste.", "Not a good movie to spend time on."])</pre></li>
				<li>一旦我们有了<code>shap_values</code>，我们就可以使用不同的可视化技术来可视化Shapley值。第一种是当预测标签为<code>POSITIVE</code> : <pre>shap.plots.text(shap_values[0, :, "POSITIVE"])</pre>时，使用<code>shap.plot.text</code>可视化第一句话的Shapley值</li>
			</ol>
			<p>这将产生如下图:</p>
			<div><div><img src="img/B18120_09_002.jpg" alt="Figure 9.2 – SHAP visualization for sentence 1 with a positive prediction &#13;&#10;" width="1263" height="216"/>
				</div>
			</div>
			<p class="figure-caption">图9.2–句子1的SHAP可视化，有正面预测</p>
			<p>从<em class="italic">图9.2 </em>中可以看出，<code>great</code>这个词的SHAP值非常大，主导了最终预测的影响，而<code>no</code>这个词对最终预测的影响较小。这导致了<code>POSITIVE</code>的最终预测结果。那么，第二句带有<code>NEGATIVE</code>预测的话呢？运行以下命令将产生类似的图形:</p>
			<pre>shap.plots.text(shap_values[1, :, "NEGATIVE"])</pre>
			<p>该命令<a id="_idIndexMarker603"/>创建如下图形:</p>
			<div><div><img src="img/B18120_09_003.jpg" alt="Figure 9.3 – SHAP visualization for sentence 2 with a negative prediction &#13;&#10;" width="1265" height="216"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">图9.3-带有负面预测的句子2的SHAP可视化</p>
			<p>从<em class="italic">图9.3 </em>可以看出，<code>Not</code>这个词对最终预测的影响很大，而<code>good</code>这个词的影响非常小，导致最终预测出一个<code>NEGATIVE</code>的情绪。这很有道理，很好的解释了模型的行为。</p>
			<ol>
				<li value="7">我们也可以使用不同的图来可视化<code>shap_values</code>。常见的一种是柱状图，它描绘了特征对最终预测的贡献。运行以下命令将为第一句话生成一个情节:<pre>bar(shap_values[0, :,'POSITIVE'])</pre></li>
			</ol>
			<p>这将生成如下条形图:</p>
			<div><div><img src="img/B18120_09_004.jpg" alt="Figure 9.4 – SHAP bar chart for sentence 1 with a positive prediction &#13;&#10;" width="625" height="401"/>
				</div>
			</div>
			<p class="figure-caption">图9.4-第1句的SHAP条形图，有正面预测</p>
			<p>从<em class="italic">图9.4 </em>中<a id="_idIndexMarker604"/>可以看出，图表从上到下排列最重要的特征，其中对最终预测有积极影响的前几个特征绘制在<em class="italic"> x </em>轴的正侧，而负贡献绘制在<em class="italic"> x </em>轴的负侧。<em class="italic"> x </em>轴是带有符号(+或-)的每个标记或单词的SHAP值。这清楚地表明单词<code>great</code>是影响最终预测的一个强有力的正面因素，而<code>have no taste</code>具有一些负面影响，但不足以改变最终预测的方向。</p>
			<p>同样，我们可以为第二句话绘制一个条形图，如下所示:</p>
			<pre>bar(shap_values[1, :,'NEGATIVE'])</pre>
			<p>这将产生以下条形图:</p>
			<div><div><img src="img/B18120_09_005.jpg" alt="Figure 9.5 – SHAP bar chart for sentence 2 with a negative prediction &#13;&#10;" width="661" height="401"/>
				</div>
			</div>
			<p class="figure-caption">图9.5-带有负面预测的句子2的SHAP条形图</p>
			<p>从<em class="italic">图9.5 </em>可以看出，<code>Not</code>一词对最终预测的贡献很大，而<code>good</code>一词次之。这两个单词对最终的预测有相反的影响，但是显然，单词<code>Not</code>更强，并且有更大的SHAP值。</p>
			<p>如果你遵循了这个例子，并且在笔记本上看到了SHAP图表，那么恭喜你！这意味着你已经成功运行了SHAP可解释性工具来解释NLP文本情感分析的DL transformer模型。</p>
			<p>让我们进一步探索另一个流行的可解释性工具，看看它们如何执行不同的解释。</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>探索变形金刚解读工具箱</h1>
			<p>正如我们已经在本章第一节回顾的，有两种主要的方法:基于扰动和基于梯度的事后可解释性工具。SHAP属于微扰论家族。现在，让我们来看一个基于渐变的工具箱，叫做<strong class="bold">变形金刚解读</strong>(<a href="https://github.com/cdpierse/transformers-interpret">https://github.com/cdpierse/transformers-interpret</a>)。这个<a id="_idIndexMarker607"/>是一个相对较新的工具，但它是建立在PyTorch <a id="_idIndexMarker608"/>的统一模型可解释性和理解库之上的，这个库叫做<strong class="bold">Captum</strong>(<a href="https://github.com/pytorch/captum">https://github.com/pytorch/captum</a>)，它<a id="_idIndexMarker609"/>提供了一个统一的API来使用扰动或基于梯度的工具(<a href="https://arxiv.org/abs/2009.07896">https://arxiv.org/abs/2009.07896</a>)。Transformers Interpret进一步简化了Captum的API，以便我们可以快速探索基于梯度的可解释性方法，获得一些实践经验。</p>
			<p>要开始，首先确保您已经设置并激活了<code>dl-explain</code>虚拟环境，如前一节所述。然后，我们可以使用相同的拥抱脸变压器情感分析模型来探索一些NLP情感分类的例子。然后，我们可以执行以下步骤来学习如何使用Transformers <a id="_idIndexMarker610"/> Interpret来做模型解释。您可能想要查看<code>gradient_explain.ipynb</code>笔记本，按照说明进行操作:</p>
			<ol>
				<li value="1">将相关包导入笔记本，如下:<pre>from transformers import AutoModelForSequenceClassification, AutoTokenizer from transformers_interpret import SequenceClassificationExplainer</pre></li>
			</ol>
			<p>这将使用拥抱脸的transformer模型和tokenizer，以及来自<code>transformers_interpret</code>的可解释函数。</p>
			<ol>
				<li value="2">使用与上一节相同的预训练模型创建模型和记号赋予器，它是<code>distilbert-base-uncased-finetuned-sst-2-english</code>模型:<pre>model_name = "distilbert-base-uncased-finetuned-sst-2-english" model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name)</pre></li>
			</ol>
			<p>现在我们有了模型和标记器，我们可以使用<code>SequenceClassificationExplainer</code> API创建一个可解释变量。</p>
			<ol>
				<li value="3">创建一个解释者，给出一个例句，从解释者那里得到<code>word</code>属性:<pre>cls_explainer = SequenceClassificationExplainer(model, tokenizer) word_attributions = cls_explainer("Not a good movie to spend time on.")</pre></li>
				<li>我们还可以在检查<code>word</code>属性之前，通过运行下面的命令获得预测标签:<pre>cls_explainer.predicted_class_name</pre></li>
			</ol>
			<p>这会产生一个结果<code>Negative</code>，表示预测是负面情绪。那么，我们来看看解释者是如何为这个预测提供解释的。</p>
			<ol>
				<li value="5">我们可以只<a id="_idIndexMarker611"/>显示<code>word_attributions</code>值，或者我们可以可视化它。<code>word_attributions</code>的值如下:</li>
			</ol>
			<div><div><img src="img/B18120_09_006.jpg" alt="Figure 9.6 – Layered integrated gradient word attribution values with a negative prediction &#13;&#10;" width="301" height="300"/>
				</div>
			</div>
			<p class="figure-caption">图9.6–带有负面预测的分层综合梯度单词属性值</p>
			<p>从<em class="italic">图9.6 </em>中可以看出，使用分层整合梯度法，这是当前解释者在变形金刚解释库中实现的默认方法，单词<code>not</code>对最终预测结果的贡献是正面的，是负面情绪。这是有道理的。请注意，其他几个词，如<code>to spend time on</code>，对最终预测也有很大的积极影响。鉴于交叉注意机制，似乎该模型正试图提取<code>not to spend time on</code>作为最终预测的主要归因。注意，我们也可以将这些<code>word</code>属性形象化如下:</p>
			<pre>cls_explainer.visualize("distilbert_viz.html")</pre>
			<p>这将<a id="_idIndexMarker612"/>产生以下情节:</p>
			<div><div><img src="img/B18120_09_007.jpg" alt="Figure 9.7 – Layered integrated gradient word attribution values with a negative prediction &#13;&#10;" width="882" height="100"/>
				</div>
			</div>
			<p class="figure-caption">图9.7–带有负面预测的分层综合梯度单词属性值</p>
			<p>从<em class="italic">图9.7 </em>中可以看出，它突出了<code>not to spend time on</code>对最终负面预测产生正面影响的单词重要性。</p>
			<p>既然我们已经试验了扰动和基于梯度的可解释性方法，我们已经成功地完成了使用<a id="_idIndexMarker613"/>可解释性工具进行事后局部解释的实践探索。</p>
			<p>接下来，我们将总结一下本章所学的内容。</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>总结</h1>
			<p>在这一章中，我们通过八个维度的分类回顾了人工智能/人工智能中的可解释性。虽然这不一定是一个全面或详尽的概述，但它确实给了我们一个大画面，向谁解释，解释的不同阶段和范围，解释的各种输入和输出格式，常见的ML问题和目标类型，以及最后，不同的事后解释方法。然后，我们提供了两个具体的练习来探索SHAP和变压器解释工具箱，它们可以为NLP文本情感DL模型提供扰动和基于梯度的特征属性解释。</p>
			<p>这为我们使用DL模型的可解释工具提供了坚实的基础。然而，鉴于XAI的积极发展，这只是在DL模型中使用XAI的开始。其他可解释性工具箱如TruLens(<a href="https://github.com/truera/trulens">https://github.com/truera/trulens</a>)、Alibi(【https://github.com/SeldonIO/alibi】<a href="https://github.com/SeldonIO/alibi">)、微软Responsible AI工具箱(</a><a href="https://github.com/microsoft/responsible-ai-toolbox">https://github.com/microsoft/responsible-ai-toolbox</a>)、IBM AI explain ability 360 Toolkit(<a href="https://github.com/Trusted-AI/AIX360">https://github.com/Trusted-AI/AIX360</a>)都在积极开发中，值得研究和未来学习。在<em class="italic">进一步阅读</em>部分还提供了其他链接，以帮助您继续学习该主题。</p>
			<p>现在我们知道了可解释性的基础，在下一章，我们将学习如何在MLflow框架中实现可解释性，这样我们就可以提供一种统一的方式来支持MLflow框架中的解释。</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>延伸阅读</h1>
			<ul>
				<li><em class="italic">可解释AI中的新前沿</em>:<a href="https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348">https://towards data science . com/New-frontiers-in-explain-AI-af 43 BBA 18348</a></li>
				<li><em class="italic">走向可解释机器学习的严谨科学</em>:<a href="https://arxiv.org/pdf/1702.08608.pdf">https://arxiv.org/pdf/1702.08608.pdf</a></li>
				<li><em class="italic">可信人工智能的工具包方法</em>:<a href="https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/">https://opendata science . com/The-Toolkit-Approach-to-trust-AI/</a></li>
				<li><em class="italic">通过概念学习特定可解释模型的框架</em>:<a href="https://arxiv.org/abs/2108.11761">https://arxiv.org/abs/2108.11761</a></li>
				<li><em class="italic">揭开ML模型的事后可解释性</em>:<a href="https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability">https://spectra . math pix . com/article/2021 . 09 . 00007/demystify-事后可解释性</a></li>
				<li><em class="italic">对全球、群组和局部模型可解释性的研究</em>:<a href="https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f">https://towards data science . com/A-Look-Into-Global-Cohort-and-Local-Model-explability-973 BD 449969 f</a></li>
				<li>流行的解释方法有哪些？<a href="https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df">https://towards data science . com/what-the-prevailing-explability-methods-3 BC 1a 44 f 94 df</a></li>
				<li><em class="italic">可解释的人工智能:目标、利益相关者和未来的研究机会</em>:<a href="https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465">https://www . tandfonline . com/doi/full/10.1080/10580530 . 2020 . 1849465</a></li>
			</ul>
		</div>
	</div>
</body></html>