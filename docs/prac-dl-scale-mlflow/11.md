# *第七章*:多步深度学习推理流水线

既然我们已经成功地运行了**(**超参数优化**)并生成了一个满足业务需求的良好的 DL 模型，那么是时候进入下一步，使用这个模型进行预测了。这就是模型推理管道发挥作用的地方，其中模型用于实时或批处理模式下对生产中的真实世界数据进行预测或评分。然而，推理管道通常不仅仅依赖于单个模型，而是需要预处理和后处理逻辑，这些逻辑在模型开发阶段不一定能看到。预处理步骤的例子包括在将输入数据传递给模型进行评分之前检测语言环境(英语或一些其他语言)。后处理可以包括用额外的元数据丰富预测的标签，以满足业务应用的要求。还有一些 ML/DL 推理管道的模式，甚至可能涉及一组模型来解决现实世界的业务问题。许多 ML 项目经常低估实现生产推理管道所需的努力，这可能导致模型在生产中的性能下降，或者在最坏的情况下，整个项目失败。因此，在我们将模型部署到产品中之前，学习如何识别不同推理管道的模式并正确地实现它们是很重要的。**

 **在本章结束时，您将能够使用 MLflow 自信地实现多步推理管道的预处理和后处理步骤，并准备在后续章节中用于生产。

在本章中，我们将讨论以下主要话题:

*   理解 DL 推理管道的模式
*   了解 MLflow 模型 Python 函数 API
*   实现自定义 MLflow Python 模型
*   在 DL 推理流水线中实现预处理和后处理步骤
*   在主 ML 项目中实现推理管道作为新的入口点

# 技术要求

本章的技术要求如下:

*   本章 GitHub 代码:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 07](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter07)
*   一个成熟的本地 MLflow 跟踪服务器，如第 3 章[](B18120_03_ePub.xhtml#_idTextAnchor040)*、*跟踪模型、参数和指标*中所述。*

 *# 理解 DL 推理管道的模式

随着模型开发进入为即将到来的生产使用实现推理管道的阶段，重要的是要理解，拥有一个经过良好调整和训练的 DL 模型只是商业 AI 战略成功故事的一半。另一半包括模型投入生产后的部署、服务、监控和持续改进。设计和实现 DL 推理管道是故事后半部分的第一步。虽然该模型已经在精选的离线数据集上进行了训练、调整和测试，但现在它需要以两种方式处理预测:

*   **批量推理**:这通常需要为一些离线批量观察数据执行一些预定的或特别的推理管道。产生预测结果的周转时间是每天、每周或其他时间表。
*   **在线推理**:这通常需要一个 web 服务来实时执行一个推理管道，根据用户场景，该管道在不到一秒甚至不到 100 毫秒的时间内为输入数据生成预测结果。

请注意，由于执行环境和数据特征可能与离线培训和测试环境不同，因此在模型培训和调整步骤期间，将围绕核心模型逻辑进行额外的预处理或后处理步骤。虽然应该强调的是，任何可共享的数据预处理步骤都应该在训练管道和推理管道中使用，但不可避免的是，一些业务逻辑将发挥作用，这将允许推理管道具有附加的预处理和后处理逻辑。例如，DL 推理管道中一个非常常见的步骤是使用缓存来存储和返回基于最近看到的输入的预测结果，以便不需要调用昂贵的模型评估。在模型开发阶段，培训/测试管道不需要此步骤。

虽然推理管道的模式仍在出现，但现在普遍认为在现实世界的生产环境中至少有四种模式:

*   **多步流水线**:这个是模型在生产中最典型的用法，它包括模型逻辑被调用前的预处理步骤和模型评估结果返回后的一些后处理步骤的线性工作流。虽然这在概念上很简单，但实现仍然可以是多种多样的。我们将在本章中看到如何使用 MLflow 高效地完成这项工作。
*   **模型集合**:这是一个更复杂的场景，可以使用多个不同的模型。这些可能是具有用于 A/B 测试目的的不同版本的相同类型的模型，或者是不同类型的模型。例如，对于复杂的对话式 AI 聊天机器人场景，需要用户查询的意图分类模型来将用户意图分类到特定类别中。则还需要内容相关性模型来基于检测到的用户意图检索相关答案以呈现给用户。
*   **业务逻辑和模型**:这通常涉及到额外的业务逻辑，关于模型的输入应该如何以及从哪里来，比如从企业数据库中查询用户信息和验证，或者在调用模型之前从特性库中检索预先计算的额外特性。此外，后处理业务逻辑还可以将预测结果转换成一些特定于应用程序的逻辑，并将结果存储在一些后端存储中。虽然这可能像线性多步骤流水线一样简单，但它也可以快速地成为 **DAG** ( **有向无环图**)，在模型被调用之前和之后有多个扇入和扇出并行任务。
*   **在线学习**:这是生产中最复杂的推理任务之一，其中模型不断学习和更新其参数，如强化学习。

虽然有必要了解生产中推理管道复杂性的全貌，但本章的目的是了解我们如何通过强大的通用 MLflow 模型 API 创建可在多种场景中使用的推理管道的可重用构建块，该 API 可以封装预处理和后处理步骤以及经过训练的模型。鼓励感兴趣的读者从这篇文章([https://www . any scale . com/blog/serving-ml-models-in-production-common-patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns))和*延伸阅读*部分的其他参考资料中了解更多关于生产中的模型模式的信息。

那么，什么是 MLflow 模型 API，以及如何使用它来实现多步推理管道的预处理和后处理逻辑？让我们在下一节中找出答案。

作为多流模型的多步推理流水线

之前，在 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040) 、*跟踪模型、参数和度量*中，我们介绍了使用 MLflow **MLproject** 的灵活的松散耦合多步流水线实现，以便我们可以在 MLflow 中显式地执行和跟踪多步训练流水线。然而，在推理期间，除了已经记录在模型库中的经过训练的模型之外，还需要实现轻量级的预处理和后处理逻辑。MLflow 模型 API 提供了一种机制，用预处理和后处理逻辑包装训练模型，然后将新包装的模型保存为封装推理管道逻辑的新模型。这统一了使用 MLflow 模型 API 加载原始模型或推理管道模型的方式。这对于使用 MLflow 进行灵活部署至关重要，并为创造性的推理管道构建打开了大门。

## 了解 MLflow 模型 Python 函数 API

MLflow 模型([https://www.mlflow.org/docs/latest/models.html#id25](https://www.mlflow.org/docs/latest/models.html#id25))是 MLflow 提供的核心组件之一，用于加载、保存和记录不同风格的模型(例如，`MLmodel`文件:

![Figure 7.1 – MLmodel content for a fine-tuned PyTorch model
](img/B18120_07_01.jpg)

图 7.1-微调 PyTorch 模型的 MLmodel 内容

从*图 7.1* 可以看出，这款的味道是 PyTorch。还有一些关于模型的其他元数据，比如 conda 环境，它定义了运行模型的依赖关系，以及其他许多元数据。给定这个自包含的信息，应该足以允许 MLflow 使用如下的`mlflow.pytorch.load_model` API 加载模型:

```
logged_model = f'runs:/{run_id}/model'
model = mlflow.pytorch.load_model(logged_model)
```

这将允许使用`run_id`将 MLflow 运行记录的模型加载回内存并进行推断。现在假设我们有以下场景，我们需要添加一些预处理逻辑来检查输入文本的语言类型。这需要加载一个语言检测器模型([https://amitness.com/2019/07/identify-text-language-python/](https://amitness.com/2019/07/identify-text-language-python/))比如如 **FastText** 语言检测器([https://fasttext.cc/](https://fasttext.cc/))，或者谷歌的 **Compact 语言检测器 v3**([https://pypi.org/project/gcld3/](https://pypi.org/project/gcld3/))。此外，我们还想检查对于完全相同的输入是否有任何缓存的预测。如果它存在，那么我们应该只返回缓存的结果，而不调用昂贵的模型预测部分。这是非常典型的预处理逻辑。对于后处理，一种常见的情况是返回预测以及一些关于模型 URIs 的元数据，以便我们可以在生产中调试任何潜在的预测问题。根据这种预处理和后处理逻辑，推理管道现在看起来如下图所示:

![Figure 7.2 – Multi-step inference pipeline
](img/B18120_07_02.jpg)

图 7.2–多步推理管道

从*图 7.2* 中可以看出，这五个步骤包括以下内容:

*   一个用于预测的原始微调模型(PyTorch DL 模型)
*   一个额外的语言检测模型，它不属于我们之前的培训项目
*   用于提高响应性能的缓存操作(检查缓存和存储到缓存)
*   一个响应消息组成步骤

与其将这五个步骤分成一个 **ML 项目**中的五个不同的入口点(回想一下 **ML 项目**中的入口点可以是 Python 或其他可执行文件中的任意执行代码)，不如在一个入口点中构建这个多步骤推理管道，因为这些步骤与模型的预测步骤密切相关。此外，将这些密切相关的步骤封装到单个推理管道中的优点是，我们可以将推理管道作为 MLmodel 工件进行保存和加载。MLflow 提供了一种通用方法来将这种多步推理管道实现为一种新的 Python 模型，同时又不失灵活性，可以根据需要添加额外的预处理和后处理功能，如下图所示:

![ Figure 7.3 – Encapsulate the multi-step preprocessing and postprocessing logic into 
a new MLflow Python model
](img/B18120_07_03.jpg)

图 7.3–将多步预处理和后处理逻辑封装到新的 MLflow Python 模型中

从*图 7.3* 中可以看出，如果我们将预处理和后处理逻辑封装到一个名为`inference_pipeline_model`的新 MLflow 模型中，那么我们可以加载整个推理管道，就好像它只是另一个模型一样。这也将允许我们为推理管道形式化输入和输出格式(称为**模型签名**),以便任何想要使用该推理管道的人都不需要猜测输入和输出的格式。

实现这一点的高级机制如下:

1.  首先创建一个自定义 MLflow `pyfunc` (Python 函数)模型([https://www . ml flow . org/docs/latest/Python _ API/ml flow . py func . html # creating-custom-py func-models](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models))来包装已有的训练好的模型。具体来说，我们需要超越 mlflow 提供的内置模型风格([https://www . ml flow . org/docs/latest/models . html # built-in-model-flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors))，并实现一个继承自`mlflow.pyfunc.PythonModel` ([https://www . ml flow . org/docs/latest/Python _ API/ml flow . py func . html # ml flow . py func . Python model](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))的新 Python 类，定义`predict()`并可选地定义

此外，我们可以通过定义模型输入和输出的模式来指定**模型签名**([https://mlflow.org/docs/latest/models.html#model-signature](https://mlflow.org/docs/latest/models.html#model-signature))。这些模式可以是基于列的，也可以是基于张量的。强烈建议在生产环境中为自动输入验证和模型诊断实现这些模式。

1.  然后在这个 MLflow `pyfunc`中实现预处理和后处理逻辑。这些可能包括缓存、语言检测、响应消息和任何其他需要的逻辑。
2.  最后，在 ML 项目中实现推理管道的入口点，这样我们就可以像调用单个模型工件一样调用推理管道。

现在，我们已经了解了 MLflow 的自定义 Python 模型的基本原理，以表示多步推理管道，让我们看看如何使用下面几节中的*图 7.3* 中描述的预处理和后处理步骤为我们的 NLP 情感分类模型实现它。

# 实现定制的 MLflow Python 模型

让我们首先描述在没有任何额外预处理和后处理逻辑的情况下实现定制 MLflow Python 模型的步骤:

1.  首先，确保我们有一个经过训练的 DL 模型，可以用于推理目的。为了便于本章的学习，我们将训练管道`README`文件包含在本章的 GitHub 存储库中，并*相应地设置环境变量*([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/readme . MD](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/README.md))。然后，在命令行中运行以下命令，在本地 MLflow 跟踪服务器中生成一个微调模型:

    ```
    mlflow run . --experiment-name dl_model_chapter07 -P pipeline_steps=download_data,fine_tuning_model
    ```

完成后，您将在 MLflow 跟踪服务器中记录一个经过微调的 DL 模型。现在，我们将使用记录的模型 URI 作为推理管道的输入，因为我们将包装它并将其保存为新的 MLflow 模型。记录的模型 URI 如下所示，其中长的随机字母数字字符串是`fine_tuning_model` MLflow 运行的`run_id`，您可以在 MLflow 跟踪服务器中找到它:

```
runs:/1290f813d8e74a249c86eeab9f6ed24e/model
```

1.  一旦您有了一个经过训练/微调的模型，我们就可以实现一个新的定制 MLflow Python 模型，如下所示。您可能想要查看 GitHub repo 中的`basic_custom_dl_model.py`([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/notebooks/basic _ custom _ dl _ model . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/basic_custom_dl_model.py))来完成这里概述的步骤:

    ```
    class InferencePipeline(mlflow.pyfunc.PythonModel):     def __init__(self, finetuned_model_uri):         self.finetuned_model_uri = finetuned_model_uri     def sentiment_classifier(self, row):         pred_label = self.finetuned_text_classifier.predict({row[0]})         return pred_label     def load_context(self, context):         self.finetuned_text_classifier = mlflow.pytorch.load_model(self.finetuned_model_uri)     def predict(self, context, model_input):         results = model_input.apply(                     self.sentiment_classifier, axis=1,                     result_type='broadcast')         return results
    ```

让我们看看我们实现了什么。首先，`InferencePipeline`类继承自`MLflow.pyfunc.PythonModel`模块，并实现如下四个方法:

*   `predict`:这是`mlflow.pyfunc.PythonModel`需要的方法，返回预测结果。这里，`model_input`参数是一个`pandas`数据帧，它包含一列需要分类的输入文本。我们利用`pandas`数据帧的`apply`方法运行`sentiment_classifier`方法对数据帧文本的每一行进行评分，并且结果是一个数据帧，每一行都是预测的标签。由于我们最初的微调模型不接受`pandas` DataFrame 作为输入(它接受一列文本字符串作为输入)，我们需要实现一个新的分类器作为原始模型的包装器。就是这个`sentiment_classifier`法。另一个`context`参数是 MLflow 上下文，用于描述模型工件的存储位置。由于我们将传递一个 MLflow 日志模型 URI，这个`context`参数不在我们的实现中使用，因为日志模型 URI 包含 MLflow 加载模型所需的一切。
*   `sentiment_classifier`:这是一个包装器方法，允许通过调用微调的 DL 模型的预测函数对输入`pandas`数据帧的每一行进行评分。请注意，我们将该行的第一个元素包装到一个列表中，以便 DL 模型可以正确地将它用作输入。
*   `init`:这是一个标准的 Python 构造器方法。在这里，我们使用它来传入一个先前微调过的 DL 模型 URI，`finetuned_model_uri`，这样我们就可以在`load_context`方法中加载它。请注意，我们不希望直接在`init`方法中加载模型，因为这会导致序列化问题(如果您想尝试，您会发现天真地序列化 DL 模型并不是一种有趣的体验)。由于经过微调的 DL 模型已经通过`mlflow.pytorch`API 进行了序列化和反序列化，我们不应该在这里重新发明轮子。推荐的方法是用`load_context`方法加载模型。
*   `load_context`:用`mlflow.pyfunc.load_model` API 加载 MLflow 模型时调用该方法。这将在构建 Python 模型后立即执行。这里，我们通过使用`mlflow.pytorch.load_model` API 来加载微调后的 DL 模型。注意，这个方法中加载的任何模型都可以使用它们对应的反序列化方法。这将为加载其他模型打开大门，例如语言检测模型，它可能包含无法使用 Python 序列化协议序列化的本机代码(例如，C++代码)。这是 MLflow 模型 API 框架提供的一个很好的特性。

1.  现在我们有了一个可以接受基于列的输入的 MLflow 定制模型，我们还可以如下定义模型签名:

    ```
    input = json.dumps([{'name': 'text', 'type': 'string'}]) output = json.dumps([{'name': 'text', 'type': 'string'}]) signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})
    ```

这个签名定义了一个输入格式，其中有一个名为`text`的列，数据类型为`string`，还有一个输出格式，其中有一个名为`text`的列，数据类型为`string`。`mlflow.models.ModelSignature`类用于创建这个`signature`对象。这将在我们在 MLflow 中记录新的定制模型时使用，我们将在下一步中看到。

1.  接下来，我们可以在 MLflow 中记录这个新的定制模型，就像这是一个使用如下`mlflow.pyfunc.log_model` API 的通用 MLflow `pyfunc`模型一样:

    ```
    MODEL_ARTIFACT_PATH = 'inference_pipeline_model' with mlflow.start_run() as dl_model_tracking_run:     finetuned_model_uri = 'runs:/1290f813d8e74a249c86eeab9f6ed24e/model'     inference_pipeline_uri = f'runs:/{dl_model_tracking_run.info.run_id}/{MODEL_ARTIFACT_PATH}'     mlflow.pyfunc.log_model(       artifact_path=MODEL_ARTIFACT_PATH,       conda_env=CONDA_ENV,       python_model=InferencePipeline(         finetuned_model_uri),       signature=signature)
    ```

前面的代码将用名为`inference_pipeline_model`的顶级文件夹在 MLflow tracking server 中记录一个模型，因为我们用这个字符串值定义了`MODEL_ARTIFACT_PATH`变量，并将这个值赋给了`mlflow.pyfunc.log_model`方法的`artifact_path`参数。我们指定的其他三个参数如下:

*   `conda_env`:这是为了定义定制模型运行的 conda 环境。这里我们可以通过`CONDA_ENV`变量定义的本章根文件夹中`conda.yaml`文件的绝对路径(这个变量的详细信息可以在 GitHub 上这个`basic_custom_dl_model.py`笔记本的源代码中找到)。
*   `python_model`:这里我们调用刚刚实现的新`InferencePipeline`类，传入`finetuned_model_uri`的参数。这样，推理管道将为预测目的加载正确的微调模型。
*   `signature`:我们还为刚刚定义的输入和输出传递签名，并将其分配给 signature 参数，以便可以记录模型输入和输出模式，并出于验证目的而强制执行。

作为提醒，确保用您自己在*步骤 1* 中生成的微调模型 URI 替换`finetuned_model_uri`变量的`'runs:/1290f813d8e74a249c86eeab9f6ed24e/model'`值，这样代码将正确加载原始微调模型。

1.  如果您按照`basic_custom_dl_model.py`一个单元一个单元地运行，直到*步骤 4* ，您应该能够在 MLflow tracking server 的**工件**部分找到一个新登录的模型，如下图所示:

![Figure 7.4 – Inference MLflow model with model schema and a root folder of inference_pipeline_model
](img/B18120_07_04.jpg)

图 7.4–推理 MLflow 模型，具有模型模式和推理 _ 管道 _ 模型的根文件夹

从*图 7.4* 中可以看出，根文件夹名称(截图左上方)是`inference_pipeline_model`，这是调用`mlflow.pyfunc.log_model`时`artifact_path`参数的赋值。注意，如果我们不指定`artifact_path`参数，默认情况下它将只是`model`。您可以通过查看本章前面的*图 7.1* 来确认这一点。还要注意，现在在`inference_pipeline_model`文件夹下有一个`MLmodel`文件，我们可以看到完整的内容如下:

![Figure 7.5 – The content of inference_pipeline_model's MLmodel file
](img/B18120_07_05.jpg)

图 7.5–推理管道模型的 MLmodel 文件的内容

从*图 7.5* 中可以看出图中`signature`部分的内容接近底部，与*图 7.1* 相比是一个新的部分。然而，在模型风格方面有一些更重要的区别。`inference_pipeline_model`的味道是通用`mlflow.pyfunc.model`型号，不再是`mlflow.pytorch`型号。其实你对比一下*图 7.5* 和*图 7.1* ，也就是我们 PyTorch 微调的 DL 车型，有一段关于`pytorch`及其`model_data`和`pytorch_version`的内容，现在已经完全消失在*图 7.5* 中了。对于 MLflow，它不知道原始模型，即 PyTorch 模型，而只是作为新包装模型的通用 MLflow `pyfunc`模型。这是一个好消息，因为现在我们只需要一个通用的 MLflow `pyfunc` API 来加载模型，而不管包装的模型有多复杂，以及当我们在下一节实现它时，这个通用的`pyfunc`模型中还有多少预处理和后处理步骤。

1.  我们现在可以使用泛型`mlflow.pyfunc.load_model`加载`inference_pipeline_model`来加载模型，并使用输入`pandas`数据帧进行预测，如下所示:

    ```
    input = {"text":["what a disappointing movie","Great movie"]} input_df = pd.DataFrame(input) with mlflow.start_run():     loaded_model = \     mlflow.pyfunc.load_model(inference_pipeline_uri)     results = loaded_model.predict(input_df)
    ```

这里，`inference_pipeline_uri`是在*步骤 4* 中产生的 URI，作为`inference_pipeline_model`的唯一标识。例如，`inference_pipeline_uri`值可能如下所示:

```
'runs:/6edf6013d2454f7f8a303431105f25f2/inference_pipeline_model'
```

一旦加载了模型，我们就可以调用`predict`函数来对`input_df`数据帧进行评分。这调用了我们新实现的`InferencePipleine`类的`predict`函数，如*步骤 2* 所述。结果将如下所示:

![Figure 7.6 – Output of the inference pipeline in a pandas DataFrame format
](img/B18120_07_06.jpg)

图 7.6-熊猫数据帧格式的推理管道输出

如果您看到如图*图 7.6* 所示的预测结果，那么您应该感到自豪的是，您刚刚实现了一个有效的自定义 MLflow Python 模型，该模型具有巨大的灵活性和功能，使我们能够在不更改任何日志记录和加载模型部分的情况下实现预处理和后处理逻辑，我们将在下一节中看到这一点。

创建 MLflow 定制模型的新风格

如本章所示，我们可以使用一个已经训练好的模型构建一个包装好的 MLflow 定制模型，用于推理目的。应该注意的是，也可以构建一个全新风格的 MLflow 定制模型用于训练目的。当您有一个内置 MLflow 模型风格还不支持的模型时，这是需要的。例如，如果您想要基于您自己的语料库训练一个全新的 **FastText** 模型，但是在 MLflow 版本 1.23.1 中，还没有 **FastText** MLflow 模型风格，那么您可以构建一个新的 **FastText** MLflow 模型风格(参见参考:[https://medium . com/@ how-save-and-load-fast text-model-in-ml flow-format-33)感兴趣的读者还可以在本章末尾的*延伸阅读*部分找到更多参考资料。](mailto:https://medium.com/@pennyqxr/how-save-and-load-fasttext-model-in-mlflow-format-37e4d6017bf0)

# 在 DL 推理流水线中实现预处理和后处理步骤

现在我们有了一个基本的通用 MLflow Python 模型，它可以对一个输入`pandas`数据帧进行预测，并在另一个`pandas`数据帧中产生输出，我们已经准备好处理前面描述的多步推理场景。请注意，虽然上一节中的初始实现可能看起来并不是翻天覆地的，但这为实现之前不可能实现的预处理和后处理逻辑打开了大门，同时保持了使用通用的`mlflow.pyfunc.log_model`和`mlflow.pyfunc.load_model`将整个推理管道视为通用的`pyfunc`模型的能力，不管原始的 DL 模型有多复杂，也不管有多少额外的预处理和后处理步骤。让我们看看如何在这一部分做到这一点。您可能想从 GitHub([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/notebooks/multi step _ inference _ model . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/multistep_inference_model.py))查看用于`multistep_inference_model.py`的 VS 代码笔记本，以完成本节中的步骤。

在*图 7.3* 中，我们描述了模型预测之前的两个预处理步骤，以及模型预测之后的两个后处理步骤。那么，我们在哪里以及如何添加预处理和后处理逻辑，同时将整个推理管道保持为单个 MLflow 模型呢？原来主要的变化将发生在前面章节中实现的的`InferencePipeline`类中。让我们在下面的小节中一步一步地了解实现和变化。

## 实现语言检测预处理逻辑

让我们首先实现语言检测预处理逻辑:

1.  为了检测输入文本的语言类型，我们可以使用 Google 的`pyfunc`模型。好消息是 MLflow 的`load_context`方法允许我们加载这个模型，而不用担心序列化和反序列化。我们只需要在`InferencePipeline`类的`load_context`方法中添加两行代码来加载语言检测器模型:

    ```
    import gcld3 self.detector = gcld3.NNetLanguageIdentifier(     min_num_bytes=0,     max_num_bytes=1000)
    ```

前面两行被添加到`load_context`方法中，同时添加的还有预先存在的语句，该语句加载了用于情感分类的微调 DL 模型。这将允许语言检测器在`InferencePipeline`类初始化完成后立即加载。该语言检测器将使用输入的前 1000 个字节来确定语言类型。一旦加载了这个语言检测器，我们就可以用它在预处理方法中检测语言。

1.  在语言检测的预处理方法中，我们将接受输入文本的每一行，检测语言，并将语言类型作为`string`返回，如下:

    ```
    def preprocessing_step_lang_detect(self, row):     language_detected = \     self.detector.FindLanguage(text=row[0])     if language_detected.language != 'en':         print("found Non-English Language text!")     return language_detected.language
    ```

的实现很简单。我们还添加了一个打印输出，以查看控制台的输入中是否有任何非英语文本。如果您的业务逻辑要求您在处理某些特定语言时实现任何抢先的操作，那么您可以在这个方法中添加更多的逻辑。这里，我们只返回检测到的语言类型。

1.  然后，在对输入的每一行进行评分的`sentiment_classifier`方法中，我们可以只在预测之前添加一行来首先检测语言，如下所示:

    ```
    language_detected = self.preprocessing_step_lang_detect(row)
    ```

稍后，我们将变量`language_detected`传递给响应，我们将在后处理逻辑实现中看到。

这就是实现语言检测作为推理管道中的预处理步骤所要做的全部工作。

现在让我们看看如何实现另一个步骤:缓存，它需要预处理(检测对于相同的输入是否有任何预先存在的匹配预测结果)和后处理(在缓存中存储输入和预测结果的键值对)。

## 实现缓存预处理和后处理逻辑

让我们看看如何在`InferencePipeline`类中实现缓存:

1.  我们可以在`init`方法中添加一个新的语句来初始化缓存存储，因为它在序列化或反序列化时没有问题:

    ```
    from cachetools import LRUCache self.cache = LRUCache(100)
    ```

这将初始化一个存储了 100 个对象的最近最少使用的缓存。

1.  接下来，我们将添加一个预处理方法来检测缓存中是否有输入:

    ```
    def preprocessing_step_cache(self, row):     if row[0] in self.cache:         print("found cached result")         return self.cache[row[0]]
    ```

如果在缓存中找到作为键的精确输入行，则返回缓存的值。

1.  在`sentiment_classifier`方法中，我们可以添加预处理步骤来检查缓存，如果它找到缓存，那么它将立即返回缓存的响应，而不调用昂贵的 DL 模型分类器:

    ```
        cached_response = self.preprocessing_step_cache(row)     if cached_response is not None:         return cached_response
    ```

这个预处理步骤应该作为`sentiment_classifier`方法的第一步，在进行语言检测和模型预测之前。当有许多重复的输入时，这可以显著地加速实时预测响应。

1.  同样在`sentiment_classifier`方法中，我们需要添加一个后处理步骤来在缓存中存储新的输入和预测响应:

    ```
    self.cache[row[0]]=response
    ```

就是这样。我们已经成功地在`InferencePipeline`类中添加了缓存作为预处理和后处理步骤。

## 实现响应组合后处理逻辑

现在，让我们看看如何在调用原始 DL 模型预测并返回结果后，将响应组合逻辑作为后处理步骤来实现。仅仅返回一个预测标签`positive`或`negative`通常是不够的，因为我们想知道使用了哪个版本的模型，以及在生产环境中为调试和诊断检测了什么语言。对推理管道调用者的响应将不再是普通的字符串，而是序列化的 JSON 字符串。按照以下步骤实现后处理逻辑:

1.  在`InferencePipeline`类的`init`方法中，我们需要添加一个新的`inference_pipeline_uri`参数，这样我们就可以捕获这个通用 MLflow `pyfunc`模型的引用，以便进行出处跟踪。`finetuned_model_uri`和`inference_pipeline_uri`参数都将是响应的 JSON 对象的一部分。`init`方法现在看起来如下:

    ```
    def __init__(self,               finetuned_model_uri,              inference_pipeline_uri=None):     self.cache = LRUCache(100)     self.finetuned_model_uri = finetuned_model_uri     self.inference_pipeline_uri = inference_pipeline_uri
    ```

2.  在`sentiment_classifier`方法中，添加一个新的后处理语句，以根据检测到的语言、预测的标签和模型元数据(包括`finetuned_model_uri`和`inference_pipeline_uri` :

    ```
    response = json.dumps({                 'response': {                     'prediction_label': pred_label                 },                 'metadata': {                     'language_detected': language_detected,                 },                 'model_metadata': {                     'finetuned_model_uri': self.finetuned_model_uri,                     'inference_pipeline_model_uri': self.inference_pipeline_uri                 },             })                    
    ```

    组成一个新的响应

请注意，我们使用`json.dumps`将嵌套的 Python 字符串对象编码成 JSON 格式的字符串，这样调用者就可以使用 JSON 工具轻松解析出响应。

1.  在`mlflow.pyfunc.log_model`语句中，我们需要在调用`InferencePipeline`类时添加一个新的`inference_pipeline_uri`参数:

    ```
    mlflow.pyfunc.log_model(   artifact_path=MODEL_ARTIFACT_PATH,   conda_env=CONDA_ENV,   python_model=InferencePipeline(finetuned_model_uri,   inference_pipeline_uri),   signature=signature)
    ```

这将记录一个新的推理管道模型，其中包含我们实现的所有附加处理逻辑。这就完成了*图 7.3* 中描述的多步推理流水线的实现。

请注意，一旦用所有这些新步骤记录了模型，要使用这个新的推理管道，也就是说，要加载这个模型，需要零代码更改。我们可以像以前一样加载新登录的模型:

```
loaded_model = mlflow.pyfunc.load_model(inference_pipeline_uri)
```

如果到目前为止您已经完成了这些步骤，您还应该逐个单元地运行 VS 代码笔记本，直到本小节中描述的*步骤 3* 。现在我们可以尝试使用这个新的多步推理管道来测试它。我们可以准备一组新的输入数据，其中有重复项和一个非英语文本字符串，如下所示:

```
input = {"text":["what a disappointing movie", "Great movie", "Great movie", "很好看的电影"]}
input_df = pd.DataFrame(input)
```

该输入包括两个重复条目(`Great movie`)和一个中文文本串(输入列表中的最后一个元素，其中中文文本的含义与`Great Movie`相同)。现在我们可以像以前一样加载模型并调用`results = loaded_model.predict(input_df)`。在执行这条 predict 语句的过程中，您应该会在控制台输出中看到以下两条语句:

```
found cached result 
found Non-English language text.
```

这意味着我们的缓存和语言检测器起作用了！

我们还可以使用以下代码打印出结果，以仔细检查我们的多步管道是否工作:

```
for i in range(results.size):
    print(results['text'][i])
```

这将打印出响应的每一行的完整内容。这里，我们以显示最后一个的的输出(有中文文本)为例:

![Figure 7.7 – JSON response for the Chinese text string input using the multi-step inference pipeline
](img/B18120_07_07.jpg)

图 7.7–使用多步推理管道对中文文本字符串输入的 JSON 响应

从*图 7.7* 中可以看出，`prediction_label`包含在响应中(即`negative`)。由于我们一直在使用 JSON 响应中的`metadata`部分下的`language_detected`字段，我们看到了代表中文的字符串`"zh"`。这是语言检测器在预处理步骤中产生的结果。另外，`model_metadata`部分包括原来的`finetuned_model_uri`和`inference_pipeline_model_uri`。这些是特定于 MLflow 跟踪服务器的 URIs，我们可以使用它们来唯一地跟踪和识别哪个微调模型和推理管道用于此预测结果。这对于生产环境中的来源跟踪和诊断分析非常重要。将这个完整的 JSON 输出与图 7.6 中的早期预测标签输出进行比较，这为推理管道的消费者提供了更丰富的上下文信息。

如果您看到笔记本中的 JSON 输出如图*图 7.7* 所示，请为自己鼓掌，因为您刚刚完成了一个重要的里程碑，实现了一个多步推理管道，该管道可以被重用并部署到实际业务场景的生产中。

# 在主 MLproject 中实现一个推理管道作为新的入口点

现在我们已经成功地实现了一个多步推理管道作为一个新的定制 MLflow 模型，我们可以更进一步，将它作为一个新的入口点合并到主 **MLproject** 中，这样我们就可以端到端地运行下面的整个管道(*图 7.8* )。从 GitHub 查看本章的代码，在您的本地环境中跟踪和运行管道。

![Figure 7.8 – End-to-end pipeline using MLproject
](img/B18120_07_08.jpg)

图 7.8–使用 MLproject 的端到端管道

我们可以将新的入口点`inference_pipeline_model`添加到`MLproject`文件中。您可以在 GitHub 存储库([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/ml project](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/MLproject))上查看该文件:

```
inference_pipeline_model:
    parameters:
      finetuned_model_run_id: { type: str, default: None }
    command: "python pipeline/inference_pipeline_model.py --finetuned_model_run_id {finetuned_model_run_id}"
```

该入口点或步骤可以单独调用，也可以作为图 7.8 中*所示的整个管道的一部分调用。提醒一下，在执行 MLflow `run`命令之前，请确保您已经为 MLflow 跟踪服务器和后端存储 URIs 设置了本章`README`文件中描述的环境变量。这一步记录并注册一个新的`inference_pipeline_model`，它本身包含多步预处理和后处理逻辑。如果您知道`finetuned_model_run_id`，可以使用下面的命令在`chapter07`文件夹的根级别运行这个步骤:*

```
mlflow run . -e inference_pipeline_model  --experiment-name dl_model_chapter07 -P finetuned_model_run_id=07b900a96af04037a956c74ef691396e
```

这不仅会在 MLflow 跟踪服务器中记录新的`inference_pipeline_model`,还会在 MLflow 模型注册表中注册新版本的`inference_pipeline_model`。您可以通过以下链接在本地 MLflow 服务器中找到已注册的`inference_pipeline_model`:

```
http://localhost/#/models/inference_pipeline_model/
```

例如，下面的截图显示了一个已注册的`inference_pipeline_model`版本 6:

![Figure 7.9 – A registered inference_pipeline_model at version 6
](img/B18120_07_09.jpg)

图 7.9-版本 6 中注册的推理管道模型

也可以按如下方式运行图 7.8 中*所示的到整条端到端管道:*

```
mlflow run . --experiment-name dl_model_chapter07
```

这将运行这个端到端管道中的所有步骤，并以在模型注册表中记录和注册的`inference_pipeline_model`结束。

调用入口点`inference_pipeline_model`时执行的`inference_pipeline_model.py`的 Python 代码的实现，基本上复制了我们在 VS 代码笔记本中为`multistep_inference_model.py`实现的`InferencePipeline`类，并做了一些小的改动，如下所示:

*   添加一个要执行的任务函数作为该步骤的参数化入口点:

    ```
    def task(finetuned_model_run_id, pipeline_run_name):
    ```

这个函数的作用是启动一个新的 MLflow 运行来记录和注册一个新的推理管道模型。

*   登录时打开型号注册，如下:

    ```
    mlflow.pyfunc.log_model(     artifact_path=MODEL_ARTIFACT_PATH,     conda_env=CONDA_ENV,               python_model=InferencePipeline(         finetuned_model_uri,          inference_pipeline_uri),     signature=signature,     registered_model_name=MODEL_ARTIFACT_PATH)
    ```

注意我们给`registered_model_name`赋值`MODEL_ARTIFACT_PATH`的值，也就是`inference_pipeline_model`。这使得模型可以在 MLflow 模型注册表中以该名称注册，如*图 7.9* 所示。

这个新入口点的完整代码可以在 GitHub 存储库中找到:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/pipeline/inference _ pipeline _ model . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/pipeline/inference_pipeline_model.py)。

注意，我们还需要在`main.py`文件中添加一个新的部分，以允许从`main`入口点中调用`inference_pipeline_model`入口点。实现很简单，就像添加之前在第 4 章 、*跟踪代码和数据版本*中描述的其他步骤一样。有兴趣的读者可以查看一下 GitHub 的`main.py`文件来看看实现:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 07/main . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/main.py)。

这就完成了在 **MLproject** 中添加新入口点的实现，这样我们就可以使用 MLflow run 命令工具运行多步推理管道创建、日志记录和注册。

# 总结

在这一章中，我们讨论了一个非常重要的主题:使用 MLflow 的定制 Python 模型方法创建多步推理管道，即`mlflow.pyfunc.PythonModel`。

我们讨论了生产中推理工作流的四种模式，其中通常单个训练模型不足以完成业务应用程序需求。在模型训练和开发阶段，很可能看不到某些预处理和后处理逻辑。这就是为什么 MLflow 的`pyfunc`方法是一种实现定制 MLflow 模型的优雅方法，它可以用额外的预处理和后处理逻辑包装经过训练的 DL 模型。

我们成功实现了一个推理管道模型，该模型使用 Google 的紧凑语言检测器、缓存以及除预测标签之外的其他模型元数据，将我们的 DL 情感分类器与语言检测包装在一起。我们进一步将推理管道模型创建步骤合并到端到端模型开发工作流中，以便我们可以使用一个 MLflow run 命令生成一个注册的推理管道模型。

本章中的技巧和经验对于任何想使用 MLflow `pyfunc`方法实现真实世界推理管道的人来说都是至关重要的。这也为支持在生产场景中灵活而强大的部署打开了大门，我们将在下一章中讨论。

# 延伸阅读

*   *MLflow 模型* (MLflow 文档):[https://www.mlflow.org/docs/latest/models.html#](https://www.mlflow.org/docs/latest/models.html#)
*   *在 MLflow 中实现 statsmodels 风格*:[https://blog . stratio . com/Implementing-the-stats models-flavor-in-ml flow/](https://blog.stratio.com/implementing-the-statsmodels-flavor-in-mlflow/)
*   *InferLine: ML 推理管道组成框架*:[https://www2 . eecs . Berkeley . edu/Pubs/techr pts/2018/EECS-2018-76 . pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-76.pdf)
*   *批量推理 vs 在线推理*:[https://mlin production . com/Batch-Inference-vs-Online-Inference/](https://mlinproduction.com/batch-inference-vs-online-inference/)
*   *构建小型 MLOps 管道的经验教训*:[https://www . nestorsag . com/blog/Lessons-from-building-a-small-ml-ops-pipeline/](https://www.nestorsag.com/blog/lessons-from-building-a-small-ml-ops-pipeline/)
*   【https://vishsubramanian.me/hugging-face-with-mlflow/】与 MLflow : [拥抱脸上的文字总结者](https://vishsubramanian.me/hugging-face-with-mlflow/)***