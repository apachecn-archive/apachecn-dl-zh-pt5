# 十、使用 MLflow 实现 DL 可解释性

正如我们在前一章所了解到的，深度学习 ( **DL** )可解释性的重要性现在已经被很好地确立了。为了在现实世界的项目中实现 DL 可解释性，最好将解释者和解释作为工件记录下来，就像 MLflow 服务器中的其他模型工件一样，这样我们就可以轻松地跟踪和再现解释。DL 可解释性工具的集成，如 SHAP([https://github.com/slundberg/shap](https://github.com/slundberg/shap))与 MLflow 可以支持不同的实现机制，理解这些集成如何用于我们的 DL 可解释性场景是很重要的。在本章中，我们将通过使用不同的 MLflow 功能来探索将 SHAP 解释集成到 MLflow 中的几种方法。由于可解释性工具和 DL 模型都在快速发展，我们也将强调使用 MLflow 实现 DL 可解释性时当前的局限性和解决方法。在本章结束时，你将会对使用 MLflow APIs 实现可扩展模型解释能力的 SHAP 解释器和解释器感到舒适。

在本章中，我们将讨论以下主要话题:

*   了解当前 MLflow 可解释性集成
*   使用 MLflow 工件日志记录 API 实现 SHAP 解释
*   使用 MLflow pyfunc API 实现 SHAP 解释器

# 技术要求

完成本章需要满足以下要求:

*   MLflow 成熟的本地服务器:这是我们从 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040) 、*跟踪模型、参数和指标*以来一直使用的服务器。
*   https://github.com/slundberg/shap SHAP Python 库:。
*   Spark 3.2.1 和 PySpark 3.2.1:详见本章 GitHub 库的`README.md`文件。
*   本章代码来自 GitHub 知识库:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 10](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10)。

# 了解当前 MLflow 可解释集成

MLflow 有几种方法来支持可解释性集成。当实现可解释性时，我们提到两种类型的工件:解释者和解释:

*   解释器是可解释性模型，常见的是 SHAP 模型，可以是不同种类的 SHAP 解释器，如 **TreeExplainer** 、 **KernelExplainer** 和**partition explainer**([https://shap . readthedocs . io/en/latest/generated/shap . explainers . html](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html))。为了计算效率，我们通常为 DL 模型选择 **PartitionExplainer** 。
*   解释是展示解释者某种形式的输出的工件，可以是文本、数值或图表。解释可能发生在离线培训或测试中，也可能发生在在线生产中。因此，如果我们想知道为什么模型提供某些预测，我们应该能够为离线评估提供一个解释器，或者为在线查询提供一个解释器端点。

在此，我们简要概述了从 MLflow 版本 1.25.1 开始的当前能力(【https://pypi.org/project/mlflow/1.25.1/】T21)。使用 MLflow 进行解释有四种不同的方式，如下所示:

*   使用的`mlflow.log_artifact`API([https://www . ml flow . org/docs/latest/python _ API/ml flow . html # ml flow . log _ artifact](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact))来记录相关的解释工件，例如条形图和 Shapley 值数组。这为日志解释提供了最大的灵活性。这既可以作为批处理离线使用，也可以在我们为某个预测自动记录 SHAP 条形图时在线使用。请注意，在在线生产场景中记录每个预测的解释是非常昂贵的，因此我们应该为按需查询提供单独的解释 API。
*   使用的`mlflow.pyfunc.PythonModel`API([https://www . mlflow . org/docs/latest/python _ API/ml flow . py func . html # ml flow . py func . python model](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))创建一个解释器，可以使用 ml flow 的`pyfunc`方法、`mlflow.pyfunc.log_model`进行日志记录，使用`mlflow.pyfunc.load_model`或`mlflow.pyfunc.spark_udf`加载一个解释器。这给了我们最大的灵活性来创建定制的解释器作为 MLflow 通用`pyfunc`模型，并且可以用于离线批处理解释或者在线作为**解释作为服务** ( **EaaS** )。
*   使用的`mlflow.shap`API([https://www . ml flow . org/docs/latest/python _ API/ml flow . shap . html](https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html))。这有一定的局限性。例如，`mlflow.shap.log_explainer`方法只支持 scikit-learn 和 PyTorch 模型。`mlflow.shap.log_explanation`方法只支持`shap.KernelExplainer`([https://shap-lrjball . readthedocs . io/en/latest/generated/shap。KernelExplainer.html](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html))。这是非常计算密集型的，因为计算时间相对于特征的数量呈指数增长；因此，即使是中等大小的数据集，也不可能计算出解释(见 GitHub 发布的一期[https://github.com/mlflow/mlflow/issues/4071](https://github.com/mlflow/mlflow/issues/4071))。MLflow 提供的现有示例针对 scikit-learn 包中的经典 ML 模型，如线性回归或随机森林，没有 DL 模型可解释性示例([https://github.com/mlflow/mlflow/tree/master/examples/shap](https://github.com/mlflow/mlflow/tree/master/examples/shap))。我们将在本章的后面部分展示这个 API 目前不支持基于变形金刚的 SHAP 解释器和解说，因此我们不会在本章使用这个 API。我们将在本章的例子中强调一些问题。
*   使用的`mlflow.evaluate`API([https://www . ml flow . org/docs/latest/python _ API/ml flow . html # ml flow . evaluate](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate))。这可以在模型已经被训练和测试之后用于评估。这是一个实验性的特性，将来可能会改变。它支持 MLflow `pyfunc`型号。但是，它有一些限制，评估数据集标签值必须是数字或布尔值，所有特征值必须是数字，每个特征列必须只包含标量值([https://www . ml flow . org/docs/latest/models . html # model-evaluation](https://www.mlflow.org/docs/latest/models.html#model-evaluation))。还是那句话，MLflow 提供的现有例子只针对 scikit-learn 包中的经典 ML 模型([https://github . com/ML flow/ML flow/tree/master/examples/evaluation](https://github.com/mlflow/mlflow/tree/master/examples/evaluation))。我们可以使用这个 API 来记录 NLP 情感模型的分类器指标，但是这个 API 会自动跳过解释部分，因为它需要一个包含标量值的特征列(NLP 模型输入是一个文本输入)。因此，这不适用于我们需要的 DL 模型可解释性。所以，我们不会在本章中使用这个 API。

考虑到这些 API 中的一些仍然是实验性的，并且仍然在发展中，用户应该意识到使用 MLflow 成功实现可解释性的局限性和变通方法。对于 DL 模型的可解释性，正如我们将在本章中了解到的，使用 MLflow 实现相当具有挑战性，因为从 MLflow 版本 1.25.1 起，MLflow 与 SHAP 的集成仍在进行中。在接下来的部分中，我们将学习何时以及如何使用这些不同的 API 来实现解释，并为 DL 模型记录和加载解释器。

# 使用 MLflow 工件记录 API 实现 SHAP 解释

MLflow 有一个通用的跟踪 API，可以记录任何工件:`mlflow.log_artifact`。然而，MLflow 文档中给出的示例通常使用 scikit-learn 和表格数字数据进行训练、测试和解释。在这里，我们想要展示如何为 NLP 感性 DL 模型使用`mlflow.log_artifact`来记录相关的工件，比如 Shapley 值数组和 Shapley 值条形图。您可以在本章的 GitHub 资源库([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ log _ artifact . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py))中查看 Python VS Code 笔记本`shap_mlflow_log_artifact.py`，并按照以下步骤进行操作:

1.  确保您已经准备好了先决条件，包括本地成熟的 MLflow 服务器和 conda 虚拟环境。按照 [*第 10 章*](B18120_10_ePub.xhtml#_idTextAnchor127) 文件夹中`README.md`([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/Chapter 10/readme . MD](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))文件中的说明做好这些准备。
2.  在开始运行本章中的任何代码之前，确保您激活了如下的`chapter10-dl-explain`虚拟环境:

    ```py
    conda activate chapter10-dl-explain
    ```

3.  导入笔记本开头的相关库如下:

    ```py
    import os import matplotlib.pyplot as plt import mlflow from mlflow.tracking import MlflowClient from mlflow.utils.file_utils import TempDir import shap import transformers from shap.plots import * import numpy as np
    ```

4.  下一步是设置一些环境变量。前三个环境变量用于本地 MLflow URIs，第四个用于禁用由于已知的拥抱脸令牌化问题而产生的拥抱脸警告:

    ```py
    os.environ["AWS_ACCESS_KEY_ID"] = "minio" os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123" os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://localhost:9000" os.environ["TOKENIZERS_PARALLELISM"] = "False"
    ```

5.  我们还需要设置 MLflow 实验，并在屏幕上显示 MLflow 实验 ID 作为输出:

    ```py
    EXPERIMENT_NAME = "dl_explain_chapter10" mlflow.set_tracking_uri('http://localhost') mlflow.set_experiment(EXPERIMENT_NAME) experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME) print("experiment_id:", experiment.experiment_id)
    ```

如果您一直在运行笔记本，您应该会看到如下输出:

```py
experiment_id: 14
```

这意味着实验名`dl_explain_chapter10`的 MLflow 实验 ID 是`14`。请注意，您也可以将 MLflow 追踪 URI 设置为环境变量，如下所示:

```py
export MLFLOW_TRACKING_URI=http://localhost
```

这里，我们使用 MLflow 的`mlflow.set_tracking_uri` API 来定义 URI 的位置。哪种方式都可以。

1.  现在，我们可以使用 Hugging Face 的 transformer pipeline API 创建一个 DL 模型，将一个句子分为积极情绪或消极情绪。因为这已经被微调了，我们将关注于如何获得和模型的解释者和说明，而不是关注于如何训练或微调一个模型:

    ```py
    dl_model = transformers.pipeline('sentiment-analysis', return_all_scores=False) explainer = shap.Explainer(dl_model) shap_values = explainer(["Not a good movie to spend time on.", "This is a great movie."])
    ```

代码片段创建了一个情感分析模型`dl_model`，然后为这个模型创建了一个 SHAP `explainer`。然后我们为这个解释者提供一个两句话的列表来获取`shap_values`对象。这将用于登录 MLflow。

给定`shap_values`对象，我们现在可以开始新的 MLflow 运行，并记录我们在上一章中看到的 Shapley 值和条形图( [*第 9 章*](B18120_09_ePub.xhtml#_idTextAnchor112) *，深度学习可解释性的基础*)。第一行代码确保所有活动的 MLflow 运行都已结束。如果我们希望以交互方式多次重新运行这段代码，这将非常有用:

```py
mlflow.end_run()
```

然后我们定义两个常数。一个是`artifact_root_path`，用于 MLflow 工件存储中的根路径，它将用于存储所有 SHAP 解释对象。另一个是`shap_bar_plot`，用于工件文件名，将用于条形图:

```py
artifact_root_path = "model_explanations_shap"
artifact_file_name = 'shap_bar_plot'
```

1.  然后，我们开始一个新的 MLflow 运行，在这个运行下，我们将生成三个 SHAP 文件并将其记录到 MLflow 工件存储中的路径`model_explanations_shap` :

    ```py
    with mlflow.start_run() as run:    with TempDir() as temp_dir:         temp_dir_path = temp_dir.path()         print("temp directory for artifacts: {}".format(temp_dir_path))
    ```

    下

我们还需要到有一个临时的本地目录，如前面的代码片段所示，首先保存 SHAP 文件，然后将这些文件记录到 MLflow 服务器。如果到目前为止您已经运行了笔记本，您应该会在输出中看到一个临时目录，如下所示:

```py
temp directory for artifacts: /var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpgw520wu1
```

1.  现在我们准备生成 SHAP 文件并保存它们。第一个是条形图，保存和记录有点棘手。让我们通过下面的代码来理解我们是如何做到这一点的:

    ```py
    try:      plt.clf()      plt.subplots_adjust(bottom=0.2, left=0.4)      shap.plots.bar(shap_values[0, :, "NEGATIVE"],                     show=False)      plt.savefig(f"{temp_dir_path}/{artifact_file_name}") finally:      plt.close(plt.gcf()) mlflow.log_artifact(f"{temp_dir_path}/{artifact_file_name}.png", artifact_root_path)
    ```

请注意，我们使用的是作为`plt`导入的`matplotlib.pyplot`，首先使用`plt.clf()`清除图形，然后创建一个带有一些调整的子情节。在这里，我们定义了`bottom=0.2`，这意味着支线剧情底边的位置在人物高度的 20%。同样，我们调整支线剧情的左边缘。然后，我们使用`shap.plots.bar` SHAP API 来绘制第一句对预测的特征贡献的条形图，但是将`show`参数设为`False`。这意味着，我们在交互运行中看不到图形，但是图形存储在 pyplot `plt`变量中，然后可以使用`plt.savefig`保存到文件名前缀为`shap_bar_plot`的本地临时目录中。`pyplot`会在文件保存后自动添加文件扩展名`.png`。因此，这将在临时文件夹中保存一个名为`shap_bar_plot.png`的本地图像文件。最后一条语句调用 MLflow 的`mlflow.log_artifact`将这个 PNG 文件上传到 MLflow 跟踪服务器的工件存储在根文件夹`model_explanations_shap`中。我们还需要确保通过调用`plt.close(plt.gcf())`来结束当前的数字。

1.  除了将`shap_bar_plot.png`记录到 MLflow 服务器，我们还希望将 Shapley `base_values`数组和`shap_values`数组作为 NumPy 数组记录到 MLflow track 服务器。这可以通过下面的语句来完成:

    ```py
    np.save(f"{temp_dir_path}/shap_values",          shap_values.values) np.save(f"{temp_dir_path}/base_values",          shap_values.base_values)         mlflow.log_artifact(             f"{temp_dir_path}/shap_values.npy",              artifact_root_path)         mlflow.log_artifact(             f"{temp_dir_path}/base_values.npy",              artifact_root_path)      
    ```

这将首先在本地临时文件夹中保存`shap_values.npy`和`base_values.npy`的本地副本，然后将其上传到 MLflow 跟踪服务器的工件存储。

1.  如果您跟踪笔记本直到这里，您应该能够在本地 MLflow 服务器中验证这些工件是否被成功存储。转到 localhost–`http://localhost/`的 MLflow UI，然后找到实验`dl_explain_chapter10`。然后，您应该能够找到刚刚运行的实验。它看起来应该类似于*图 10.1* ，在这里你可以在`model_explanations_shap`文件夹中找到三个文件:`base_values.npy`、`shap_bar_plot.png`和`shap_values.npy`。*图 10.1* 显示了不同的表征或词对句子的预测结果的特征贡献的柱状图——T12。这个实验页面的 URL 如下所示:

    ```py
    http://localhost/#/experiments/14/runs/10f0655189f740aeb813a015f1f6e115
    ```

![Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image 
in the MLflow tracking server
](img/B18120_10_001.jpg)

图 10.1–MLflow log _ artifact API 将 SHAP 条形图保存为 ml flow 跟踪服务器中的图像

或者，您也可以使用代码以编程方式下载这些存储在 MLflow 跟踪服务器中的文件，并在本地检查它们。我们在笔记本的最后一个单元格中提供了这样的代码。

1.  如果您运行笔记本代码的最后一个单元块，即从我们刚刚保存的 MLflow 服务器下载三个文件并将其打印出来，您应该能够看到以下输出，如图*图 10.2* 所示。从 MLflow 跟踪服务器下载工件的机制是使用`MlflowClient().download_artifacts` API，其中您提供 MLflow 运行 ID(在我们的例子中是`10f0655189f740aeb813a015f1f6e115`)和工件根路径`model_explanations_shap`作为 API 的参数:

    ```py
    downloaded_local_path = MlflowClient().download_artifacts(run.info.run_id, artifact_root_path)
    ```

这将把 MLflow tracking 服务器上`model_explanations_shap`中的所有文件下载到一个本地路径，这是返回变量`downloaded_local_path`:

![Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow tracking server to a local path and display them
](img/B18120_10_002.jpg)

图 10.2–将 SHAP 基值和形状值数组从 MLflow 追踪服务器下载到本地路径并显示它们

为了显示这两个 NumPy 数组，我们需要调用 NumPy 的`load` API 来加载它们，然后打印它们:

```py
base_values = np.load(os.path.join(downloaded_local_path, "base_values.npy"), allow_pickle=True)
shap_values = np.load(os.path.join(downloaded_local_path, "shap_values.npy"), allow_pickle=True)
```

注意，在调用`np.load` API 时，我们需要将`allow_pickle`参数设置为`True`，以便 NumPy 可以正确地将这些文件加载回内存。

虽然您可以在 VS 代码环境中以交互方式运行这个笔记本，但是您也可以在命令行中运行它，如下所示:

```py
python shap_mlflow_log_artifact.py
```

这将在控制台中生成所有输出，并将所有工件记录到 MLflow 服务器中，正如我们在笔记本的交互式运行中看到的那样。

如果到目前为止您已经运行了代码，那么恭喜您使用 MLflow 的`mlflow.log_artifact` API 成功实现了对 MLflow 跟踪服务器的日志记录 SHAP 解释！

尽管记录所有解释的过程似乎有点长，但是这种方法确实具有不依赖于所使用的解释器类型的优点，因为解释器是在 MLflow 工件记录 API 之外定义的。

在下一节中，我们将看到如何使用内置的`mlflow.pyfunc.PythonModel` API 将一个 SHAP 解释器记录为一个 MLflow 模型，然后部署为一个端点或者以批处理模式使用它，就像它是一个通用的 MLflow `pyfunc`模型一样。

# 使用 MLflow pyfunc API 实现 SHAP 解释器

正如我们从上一节中了解到的那样，通过使用 SHAP API 创建一个解释器的新实例，SHAP 解释器可以在任何需要的时候离线使用。然而，由于底层的 DL 模型经常被记录到 MLflow 服务器中，所以最好也将相应的解释器记录到 MLflow 服务器中，这样我们不仅可以跟踪 DL 模型，还可以跟踪它们的解释器。此外，我们可以为解释器使用通用的 MLflow pyfunc 模型日志和加载 API，从而统一对 DL 模型及其解释器的访问。

在本节中，我们将逐步学习如何将 SHAP 解释器实现为通用 MLflow pyfunc 模型，以及如何使用它进行离线和在线解释。我们将把这个过程分成三个部分:

*   创建和记录 MLflow pyfunc 解释器
*   为 EaaS 部署 MLflow pyfunc 解释器
*   使用 MLflow pyfunc 解释器进行批处理解释

让我们从创建和记录 MLflow pyfunc 解释器的第一小节开始。

## 创建和记录 MLflow pyfunc 解释器

为了让了解本节内容，请查看 GitHub 知识库中的`nlp_sentiment_classifier_explainer.py`([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/pipeline/NLP _ sensation _ classifier _ explainer . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py)):

1.  首先，通过子类化`mlflow.pyfunc.PythonModel`，我们可以创建一个定制的 MLflow 模型来封装一个 SHAP 解释器。所以，让我们将这个类声明如下:

    ```py
    class SentimentAnalysisExplainer(mlflow.pyfunc.PythonModel):
    ```

2.  接下来，我们需要实例化一个解释器。我们将使用`load_context`方法为拥抱脸 NLP 情感分析分类器加载一个 SHAP 解释器，而不是在这个类的`init`方法中创建一个解释器，如下:

    ```py
    def load_context(self, context):   from transformers import pipeline   import shap   self.explainer = shap.Explainer(pipeline('sentiment-analysis', return_all_scores=True))
    ```

每当这个`SentimentAnalysisExplainer`类被执行时，这将创建一个 SHAP 解释器。请注意，情感分类器是一个拥抱面部管道对象，其`return_all_scores`参数设置为`True`。这意味着这将返回每个输入文本的正面和负面情绪的标签和概率分数。

为 SHAP 解释者避免运行时错误

如果我们在这个类的`init`方法中实现`self.explainer`，我们会遇到一个与 SHAP 包的`_masked_model.py`文件相关的运行时错误，该文件抱怨`init`方法会被 MLflow 序列化，所以很明显这个运行时错误来自于 MLflow 的序列化。然而，在`load_context`函数中实现`self.explainer`避免了 MLflow 的序列化，并且在运行时调用这个解释器时工作正常。

1.  然后我们将实现`sentiment_classifier_explanation`方法，该方法接受一个 pandas DataFrame 行的输入，并生成一个 pickled `shap_values`输出，作为单行文本输入的解释:

    ```py
    def sentiment_classifier_explanation(self, row):   shap_values = self.explainer([row['text']])   return [pickle.dumps(shap_values)]
    ```

注意，我们需要用一对方括号将`row['text']` 值括起来，这样它就成为一个列表，而不仅仅是一个值。这是因为这个 SHAP 解释器需要一个文本列表，而不仅仅是一个字符串。如果我们不将值括在方括号内，那么解释器将一个字符一个字符地拆分整个字符串，将每个字符视为一个单词，这不是我们想要的。一旦我们从解释器得到 Shapley 值作为输出作为`shap_values`，我们就需要在返回给调用者之前使用`pickle.dumps`序列化它们。MLflow pyfunc 模型输入和输出签名不支持无序列化的复杂对象，因此该酸洗步骤确保模型输出签名符合 MLflow。我们将很快在*步骤 5* 中看到这个 MLflow pyfunc 解释器的输入和输出签名的定义。

1.  接下来，我们需要为这个类实现所需的`predict`方法。这将把`sentiment_classifier_explanation`方法应用于整个输入熊猫数据帧，如下:

    ```py
    def predict(self, context, model_input):   model_input[['shap_values']] = model_input.apply(     self.sentiment_classifier_explanation, axis=1,      result_type='expand')   model_input.drop(['text'], axis=1, inplace=True)   return model_input
    ```

这将在`text`列中为输入 pandas 数据帧的每一行生成一个名为`shap_values`的新列。然后我们删除`text`列并返回一个单列的`shap_values`数据帧作为最终的预测结果:在本例中，解释结果是一个数据帧。

1.  现在我们已经有了`SentimentAnalysisExplainer`类实现，我们可以使用标准的 MLflow pyfunc 模型日志 API 将这个模型记录到 MLflow 跟踪服务器中。在进行 MLflow 日志记录之前，让我们确保声明了这个解释器的模型签名，如下:

    ```py
    input = json.dumps([{'name': 'text', 'type': 'string'}]) output = json.dumps([{'name': 'shap_values', 'type': 'string'}]) signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})
    ```

这些语句声明输入是具有单个`string`类型`text`列的数据帧，输出是具有单个`string`类型`shap_values`列的数据帧。回想一下，这个`shap_values`列是一个经过酸洗的序列化字节字符串，它包含 Shapley 值对象。

1.  最后，我们可以在任务方法中使用`mlflow.pyfunc.log_model`方法来实现解释者日志记录步骤，如下:

    ```py
    with mlflow.start_run() as mlrun:             mlflow.pyfunc.log_model(     artifact_path=MODEL_ARTIFACT_PATH,      conda_env=CONDA_ENV,                                python_model=SentimentAnalysisExplainer(),      signature=signature)
    ```

我们使用的`log_model`方法中有四个参数。`MODEL_ARTIFACT_PATH`是 MLflow 跟踪服务器中存储解释器的文件夹的名称。这里，该值在您签出的 Python 文件中被定义为`nlp_sentiment_classifier_explainer`。`CONDA_ENV`是本章根文件夹中的`conda.yaml`文件。`python_model`参数是我们刚刚实现的`SentimentAnalysisExplainer`类，`signature`是我们定义的解释器输入和输出签名。

1.  现在我们准备在命令行中运行整个文件，如下所示:

    ```py
    python nlp_sentiment_classifier_explainer.py
    ```

假设您已经按照 GitHub 存储库中本章的`README.md`文件正确设置了本地 MLflow 跟踪服务器和环境变量，这个将在控制台输出中产生下面两行:

```py
2022-05-11 17:49:32,181 Found credentials in environment variables.
2022-05-11 17:49:32,384 finished logging nlp sentiment classifier explainer run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9
```

这意味着我们已经成功地在本地 MLflow 跟踪服务器中记录了 explainer。

1.  在网络浏览器中进入位于`http://localhost/`的 MLflow 网络界面，点击`dl_explain_chapter10`实验文件夹。你应该可以在`nlp_sentiment_classifier_explainer`下的`Artifacts`文件夹中找到这次运行和记录的解释者，应该如图*图 10.3* 所示:

![Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model 
](img/B18120_10_003.jpg)

图 10.3-SHAP 解释器被记录为 MLflow pyfunc 模型

请注意，*图 10.3* 中显示的`MLmodel`元数据与我们之前记录为 MLflow pyfunc 模型的普通 DL 推理管道没有太大区别，除了`artifact_path`名称和`signature`。这就是使用这种方法的优势，因为现在我们可以使用通用的 MLflow pyfunc 模型方法来加载这个解释器或将其部署为服务。

mlflow.shap.log_explainer API 的问题

正如我们前面提到的，MLflow 有一个`mlflow.shap.log_explainer` API，它提供了一个记录解释器的方法。然而，这个 API 不支持我们的 NLP 情感分类器解释器，因为我们的 NLP 管道不是 MLflow 当前支持的已知模型风格。因此，即使`log_explainer`可以将这个解释器对象写入跟踪服务器，当使用`mlflow.shap.load_explainer` API 将解释器加载回内存时，它也会失败，并显示以下错误消息:本书中的`mlflow.shap.log_explainer` API。

现在我们有了一个已记录的解释器，我们可以以两种方式使用它:将它部署到 web 服务中，这样我们就可以创建一个端点来建立 EaaS，或者使用 MLflow `run_id`通过 MLflow pyfunc `load_model`或`spark_udf`方法直接加载解释器。让我们通过设置本地 web 服务来开始 web 服务部署。

## 为 EaaS 部署 MLflow pyfunc 解释器

我们可以用标准的 MLflow 方式设置本地 EaaS，因为现在 SHAP 解释器就像一个通用的 MLflow pyfunc 模型。执行以下步骤，了解如何在本地实现这一点:

1.  运行下面的 MLflow 命令，为我们刚刚记录的解释器设置一个本地 web 服务。本例中的`run_id`为`ad1edb09e5ea4d8ca0332b8bc2f5f6c9` :

    ```py
    mlflow models serve -m runs:/ ad1edb09e5ea4d8ca0332b8bc2f5f6c9/nlp_sentiment_classifier_explainer
    ```

这将产生以下控制台输出:

![ Figure 10.4 – SHAP EaaS console output
](img/B18120_10_004.jpg)

图 10.4–SHAP EaaS 控制台输出

注意在*图 10.4* 中，默认的底层预训练语言模型是在`gunicore` HTTP 服务器启动并运行后加载的。这是因为我们的解释器实现在`load_context`方法中，这正是我们所期望的:在 web 服务启动并运行后立即加载解释器。

1.  在另一个终端窗口中，键入以下命令，在本地主机的端口`5000`调用 explainer web 服务，输入两个示例文本:

    ```py
    curl -X POST -H "Content-Type:application/json; format=pandas-split" --data '{"columns":["text"],"data":[["This is meh weather"], ["This is great weather"]]}' http://127.0.0.1:5000/invocations
    ```

这将使产生以下输出:

![Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS
](img/B18120_10_005.jpg)

图 10.5–调用我们的 SHAP EaaS 后数据帧中的响应

注意在*图 10.5* 中，列名为`shap_values`，而值为酸洗字节十六进制数据。这些不是人类可读的，但是可以在调用者端使用`pickle.loads`方法转换回原始的`shap_values`。因此，如果您看到类似*图 10.5* 的响应输出，那么恭喜您建立了本地 EaaS！您可以像其他 MLflow 服务部署一样部署此解释器服务，如第 8 章 *中所述，在规模*上部署 DL 推理管道，因为此解释器现在可以像通用 MLflow pyfunc 模型服务一样调用。

接下来，我们将看到如何使用 MLflow pyfunc 解释器进行批量解释。

## 使用 MLflow pyfunc 解释器进行批量解释

使用 MLflow pyfunc 解释器有两种方法实现离线批处理解释:

*   将 pyfunc 解释器作为 MLflow pyfunc 模型加载，以解释给定的 pandas DataFrame 输入。
*   将 pyfunc 解释器作为 PySpark UDF 加载，以解释给定的 PySpark 数据帧输入。

让我们从加载解释器作为 MLflow pyfunc 模型开始。

### 将 MLflow pyfunc 解释器作为 MLflow pyfunc 模型加载

正如我们已经提到的，使用 MLflow 日志解释器的另一种方式是直接使用 MLflow 的 pyfunc `load_model`方法在本地 Python 代码中加载解释器，而不是将其部署到 web 服务中。这非常简单，我们将向您展示如何做到这一点。您可以在 GitHub 存储库中查看`shap_mlflow_pyfunc_explainer.py`文件中的代码([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ py func _ explainer . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py)):

1.  第一步是将记录的解释器加载回内存。下面的代码使用`mlflow.pyfunc.load_model`和解释器`run_id` URI:

    ```py
    run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9" logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer' explainer = mlflow.pyfunc.load_model(logged_explainer)
    ```

    来完成这个任务

这应该加载解释器，就好像它只是一个通用的 MLflow pyfunc 模型。我们可以通过运行以下代码打印出解释器的元数据:

```py
explainer
```

这将显示以下输出:

```py
mlflow.pyfunc.loaded_model: artifact_path: nlp_sentiment_classifier_explainer flavor: mlflow.pyfunc.model run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9
```

这意味着这是一个`mlflow.pyfunc.model`风格，这是一个好消息，因为我们可以使用相同的 MLflow pyfunc API 来使用这个解释器。

1.  接下来，我们将获得一些示例数据来测试新加载的解释器:

    ```py
    import datasets dataset = datasets.load_dataset("imdb", split="test") short_data = [v[:500] for v in dataset["text"][:20]] df_test = pd.DataFrame (short_data, columns = ['text'])
    ```

这将加载 IMDb 测试数据集，将每个评论文本截短为 500 个字符，并挑选前 20 行来制作一个 pandas 数据帧，用于下一步的解释。

1.  现在，我们可以如下运行解释器:

    ```py
    results = explainer.predict(df_test)
    ```

这将为输入数据帧`df_test`运行 SHAP 分区解释器。运行时，它将为数据帧的每一行显示以下输出:

```py
Partition explainer: 2it [00:38, 38.67s/it]
```

结果将是一个只有一列的 pandas 数据框架，`shap_values`。这可能需要几分钟的时间，因为它需要标记每一行，执行解释器，并序列化输出。

1.  一旦解释器执行完成，我们可以通过反序列化行内容来检查结果。下面是检查第一个输出的代码:

    ```py
    results_deserialized = pickle.loads(results['shap_values'][0]) print(results_deserialized)
    ```

这将打印出第一行的`shap_values`。*图 10.6* 显示了`shap_values`输出的部分截图:

![Figure 10.6 – Partial output of the deserialized shap_values from the explanation
](img/B18120_10_006.jpg)

图 10.6–解释中反序列化的 shap_values 的部分输出

正如我们可以在*图 10.6* 中看到一样，`shap_values`的输出与我们在 [*第九章*](B18120_09_ePub.xhtml#_idTextAnchor112) *、深度学习可解释性基础*中所学的没有什么不同，当时我们没有使用 MLflow 来记录和加载解释器。我们还可以生成 Shapley 文本图来突出文本对预测情感的贡献。

1.  在笔记本中运行下面的语句，查看匀称的文本情节:

    ```py
    shap.plots.text(results_deserialized[:,:,"POSITIVE"])
    ```

这将生成一个显示在*图 10.7* 中的图:

![Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow logged explainer
](img/B18120_10_007.jpg)

图 10.7–使用来自 MLflow 日志解释器的反序列化 shap_values 的 Shapley 文本图

从*图 10.7* 中可以看出，该评论的情绪是积极的，对预测情绪有贡献的关键词或短语是`good`、`love`，以及其他一些用红色突出显示的短语。当你看到这个 Shapley 文本情节时，你应该给自己一点掌声，因为你已经学会了如何使用 MLflow logged explainer 来生成批处理解释。

正如在这个批量解释的逐步实现过程中提到的，使用这个 pyfunc 模型方法做一个大批量解释有点慢。幸运的是，我们有另一种方法来使用 PySpark UDF 函数实现批处理解释，我们将在下一小节中解释。

### 将 pyfunc 解释器作为 PySpark UDF 加载

对于可伸缩的批处理解释，我们可以使用 Spark 的分布式计算能力，这是通过将 pyfunc 解释器加载为 PySpark UDF 来支持的。使用这个功能不需要额外的工作，因为 MLflow pyfunc API 已经通过`mlflow.pyfunc.spark_udf`方法提供了这个功能。我们将向您展示如何逐步实现这种大规模的解释:

1.  首先，确保您已经完成了`README.md`文件([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/readme . MD](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))来安装 Spark，创建并激活`chapter10-dl-pyspark-explain`虚拟环境，并在运行 PySpark UDF 代码进行大规模解释之前设置所有的环境变量。
2.  然后就可以开始运行 VS 代码笔记本了，`shap_mlflow_pyspark_explainer.py`，可以在 GitHub 的资源库中查看:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 10/notebooks/shap _ ml flow _ py spark _ explainer . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py)。在`chapter10/notebooks/`运行以下命令:

    ```py
    python shap_mlflow_pyspark_explainer.py
    ```

您将得到在*图 10.8* 中显示的最终输出，在这最后几行之前的几行输出中:

![Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's shap_values along with their input texts
](img/B18120_10_008.jpg)

图 10.8-PySpark UDF 解释器输出的前两行文本的 shap_values 以及它们的输入文本

从*图 10.8* 中可以看出，PySpark UDF 解释器的输出是一个包含两列的 PySpark 数据帧:`text`和`shap_values`。`text`列是原始输入文本，而`shap_values`列包含经过腌制的序列化 Shapley 值，就像我们在前一小节中看到的对 pandas 数据帧使用 pyfunc 解释器一样。

现在让我们看看代码中发生了什么。我们将解释`shap_mlflow_pyspark_explainer.py`文件中的关键代码块。由于这是一个 VS 代码笔记本，您可以像我们刚才做的那样在命令行中运行它，也可以在 VS 代码 ide 窗口中交互运行它。

1.  第一个关键代码块使用`mflow.pyfunc.spark_udf`方法加载解释器，如下:

    ```py
    spark = SparkSession.builder.appName("Batch explanation with MLflow DL explainer").getOrCreate() run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9" logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer' explainer = mlflow.pyfunc.spark_udf(spark, model_uri=logged_explainer, result_type=StringType())
    ```

第一个语句是初始化一个`SparkSession`变量，然后使用`run_id`将记录的解释器加载到内存中。运行解释器来获取元数据，如下所示:

```py
explainer
```

我们将得到以下结果:

```py
<function mlflow.pyfunc.spark_udf.<locals>.udf(iterator: Iterator[Tuple[Union[pandas.core.series.Series, pandas.core.frame.DataFrame], ...]]) -> Iterator[pandas.core.series.Series]>
```

这意味着我们现在有了一个包装成火花 UDF 函数的 SHAP 解释器。这允许我们在下一步中为输入 PySpark 数据帧直接应用 SHAP 解释器。

1.  我们像以前一样加载 IMDb 测试数据集，得到一个`short_data`列表，然后为测试数据集的前 20 行创建一个 PySpark 数据帧，用于解释:

    ```py
    df_pandas = pd.DataFrame (short_data, columns = ['text']) spark_df = spark.createDataFrame(df_pandas) spark_df = spark_df.withColumn('shap_values', explainer())
    ```

注意最后一条语句，它使用 PySpark 的`withColumn`函数向输入数据帧`spark_df`添加一个新的`shap_values`列，该数据帧最初只包含一列`text`。这是使用 Spark 并行和分布式计算能力的自然方式。如果您已经使用 MLflow pyfunc `load_model`方法运行了以前的非 Spark 方法和当前的 PySpark UDF 方法，您会注意到 Spark 方法运行得更快，甚至在本地计算机上也是如此。这允许我们对输入文本的许多实例进行大规模的 SHAP 解释。

1.  最后，为了验证结果，我们展示了`spark_df`数据帧的顶部两行，如图*图 10.8* 所示。

至此，有了 MLflow 的 pyfunc Spark UDF 包装的 SHAP 讲解器，我们可以理直气壮地做大规模批量讲解了。恭喜你！

现在让我们在下一节总结一下我们在本章中学到的内容。

# 摘要

在本章中，我们首先回顾了 MLflow APIs 中可用于实现可解释性的现有方法。现有的两个 ml flow API`mlflow.shap`和`mlflow.evaluate`有局限性，因此不能用于我们需要的复杂 DL 模型和管道可解释性场景。然后，我们重点介绍了在 MLflow API 框架中实现 SHAP 解释和解释器的两种主要方法:`mlflow.log_artifact`用于记录解释，而`mlflow.pyfunc.PythonModel`用于记录 SHAP 解释器。使用`log_artifact` API 可以允许我们将 Shapley 值和解释图记录到 MLflow 跟踪服务器中。使用`mlflow.pyfunc.PythonModel`允许我们将 SHAP 解释器作为 MLflow pyfunc 模型登录，从而打开了将 SHAP 解释器作为 web 服务部署以创建 EaaS 端点的大门。它还打开了通过 MLflow pyfunc `load_model`或`spark_udf` API 使用 SHAP 解释器进行大规模离线批量解释的大门。这使我们能够自信地为 DL 模型大规模实现可解释性。

随着可解释性领域的继续发展，MLflow 与 SHAP 和其他可解释性工具箱的集成也将继续改进。鼓励有兴趣的读者通过延伸阅读部分提供的链接继续他们的学习之旅。快乐的不断学习和成长！

# 延伸阅读

*   比例下的 Shapley 值:[https://neowaylabs . github . io/data-science/Shapley-Values-at-Scale/](https://neowaylabs.github.io/data-science/shapley-values-at-scale/)
*   使用 PySpark 和 Pandas UDF 缩放 SHAP 计算:[https://databricks . com/blog/2022/02/02/scaling-shap-Calculations-With-py spark-and-Pandas-UDF . html](https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html)
*   使用分布式计算系统 Ray 加速 Shapley 值计算:[https://www . telesens . co/2020/10/05/speeding-up-Shapley-value-computing-using-Ray-a-distributed-computing-system/](https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/)
*   用莱姆和 SHAP 解释 NLP 模型:[https://medium . com/@ kalia _ 65609/interpreting-an-NLP-model-with-LIME-and-shap-834 CFA 124 E4](mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4)
*   MLflow 中的模型评估:[https://databricks . com/blog/2022/04/19/model-Evaluation-in-ml flow . html](https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html)