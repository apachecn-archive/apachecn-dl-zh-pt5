# 零、前言

从 2012 年赢得大型 ImageNet 比赛的 AlexNet 开始，到 2018 年登顶众多**自然语言处理** ( **NLP** )排行榜的 BERT 预训练语言模型，现代**深度学习** ( **DL** )在更广泛的**人工智能** ( **AI** )和**机器学习** ( **ML** 中的革命然而，将这些 DL 模型从离线实验转移到生产环境的挑战仍然存在。这在很大程度上是由于支持 DL 全生命周期开发的统一开源框架的复杂性和缺乏。这本书将帮助您了解 DL 全生命周期开发的全貌，并实现可以从本地离线实验扩展到分布式环境和在线生产云的 DL 管道，重点是基于项目的实践学习，以支持使用流行的开源 MLflow 框架的端到端 DL 流程。

该书首先概述了 DL 的全生命周期和新兴的**机器学习操作** ( **MLOps** )领域，清晰地描述了 DL 的四大支柱(数据、模型、代码和可解释性)以及 MLflow 在这些领域的作用。在第一章中，使用 PyTorch Lightning Flash 构建了一个基于迁移学习的 NLP 情感模型，在本书的其余部分中，该模型被进一步开发、调整并部署到生产中。从那里开始，它指导您逐步理解 MLflow 实验和使用模式的概念，使用 MLflow 作为统一框架来跟踪 DL 数据、代码和管道、模型、参数和大规模指标。我们将在分布式执行环境中运行具有可再现性和来源跟踪的 DL 管道，并通过使用 Ray Tune、Optuna 和 HyperBand 的**超参数优化** ( **HPO** )来调整 DL 模型。我们还将构建一个包含预处理和后处理步骤的多步骤 DL 推理管道，使用 Ray Serve 和 AWS SageMaker 为生产部署一个 DL 推理管道，最后，使用**SHapley Additive expansions**(**SHAP**)和 MLflow 集成提供一个 DL 解释即服务。

到本书结束时，您将具备构建 DL 管道的基础和实践经验，从最初的离线实验到最终的部署和生产，所有这些都在一个可复制的开源框架内完成。在此过程中，您还将了解 DL 管道面临的独特挑战，以及我们如何通过实用且可扩展的解决方案来克服这些挑战，例如使用多核 CPU、**图形处理单元**(**GPU**)、分布式和并行计算框架以及云。

# 这本书是给谁的

这本书是为数据科学家、ML 工程师和 AI 从业者编写的，他们希望使用开源的 MLflow 框架和相关工具(如 Ray Tune、SHAP 和 Ray Serve)掌握 DL 开发从开始到生产的整个生命周期。本书中介绍的可伸缩、可复制和来源感知的实现确保您成功构建企业级 DL 管道。这本书将支持任何人构建强大的 DL 云应用程序。

# 这本书涵盖了什么

[*第一章*](B18120_01_ePub.xhtml#_idTextAnchor015) ，*深度学习生命周期与 MLOps 挑战*，涵盖了 DL 全生命周期的五个阶段，也是本书中第一个使用迁移学习方法进行文本情感分类的 DL 模型。它还定义了 MLOps 的概念、三个基础层和四个支柱，以及 MLflow 在这些领域中的作用。还概述了 DL 数据、模型、代码和可解释性方面的挑战。这一章旨在让每个人都达到相同的基础水平，并对本书其余部分的范围提供清晰的说明和指导。

[*第 2 章*](B18120_02_ePub.xhtml#_idTextAnchor027) ，*深度学习 MLflow 入门*，作为 MLflow 入门和第一手操作学习模块，快速设置基于本地文件系统的 MLflow 跟踪服务器或在 Databricks 中与远程管理的 MLflow 跟踪服务器交互，并使用 MLflow 自动日志记录执行第一个 DL 实验。它还通过具体的例子解释了一些基本的 MLflow 概念，例如实验、运行、关于实验和运行的元数据以及它们之间的关系、代码跟踪、模型日志和模型风格。具体来说，我们强调实验应该是一流的实体，可以用来弥合 DL 模型的离线和在线生产生命周期之间的差距。本章构建了 MLflow 的基础知识。

[*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040)*跟踪模型、参数和指标*，涵盖了关于使用成熟的本地 MLflow 跟踪服务器进行跟踪的第一个深入学习模块。它首先建立一个本地成熟的 MLflow 跟踪服务器，运行在 Docker Desktop 中，带有一个 MySQL 后端存储和一个 MinIO 工件存储。在实现跟踪之前，本章提供了一个基于开放起源模型词汇表规范的开放起源跟踪框架，并提出了六种可以使用 MLflow 实现的起源问题。然后，它提供了实际操作的实现示例，说明如何使用 MLflow 模型日志记录 API 和注册表 API 来跟踪模型来源、模型度量和参数，有或没有自动日志记录。与其他典型的 MLflow API 教程不同，这些教程只提供关于使用 API 的指导，相反，本章重点关注如何成功地使用 MLflow 来回答出处问题。到本章结束时，我们可以回答六个起源问题中的四个，剩下的两个问题只有在我们有多步管道或部署到生产时才能回答，这将在后面的章节中介绍。

[*第 4 章*](B18120_04_ePub.xhtml#_idTextAnchor050) ，*跟踪代码和数据版本*，涵盖了关于 MLflow 跟踪的第二个深入学习模块。它分析了在 ML/DL 项目中使用笔记本和管道的当前实践。它推荐使用 VS 代码笔记本，并展示了一个具体的 DL 笔记本示例，该示例可以在启用 MLflow 跟踪的情况下以交互或非交互方式运行。它还推荐使用 MLflow 的 **MLproject** 来使用 MLflow 的入口点和管道链接实现多步 DL 管道。为 DL 模型训练和注册创建了三步 DL 流水线。此外，它还显示了通过 MLflow 中的父子嵌套运行进行的管道级跟踪和单个步骤跟踪。最后，它展示了如何使用 MLflow 在 **Delta Lake** 中跟踪公共和私有构建的 Python 库和数据版本。

[*第五章*](B18120_05_ePub.xhtml#_idTextAnchor060) 、*在不同环境下运行 DL 管道*，讲述了如何在不同环境下运行 DL 管道。它从在不同环境中执行 DL 管道的场景和需求开始。然后展示了如何使用 MLflow 的**命令行接口** ( **CLI** )在四种场景下提交运行:使用本地代码在本地运行，使用 GitHub 中的远程代码在本地运行，使用本地代码在云中远程运行，以及使用 GitHub 中的远程代码在云中远程运行。MLflow 支持的执行 DL 管道的灵活性和可再现性也为需要时的**持续集成/持续部署** ( **CI/CD** )自动化提供了构建模块。

[*第 6 章*](B18120_06_ePub.xhtml#_idTextAnchor069) 、*运行大规模超参数调优*，涵盖了使用 MLflow 支持大规模 HPO，使用最先进的 HPO 框架，如 Ray Tune。它首先回顾了 DL 流水线超参数的类型和挑战。然后，比较了三个 HPO 框架 Ray Tune、Optuna 和 HyperOpt，并详细分析了它们的优缺点及其与 MLflow 的集成成熟度。然后，它推荐并展示了如何使用射线调整与 MLflow 做 HPO 调整的 DL 模型，我们一直在这本书到目前为止的工作。此外，它涵盖了如何切换到其他 HPO 搜索和调度算法，如 Optuna 和 HyperBand。这使我们能够以经济高效且可扩展的方式生产出满足业务需求的高性能 DL 模型。

[*第 7 章*](B18120_07_ePub.xhtml#_idTextAnchor083) ，*多步深度学习推理管道*，涵盖了使用 MLflow 的自定义 Python 模型方法创建多步推理管道。它首先概述了生产中推理工作流的四种模式，其中单一的训练模型通常不足以满足业务应用程序的需求。需要额外的预处理和后处理步骤。然后，它给出了实现多步推理管道的分步指南，该管道用语言检测、缓存和附加的模型元数据包装了之前微调过的 DL 情感模型。然后，这个推理管道被记录为一个通用的 MLflow **PyFunc** 模型，可以使用通用的 MLflow PyFunc load API 加载该模型。将推理管道包装成 MLflow 模型为在同一个 MLflow 框架内实现模型管道的自动化和一致性管理打开了大门。

[*第 8 章*](B18120_08_ePub.xhtml#_idTextAnchor095) ，*大规模部署 DL 推理管道*，涵盖将 DL 推理管道部署到不同的主机环境中以供生产使用。它首先概述了部署和托管环境的情况，包括批量推理和大规模流推理。然后描述了不同的部署机制，如 MLflow 内置模型服务工具、定制部署插件和通用模型服务框架(如 Ray Serve)。它展示了如何使用 MLflow 的 Spark `mlflow-ray-serve`部署批处理推理管道的例子。然后描述了一个完整的分步指南，将 DL 推理管道部署到受管 AWS SageMaker 实例以供生产使用。

[*第 9 章*](B18120_09_ePub.xhtml#_idTextAnchor112) ，*深度学习可解释性的基础*，涵盖了可解释性的基本概念，并探索了两种流行的可解释工具的使用。它首先概述了可解释性的八个维度和**可解释的 AI** ( **XAI** )，然后提供了具体的学习示例来探索 SHAP 和变形金刚-解释工具箱在 NLP 情感管道中的使用。它强调当开发一个 DL 应用程序时，可解释性应该被提升为一级工件，因为在各种业务应用程序和领域中，对模型和数据解释的需求和期望越来越多。

[*第十章*](B18120_10_ePub.xhtml#_idTextAnchor127) ，*用 MLflow* 实现 DL 可解释性，涵盖了如何用 MLflow 实现 DL 可解释性，以提供**解释即服务** ( **EaaS** )。它首先概述了 MLflow 当前支持解释器和解释的能力。具体来说，MLflow APIs 中现有的与 SHAP 的集成不支持大规模的 DL 可解释性。因此，它为实现提供了两种使用 MLflow 的工件日志 API 和**py func**API 的通用方法。提供了用于实现 SHAP 解释的示例，该解释将 SHAP 值记录在 MLflow 跟踪服务器的工件存储中的条形图中。SHAP 解释器可以作为 MLflow Python 模型登录，然后作为批量解释的 Spark UDF 或在线 EaaS 的 web 服务加载。这在统一的 MLflow 框架中为实现可解释性提供了最大的灵活性。

# 为了充分利用这本书

本书中的大多数代码都可以使用开源的 MLflow 工具来实现和执行，只有少数例外，即需要 14 天的完整 Databricks 试用(在 https://databricks.com/try-databricks 的[注册)以及 AWS 免费层帐户(在 https://aws.amazon.com/free/](https://databricks.com/try-databricks)的[注册)。下面列出了本书涉及的一些主要软件包:](https://aws.amazon.com/free/)

*   MLflow 1.20.2 及以上
*   Python 3.8.10
*   闪电 0.5.0
*   转换器 4.9.2
*   SHAP
*   PySpark 3.2.1
*   雷[调] 1.9.2
*   Optuna 2.10.0

完整的包依赖关系列在本书 GitHub 库中每章的`requirements.txt` 文件或`conda.yaml`文件中。所有代码都已经过测试，可以在 macOS 或 Linux 环境下成功运行。如果你是微软 Windows 用户，建议安装 **WSL2** 运行本书提供的 bash 脚本:[https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10)。众所周知，MLflow CLI 在 Microsoft Windows 命令行中无法正常工作。

从本书的 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040) *、* *跟踪模型、参数和指标*开始，您还需要安装 Docker Desktop([https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/))来设置一个完全成熟的本地 MLflow 跟踪服务器，用于执行本书中的代码。 [*第八章*](B18120_08_ePub.xhtml#_idTextAnchor095) *需要 AWS SageMaker，规模化部署 DL 推理流水线，*为云部署示例。本书中的**集成开发环境** ( **IDE** )采用 VS 代码版本 1.60 或以上([https://code.visualstudio.com/updates/v1_60](https://code.visualstudio.com/updates/v1_60))。Miniconda 版本 4.10.3 或更高版本([https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html))在本书中用于创建和激活虚拟环境。

**如果你使用的是这本书的数字版，我们建议你自己输入代码，或者从这本书的 GitHub 库中获取代码(下一节有链接)。这样做将帮助您避免任何与复制和粘贴代码相关的潜在错误。**

最后，为了最大限度地利用这本书，您应该有 Python 编程的经验，并对流行的 ML 和数据操作库(如 pandas 和 PySpark)有基本的了解。

# 下载示例代码文件

你可以从 GitHub 的[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow)下载本书的示例代码文件。如果代码有更新，它会在 GitHub 库中更新。

在 https://github.com/PacktPublishing/[网站](https://github.com/PacktPublishing/)上，我们也有来自我们丰富的书籍和视频目录的其他代码包。看看他们！

# 下载彩色图片

我们还提供了一个 PDF 文件，其中有本书中使用的截图和图表的彩色图像。可以在这里下载:[https://static . packt-cdn . com/downloads/9781803241333 _ color images . pdf](_ColorImages.pdf)。

# 使用的约定

本书通篇使用了许多文本约定。

`Code in text`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。这里有一个例子:“出于学习目的，我们在 GitHub 存储库中的`chapter08`文件夹下提供了两个示例`mlruns`工件和`huggingface`缓存文件夹。”

代码块设置如下:

```
client = boto3.client('sagemaker-runtime') 
```

```
response = client.invoke_endpoint(
```

```
        EndpointName=app_name, 
```

```
        ContentType=content_type,
```

```
        Accept=accept,
```

```
        Body=payload
```

```
        )
```

当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:

```
loaded_model = mlflow.pyfunc.spark_udf(
```

```
    spark,
```

```
    model_uri=logged_model, 
```

```
    result_type=StringType())
```

任何命令行输入或输出都按如下方式编写:

```
mlflow models serve -m models:/inference_pipeline_model/6
```

**粗体**:表示新术语、重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词以**粗体**显示。下面是一个例子:“要执行该单元格中的代码，您只需点击右上角下拉菜单中的**运行单元格**。”

提示或重要注意事项

像这样出现。

# 取得联系

我们随时欢迎读者的反馈。

**总体反馈**:如果您对本书的任何方面有疑问，请发邮件至 customercare@packtpub.com[并在邮件主题中提及书名。](mailto:customercare@packtpub.com)

**勘误表**:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请游览 www.packtpub.com/support/errata 并填写表格。

盗版:如果您在互联网上发现我们作品的任何形式的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过[copyright@packt.com](mailto:copyright@packt.com)联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有你擅长的主题，并且你有兴趣写书或投稿，请访问 authors.packtpub.com。

# 分享你的想法

一旦你阅读了*ml flow*的实用深度学习，我们很想听听你的想法！请点击这里直接进入亚马逊对这本书的评论页面并分享您的反馈。

您的评论对我们和技术社区非常重要，将有助于我们确保提供高质量的内容。