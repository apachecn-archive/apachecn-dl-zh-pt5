# *第四章*:跟踪代码和数据版本化

DL 模型不仅仅是模型——它们与训练和测试模型的代码以及用于训练和测试的数据密切相关。如果我们不跟踪用于模型的代码和数据，就不可能重现或改进模型。此外，最近整个行业都出现了觉醒和范式转变，转向以数据为中心的人工智能([https://www . Forbes . com/sites/Gil press/2021/06/16/Andrew-ng-launches-a-campaign-for-data-centric-AI/？sh=5cbacdc574f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5cbacdc574f5) )，其中数据的重要性被提升到构建 ML，尤其是 DL 模型的一流工件。因此，在本章中，我们将学习如何使用 MLflow 跟踪代码和数据版本。我们将了解跟踪代码和管道版本的不同方法，以及如何使用 Delta Lake 进行数据版本控制。在本章结束时，你将能够理解并使用 MLflow 实现代码和数据的跟踪技术。

在本章中，我们将讨论以下主要话题:

*   跟踪笔记本和管道版本
*   跟踪本地私有的 Python 库
*   跟踪三角洲湖中的数据版本

# 技术要求

本章的技术要求如下:

*   使用 Jupyter 笔记本扩展的 VS 代码:[https://github . com/Microsoft/VS Code-Jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks)。
*   本章的代码，可以在本书的 GitHub 知识库中找到:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 04](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04)。
*   访问 Databricks 实例，以便您可以了解如何使用 Delta Lake 来启用版本化数据访问。

# 跟踪笔记本和管道版本

数据科学家通常从离线试用 Python 笔记本开始，在这种情况下，交互式执行是一个关键优势。自`.ipynb`时代以来，Python 笔记本已经走过了漫长的道路。您可能也无法使用 Jupyter 笔记本在 MLflow 跟踪服务器中看到每次运行的确切 Git 散列。关于是否或何时应该使用 Jupyter 笔记本，尤其是在生产环境中，有很多有趣的争论(参见这里的讨论:[https://medium . com/mlops-community/Jupyter-notebooks-in-production-4e0d 38803251](https://medium.com/mlops-community/jupyter-notebooks-in-production-4e0d38803251))。我们不应该在生产环境中使用 Jupyter 笔记本电脑的原因有很多，特别是当我们需要端到端流水线方式的可再现性时，在这种情况下，大量笔记本电脑可能很难进行单元测试、正确的代码版本控制和依赖性管理。使用网飞([https://papermill.readthedocs.io/en/latest/index.html](https://papermill.readthedocs.io/en/latest/index.html))的开源工具 **papermill** ，在工作流方式中围绕调度、参数化和执行 Jupyter 笔记本有一些早期的创新。然而，Databricks 和 VS Code 最近的一项创新使笔记本更容易进行版本控制并与 MLflow 集成。让我们来看看这两种工具带来的笔记本电脑特征:

*   **交互式执行**:data bricks 的笔记本和 VS Code 的笔记本都可以像传统的 Jupyter 笔记本一样，以逐个单元格的执行模式运行。通过这样做，您可以立即看到结果的输出。
*   `.py`文件扩展名。这允许所有常规的 Python 代码林挺(代码格式和样式检查)应用于一个笔记本。
*   **用于呈现代码单元格和标记单元格的特殊符号**:data bricks 和 VS 代码都利用一些特殊符号将 Python 文件呈现为交互式笔记本。在数据块中，将代码划分为不同的可执行单元的特殊符号如下:

    ```
    # COMMAND ----------  import mlflow import torch from flash.core.data.utils import download_data from flash.text import TextClassificationData, TextClassifier import torchmetrics
    ```

特殊`COMMAND`行下面的代码将在 Databricks web UI 门户中呈现为可执行单元，如下所示:

![Figure 4.1 – Databricks executable cell
](img/B18120_Figure_4.1.jpg)

图 4.1-数据块可执行单元

要执行该单元格中的代码，只需通过右上角的下拉菜单点击**运行单元格**。

要添加一大块文本来描述和注释数据块(也称为标记单元格)中的代码，可以在行首使用`# MAGIC`符号，如下所示:

```
# MAGIC %md
# MAGIC #### Notebooks for fine-tuning a pretrained language model to do text-based sentiment classification
```

然后，这将在 Databricks 记事本中呈现为降价注释单元格，如下所示:

![Figure 4.2 – Databricks Markdown text cell
](img/B18120_Figure_4.2.jpg)

图 4.2-数据块降价文本单元格

在 VS 代码中，这两种类型的单元格使用的符号略有不同。对于代码单元，在单元块的开头使用`# %%`符号:

```
# %%
download_data("https://pl-flash-data.s3.amazonaws.com/imdb.zip", "./data/")
datamodule = TextClassificationData.from_csv(
    input_fields="review",
    target_fields="sentiment",
    train_file="data/imdb/train.csv",
    val_file="data/imdb/valid.csv",
    test_file="data/imdb/test.csv"
)
```

然后在 VS 代码编辑器中呈现，如下所示:

![Figure 4.3 – VSCode code cell
](img/B18120_Figure_4.3.jpg)

图 4.3–VS 代码代码单元

正如你所看到的，在代码块之前有一个**运行单元**按钮，你可以点击它来交互式地运行代码块。如果点击**运行单元格**按钮，代码块将在编辑器窗口的侧面板开始执行，如下图所示:

![ Figure 4.4 – Running code interactively in VSCode
](img/B18120_Figure_4.4.jpg)

图 4.4–在 VS 代码中交互运行代码

要添加包含注释的 Markdown 单元格，请在行首添加以下内容以及必要的符号:

```
# %% Notebook for fine-tuning a pretrained language model and sentiment classification
```

这将确保文本不是 VS 代码中的可执行代码块。

鉴于 Databricks 和 VS Code notebooks 的优势，我们建议使用其中任何一种进行版本跟踪。我们可以使用 GitHub 来跟踪这两种笔记本的版本，因为它们使用常规的 Python 文件格式。

使用 Databricks 笔记本版本控制的两种方法

对于托管 Databricks 实例，可以通过两种方式跟踪笔记本版本:在 Databricks web UI 上查看笔记本侧面板上的修订历史，或者链接到远程 GitHub 存储库。详细描述见 Databricks 笔记本文档:[https://docs . data bricks . com/notebooks/notebooks-use . html # version-control](https://docs.databricks.com/notebooks/notebooks-use.html#version-control)。

虽然 Databricks web 门户为笔记本版本控制和与 MLflow 实验跟踪的集成提供了出色的支持(请参见本章的标注框中的*使用 Databricks 笔记本版本控制的两种方法*和*Databricks 笔记本中的两种 MLflow 实验*)，但是在 data bricks 笔记本 web UI 中编写代码有一个主要缺点。这是因为与 VS 代码相比，web UI 不是典型的**集成开发环境** ( **IDE** )，在 VS 代码中，代码风格和格式工具如**flake 8**([https://flake8.pycqa.org/en/latest/](https://flake8.pycqa.org/en/latest/))和 autopep 8([https://pypi.org/project/autopep8/](https://pypi.org/project/autopep8/))可以很容易地实施。这可能会对代码质量和可维护性产生重大影响。因此，强烈建议您使用 VS 代码来创作笔记本代码(Databricks 笔记本或 VS 代码笔记本)。

Databricks 笔记本中的两种 MLflow 实验

对于托管 Databricks web 门户实例，您可以执行两种类型的 MLflow 实验:工作区和笔记本实验。工作区实验主要是针对不与单个笔记本相关联的共享实验文件夹。如果需要，远程代码执行可以写入工作区实验文件夹。另一方面，笔记本电脑范围的实验与特定的笔记本电脑相关联，可以直接在 Databricks 门户网站的笔记本电脑页面的右上角菜单项**实验**中找到。更多详情请看 Databricks 文档网站:[https://docs . data bricks . com/applications/ml flow/tracking . html # experiments](https://docs.databricks.com/applications/mlflow/tracking.html#experiments)。

使用本章的 VS 代码笔记本`fine_tuning.py`，它可以在本章的 GitHub 资源库([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/Chapter 04/notebooks/fine _ tuning . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/notebooks/fine_tuning.py))中找到，您将能够在 VS 代码编辑器中交互式地运行它，并在我们在 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040) *中设置的 MLflow Docker 服务器中记录实验提醒一下，要想在 VS 代码中成功运行这个笔记本，你需要设置你的虚拟环境`dl_model`，如本章 GitHub 库的`README.md`文件所述。它包括以下三个步骤:*

```
conda create -n dl_model python==3.8.10
```

```
conda activate dl_model
```

```
pip install -r requirements.txt
```

如果您从头到尾一个单元一个单元地运行这个笔记本，您的实验页面将如下所示:

![Figure 4.5 – Logged experiment page after running a VSCode notebook interactively 
](img/B18120_Figure_4.5.jpg)

图 4.5–交互运行 VS 代码笔记本后记录的实验页面

您可能会立即注意到前面截图中的一个问题—`fine_tuning.py`文件。这是因为 VS 代码笔记本并没有本地集成到 MLflow 跟踪服务器中进行源文件跟踪；它只能显示 VS 代码用来执行一个 VS 代码笔记本的**ipykernel**([https://pypi.org/project/ipykernel/](https://pypi.org/project/ipykernel/))([https://github.com/microsoft/vscode-jupyter](https://github.com/microsoft/vscode-jupyter))。不幸的是，在撰写本文时，这是一个无法通过交互式运行 VS 代码笔记本*进行实验代码跟踪来解决的限制。在托管的 Databricks web UI 中运行的 data brick 笔记本没有这样的问题，因为它们与捆绑在 data brick web 门户中的 MLflow 跟踪服务器进行了本机集成。*

然而，由于 VS 代码笔记本只是 Python 代码，我们可以在命令行*中非交互地运行笔记本*，如下所示:

```
python fine_tuning.py
```

这将在 MLflow 实验页面中记录实际的源代码文件名和 Git 提交散列，不会出现任何问题，如下所示:

![Figure 4.6 – Logged experiment page after running a VSCode notebook in the command line 
](img/B18120_Figure_4.6.jpg)

图 4.6–在命令行中运行 VS 代码笔记本后记录的实验页面

前面的截图显示了正确的源文件名(`fine_tuning.py`)和正确的 git 提交散列(**661 ffeda 5a e 53 CFF 3623 F2 FCC 8227d 822 e 877 e 2d**)。这种变通方法不需要我们更改笔记本的代码，如果我们的初始交互式笔记本调试已经完成，并且我们希望获得笔记本的完整运行，以及 MLflow 跟踪服务器中的正确代码版本跟踪，这种变通方法将非常有用。请注意，所有其他参数、指标和模型都被正确地跟踪，不管我们是否以交互方式运行笔记本。

## 管道跟踪

讨论了笔记本代码跟踪(版本和文件名)之后，让我们转向管道跟踪的主题。然而，在我们讨论管道跟踪之前，我们将讨论 ML/DL 生命周期中管道的定义。从概念上讲，管道是一个多步骤的数据处理和任务工作流。然而，这种数据/任务工作流的实现可能非常不同。在一些 ML 包中，可以将管道定义为一级 Python API。两个最著名的管道 API 如下:

*   `sklearn.pipeline.Pipeline`([https://sci kit-learn . org/stable/modules/generated/sk learn . pipeline . html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)):这广泛用于构建紧密集成的多步管道，用于经典机器学习或数据**提取、转换和加载** ( **ETL** )管道，使用**pandas data frames**(https://pandas.pydata.org/docs/reference/api/pandas。DataFrame.html)。
*   `pyspark.ml.Pipeline`([https://Spark . Apache . org/docs/latest/API/python/reference/API/pyspark . ml . pipeline . html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html)):这是一个 py Spark 版本，用于使用 **Spark DataFrames** 为机器学习或数据 ETL 管道构建简单且紧密集成的多步管道(https://Spark . Apache . org/docs/latest/API/python/reference/API/py Spark . SQL . data frame . html)。

然而，当我们构建 DL 模型管道时，我们需要在管道的不同步骤使用多个不同的 Python 包，因此使用单一管道 API 的一刀切方法通常是行不通的。此外，上述两个管道 API 都没有对当前流行的 DL 包的原生支持，如 **Huggingface** 或 **PyTorch-Lightning** ，这些都需要额外的集成工作。虽然存在一些开源的 DL 管道 API，如**neur axle**([https://github.com/Neuraxio/Neuraxle](https://github.com/Neuraxio/Neuraxle))，试图提供一个类似 sklearn 的管道接口和框架，但并没有被广泛使用。此外，使用这些基于 API 的管道意味着当您需要向管道添加更多步骤时，您将被锁定，这可能会降低您在新需求出现时扩展或发展 DL 管道的灵活性。

在本书中，我们将采用不同的方法来定义和构建一个基于 MLflow 的`fine_tuning.py`的 DL 管道，成为一个多步管道。该管道可以被视为一个三步流程图，如下所示:

![Figure 4.7 – A three-step DL pipeline 
](img/B18120_Figure_4.7.jpg)

图 4.7-三步 DL 管道

这个三步流程如下:

1.  将数据下载到本地执行环境
2.  微调模型
3.  注册模型

对于我们当前的例子来说，这些模块化步骤可能看起来有些多余，但是当涉及到更复杂的情况时，或者当每个步骤都需要改变时，拥有独特的功能步骤的力量是显而易见的。如果我们定义了需要在它们之间传递的参数，每个步骤都可以被修改，而不会影响其他步骤。每个步骤都是一个独立的 Python 文件，可以使用一组输入参数独立执行。将有一个主管道 Python 文件，可以运行整个管道或管道步骤的一个子部分。在没有文件扩展名的标准 YAML 文件`MLproject`中，我们可以定义四个入口点(`main`、`download_data`、`fine_tuning_model`和`register_model`)、它们所需的输入参数、它们的类型和默认值，以及执行每个入口点的命令行。在我们的示例中，这些入口点将在 Python 命令行执行命令中提供。但是，如果任何特定步骤需要，您可以调用任何类型的执行，比如批处理 shell 脚本。例如，本章的`MLproject`文件中的以下行([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 04/ml project](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/MLproject))描述了项目的名称、`conda`环境定义文件名和主入口点:

```
name: dl_model_chapter04
```

```
conda_env: conda.yaml
```

```
entry_points:
```

```
  main:
```

```
    parameters:
```

```
      pipeline_steps:
```

```
        description: Comma-separated list of dl pipeline steps to execute 
```

```
        type: str
```

```
        default: all
```

```
    command: "python main.py --steps {pipeline_steps}"
```

在这里，项目的名称是`dl_model_chapter04`。`conda_env`是指本地 conda 环境的 YAML 定义文件`conda.yaml`，与`MLproject`文件位于同一个目录下。`entry_points`部分列出了第一个入口点，称为`main`。在`parameters`部分，有一个名为`pipeline_steps`的参数，它允许用户定义要执行的 DL 管道步骤的逗号分隔列表。该参数属于`str`类型，其默认值为`all`，这意味着所有管道步骤都将运行。最后，`command`部分列出了如何在命令行中执行这个步骤。

文件的其余部分通过遵循与主入口点相同的语法约定来定义其他三个管道步骤入口点。例如，同一`MLproject`文件中的下列行定义了`download_data`的入口点:

```
  download_data:
```

```
    parameters:
```

```
      download_url:
```

```
        description: a url to download the data for fine tuning a text sentiment classifier
```

```
        type: str
```

```
        default: https://pl-flash-data.s3.amazonaws.com/imdb.zip
```

```
      local_folder:
```

```
        description: a local folder to store the downloaded data
```

```
        type: str
```

```
        default: ./data
```

```
      pipeline_run_name:
```

```
        description: an mlflow run name
```

```
        type: str
```

```
        default: chapter04
```

```
    command:
```

```
      "python pipeline/download_data.py --download_url {download_url} --local_folder {local_folder} \
```

```
      --pipeline_run_name {pipeline_run_name}"
```

与主入口点类似，`download_data`部分也定义了参数、类型和默认值的列表，以及执行该步骤的命令行。我们可以像在刚从这本书的 GitHub 资源库中签出的`MLproject`文件中一样定义其余的步骤。要了解更多细节，请看一下那个`MLproject`文件的完整内容。

在定义了`MLproject`文件之后，很明显我们已经以声明的方式定义了一个多步管道。这就像是管道的规范，说明了每个步骤的名称、它期望的输入参数以及如何执行它们。现在，下一步是实现 Python 函数来执行管道的每一步。那么，我们来看看主入口点的 Python 函数的核心实现，这个函数叫做`main.py`。下面几行代码(不是`main.py`中的全部 Python 代码)说明了仅用管道中的一个步骤实现整个管道的核心组件(`download_data`):

```
@click.command()
```

```
@click.option("--steps", default="all", type=str)
```

```
def run_pipeline(steps):
```

```
    with mlflow.start_run(run_name='pipeline', nested=True) as active_run:
```

```
        download_run = mlflow.run(".", "download_data", parameters={})
```

```
if __name__ == "__main__":
```

```
    run_pipeline()
```

这个主函数片段包含一个`run_pipeline`函数，当在命令行中执行`main.py`文件时将运行这个函数。有一个名为`steps`的参数，当它被提供时将被传递给这个函数。在这个例子中，我们使用`click`Python 包([https://click.palletsprojects.com/en/8.0.x/](https://click.palletsprojects.com/en/8.0.x/))来解析命令行参数。`run_pipeline`函数通过调用`mlflow.start_run`并传递两个参数(`run_name`和`nested`)来启动 MLflow 实验运行。我们以前用过`run_name`——它是这次跑步的描述性短语。然而，`nested`参数是新的，这意味着这是一个父实验运行。此父实验运行包含一些将在 MLflow 中分层跟踪的子实验运行。每个父运行可以包含一个或多个子运行。在示例代码中，这包含管道运行的一个步骤，称为`download_data`，通过调用`mlflow.run`来调用它。这是以编程方式调用 MLproject 入口点的关键 MLflow 函数。一旦`download_data`被调用并且运行结束，父运行也将结束，从而结束流水线的运行。

执行 MLproject 入口点的两种方法

有两种方法可以执行 MLproject 的入口点。首先可以使用 MLflow 的 Python API，称为`mlflow.run`([https://www . ml flow . org/docs/latest/Python _ API/ml flow . projects . html # ml flow . projects . run](https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run))。或者，您可以使用 MLflow 的命令行接口工具，称为`mlflow run`，可以在命令行 shell 环境中调用它来直接执行任何入口点([https://www.mlflow.org/docs/latest/cli.html#mlflow-run](https://www.mlflow.org/docs/latest/cli.html#mlflow-run))。

现在，让我们学习如何一般地实现管道中的每个步骤。对于每个管道步骤，我们将 Python 文件放在一个`pipeline`文件夹中。在这个例子中，我们有三个文件:`download_data.py`、`fine_tuning_model.py`和`register_model.py`。因此，成功构建 MLflow 支持的管道项目的相关文件如下:

```
MLproject
```

```
conda.yaml
```

```
main.py
```

```
pipeline/download_data.py
```

```
pipeline/fine_tuning_model.py
```

```
pipeline/register_model.py
```

对于每个管道步骤的实现，我们可以使用以下 Python 函数模板。预留占位符部分用于实现实际的流水线步骤逻辑:

```
import click
```

```
import mlflow
```

```
@click.command()
```

```
@click.option("input")
```

```
def task(input):
```

```
    with mlflow.start_run() as mlrun:
```

```
        # Implement pipeline step logic here 
```

```
        mlflow.log_parameter('parameter', parameter)
```

```
        mlflow.set_tag('pipeline_step', __file__)
```

```
        mlflow.log_artifacts(artifacts, artifact_path="data")
```

```
if __name__ == '__main__':
```

```
    task()
```

该模板允许我们标准化实现管道步骤任务的方式。这里的主要思想是，对于每个管道步骤任务，它需要从`mlflow.start_run`开始启动 MLflow 实验运行。一旦我们在函数中实现了特定的执行逻辑，我们需要使用`mlflow.log_parameter`记录一些参数，或者使用`mlflow.log_artifacts`记录工件存储中的一些工件，它们可以被传递给管道的下一步使用。这叫`mlflow.set_tag`。

例如，在`download_data.py`步骤中，核心实现如下:

```
import click
```

```
import mlflow
```

```
from flash.core.data.utils import download_data
```

```
@click.command()
```

```
@click.option("--download_url")
```

```
@click.option("--local_folder")
```

```
@click.option("--pipeline_run_name")
```

```
def task(download_url, local_folder, pipeline_run_name):
```

```
    with mlflow.start_run(run_name=pipeline_run_name) as mlrun:
```

```
        download_data(download_url, local_folder)
```

```
        mlflow.log_param("download_url", download_url)
```

```
        mlflow.log_param("local_folder", local_folder)
```

```
        mlflow.set_tag('pipeline_step', __file__)
```

```
        mlflow.log_artifacts(local_folder, artifact_path="data")
```

```
if __name__ == '__main__':
```

```
    task()
```

在这个`download_data.py`实现中，任务是从远程 URL 下载建模数据到本地文件夹(`download_data(download_url, local_folder)`)。一旦我们完成了这些，我们将记录一些参数，比如`download_url`和`local_folder`。我们还可以使用`mlflow.log_artifacts`将新下载的数据记录到 MLflow 工件存储中。对于本例，这似乎没有必要，因为我们只想在本地开发环境中执行下一步。然而，对于分布式执行环境中更现实的场景，其中每个步骤可以在不同的执行环境中运行，这是非常理想的，因为我们只需要将工件 URL 路径传递给管道的下一个步骤来使用；我们不需要知道上一步是如何执行的，在哪里执行的。在这个例子中，当调用`mlflow.log_artifacts(local_folder, artifact_path="data")`语句时，下载的数据文件夹被上传到 MLflow 工件存储。然而，我们不会在本章的下游管道步骤中使用这个工件路径。在本书的后面，我们将探索如何使用这种工件存储将工件传递到管道中的下一步。这里，我们将使用日志参数将下载的数据路径传递到管道的下一步(`mlflow.log_param("local_folder", local_folder)`)。因此，让我们看看如何通过扩展`main.py`来实现这一点，使其包含下一步，即`fine_tuning_model`入口点，如下所示:

```
        with mlflow.start_run(run_name='pipeline', nested=True) as active_run:
```

```
            download_run = mlflow.run(".", "download_data", parameters={})
```

```
            download_run = mlflow.tracking.MlflowClient().get_run(download_run.run_id)
```

```
            file_path_uri = download_run.data.params['local_folder']
```

```
            fine_tuning_run = mlflow.run(".", "fine_tuning_model", parameters={"data_path": file_path_uri})
```

我们使用`mlflow.tracking.MlflowClient().get_run`来获取`download_run` MLflow run 对象，然后使用`download_run.data.params`来获取`file_path_uri`(在本例中，它只是一个本地文件夹路径)。这然后被传递到下一个`mlflow.run`，也就是`fine_tuning_run`，作为键值参数(`parameters={"data_path": file_path_uri`)。这样,`fine_tuning_run`管道步骤可以使用这个参数作为其数据源路径的前缀。这是一个非常简单的场景，用来说明我们如何将数据从一个步骤传递到下一个步骤。使用 ml flow([https://www . ml flow . org/docs/latest/python _ API/ml flow . tracking . html](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html))提供的`mlflow.tracking.MlflowClient()` API，可以直接访问运行的信息(参数、指标和工件)。

我们还可以通过添加`register_model`步骤，用管道的第三步来扩展`main.py`文件。这一次，我们需要已登录的模型 URI 来注册一个已训练的模型，这取决于`fine_tuning_model`步骤的`run_id`。因此，在`fine_tuning_model`步骤中，我们需要获取`fine_tuning_model`运行的`run_id`属性，然后通过`register_model`运行的输入参数传递它，如下所示:

```
fine_tuning_run_id = fine_tuning_run.run_id
```

```
register_model_run = mlflow.run(".", "register_model", parameters={"mlflow_run_id": fine_tuning_run_id})
```

现在，`register_model`步骤可以使用`fine_tuning_run_id`来定位记录的模型。`register_model`步骤的核心实现如下:

```
    with mlflow.start_run() as mlrun:
```

```
        logged_model = f'runs:/{mlflow_run_id}/model'
```

```
        mlflow.register_model(logged_model, registered_model_name)
```

这将在由变量`logged_model`定义的 URI 处注册一个微调模型到 MLflow 模型注册表。

如果您遵循了这些步骤，那么您应该有一个可以被 MLflow 从头到尾跟踪的工作管道。提醒一下，一个先决条件是已经设置了本地成熟的 MLflow 服务器，如 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040) *所示，跟踪模型、参数和指标*。您应该已经在上一节中设置了虚拟环境`dl_model`。要测试此管道，请查看位于[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 04](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04)的本章 GitHub 存储库，并运行以下命令:

```
python main.py
```

这将运行整个三步管道，并在 MLflow 跟踪服务器中记录管道的`run_id`(即父运行)以及子运行时每一步的运行。当它完成运行时，控制台屏幕输出的最后几行将显示如下内容(当您运行管道时，您将在屏幕上看到许多输出):

![Figure 4.8 – Console output of running the pipeline with MLflow run_ids
](img/B18120_Figure_4.8.jpg)

图 4.8–使用 MLflow run_ids 运行管道的控制台输出

这显示了管道的`run_id`，也就是`f8f21fdf8fff4fd6a400eeb403b776c8`；最后一步是`fine_tuning_model`的`run_id`房产，也就是`5ba38e059695485396e709b809e9bb8d`。如果我们通过点击`http://localhost`转到 MLflow tracking 服务器的 UI 网页，我们应该能够看到`dl_model_chapter04`实验文件夹中运行的以下嵌套实验，如下所示:

![Figure 4.9 – A pipeline being run with nested three-step child runs in the MLflow tracking server
](img/B18120_Figure_4.9.jpg)

图 4.9–在 MLflow 跟踪服务器中运行嵌套三步子管道的管道

前面的屏幕截图显示了管道运行，以及源文件`main.py`和管道三个步骤的嵌套运行。每一步都有一个对应的入口点名称，在`MLproject`中定义有 GitHub 提交哈希代码版本的`register_model`运行页面，你会看到以下信息:

![Figure 4.10 – Entry point register_model's run page on the MLflow tracking server
](img/B18120_Figure_4.10.jpg)

图 4.10–ml flow 跟踪服务器上的入口点 register_model 运行页面

前面的截图不仅显示了一些我们已经看到的熟悉信息，还显示了一些新信息，如`file:///`、GitHub 哈希代码版本、入口点`(-e register_model`、执行环境(这是一个本地开发环境(`-b local`)以及`register_model`函数的预期参数(`-P`)。我们将在本书的后面学习如何使用 MLflow 的`MLproject`运行命令来远程执行任务。这里，我们只需要理解源代码是通过入口点(`register_model`)引用的，而不是文件名本身，因为引用是在`MLproject`文件中声明为入口点的。

如果您在 MLflow 跟踪服务器中看到了图 4.9 和图 4.10 所示的输出，那么是时候庆祝了——您已经使用 MLflow 成功地执行了多步 DL 管道！

总之，要在 MLflow 中跟踪多步骤 DL 管道，我们可以使用`MLproject`来定义每个管道步骤的入口点和一个主管道入口点。在主管道函数中，我们实现了一些方法，以便数据可以在管道步骤之间传递。然后，每个管道步骤使用共享的数据以及其他输入参数来执行特定的任务。使用 MLflow 跟踪服务器来跟踪主要管道级函数和管道的每个步骤，MLflow 跟踪服务器生成一个父级`run_id`来跟踪主要管道运行，并生成多个 ml flow 嵌套运行来跟踪每个管道的步骤。我们为每个管道步骤引入了一个模板，以标准的方式实现这个任务。我们还探索了通过 MLflow 的`run`参数和工件存储完成的强大管道链接，以了解如何在管道步骤之间传递数据。

现在您已经知道了如何跟踪笔记本和管道，让我们学习如何跟踪 Python 库。

# 跟踪本地私有的 Python 库

现在，让我们把注意力转移到跟踪本地私有的 Python 库上。对于公开发布的 Python 库，我们可以在一个需求文件或者一个`conda.yaml`文件中明确指定它们的发布版本，这个版本是在 PyPI 中发布的。例如，本章的`conda.yaml`文件([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 04/conda . YAML](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/conda.yaml))定义了 Python 版本并提供了对需求文件的引用，如下所示:

```
name: dl_model 
```

```
channels:
```

```
  - conda-forge
```

```
dependencies:
```

```
  - python=3.8.10
```

```
  - pip
```

```
  - pip:
```

```
    - -r requirements.txt
```

Python 版本定义为`3.8.10`，正在强制执行。这个`conda.yaml`文件还引用了一个`requirements.txt`文件，该文件包含以下版本化的 Python 包作为一个`requirements.txt`文件，该文件与`conda.yaml`文件位于同一目录中:

```
ipykernel==6.4.1
```

```
lightning-flash[all]==0.5.0
```

```
mlflow==1.20.2
```

```
transformers==4.9.2
```

```
boto3==1.19.7
```

```
pytorch-lightning==1.4.9
```

```
datasets==1.9.0
```

```
click==8.0.3
```

正如我们所看到的，所有这些包都使用它们发布的 PyPI([https://pypi.org/](https://pypi.org/))版本号被明确跟踪。当您运行 MLflow `MLproject`时，MLflow 将使用`conda.yaml`文件和引用的`requirements.txt`文件动态创建一个 conda 虚拟环境。这确保了执行环境是可再现的，并且所有 DL 模型管道都可以成功运行。您可能已经注意到，在您第一次运行上一节的 MLflow 管道项目时，已经为您创建了这样一个虚拟环境。您可以通过运行以下命令再次执行此操作:

```
conda env list
```

这将在您当前的机器中产生一个 conda 虚拟环境的列表。您应该能够找到一个以`mlflow-`开头的虚拟环境，后面是一长串字母数字字符，如下所示:

```
mlflow-95353930ddb7b60101df80a5d64ef8bf6204a808
```

这是由 MLflow 动态创建的虚拟环境，它遵循在`conda.yaml`和`requirements.txt`中指定的依赖关系。随后，当您记录微调后的模型时，`conda.yaml`和`requirements.txt`将自动记录在 MLflow 工件存储中，如下所示:

![Figure 4.11 – Python packages are being logged and tracked in the MLflow artifact store
](img/B18120_Figure_4.11.jpg)

图 4.11–在 MLflow 工件存储中记录和跟踪 Python 包

正如我们所见，`conda.yaml`文件被自动扩展以包含`requirements.txt`的内容，以及 conda 决定包含的其他依赖项。

对于私有构建的 Python 包，这意味着 Python 包没有发布到 PyPI 供公众使用和参考，推荐的方法是使用`git+ssh`来包含这样的 Python 包。让我们假设你有一个名为`cool-dl-utils`的私人构建项目，你工作的组织名为`cool_org`，你的项目的资源库已经建立在 GitHub 中。如果您想在需求文件中包含这个项目的 Python 包，您需要确保将您的公钥添加到您的 GitHub 设置中。如果你想了解如何生成一个公钥并将其加载到 GitHub 中，可以看看 GitHub 的指南，网址是[https://docs . GitHub . com/en/authentic ation/connecting-to-GitHub-with-ssh/adding-a-new-ssh-key-to-your-GitHub-account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account)。在`requirements.txt`文件中，您可以添加下面一行，它将引用一个特定的 GitHub hash ( `81218891bbf5a447103884a368a75ffe65b17a44`)和从这个私有存储库构建的 Python `.egg`包(如果您愿意，也可以引用一个`.whl`包):

```
cool-dl-utils @ git+ssh://git@github.com/cool_org/cool-dl-utils.git@81218891bbf5a447103884a368a75ffe65b17a44#egg=cool-dl-utils
```

如果您在自己构建的包中有一个数字发布的版本，您也可以直接引用`requirements.txt`文件中的发布号，如下所示:

```
git+ssh://git@github.com/cool_org/cool-dl-utils.git@2.11.4
```

这里`cool-dl-utils`的发布号是`2.11.4`。这允许 MLflow 将这个私有构建的包拉入虚拟环境来执行`MLproject`。在本章中，我们不需要引用任何私人构建的 Python 包，但是值得注意的是 MLflow 可以利用`git+ssh`方法来做到这一点。

现在，让我们学习如何跟踪数据版本。

# 跟踪三角洲湖泊中的数据版本

在本节中，我们将了解如何在 MLflow 中跟踪数据。历史上，数据管理和版本控制通常被认为不同于机器学习和数据科学。然而，以数据为中心的人工智能的出现正在发挥越来越重要的作用，特别是在 DL 中。因此，了解使用什么数据以及如何使用数据来改进 DL 模型至关重要。在 2021 年夏天由吴恩达组织的首届以数据为中心的人工智能竞赛中，成为获胜者的要求不是改变和调整模型，而是改善固定模型的数据集([https://https-deeplearning-ai.github.io/data-centric-comp/](https://https-deeplearning-ai.github.io/data-centric-comp/))。下面是来自比赛网页的一段话:

“以数据为中心的人工智能竞赛颠覆了传统的形式，要求你在给定一个固定模型的情况下，改进一个数据集。我们将为您提供一个数据集，通过应用以数据为中心的技术进行改进，如修复不正确的标签、添加代表边缘案例的示例、应用数据扩充等。”

这种范式转变凸显了数据在深度学习中的重要性，尤其是有监督的深度学习，其中标记的数据很重要。一个隐含的潜在假设是，不同的数据将产生不同的模型度量，即使使用相同的模型架构和参数。这要求我们努力地跟踪数据版本化过程，以便我们知道哪个版本的数据正被用于产生获胜的模型。

在 ML/DL 生命周期中，有几个用于跟踪数据版本的新兴框架。这个领域的早期先驱之一是**http://dvc.org**([DVC](http://dvc.org))。它使用一组类似 GitHub 的命令来拉/推数据，就像它们是代码一样。它允许数据被远程存储在 S3 或谷歌驱动器，以及许多其他受欢迎的商店。然而，存储在远程存储中的数据变得杂乱无章，不可读。由于访问数据的唯一方式是通过 DVC 工具和配置，这就变成了一个固有的问题。此外，很难跟踪数据及其模式是如何改变的。虽然可以将 MLflow 与 DVC 集成，但它的可用性和灵活性并不像我们希望的那样理想。因此，我们不会在本书中深入探讨这种方法。如果你对此感兴趣，我们建议你利用本章末尾的*使用 DVC 的 ML 项目中的版本控制数据和模型和 AWS* 参考来找到更多关于使用 DVC 的细节。

最近开放的 sourced 和基于开放格式的**Delta Lake**([https://delta.io/](https://delta.io/))是 DL/ML 项目中集成数据管理和版本控制的实用解决方案，特别是因为 MLflow 可以直接支持这种集成。这也是基础的数据管理层，称为**lake house**([https://databricks . com/blog/2020/01/30/what-is-a-data-lake house . html](https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html))，它将数据仓库和流数据统一到一个数据基础层中。它支持模式变更跟踪和数据版本控制，这是 DL/ML 数据使用场景的理想选择。Delta 表基于开放的标准文件格式，称为**Parquet**([https://parquet.apache.org/](https://parquet.apache.org/))，广泛支持大规模数据存储。

数据块中的增量表

注意，本节假设您可以访问数据块服务，这允许您在**数据块文件系统** ( **DBFS** )中试验 Delta Lake 格式。你可以去 Databricks 门户网站[https://community.cloud.databricks.com/login.html](https://community.cloud.databricks.com/login.html)获得社区版的试用账号。

请注意，本节要求您使用 **PySpark** 通过从/向存储器(如 S3)读取/写入数据来操作数据。Delta Lake 有一个叫做**时间旅行**的功能，可以自动版本化数据。通过传递一个参数，比如时间戳或版本号，可以读取该特定版本或时间戳的任何历史数据。这使得复制和跟踪实验更加容易，因为关于数据的唯一的时间元数据是数据的版本号或时间戳。有两种方法可以查询增量表:

*   `timestampAsOf`:这允许你读取增量表，以及读取一个有特定时间戳的版本。以下代码显示了如何使用`timestampAsOf` :

    ```
    df = spark.read \   .format("delta") \   .option("timestampAsOf", "2020-11-01") \   .load("/path/to/my/table")
    ```

    读取数据
*   `versionAsOf`:定义增量表版本的数值。您还可以选择从版本 0 开始读取具有特定版本的版本。下面的 PySpark 代码使用定义为版本`52` :

    ```
    df = spark.read \   .format("delta") \   .option("versionAsOf", "52") \   .load("/path/to/my/table")
    ```

    的`versionAsOf`选项读取数据

拥有这种带时间戳或版本化的访问是使用增量表跟踪任何文件版本的一个关键优势。因此，让我们来看看 MLflow 中的一个具体示例，这样我们就可以跟踪我们一直在使用的 IMDb 数据集。

## 使用 MLflow 跟踪数据的示例

对于我们一直用来微调情感分类模型的 IMDb 数据集，我们将把这些 CSV 文件上传到 Databricks 的数据存储或您可以从 Databricks 门户访问的任何 S3 存储桶中。完成后，按照以下步骤创建一个支持版本化和时间戳数据访问的增量表:

1.  将以下 CSV 文件读入一个数据帧(假设你将`train.csv`文件上传到数据块中的`FileStore/imdb/`文件夹):

    ```
    imdb_train_df = spark.read.option('header', True).csv('dbfs:/FileStore/imdb/train.csv')
    ```

2.  将 DBFS 的`imdb_train_df`数据帧写成增量表，如下:

    ```
    imdb_train_df.write.format('delta').option("mergeSchema", "true").mode("overwrite").save('/imdb/training.delta')
    ```

3.  使用以下命令将`training.delta`文件读回内存:

    ```
    imdb_train_delta = spark.read.format('delta').load('/imdb/training.delta')
    ```

4.  现在，通过 Databricks UI 查看增量表的历史。将增量表从存储器读入内存后，点击**历史**选项卡:

![Figure 4.12 – The train_delta table's history with a version and a timestamp column
](img/B18120_Figure_4.12.jpg)

图 4.12–带有版本和时间戳列的 train_delta 表的历史

前面的截图显示版本为 **0** ，时间戳为 **2021-11-22** 。当将版本号或时间戳传递给 Spark 数据帧读取器时，我们可以使用这个值来访问版本化数据。

1.  使用以下命令读取版本化的`imdb/train_delta`文件:

    ```
    train_data_version = spark.read.format("delta").option("versionAsOf", "0").load('/imdb/train.delta')  
    ```

这将读取`train.delta`文件的版本`0`。如果我们有这个文件的其他版本，我们可以传递一个不同的版本号。

1.  使用以下命令读取带时间戳的`imdb/train_delta`文件:

    ```
    train_data_timestamped = spark.read.format("delta").option("timestampAsOf", "2021-11-22T03:39:22").load('/imdb/train.delta')  
    ```

这将读取带有时间戳的数据。在撰写本文时，这是我们仅有的时间戳，这很好。如果我们有更多的时间戳数据，我们可以传递给它一个不同的版本。

1.  现在，如果我们需要在 MLflow 跟踪实验运行中记录这个数据版本，我们可以使用`mlflow.log_parameter()`记录数据的路径、版本号和/或时间戳。这会将这些记录为实验参数键值列表的一部分:

    ```
    mlflow.log_parameter('file_path', '/imdb/train.delta') mlflow.log_parameter('file_version', '0') mlflow.log_parameter('file_timestamp', '2021-11-22T03:39:22') 
    ```

使用增量表的唯一要求是，数据需要以支持增量表的存储形式存储，如 Databricks 支持的 Lakehouse。这对于企业 ML/DL 场景有很大的价值，因为我们可以跟踪数据版本以及代码和模型版本。

总之，Delta Lake 提供了一种简单而强大的数据版本化方法。MLflow 可以轻松地记录这些版本号和时间戳，作为实验参数列表的一部分，以跟踪数据，以及所有其他参数、指标、工件、代码和模型。

# 总结

在本章中，我们深入探讨了如何在 MLflow 实验运行中跟踪代码和数据版本。我们首先回顾了不同类型的笔记本:Jupyter 笔记本、Databricks 笔记本和 VS Code 笔记本。我们对它们进行了比较，并推荐使用 VS 代码来创作笔记本，因为它支持 IDE、Python 风格、自动完成以及许多更丰富的特性。

然后，在回顾了现有 ML 管道 API 框架的局限性之后，我们讨论了如何使用 MLflow 的`run_id`创建多步 DL 管道，然后为每个管道步骤使用子管道`run_id`。通过将参数或工件存储位置传递给下一步，可以使用`mlflow.run()`和`mlflow.tracking.MlflowClient()`灵活地进行管道链接和跟踪。我们使用 MLflow 嵌套运行跟踪功能成功运行了端到端的三步管道。这也将为我们在以后的章节中扩展 MLproject 的使用，以分布式方式运行不同的步骤打开大门。

我们还学习了如何使用`git+ssh`方法跟踪私有构建的 Python 包。然后，我们使用 Delta Lake 方法获得对数据的版本化和时间戳访问。这允许使用版本号或时间戳以两种方式跟踪数据。MLflow 然后可以在 MLflow 实验运行期间记录这些版本号或时间戳作为参数。由于我们正在进入以数据为中心的人工智能时代，能够跟踪数据版本对于可重复性和时间旅行至关重要。

至此，我们已经学习了如何使用 MLflow 全面地跟踪代码、数据和模型。在下一章，我们将学习如何以分布式方式扩展我们的 DL 实验。

# 延伸阅读

有关本章涵盖的主题的更多信息，请查看以下资源:

1.  数据块中的 MLflow 笔记本实验跟踪:[https://docs . data bricks . com/applications/ml flow/tracking . html # create-notebook-experiment](https://docs.databricks.com/applications/mlflow/tracking.html#create-notebook-experiment)
2.  *构建多步骤工作流*:[https://www . ml flow . org/docs/latest/projects . html # Building-multi step-Workflows](https://www.mlflow.org/docs/latest/projects.html#building-multistep-workflows)
3.  *带有 MLflow 项目的端到端 ML 管道*:[https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/](https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/)
4.  安装私有构建的 Python 包:[https://medium . com/@ ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b 19436 f3e](mailto:https://medium.com/@ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b19436f3e)
5.  *使用 DVC 和 AWS 对 ML 项目中的数据和模型进行版本控制*:[https://medium . com/analytics-vid hya/Versioning-data-and-models-in-ML-projects-using-DVC-and-AWS-S3-286 e664 a 7209](https://medium.com/analytics-vidhya/versioning-data-and-models-in-ml-projects-using-dvc-and-aws-s3-286e664a7209)
6.  *引入大规模数据湖的 Delta 时间旅行*:[https://databricks . com/blog/2019/02/04/Introducing-Delta-Time-Travel-for-Large-Scale-Data-Lakes . html](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)
7.  *我们如何赢得首届以数据为中心的人工智能大赛:Synaptic-AnN*:[https://www . deep learning . AI/Data-Centric-AI-Competition-Synaptic-AnN/](https://www.deeplearning.ai/data-centric-ai-competition-synaptic-ann/)
8.  *复制任何东西:机器学习遇到数据湖屋*:[https://databricks . com/blog/2021/04/26/reproduct-any-Machine-Learning-Meets-Data-lake house . html](https://databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html)
9.  *DATABRICKS 社区版:初学者指南*:[https://www . top coder . com/thrive/articles/data bricks-COMMUNITY-EDITION-A-初学者指南](https://www.topcoder.com/thrive/articles/databricks-community-edition-a-beginners-guide)