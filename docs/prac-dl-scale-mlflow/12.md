# 八、大规模部署 DL 推理流水线

为生产使用部署一个**深度学习** ( **DL** )推理管道既令人兴奋又富有挑战性。令人兴奋的是，最后，DL 模型管道可以用于预测现实世界的生产数据，这将为业务场景提供真正的价值。然而，具有挑战性的部分是有不同的 DL 模型服务于平台和主机环境。为正确的模型服务场景选择正确的框架并不容易，它可以最小化部署复杂性，但以可伸缩和经济高效的方式提供最佳的模型服务体验。本章将概述不同的部署场景和主机环境，然后提供关于如何部署到不同环境的实践学习，包括使用 MLflow 部署工具的本地和远程云环境。学完本章后，您应该能够自信地将 MLflow DL 推理管道部署到各种主机环境中，用于批处理或实时推理服务。

在本章中，我们将讨论以下主要话题:

*   了解部署和托管环境的前景
*   为批处理和 web 服务推理进行本地部署
*   使用 Ray Serve 和 MLflow 部署插件进行部署
*   部署到 AWS SageMaker——完整的端到端指南

# 技术要求

本章学习需要以下项目:

*   本章 GitHub 库代码:[https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08)。
*   雷服和`mlflow-ray-serve`外挂:[https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve)。
*   AWS SageMaker:你需要有一个 AWS 账户。您可以通过位于[https://aws.amazon.com/free/](https://aws.amazon.com/free/)的免费注册网站轻松创建一个免费的 AWS 帐户。
*   AWS **命令行界面**(**CLI**):[https://docs . AWS . Amazon . com/CLI/latest/user guide/getting-started-install . html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)。
*   Docker 桌面:【https://www.docker.com/products/docker-desktop/】T21。
*   完成本书第七章 、*多步深度学习推理流水线、*中的例子。这将为您提供一个可以在本章中使用的现成的推理管道。

# 了解不同的部署工具和主机环境

MLOps 技术堆栈中有不同的部署工具，它们具有不同的目标用例以及用于部署不同模型推理管道的宿主环境。在 [*第 7 章*](B18120_07_ePub.xhtml#_idTextAnchor083) 、*多步深度学习推理管道*中，我们学习了不同的推理场景和需求，并实现了一个多步 DL 推理管道，可以部署到模型托管/服务环境中。现在，我们将学习如何将这样的模型部署到几个特定的模型托管和服务环境中。这在图 8.1 中显示如下:

![Figure 8.1 – Using model deployment tools to deploy a model inference pipeline to 
a model hosting and serving environment
](img/B18120_08_01.jpg)

图 8.1–使用模型部署工具将模型推理管道部署到模型托管和服务环境中

从*图 8.1* 可以看出，不同的模型托管和服务环境可以有不同的部署工具。在这里，我们列出了如下三种典型场景:

*   **批量推理**:如果我们希望定期进行批量推理，我们可以使用 PySpark **用户定义函数** ( **UDF** )来加载一个 MLflow 模型来实现这一点，因为我们可以在分布式集群上利用 Spark 的可伸缩计算方法([https://ml flow . org/docs/latest/models . html # export-a-python-function-model-as-an-Apache-Apache)我们将在下一节展示一个如何做到这一点的例子。](https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf)
*   **大规模流推理**:这通常需要一个端点来托管**模型即服务** ( **MaaS** )。有相当多的工具和框架用于产品级部署和模型服务。在本章开始学习如何进行这种类型的部署之前，我们将在本节中比较几个工具，以了解它们如何工作以及它们与 MLflow 的集成情况。
*   **On-device model 推断**:这是一个名为 **TinyML** 的新兴领域，它在移动、传感器或边缘设备等资源受限的环境中部署 ML/DL 模型([https://www . kdnugges . com/2021/11/On-device-deep-learning-py torch-mobile-tensor flow-lite . html](https://www.kdnuggets.com/2021/11/on-device-deep-learning-pytorch-mobile-tensorflow-lite.html))。两个流行的框架是 py torch Mobile([https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/))和 tensor flow Lite([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite))。这不是本书的重点。鼓励你在这一章的末尾查阅这方面的进一步阅读。

现在，让我们看看有哪些工具可用于将模型推理部署为服务，尤其是那些支持 MLflow 模型部署的工具。有三种类型的模型部署和服务工具，如下所示:

*   **MLflow 内置模型部署**:这是 MLflow 版本的现成产品，包括部署到本地 web 服务器、AWS SageMaker 和 Azure ML。在撰写本文时，Databricks 上还有一个托管的 MLflow，它支持公共审核中的模型服务，我们不会在本书中介绍，因为这在官方 Databricks 文档中有很好的介绍(感兴趣的读者应该在以下网站查找关于此 Databricks 功能的官方文档:[https://docs . data bricks . com/applications/ml flow/model-serving . html](https://docs.databricks.com/applications/mlflow/model-serving.html))。然而，我们将在本章中向您展示如何使用 MLflow 内置模型部署来部署到本地和远程 AWS SageMaker。
*   `mlflow-torchserv`([https://github.com/mlflow/mlflow-torchserve](https://github.com/mlflow/mlflow-torchserve))`mlflow-ray-serve`([https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve))`mlflow-triton-plugin`([https://github . com/triton-inference-server/server/tree/v 2 . 17 . 0/deploy/ml flow-triton-plugin](https://github.com/triton-inference-server/server/tree/v2.17.0/deploy/mlflow-triton-plugin))。我们将在本章展示如何使用`mlflow-ray-serve`插件进行部署。
*   `mlflow-ray-serve` plugin to deploy the MLflow Python model. Note that, although in this book we show how to use an MLflow customized plugin to deploy with a generic ML serve tool such as Ray Serve, it is important to note that a generic ML serve tool can do much more with or without an MLflow customized plugin.

    通过专门的推理引擎优化 DL 推理

    还有一些特殊的 MLflow 模型口味比如**ONNX**([https://onnx.ai/](https://onnx.ai/))**torch script**([https://hugging face . co/docs/transformers/v 4 . 17 . 0/en/serialization # torch script](https://huggingface.co/docs/transformers/v4.17.0/en/serialization#torchscript))是专门为 DL 模型推理运行时设计的。我们可以将 DL 模型转换成 ONNX 模型风格([https://github.com/microsoft/onnxruntime](https://github.com/microsoft/onnxruntime))或者 TorchScript 服务器([https://pytorch.org/serve/](https://pytorch.org/serve/))。由于 ONNX 和 TorchScript 仍在发展中，并且是专门为最初的 DL 模型部分而设计的，而不是整个推理管道，所以我们不在本章中讨论它们。

既然我们已经对各种部署工具和模型服务框架有了很好的理解，让我们在接下来的章节中通过具体的例子来学习如何进行部署。

# 本地部署批处理和 web 服务推理

出于开发和测试的目的，我们通常需要在本地部署我们的模型，以验证它是否按预期工作。我们来看两个场景是如何做到的:批量推理和 web 服务推理。

## 批量推断

对于批次推断，遵循以下说明:

1.  确保你已经完成了 [*第七章*](B18120_07_ePub.xhtml#_idTextAnchor083) 、*多步深度学习推理流水线*。这将产生一个 MLflow `pyfunc` DL 推理模型管道 URI，可以使用标准 MLflow Python 函数加载。记录的型号可以通过`run_id`和型号名称进行唯一定位，如下:

    ```
    logged_model = 'runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model'
    ```

还可以使用型号注册表通过型号名称和版本号来标识型号，如下所示:

```
logged_model = 'models:/inference_pipeline_model/6'
```

1.  按照本`README.md`文件([https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter 08/readme . MD](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/README.md))的*批量推理使用 PySpark UDF 函数*部分中的说明，设置本地虚拟环境、一个成熟的 ml flow 跟踪服务器和一些环境变量，以便我们可以在您的本地环境中执行代码。
2.  用 MLflow `mlflow.pyfunc.spark_udf` API 加载模型，创建一个 PySpark UDF 函数，如下所示。您可能想从 GitHub 查看一下`batch_inference.py`文件，以便跟进([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 08/batch/batch _ inference . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py)):

    ```
    loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model, result_type=StringType())
    ```

这将把推理管道包装成一个 PySpark UDF 函数，返回结果类型为`String`。这是因为我们的模型推理管道有一个模型签名，要求输出为一个`string`类型的列。

1.  现在，我们可以将 PySpark UDF 函数应用于输入数据帧。注意，输入数据帧必须有一个数据类型为`string`的`text`列，因为这是模型签名所要求的:

    ```
    df = df.withColumn('predictions', loaded_model())
    ```

因为我们的模型推理管道已经定义了一个模型签名，如果它在输入数据帧中找到了`text`列，在本例中是`df`，我们就不需要指定任何列参数。注意，我们可以使用 Spark 的`read` API 读取大量数据，它支持不同的数据格式读取，比如 CSV、JSON、Parquet 等等。在我们的示例中，我们从 IMDB 数据集中读取了`test.csv`文件。如果我们有大量数据，这将利用 Spark 在集群上强大的分布式计算。这使我们能够毫不费力地进行批量推断。

1.  要从头到尾运行批处理推理代码，您应该在以下位置查看存储库中提供的完整代码:[https://github . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/blob/main/chapter 08/batch/batch _ inference . py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter08/batch/batch_inference.py)。在`batch`文件夹中运行以下命令之前，确保用您自己的`run_id`和型号名称或注册的型号名称和版本替换`logged_model`变量:

    ```
    python batch_inference.py
    ```

2.  您应该在屏幕上看到图 8.2 中的输出:

![Figure 8.2 – Batch inference using PySpark UDF function
](img/B18120_08_02.jpg)

图 8.2–使用 PySpark UDF 函数进行批量推断

从*图 8.2* 中可以看到，我们加载的多步推理管道工作正常，甚至检测到了非英语文本和副本，尽管语言检测器可能产生了一些误报。输出是一个两列数据帧，其中模型预测的 JSON 响应保存在`predictions`列中。请注意，您可以在 Databricks 笔记本中使用`batch_inference.py`中提供的相同代码，并通过更改输入数据和记录的模型位置，使用 Spark cluster 处理大量输入数据。

既然我们知道了如何大规模地进行批量推理，那么让我们看看如何为相同的模型推理管道部署到本地 web 服务。

## 作为 web 服务的模型

我们可以在本地将同一个日志模型推理管道部署到一个 web 服务，并拥有一个接受 HTTP 请求和 HTTP 响应的端点。

本地部署非常简单，只需一个命令行。我们可以像在前面的批处理推断中一样，使用模型 URI 来部署一个日志模型或一个注册模型，如下所示:

```
mlflow models serve -m models:/inference_pipeline_model/6
```

您应该能够看到以下内容:

```
2022/03/06 21:50:19 INFO mlflow.models.cli: Selected backend for flavor 'python_function'
2022/03/06 21:50:21 INFO mlflow.utils.conda: === Creating conda environment mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 ===
± |main U:1 ?:8 X| done
Solving environment: done
```

这将使用记录的模型创建 conda 环境，这样它将有所有的依赖项来运行。创建 conda 环境后，您应该看到以下内容:

```
2022/03/06 21:52:11 INFO mlflow.pyfunc.backend: === Running command 'source /Users/yongliu/opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'
[2022-03-06 21:52:12 -0800] [97554] [INFO] Starting gunicorn 20.1.0
[2022-03-06 21:52:12 -0800] [97554] [INFO] Listening at: http://127.0.0.1:5000 (97554)
[2022-03-06 21:52:12 -0800] [97554] [INFO] Using worker: sync
[2022-03-06 21:52:12 -0800] [97561] [INFO] Booting worker with pid: 97561
```

现在，模型被部署为 web 服务，并准备好接受模型预测的 HTTP 请求。打开不同的终端窗口，键入以下命令调用模型 web 服务以获得预测响应:

```
curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{
    "columns": ["text"],
    "data": [["This is the best movie we saw."], ["What a movie!"]]
}'
```

我们可以立即看到以下预测响应:

```
[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/07b900a96af04037a956c74ef691396e/model\", \"inference_pipeline_model_uri\": \"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/07b900a96af04037a956c74ef691396e/model\", \"inference_pipeline_model_uri\": \"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\"}}"}]
```

如果到目前为止您已经遵循了这些步骤，并且看到了预测结果，那么您应该会感到非常自豪，因为您刚刚在本地 web 服务中部署了一个 DL 模型推理管道！这对于测试和调试来说是非常好的，并且模型的行为在生产 web 服务器上不会改变，所以我们应该确保它在本地 web 服务器上工作。

到目前为止，我们已经学习了如何使用内置的 MLflow 部署工具。接下来，我们将了解如何使用通用部署工具 Ray Serve 来部署 MLflow 推理管道。

# 使用 Ray Serve 和 MLflow 部署插件进行部署

一种更通用的部署方式是使用一个框架，比如 Ray Serve([https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html))。Ray Serve 有几个优点，比如 DL 模型框架不可知论者，原生 Python 支持，支持复杂模型组合推理模式。Ray Serve 支持所有主要的 DL 框架和任何任意的业务逻辑。那么，我们可以同时利用 Ray Serve 和 MLflow 来进行模型部署和服务吗？好消息是，我们可以使用 Ray Serve 提供的 MLflow 部署插件来做到这一点。让我们来看看如何使用`mlflow-ray-serve`插件通过 Ray Serve([https://github.com/ray-project/mlflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve))进行 MLflow 模型部署。在我们开始之前，我们需要安装`mlflow-ray-serve`包:

```
pip install mlflow-ray-serve
```

然后，我们需要首先使用以下两个命令在本地启动单节点光线簇:

```
ray start --head
serve start
```

这将在本地启动一个光线簇，你可以在`http://127.0.0.1:8265/#/`从你的网络浏览器访问它的仪表板，如下所示:

![Figure 8.3 – A locally running Ray cluster
](img/B18120_08_03.jpg)

图 8.3–本地运行的光线簇

*图 8.3* 显示了一个局部运行的射线簇。然后，您可以发出以下命令将`inference_pipeline_model`部署到 Ray Serve 中，如下所示:

```
mlflow deployments create -t ray-serve -m runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model --name dl-inference-model-on-ray -C num_replicas=1
```

这将显示以下屏幕输出:

```
2022-03-20 20:16:46,564    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379
2022-03-20 20:16:46,717    INFO api.py:242 -- Updating deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray
(ServeController pid=78159) 2022-03-20 20:16:46,784    INFO deployment_state.py:912 -- Adding 1 replicas to deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray
2022-03-20 20:17:10,309    INFO api.py:249 -- Deployment 'dl-inference-model-on-ray' is ready at `http://127.0.0.1:8000/dl-inference-model-on-ray`. component=serve deployment=dl-inference-model-on-ray
python_function deployment dl-inference-model-on-ray is created
```

这意味着在`http://127.0.0.1:8000/dl-inference-model-on-ray`的一个端点准备好服务一个在线推理请求！您可以使用在`chapter08/ray_serve/query_ray_serve_endpoint.py`提供的 Python 代码测试这个部署，如下所示:

```
python ray_serve/query_ray_serve_endpoint.py
```

这将在屏幕上显示如下结果:

```
2022-03-20 21:16:45,125    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379
[{'name': 'dl-inference-model-on-ray', 'info': Deployment(name=dl-inference-model-on-ray,version=None,route_prefix=/dl-inference-model-on-ray)}]
{
    "columns": [
        "text"
    ],
    "index": [
        0,
        1
    ],
    "data": [
        [
            "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/be2fb13fe647481eafa071b79dde81de/model\", \"inference_pipeline_model_uri\": \"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\"}}"
        ],
        [
            "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/be2fb13fe647481eafa071b79dde81de/model\", \"inference_pipeline_model_uri\": \"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\"}}"
        ]
    ]
}
```

您应该会看到推理模型的预期响应。如果您一直坚持到现在，那么恭喜使用`mlflow-ray-serve` MLflow 部署插件成功完成了部署！如果不再需要这个 Ray Serve 实例，可以通过执行以下命令行来停止它:

```
ray stop
```

这将停止本地计算机上所有正在运行的 Ray 实例。

使用 MLflow 部署插件进行部署

有几个 MLflow 部署插件。我们刚刚展示了如何使用`mlflow-ray-serve`部署一个通用的 MLflow Python 模型`inference_pipeline_model`。这为部署到许多目标目的地打开了大门，您可以在任何云提供商中启动 Ray 集群。我们不会在这一章中涉及更多的细节，因为这超出了本书的范围。如果你对感兴趣，可以参考关于如何启动云集群的 Ray 文档(AWS、Azure 和 **Google 云平台**(**GCP**):[https://docs . Ray . io/en/latest/cluster/Cloud . html #:~:text = The % 20 Ray % 20 cluster % 20 launcher % 20 can，ready % 20 to % 20 launch % 20 your % 20 cluster](https://docs.ray.io/en/latest/cluster/cloud.html#:~:text=The%20Ray%20Cluster%20Launcher%20can,ready%20to%20launch%20your%20cluster)。一旦有了光线簇，就可以按照相同的过程来部署 MLflow 模型。

既然我们已经知道了几种本地部署的方法，并且如果需要的话，还可以使用 Ray Serve 进一步部署到云，那么在下一节中，让我们看看如何部署到云管理的推理服务 AWS SageMaker，因为它已经被广泛使用，并且可以为如何在现实场景中进行部署提供很好的经验。

# 部署到 AWS sage maker——完整的端到端指南

AWS SageMaker 有一个由 AWS 管理的云托管模型服务。我们将使用 AWS SageMaker 作为一个例子，向您展示如何部署到一个远程云提供商，以提供托管的 web 服务，从而服务于真实的生产流量。AWS SageMaker 有一套 ML/DL 相关的服务，包括支持注释和模型训练等等。在这里，我们展示如何**将自己的模型** ( **BYOM** )用于部署。这意味着您有一个在 AWS SageMaker 之外训练的模型推理管道，现在只需要部署到 SageMaker 进行托管。按照下面的步骤准备和部署 DL 情感模型。需要一些先决条件:

*   您必须在本地环境中运行 Docker Desktop。
*   您必须拥有 AWS 帐户。你可以通过位于[https://aws.amazon.com/free/](https://aws.amazon.com/free/)的免费注册网站轻松创建一个免费的 AWS 账户。

一旦你有了这些需求，激活`dl-model-chapter08` conda 虚拟环境，按照几个步骤部署到 SageMaker。我们将这些步骤分为如下六个小节:

1.  建立本地 SageMaker Docker 映像
2.  在 SageMaker Docker 图像上添加额外的模型工件层
3.  使用新构建的 SageMaker Docker 映像测试本地部署
4.  将 SageMaker Docker 映像推送到 AWS 弹性容器注册表
5.  部署推理管道模型以创建 SageMaker 端点
6.  查询 SageMaker 端点以进行在线推断

让我们从构建本地 SageMaker Docker 映像的第一步开始。

## 步骤 1:构建本地 SageMaker Docker 映像

我们有意从本地构建开始，而不是推到 AWS，以便我们可以了解如何在这个基本映像之上添加额外的层，并在产生任何云成本之前在本地验证一切:

```
mlflow sagemaker build-and-push-container --build --no-push -c mlflow-dl-inference
```

您会看到许多屏幕输出，最后会显示如下内容:

```
#15 exporting to image
#15 sha256:e8c613e07b0b7ff33893b694f7759a10 d42e180f2b4dc349fb57dc6b71dcab00
#15 exporting layers
#15 exporting layers 8.7s done
#15 writing image sha256:95bc539b021179e5e87087 012353ebb43c71410be535ef368d1121b550c57bd4 done
#15 naming to docker.io/library/mlflow-dl-inference done
#15 DONE 8.7s
```

如果您看到图像名称`mlflow-dl-inference`，这意味着您已经成功地创建了一个 SageMaker 兼容的 MLflow-model-serving Docker 图像。您可以通过运行以下命令来验证这一点:

```
docker images | grep mlflow-dl-inference
```

您应该会看到如下所示的输出:

```
mlflow-dl-inference          latest                  95bc539b0211   6 minutes ago   2GB
```

## 第二步:在 SageMaker Docker 图像上添加额外的模型工件层

回想一下，我们的推理管道模型建立在一个微调的 DL 模型之上，我们通过 MLflow PythonModel API 的`load_context`函数([https://www . ml flow . org/docs/latest/python _ API/ml flow . py func . html # ml flow . py func . python model](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))加载模型，而不序列化微调的模型本身。这部分是因为 MLflow 无法使用 pickle 正确序列化 PyTorch 数据加载器([https://py torch . org/docs/stable/data . html # single-and-multi-process-data-loading](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)),因为在撰写本文时，数据加载器没有实现 pickle 序列化。这确实给了我们一个机会来学习当一些依赖项不能被正确序列化时，我们如何部署，尤其是在处理真实的 DL 模型时。

允许 Docker 容器访问 MLflow 跟踪服务器的两种方式

有两种方法允许像`mlflow-dl-inference`这样的 Docker 容器在运行时访问和加载一个微调过的模型。第一种方法是允许容器包含 MLflow 跟踪服务器 URL 和访问令牌。这可能会在企业环境中引起一些安全问题，因为 Docker 映像现在包含一些安全凭证。第二种方法是直接复制所有引用的工件来创建一个新的 Docker 映像，这个映像是自给自足的。在运行时，它不需要知道原始 MLflow 跟踪服务器的位置，因为它在本地拥有所有的模型工件。这种独立的方法消除了对安全漏洞的任何担忧。我们在本章中使用第二种方法进行部署。

在这一章中，我们将把引用的微调模型复制到一个新的 Docker 映像中，该映像建立在基本的`mlflow-dl-inference` Docker 映像之上。这将生成一个新的自包含 Docker 映像，而无需依赖任何外部 MLflow 跟踪服务器。为此，您需要从模型跟踪服务器下载微调的 DL 模型到您当前的本地文件夹，或者您可以使用本地文件系统作为 MLflow 跟踪服务器后端，在本地运行我们的 MLproject 的管道。按照`README.md`文件中的*部署到 AWS SageMaker* 部分来重现本地 MLflow 运行，以准备一个微调的模型和本地文件夹中的`inference-pipeline-model`。出于学习的目的，我们在 GitHub 存储库中的`chapter08`文件夹中提供了两个示例`mlruns`工件和`huggingface`缓存文件夹([https://GitHub . com/packt publishing/Practical-Deep-Learning-at-Scale-with-ml flow/tree/main/chapter 08](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter08))，这样我们就可以通过使用这些现有工件立即开始构建一个新的 Docker 映像。

要构建新的 Docker 映像，我们需要创建一个 Docker 文件，如下所示:

```
FROM mlflow-dl-inference
```

```
ADD mlruns/1/meta.yaml  /opt/mlflow/mlruns/1/meta.yaml
```

```
ADD mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/ /opt/mlflow/mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/
```

```
ADD tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/ /opt/mlflow/tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/
```

第一行意味着它从已有的`mlflow-dl-inference` Docker 镜像开始，后面三行`ADD`会将一个`meta.yaml`文件和两个文件夹复制到 Docker 镜像中相应的位置。注意，如果您已经按照`README`文件生成了自己的运行，那么您不需要添加第三行。注意，默认情况下，当 Docker 容器启动时，它会自动转到这个`/opt/mlflow/`工作目录，因此所有内容都需要复制到这个文件夹中，以便于访问。另外，请注意，`/opt/mlflow`目录需要超级用户权限，因此您需要准备好输入您本地机器的管理员密码(通常，在您自己的笔记本电脑上，这是您自己的密码)。

将私有构建的 Python 包复制到 Docker 映像中

也可以将私有构建的 Python 包复制到 Docker 映像中，这样我们就可以在`conda.yaml`文件中直接引用它们，而不用离开容器本身。例如，我们可以将一个私有的 Python 轮包`cool-dl-package-1.0.py3-none-any.whl`复制到`/usr/private-wheels/cool-dl-package/cool-dl-package-1.0-py3-none-any.whl` Docker 文件夹，然后我们可以在`conda.yaml`文件中指向这个路径。这允许 MLflow 模型工件成功加载这些本地可访问的 Python 包。在我们当前的例子中，我们不使用这种方法，因为我们没有使用任何私有的 Python 包。如果您有兴趣探索这一点，这对于将来的参考很有用。

现在，您可以运行以下命令在`chapter08`文件夹中构建一个新的 Docker 映像，如下所示:

```
docker build . -t mlflow-dl-inference-w-finetuned-model
```

这将在`mlflow-dl-inference`之上构建一个新的 Docker 映像`mlflow-dl-inference-w-finetuned-model`。您应该会看到下面的输出(为了简洁起见，只显示了第一行和最后一行):

```
[+] Building 0.2s (9/9) FINISHED
 => [internal] load build definition from Dockerfile                                                      0.0s
…………
=> => naming to docker.io/library/mlflow-dl-inference-w-finetuned-model
```

现在，您有了一个名为`mlflow-dl-inference-w-finetuned-model`的新 Docker 映像，它包含了经过微调的模型。现在，我们准备使用这个新的 Docker 映像部署我们的推理管道模型，它是 SageMaker 兼容的。

## 步骤 3:使用新构建的 SageMaker Docker 映像测试本地部署

在我们部署到云之前，让我们使用这个新的 SageMaker Docker 映像在本地测试部署。MLflow 提供了一种使用以下命令进行本地测试的便捷方法:

```
mlflow sagemaker run-local -m runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model -p 5555 -i mlflow-dl-inference-w-finetuned-model
```

该命令将开始在本地运行`mlflow-dl-inference-w-finetuned-model` Docker 容器，并将带有`dc5f670efa1a4eac95683633ffcfdd79`运行 ID 的推理管道模型部署到该容器中。

修复潜在的 Docker 错误

请注意，您可能会遇到一个 Docker 错误，说明**路径/opt/ml flow/ml runs/1/dc5f 670 EFA 1a 4 EAC 95683633 ffcfdd 79/artifacts/inference _ pipeline _ model 不是从主机共享的，Docker** 不知道它。您可以从 **Docker** | **首选项中配置共享路径...** | **资源** | **文件共享**修复此 Docker 错误。

我们已经在 GitHub 存储库中提供了这个推理管道模型，所以当您在本地环境中签出存储库时，这应该是现成的。web 服务的端口是`5555`。命令运行后，您将在屏幕上看到许多输出，最后，您应该会看到以下内容:

```
[2022-03-18 01:47:20 +0000] [552] [INFO] Starting gunicorn 20.1.0
[2022-03-18 01:47:20 +0000] [552] [INFO] Listening at: http://127.0.0.1:8000 (552)
[2022-03-18 01:47:20 +0000] [552] [INFO] Using worker: gevent
[2022-03-18 01:47:20 +0000] [565] [INFO] Booting worker with pid: 565
[2022-03-18 01:47:20 +0000] [566] [INFO] Booting worker with pid: 566
[2022-03-18 01:47:20 +0000] [567] [INFO] Booting worker with pid: 567
[2022-03-18 01:47:20 +0000] [568] [INFO] Booting worker with pid: 568
[2022-03-18 01:47:20 +0000] [569] [INFO] Booting worker with pid: 569
[2022-03-18 01:47:20 +0000] [570] [INFO] Booting worker with pid: 570
```

这意味着服务已经启动并正在运行。您可能会看到一些关于 PyTorch 版本不兼容的警告，但是可以安全地忽略它们。一旦该服务启动并运行，您就可以在不同的终端窗口中对其进行测试，方法是发出如下的`curl` web 请求，就像我们之前尝试的那样:

```
curl http://127.0.0.1:5555/invocations -H 'Content-Type: application/json' -d '{
    "columns": ["text"],
    "data": [["This is the best movie we saw."], ["What a movie!"]]
}'
```

注意，本地主机的端口号是`5555`。然后，您应该会看到如下响应:

```
[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]
```

您可能想知道这与上一节中推理模型的本地 web 服务有什么不同。不同的是，这一次，我们在本地使用 SageMaker 容器，而以前，它只是一个没有 Docker 容器的本地 web 服务。在本地测试 SageMaker 容器非常重要，这样您就不会浪费时间和金钱将失败的模型服务部署到云中。

接下来，我们准备将这个容器部署到 AWS SageMaker。

## 步骤 4:将 SageMaker Docker 映像推送到 AWS 弹性容器注册中心

现在，您可以使用下面的命令将新构建的`mlflow-dl-inference-w-finetuned-model` Docker 映像推送到 AWS **弹性容器注册表** ( **ECR** )。确保正确设置了 AWS 访问令牌和访问 ID(真实的，不是本地开发的)。获得访问密钥 ID 和令牌后，运行以下命令来设置对真实 AWS 的访问:

```
aws configure
```

执行命令后回答所有问题，您就可以开始了。现在，您可以运行以下命令将`mlflow-dl-inference-w-finetuned-model` Docker 映像推送到 AWS ECR:

```
mlflow sagemaker build-and-push-container --no-build --push -c mlflow-dl-inference-w-finetuned-model
```

确保不要用命令中包含的`--no-build`选项构建新的映像，因为我们只是想推送映像，而不是构建新的映像。您将看到下面的输出，它显示图像正被推送到 ECR。注意，在下面的输出中，AWS 帐户被`xxxxx`屏蔽。你会在输出中看到你的账号。确保您拥有写入 AWS ECR 存储的权限:

```
2022/03/18 17:36:05 INFO mlflow.sagemaker: Pushing image to ECR
2022/03/18 17:36:06 INFO mlflow.sagemaker: Pushing docker image mlflow-dl-inference-w-finetuned-model to xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1
Created new ECR repository: mlflow-dl-inference-w-finetuned-model
2022/03/18 17:36:06 INFO mlflow.sagemaker: Executing: aws ecr get-login-password | docker login  --username AWS  --password-stdin xxxxx.dkr.ecr.us-west-2.amazonaws.com;
docker tag mlflow-dl-inference-w-finetuned-model xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1;
docker push xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1
Login Succeeded
The push refers to repository [xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model]
447db5970ca5: Pushed
9d6787a516e7: Pushed
1.23.1: digest: sha256:f49f85741bc2b82388e85c79f6621f4 d7834e19bdf178b70c1a6c78c572b4d10 size: 3271
```

一旦完成，如果你去 AWS 网站(例如，如果你使用`us-west-2`地区数据中心，网址是[https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2](https://us-west-2.console.aws.amazon.com/ecr/repositories?region=us-west-2) ，您应该在 ECR 中找到您新推送的图像，文件夹名为`mlflow-dl-inference-w-finetuned-model`。然后你会在这个文件夹中找到如下的图像(*图 8.4* ):

![Figure 8.4 – AWS ECR repositories with mlflow-dl-inference-w-finetuned-model image tag 1.23.1
](img/B18120_08_04.jpg)

图 8.4–带有 ml flow-dl-inference-w-fine tuned-model 图像标签 1.23.1 的 AWS ECR 存储库

注意图像标签号`Copy URI`选项。它将如下所示(AWS 帐户用`xxxxx`屏蔽):

```
xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1
```

在下一步中，您将需要这个映像 URI 来部署到 SageMaker。现在让我们部署到 SageMaker 来创建一个推理端点。

## 步骤 5:部署推理管道模型以创建 SageMaker 端点

现在，是时候使用我们刚刚推送到 AWS ECR 注册中心的图像 URI 将推理管道模型部署到 SageMaker 了。我们已经将`sagemaker/deploy_to_sagemaker.py`代码包含在 GitHub 库的`chapter08`文件夹中。您需要为部署使用正确的 AWS 角色。您可以在您的帐户中创建一个新的`AWSSageMakerExecutionRole`角色，并为该角色分配两个权限策略`AmazonS3FullAccess`和`AmazonSageMakerFullAccess`。在现实世界的场景中，您可能希望将权限收紧到更严格的策略，但是出于学习的目的，这将很好。下图显示了创建角色后的屏幕:

![Figure 8.5 – Create a role that can be used for deployment in SageMaker
](img/B18120_08_05.jpg)

图 8.5–创建一个可用于 SageMaker 部署的角色

您还需要为 SageMaker 创建一个 S3 存储桶，以便上传模型工件并将它们部署到 SageMaker。在我们的例子中，我们创建了一个名为`dl-inference-deployment`的桶。当我们执行部署脚本时，如这里所示，要部署的模型将首先上传到`dl-inference-deployment` bucket，然后部署到 SageMaker。我们在`chapter08/sagemaker/deploy_to_sagemaker.py` GitHub 存储库中提供了完整的部署脚本，因此您可以下载并执行它，如下所示(提醒您，在运行该脚本之前，请确保将`MLFLOW_TRACKING_URI`的环境变量重置为空，如`export MLFLOW_TRACKING_URI=`所示):

```
sudo python sagemaker/deploy_to_sagemaker.py
```

该脚本执行以下两项任务:

1.  将`chapter08`文件夹下的本地`mlruns`复制到本地`/opt/mlflow`文件夹，以便 SageMaker 部署代码可以选择`inference-pipeline-model`进行上传。因为`/opt`路径通常是受限制的，这里我们使用`sudo`(超级用户)来做这个复制。这将提示您在笔记本电脑上键入您的用户密码。
2.  使用`mlflow.sagemaker.deploy` API 创建一个新的 SageMaker 端点`dl-sentiment-model`。

代码片段如下:

```
mlflow.sagemaker.deploy(
    mode='create',
    app_name=endpoint_name,
    model_uri=model_uri,
    image_url=image_uri,
    execution_role_arn=role,
    instance_type='ml.m5.xlarge',
    bucket = bucket_for_sagemaker_deployment,
    instance_count=1,
    region_name=region
)
```

这些参数需要一些解释，以便我们完全理解所需的所有准备工作:

*   这是推理管道模型的 URI。在我们的例子中，它是`runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model`。
*   这是我们上传到 AWS ECR 的 Docker 图像。在我们的例子中，它是`xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1`。请注意，您需要用您的实际账号替换屏蔽的 AWS 账号`xxxxx`。
*   `execution_role_arn`:这是我们创建的角色，允许 SageMaker 进行部署。在我们的例子中，它是`arn:aws:iam::565251169546:role/AWSSageMakerExecutionRole`。同样，您需要用您实际的 AWS 帐号替换`xxxxx`。
*   `bucket`:这是我们创建的 S3 桶，允许 SageMaker 上传模型，然后进行实际部署。在我们的例子中，它是`dl-inference-deployment`。

其余的参数不言自明。

执行部署脚本后，您将看到以下输出(其中`xxxxx`是屏蔽的 AWS 帐号):

```
2022/03/18 19:30:47 INFO mlflow.sagemaker: Using the python_function flavor for deployment!
2022/03/18 19:30:47 INFO mlflow.sagemaker: tag response: {'ResponseMetadata': {'RequestId': 'QMAQRCTJT36TXD2H', 'HostId': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'x-amz-request-id': 'QMAQRCTJT36TXD2H', 'date': 'Sat, 19 Mar 2022 02:30:48 GMT', 'server': 'AmazonS3', 'content-length': '0'}, 'RetryAttempts': 0}}
2022/03/18 19:30:47 INFO mlflow.sagemaker: Creating new endpoint with name: dl-sentiment-model ...
2022/03/18 19:30:47 INFO mlflow.sagemaker: Created model with arn: arn:aws:sagemaker:us-west-2:xxxxx:model/dl-sentiment-model-model-qbca2radrxitkujn3ezubq
2022/03/18 19:30:47 INFO mlflow.sagemaker: Created endpoint configuration with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint-config/dl-sentiment-model-config-r9ax3wlhrfisxkacyycj8a
2022/03/18 19:30:48 INFO mlflow.sagemaker: Created endpoint with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint/dl-sentiment-model
2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for the deployment operation to complete...
2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for endpoint to reach the "InService" state. Current endpoint status: "Creating"
```

这可能需要几分钟(有时超过 10 分钟)。您可能会看到一些关于 PyTorch 版本兼容性的警告消息，正如您在进行本地 SageMaker 部署测试时看到的。您也可以直接访问 SageMaker 网站，您将看到从**创建**开始的端点状态，然后最终变为绿色**运行中**状态，如下所示:

![Figure 8.6 – AWS SageMaker dl-sentiment-model endpoint InService
](img/B18120_08_06.jpg)

图 8.6–AWS sage maker dl-服务中的情绪模型端点

如果你看到**在役**状态，那么恭喜你！您已经成功地将 DL 推理管道模型部署到 SageMaker 中，现在您可以将它用于生产流量了！

现在服务的状态是 inService，您可以在下一步中使用命令行查询它。

## 步骤 6:查询 SageMaker 端点进行在线推理

要查询 SageMaker 端点，可以使用以下命令行:

```
aws sagemaker-runtime invoke-endpoint --endpoint-name 'dl-sentiment-model' --content-type 'application/json; format=pandas-split' --body '{"columns":["text"], "data": [["This is the best movie we saw."], ["What a movie!"]]}' response.json
```

您将看到如下输出:

```
{
    "ContentType": "application/json",
    "InvokedProductionVariant": "dl-sentiment-model-model-qbca2radrxitkujn3ezubq"
}
```

实际的预测结果存储在本地`response.json`文件中，可以通过运行以下命令显示响应的内容来查看:

```
cat response.json
```

这将显示如下内容:

```
[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]
```

这是我们的推理管道模型的预期响应模式！还可以使用 Python 代码对 SageMaker 推理端点运行查询，我们已经在 GitHub 存储库中的`chapter08/sagemaker/ query_sagemaker_endpoint.py`文件中提供了这些代码。核心代码片段使用`SageMakerRuntime`客户端的`invoke_endpoint`进行查询，如下所示:

```
client = boto3.client('sagemaker-runtime') 
```

```
response = client.invoke_endpoint(
```

```
    EndpointName=app_name, 
```

```
    ContentType=content_type,
```

```
    Accept=accept,
```

```
    Body=payload
```

```
    )
```

`invoke_endpoint`的参数需要一些解释:

*   `EndpointName`:这是推理端点名。在我们的例子中，它是`dl-inference-model`。
*   `ContentType`:这是请求体中输入数据的 MIME 类型。在我们的例子中，我们使用`application/json; format=pandas-split`。
*   `Accept`:这是响应体中推理所需的 MIME 类型。在我们的示例中，我们期望`text/plain`字符串类型。
*   `Body`:这是我们想要使用 DL 模型推理服务来预测情绪的实际文本。在我们的例子中是`{"columns": ["text"],"data": [["This is the best movie we saw."], ["What a movie!"]]}`。

GitHub 存储库中提供了完整的代码，您可以在命令行中运行它，如下所示:

```
python sagemaker/query_sagemaker_endpoint.py
```

您将在终端屏幕上看到以下输出:

```
Application status is: InService
[{"text": "{\"response\": {\"prediction_label\": [\"positive\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}, {"text": "{\"response\": {\"prediction_label\": [\"negative\"]}, \"metadata\": {\"language_detected\": \"en\"}, \"model_metadata\": {\"finetuned_model_uri\": \"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\", \"inference_pipeline_model_uri\": \"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\"}}"}]
```

这就是我们对推理管道模型响应的预期！如果您已经阅读了本章，那么恭喜您成功地将我们的推理管道模型部署到远程云主机 AWS SageMaker 的生产环境中！完成本章中的课程后，请确保删除端点，以免产生不必要的成本。

让我们总结一下本章所学的内容。

# 总结

在这一章中，我们学习了为批量推理和在线实时推理部署 MLflow 推理管道模型的不同方法。我们首先对不同的模型服务场景(批处理、流和设备上)进行了简要的调查，并查看了三种不同类别的 MLflow 模型部署工具(MLflow 内置部署工具、MLflow 部署插件和可以与 MLflow 推理模型一起工作的通用模型推理服务框架)。然后，我们讨论了几个本地部署场景，使用 PySpark UDF 函数进行批量推理，并为 web 服务进行 MLflow 本地部署。之后，我们学习了如何结合使用 Ray Serve 和`mlflow-ray-serve`插件来将 MLflow Python 推理管道模型部署到本地 Ray 集群中。这为部署到任何云平台(如 AWS、Azure ML 或 GCP)打开了大门，只要我们可以在云中建立一个 Ray 集群。最后，我们提供了关于如何部署到 AWS SageMaker 的完整的端到端指南，重点关注 BYOM 的一个常见场景，其中我们有一个在 AWS SageMaker 外部构建的训练有素的推理管道模型，现在需要部署到 AWS SageMaker 以提供托管模型服务。我们的分步指南将为您提供信心，为实际生产应用部署 MLflow 推理管道模型。

请注意，部署 DL 推理管道模型的前景仍在发展，我们刚刚学习了一些基础技能。鼓励您从*延伸阅读*部分探索更多高级主题。

现在我们知道了如何部署和托管 DL 推理管道，我们将在下一章学习如何进行模型可解释性，这对于许多真实场景中可信和可解释的模型预测结果非常重要。

# 延伸阅读

*   *tiny ml 介绍*:[https://towards data science . com/An-Introduction-to-tiny ml-4617 f 314 aa 79](https://towardsdatascience.com/an-introduction-to-tinyml-4617f314aa79)
*   *性能优化和 MLFlow 集成–Seldon Core 1 . 10 . 0 发布*:[https://www . Seldon . io/Performance-Optimizations-and-ml flow-Integrations-Seldon-Core-1-10-0-Released/](https://www.seldon.io/performance-optimizations-and-mlflow-integrations-seldon-core-1-10-0-released/)
*   *Ray & MLflow:将分布式机器学习应用用于生产*:[https://medium . com/Distributed-computing-with Ray/Ray-ml flow-Taking-Distributed-Machine-Learning-Applications-to-Production-103 f 5505 cb88](https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88)
*   *用 MLflow 和 Amazon SageMaker 管理你的机器学习生命周期*:[https://AWS . Amazon . com/blogs/machine-learning/Managing-your-machine-learning-life cycle-with-ml flow-and-Amazon-sage maker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)
*   *使用 AWS sage maker*:[https://medium.com/geekculture/84af8989d065](https://medium.com/geekculture/84af8989d065)在云中部署本地培训的 ML 模型
*   *2022 年 py torch vs tensor flow*:[https://www . assembly ai . com/blog/py torch-vs-tensor flow-in-2022/](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)
*   *试用 Databricks:免费试用版或社区版*:[https://docs . data bricks . com/getting-started/Try-data bricks . html #免费试用版或社区版](https://docs.databricks.com/getting-started/try-databricks.html#free-trial-or-community-edition)
*   *带有 MLflow 和 Amazon SageMaker 管道的 MLOps*:[https://towardsdatascience . com/MLOps-with-ml flow-and-Amazon-SageMaker-Pipelines-33e 13d 43 f 238](https://towardsdatascience.com/mlops-with-mlflow-and-amazon-sagemaker-pipelines-33e13d43f238)
*   *PyTorch JIT 和 torch script*:[https://towardsdatascience . com/py torch-JIT-and-torch script-c 2 a 77 BAC 0 fff](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)
*   *ML 模特上菜最佳工具*:[https://neptune.ai/blog/ml-model-serving-best-tools](https://neptune.ai/blog/ml-model-serving-best-tools)
*   *将机器学习模型部署到生产-推理服务架构模式*:[https://medium . com/data-for-ai/Deploying-Machine-Learning-models-to-production-Inference-service-architecture-patterns-BC 8051 f 70080](https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080)
*   *如何将大规模深度学习模型部署到生产中*:[https://towards data science . com/How-to-Deploy-Large-Size-Deep-Learning-Models-into-Production-66b 851d 17 f 33](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)
*   *在 Kubernetes 上使用 Mlflow 大规模提供 ML 模型*:[https://medium . com/artefact-engineering-and-data-science/Serving-ML-models-at-scale-using-ML flow-on-Kubernetes-7a 85 c 28d 38 e](https://medium.com/artefact-engineering-and-data-science/serving-ml-models-at-scale-using-mlflow-on-kubernetes-7a85c28d38e)
*   *当 PyTorch 遇到 ml flow*:[https://ml ops . community/When-py torch-meets-ml flow/](https://mlops.community/when-pytorch-meets-mlflow/)
*   *将模型部署到 Azure Kubernetes 服务集群*:[https://docs . Microsoft . com/en-us/Azure/machine-learning/how-to-Deploy-Azure-Kubernetes-Service？tabs=python](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python)
*   *ONNX 和 Azure 机器学习:创建和加速 ML 模型*:[https://docs . Microsoft . com/en-us/Azure/Machine-Learning/concept-ONNX](https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx)