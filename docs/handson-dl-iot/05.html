<html><head/><body>

    

        <title>Image Recognition in IoT</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">物联网图像识别</h1>

                

            

            

                

<p>许多物联网应用，包括智能家居、智能城市和智能医疗，未来将广泛使用基于图像识别的决策(如智能门或锁的面部识别)。<strong>机器学习</strong> ( <strong> ML </strong>)和<strong>深度学习</strong> ( <strong> DL </strong>)算法对图像识别和决策有用。因此，它们在物联网应用中非常有前途。本章将介绍物联网应用中基于DL的图像数据处理实践。</p>

<p>本章的第一部分将简要描述不同的物联网应用及其基于图像检测的决策。此外，它还将简要讨论一个物联网应用及其在真实场景中基于图像检测的实现。在本章的第二部分，我们将介绍一个使用DL算法的应用程序的实际图像检测实现。在本章中，我们将讨论以下主题:</p>

<ul>

<li>物联网应用和图像识别</li>

<li>用例一:基于图像的道路故障检测</li>

<li>用例二:基于图像的智能固体废物分离</li>

<li>实现用例</li>

<li>物联网中图像识别的迁移学习</li>

<li>细胞神经网络在物联网图像识别中的应用</li>

<li>收集数据</li>

<li>数据预处理</li>

<li>模特培训</li>

<li>评估模型</li>

</ul>

<p class="mce-root"/>





            



            

        

    






    

        <title>IoT applications and image recognition</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">物联网应用和图像识别</h1>

                

            

            

                

<p>物联网应用中的图像识别前景正在快速变化。移动处理能力、边缘计算和机器学习的显著进步为图像识别在许多物联网应用中的广泛使用铺平了道路。例如，无处不在的移动设备(这是许多物联网应用的关键组件)配备了高分辨率摄像头，便于任何人在任何地方生成图像和视频。</p>

<p>此外，智能摄像机，如IP摄像机和带摄像头的Raspberry Pis，用于许多地方，如智能家居、校园和工厂，用于不同的应用。许多物联网应用——包括智能城市、智能家庭、智能健康、智能教育、智能工厂和智能农业——使用图像识别/分类做出决策。如下图所示，这些应用程序使用以下一种或多种图像识别服务:</p>

<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1085 image-border" src="img/c5027b9b-10a4-4fb5-9d03-75004e180697.png" style="width:50.33em;height:20.67em;"/></p>

<p class="mce-root"/>

<p>让我们详细讨论一下前面的图像:</p>

<ul>

<li><strong>人员识别</strong>:一般来说，安全友好地进入家庭、办公室和任何其他场所都是一项具有挑战性的任务。使用智能设备，包括物联网解决方案，可以安全友好地访问许多场所。让我们考虑一下办公室或家庭访问的例子。我们使用一把或多把钥匙进入我们的家或办公室。如果我们丢失了这些钥匙，这不仅会给我们带来不便，而且如果别人发现了它们，还会危及我们的安全。在这种情况下，基于图像识别的人员识别可以用作智能家居或办公室的无钥匙访问方法。</li>

<li><strong>物体识别</strong>:基于物联网的自动化物体识别在许多领域都非常受欢迎，包括无人驾驶汽车、智能城市和智能工厂。例如，智能城市应用，如智能车辆牌照识别和车辆检测，以及城市范围内的公共资产监控，可以使用基于图像识别的对象检测服务。同样，智能工厂可以使用对象检测服务进行库存管理。</li>

<li><strong>面部识别</strong>:基于图像处理的面部检测和识别领域变化如此之快，它很快就会成为一种商品。届时，具有生物识别功能的智能手机将成为常态。智能手机和基于物联网的面部识别可用于许多应用，如安全和安保以及智能教育。例如，在智能课堂(教育)中，可以使用人脸识别系统来识别对讲座的响应。</li>

<li><strong>事件检测</strong>:很多人类疾病(如手足口)、动物疾病(如口蹄疫、家禽疾病)、植物疾病的症状都是显而易见的。这些疾病可以使用集成了基于DL的图像分类的物联网解决方案进行数字化检测。</li>

</ul>





            



            

        

    






    

        <title>Use case one – image-based automated fault detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">用例一——基于图像的自动化故障检测</h1>

                

            

            

                

<p>城市中的公共资产(如道路、公共建筑和旅游场所)是异构的，分布在城市内部。世界上大多数城市都面临着监控、故障检测和报告这些资产的挑战。例如，在许多英国城市，市民经常报告故障，但在许多情况下，报告的准确性和效率是一个问题。在智能城市中，可以监控这些资产，并通过物联网应用程序检测和报告它们的故障。例如，附着有一个或多个传感器(例如照相机或麦克风)的车辆(例如市政车辆)可以用于道路故障监控和检测。</p>

<p class="mce-root">道路是一个城市的重要资产，它有许多缺点。坑洞、颠簸和道路不平是通勤者和车辆经历的最令人沮丧的危险和异常情况。重要的是，车辆可能经常面临悬挂问题、转向不准和爆胎，这也可能导致事故。与道路故障相关的损失是巨大的。例如，仅与路面坑洼相关的损失就让英国司机每年损失17亿英镑。支持适当DL算法的物联网应用可用于自动检测这些故障，并适当报告它们。这以经济高效的方式减少了与道路故障相关的损坏数量。</p>

<p>实施用例一</p>





            



            

        

    






    

        <title>Implementing use case one</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">如下图所示，用例的实现由三个主要元素组成:</h1>

                

            

            

                

<p><img src="img/c7204812-461d-4169-8ae5-af23821b9b21.png" style="width:35.75em;height:29.08em;"/></p>

<p class="CDPAlignCenter CDPAlign">让我们详细了解一下组件:</p>

<p><strong>传感器和数据采集</strong> : <strong> </strong>数据采集传感器的选择取决于资产和故障类型。如果我们使用智能手机作为边缘计算设备，它的摄像头可以用于感知和收集道路故障的数据。相反，如果我们使用Raspberry Pi作为边缘计算设备，我们需要使用外部摄像头，因为Raspberry Pi中没有内置摄像头。上图显示了用于用例实现的Raspberry Pi和camera。我们使用了一个带有1 GB RAM的Raspberry Pi 3型号B+和一个带有Omnivision OV5647传感器的500万像素传感器，该传感器位于固定焦距镜头中。相机的采样或拍摄速率将取决于车辆的速度和道路故障的可用性。例如，如果智能手机摄像头或安装在车辆上的摄像头可以每秒捕捉一张照片，那么如果车辆的速度为40公里/小时或更低，手机或Raspberry Pi将能够在两秒内检测到故障。一旦图像被感测和捕获，它将被发送到检测方法。</p>

<ul>

<li><strong>故障检测和报告</strong> : <strong> </strong>在此阶段，边缘计算设备将安装一个app。智能手机或Raspberry Pi中安装的应用程序将加载预先训练的故障检测和分类模型。一旦车辆的智能手机或Raspberry Pi摄像头拍摄到照片(按照采样率)，这些模型将检测和分类潜在的故障，并向应用服务器(地方议会)报告。</li>

<li><strong>委员会的服务器和故障检测模型</strong> : <strong> </strong>委员会的服务器负责以下内容:</li>

<li>使用参考数据集学习故障检测和分类模型<ul>

<li>传播和更新边缘计算设备的模型</li>

<li>接收并存储故障数据</li>

<li>基于图像的模型学习和道路故障检测的验证是实现的核心。本章的第二部分(在从<em>开始的节中讨论)将描述先前用例的基于DL的异常检测的实现。所有必要的代码都可以在本章的代码文件夹中找到。</em></li>

</ul>

</li>

</ul>

<div><p>用例二——基于图像的智能固体废物分离</p>

</div>





            



            

        

    






    

        <title>Use case two – image-based smart solid waste separation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">固体废物是一个全球性的挑战。固体废物的管理成本高昂，不当的废物管理也严重影响着全球经济、公共卫生和环境。一般来说，塑料、玻璃瓶和纸张等固体废物是可回收的，它们需要一种有效的回收方法来实现经济和环境效益。然而，在大多数国家，现有的回收过程是手工完成的。此外，市民或消费者经常对回收方法感到困惑。</h1>

                

            

            

                

<p>在这种背景下，物联网在机器学习和深度学习，特别是基于图像的物体识别的支持下，可以识别废物的类型，并帮助进行相应的分类，而无需任何人工干预。</p>

<p>实施用例二</p>





            



            

        

    






    

        <title>Implementing use case two </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">基于图像的智能固体废物分类的实施包括两个关键部分:</h1>

                

            

            

                

<p>带有独立腔室的垃圾箱，每个腔室带有可控制的盖子，用于存放各种类型的固体废物</p>

<ul>

<li>具有用于图像识别的DL模型的物联网基础设施</li>

<li>该实现的第一个组件不在本书的讨论范围内，我们认为该组件可用于该实现。如下图所示，该用例的物联网实施包括两个主要元素:</li>

</ul>

<p><img src="img/58f00fef-94d2-4c5c-beff-e05f95948f48.png" style="width:34.25em;height:15.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><strong>传感器和数据采集</strong>:数据采集传感器的选择取决于固体废物的种类及其特性。例如，许多玻璃和塑料瓶在颜色和外观上非常相似。然而，它们的重量通常明显不同。对于用例，我们考虑两个传感器:</p>

<ul>

<li>一个或多个摄像机，用于在垃圾通过入口点进入垃圾箱时捕捉垃圾的图像<ul>

<li>获取垃圾重量的重量传感器</li>

<li>我们使用Raspberry Pi作为计算平台。该用例使用具有1 GB RAM的Raspberry Pi 3型号B+和具有Omnivision OV5647传感器的500万像素传感器在定焦镜头中进行测试。一旦图像和重量被感测和捕获，它们就被发送到分拣方法。</li>

</ul>

</li>

</ul>

<p style="padding-left: 60px"><strong>垃圾检测和分类</strong>:这是实施的关键要素。Raspberry Pi将使用DL加载一个预训练的垃圾检测和分类模型。一旦检测算法检测到垃圾并对其进行分类，它将启动控制系统打开适当的盖子并将其移入垃圾箱。</p>

<ul>

<li>用例场景关注城市公共区域的废物管理，包括公园、旅游景点、景观和其他娱乐区域。通常，这些地区的市民和/或游客会自行处理他们的垃圾。重要的是，他们处理的物品数量很少，从一件到几件都有。</li>

</ul>

<p>以下所有部分将描述上述用例所需的基于DL的图像识别的实现。所有必要的代码都可以在本章的代码文件夹中找到。</p>

<p>物联网中图像识别的迁移学习</p>





            



            

        

    






    

        <title>Transfer learning for image recognition in IoT</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">一般来说，迁移学习是指将预先训练好的机器学习模型表示转移到另一个问题上。近年来，这正在成为将DL模型应用于问题的流行手段，特别是在图像处理和识别中，因为它能够用相对较少的数据训练DL模型。</h1>

                

            

            

                

<p>下图显示了两种模型:</p>

<p>标准DL模型的体系结构(a)</p>

<ul>

<li>迁移学习DL模型的体系结构(b):</li>

<li><img src="img/adc41d97-4ee5-461a-8bca-ef6bbabcea35.png"/></li>

</ul>

<p class="CDPAlignCenter CDPAlign">如标准DL模型的架构图所示，一个经过充分训练的神经网络在初始层获取输入值，然后通过必要的转换将该信息顺序向前馈送，直到倒数第二层(也称为<strong>瓶颈层</strong>)构建了一个更容易转换为最终输出的输入的高级表示。模型的完整训练包括在每个连接中使用的权重和偏差项的优化(用蓝色标记)。在大型异构数据集中，这些权重和偏差项的数量可能高达数百万。</p>

<p>在迁移学习中，我们可以使用早期和中间层，只对后面的层进行重新训练。一种流行的转移学习方法是对除最后一层之外的整个网络重新使用预训练的权重，并通过使用新数据集重新训练网络来重新学习最后一层或分类部分的权重。如迁移学习DL模型的架构图所示，我们重新使用橙色连接，并使用新数据集重新训练网络，以学习最后一层的绿色连接。</p>

<p>In transfer learning, we can use the early and middle layers and only re-train the latter layers. One popular approach to transfer learning is to reuse the pre-trained weights for the whole network other than the last layer and relearn the weights of the last layer or classification part by retraining the network using the new dataset. As shown in the diagram of an architecture for a transfer-learning DL model, we reused the orange connections and retrained the network using the new dataset to learn the last layer’s green connections.</p>

<p class="mce-root"/>

<p class="mce-root">许多预先训练的DL模型，包括Inception-v3和MobileNets模型，可用于迁移学习。为ImageNet <em>大型视觉识别挑战赛</em>训练的Inception-v3模型将图像分为1000类，如<em>斑马</em>、<em>大麦町</em>和<em>洗碗机</em>。Inception-v3由两部分组成:</p>

<p>具有卷积神经网络的特征提取部分，其从输入中提取特征</p>

<ul>

<li>具有全连接图层和softmax图层的分类部分，根据第一部分中确定的要素对输入数据进行分类</li>

<li>如果我们想要使用Inception-v3，我们可以重用特征提取部分，并用我们的数据集重新训练分类部分。</li>

</ul>

<p>迁移学习有两个好处:</p>

<p>对新数据的训练更快。</p>

<ul>

<li>用较少的训练数据解决问题的能力，而不是从零开始学习。</li>

<li>迁移学习的这些特征对于在物联网的资源受限的边缘设备中实现DL模型特别有用，因为我们不需要训练资源饥渴的特征提取部分。因此，可以使用较少的计算资源和时间来训练该模型。</li>

</ul>

<p>细胞神经网络在物联网图像识别中的应用</p>





            



            

        

    






    

        <title>CNNs for image recognition in IoT applications</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">一个<strong>卷积神经网络</strong> ( <strong> CNN </strong>)有不同的实现。<strong> AlexNet </strong>就是这样一个实现，它赢得了ImageNet挑战赛:ILSVRC 2012。从那以后，CNN在计算机视觉和图像检测和分类中变得无处不在。直到2017年4月，总的趋势是制作更深更复杂的网络，以实现更高的精度。然而，这些更深更复杂的网络提高了准确性，但并不总是使网络更有效，特别是在规模和速度方面。在许多现实世界的应用中，特别是在物联网应用中，例如自动驾驶汽车和病人监护，识别任务需要在资源受限(处理、存储)的平台上及时完成。</h1>

                

            

            

                

<p>A <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) has different implementations. <strong>AlexNet</strong> is one such implementation, and it won the ImageNet Challenge: ILSVRC 2012. Since then, CNNs have become omnipresent in computer vision and image detection and classification. Until April 2017, the general trend was to make deeper and more complicated networks to achieve higher accuracy. However, these deeper and complex networks offered improved accuracy but did not always make the networks more efficient, particularly in terms of size and speed. In many real-world applications, especially in IoT applications, such as a self-driving car and patient monitoring, recognition tasks need to be accomplished in a timely fashion on a resource-constrained (processing, memory) platform.</p>

<p class="mce-root">在此背景下，2017年4月推出了MobileNet V1。这个版本的Mobilenet是对其2018年4月的第二个版本(MobileNetV2)的改进。<strong> Mobilenets </strong>及其变种是高效的CNN DL模型的物联网应用，尤其是基于图像识别的物联网应用。在下面的段落中，我们将简要介绍MobileNets。</p>

<p>MobileNets是最流行和最广泛使用的DL模型的实现，即CNN。它们是专门为资源受限的移动设备设计的，以支持分类、检测和预测。安装了DL机型的个人移动设备(包括智能手机、可穿戴设备和智能手表)改善了用户体验，提供了随时随地的访问，以及安全性、隐私和能耗方面的额外优势。重要的是，移动设备中新出现的应用将需要更高效的神经网络来与现实世界实时交互。</p>

<p>下图显示了标准卷积滤波器(图a)如何被Mobilenet V1中的两层所取代。它使用深度方向卷积(图b)和点方向卷积(图c)来构建深度方向可分离滤波器:</p>

<p><img src="img/2294acd6-1260-41c1-80fc-d142b3d6cb0a.jpg"/></p>

<p class="CDPAlignCenter CDPAlign">MobileNet V1的主要动机是卷积层的计算成本很高，它们可以被所谓的<strong>深度方向可分离卷积</strong>所取代。在MobileNet V1中，深度方向卷积过程对每个输入通道使用单个滤波器，然后点方向卷积对早期深度方向卷积的输出使用1 x 1卷积过程。如标准卷积滤波器图所示，标准卷积在一个步骤中将输入滤波并组合成一组新的输出。与标准CNN不同，MobileNets中的深度方向可分离卷积(因式分解)将其分为两层(如Mobilenet V1图所示):一层用于过滤，另一层用于合并。</p>

<p class="mce-root">下图显示了V1 Mobilenet的分解架构。这种因子分解大大减少了计算量和模型大小，因为模型需要计算的参数数量要少得多。例如，V1移动网络需要计算420万个参数，而全卷积网络需要计算2930万个参数:</p>

<p class="mce-root">V2移动网是V1移动网的更新和显著改进版本。它极大地改进和推动了现有的移动视觉识别，包括分类、检测和语义分割。像MobileNet V1一样，MobileNet V2是作为TensorFlow-Slim图像分类库的一部分发布的。如果需要的话，你可以在谷歌的合作实验室中探索这一点。此外，移动网络V2在TF-Hub上作为模块提供，预训练的检查点或保存的模型可以在<a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank">https://github . com/tensor flow/models/tree/master/research/slim/nets/MobileNet</a><a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank">找到，可以作为迁移学习使用。</a></p>

<p class="mce-root"/>

<p class="mce-root"><a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank"/></p>

<p class="mce-root">下图显示了MobileNet V2的简单架构。V2移动网络是V1移动网络的延伸。它使用深度方向可分离卷积作为有效的构建模块。此外，MobileNet V2在架构中包括两个新功能。一个是各层之间的线性瓶颈，另一个是瓶颈之间的快捷连接:</p>

<p class="mceNonEditable"><img src="img/b80688e0-0d3f-465f-b2ff-976064b84f48.png" style="width:47.25em;height:25.33em;"/></p>

<p>为用例一收集数据</p>

<p>我们可以使用智能手机摄像头或Raspberry Pi摄像头收集数据，并自己准备数据集，或者从互联网上下载现有图像(即通过Google、Bing等)并准备数据集。或者，我们可以使用现有的开源数据集。对于用例一，我们使用了两者的组合。我们已经从下载了一个关于坑洞图像(最常见的道路故障之一)的现有数据集，并用来自Google images的更多图像更新了该数据集。用于坑洞识别的开源数据集(<kbd>PotDataset</kbd>)由英国克兰菲尔德大学发布。数据集包括坑洞对象和非坑洞对象的图像，包括检修孔、人行道、道路标记和阴影。这些图像被手动注释并组织到以下文件夹中:</p>

<div><img src="img/6aac716a-4376-421f-b2ee-b1bf9f081ed6.png"/></div>

<p>人孔</p>

<p class="mce-root">人行道</p>

<p>坑洞</p>

<p>道路标记</p>

<p class="CDPAlignCenter CDPAlign">阴影</p>





            



            

        

    






    

        <title>Collecting data for use case one</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">探索用例一中的数据集</h1>

                

            

            

                

<p>在对数据应用DL算法之前，研究数据集是非常重要的。为了进行探索，我们可以对数据集运行<kbd>image_explorer.py</kbd>,如下所示:</p>

<ul>

<li>下图显示了数据浏览过程的快照:</li>

<li><img src="img/07f715ce-8c41-4106-95fe-475b550a1361.jpg" style="width:49.67em;height:39.08em;"/></li>

<li>如数据探索图所示，如果我们只使用智能手机摄像头，坑洞和非坑洞物体之间的差异并不总是很明显。红外和智能手机摄像头的结合可以改善这种情况。此外，我们发现我们在此使用的坑洞图像可能不足以覆盖大范围的坑洞，例如:</li>

<li>所用数据集中的许多图像显示坑洼已经得到维护/修复。</li>

<li>在使用的数据集中有一些大型坑洞的图像。</li>

</ul>





            



            

        

    






    

        <title>Exploring the dataset from use case one</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">在这种情况下，我们决定通过从互联网上收集更多的图像来更新坑洞图像数据集。接下来，我们简要讨论一下数据收集过程:</h1>

                

            

            

                

<p><strong>搜索</strong> : <strong> </strong>使用任意浏览器(我们用的是Chrome)，上谷歌，在谷歌图片中搜索<em>坑爹图片</em>。您的搜索窗口将如下图所示:</p>

<pre>python image_explorer.py datset_original</pre>

<p class="CDPAlignLeft CDPAlign">您可以通过点击<em>工具</em>选择无版权图像，并将使用权更改为<em>，标记为修改后重复使用。</em></p>

<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-753 image-border" src="img/e95a92bb-cdea-48e0-a81d-acbe3b9b806b.png" style="width:55.08em;height:37.00em;"/></p>

<p><strong>收集图片URL</strong>:这一步使用几行JavaScript代码来收集图片URL。可以在Python中使用收集的URL来下载图像。如以下截图所示，通过点击<strong>查看</strong> | <strong>开发者</strong> | <strong> JavaScript控制台</strong>(在macOS中)选择JavaScript控制台(假设你使用Chrome web浏览器，但也可以使用Firefox)，并自定义和控制<strong> Google Chrome </strong> | <strong>更多工具</strong> | <strong>开发者工具</strong>(在Windows OS中):</p>

<ul>

<li><img class="aligncenter size-full wp-image-754 image-border" src="img/8dc8d211-0d45-458c-a81c-fa9da19a824d.png" style="width:56.42em;height:39.58em;"/></li>

<li>There are a few images of a large-sized pothole in the used dataset.</li>

</ul>

<p>In this context, we decided to update the pothole images dataset by collecting more images from the internet. Next, we briefly discuss the data collection process:</p>

<ol>

<li>选择JavaScript控制台后，您将看到一个浏览器窗口，如下图所示，这将使您能够以类似REPL的方式执行JavaScript:</li>

</ol>

<p>现在按顺序执行以下操作:</p>

<p class="CDPAlignCenter CDPAlign">向下滚动页面，直到找到所有对数据集有用的图像(注意:请使用不受版权保护的图像)。之后，您需要收集所选图像的URL。</p>

<ol start="2">

<li class="CDPAlignLeft CDPAlign">现在转到JavaScript控制台，然后将以下JavaScript代码复制并粘贴到控制台中:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-754 image-border" src="img/8dc8d211-0d45-458c-a81c-fa9da19a824d.png" style="width:56.42em;height:39.58em;"/></p>

<p class="mce-root">前面的代码行将提取jQuery JavaScript库。现在，您可以使用CSS选择器通过以下代码行收集URL列表:</p>

<p class="mce-root">最后，使用以下代码行将URL写入文件(每行一个):</p>

<p style="padding-left: 60px">一旦执行了前面几行代码，在默认下载目录中就会有一个名为<kbd>imageurls.txt</kbd>的文件。如果你想把它们下载到一个特定的文件夹中，那么在前面的代码中写下<kbd>hiddenComponents.download = 'your fooler/imageurls.txt</kbd>而不是<kbd>hiddenComponents.download = 'imageurls.txt'</kbd>。</p>

<div><img class="aligncenter size-full wp-image-755 image-border" src="img/8dd135fa-09e2-46a8-9a84-986e8a490d14.png" style="width:57.42em;height:38.58em;"/></div>

<ol start="3">

<li><strong>下载图片</strong>:现在你已经可以下载之前下载的<kbd>imageurls.txt</kbd>中的运行图片<kbd>download_images.py</kbd>(在本章的代码文件夹中)；<ul>

<li><strong>浏览</strong>:一旦我们下载了图片，我们需要浏览它们以便删除不相关的图片。我们可以通过一些手工检查来做到这一点。之后，我们需要调整它们的大小并将其转换为灰度图像，以匹配之前下载的数据集:</li>

<li><img src="img/46a249b1-8e27-4169-a8c4-50d62acc6e8d.png" style="width:19.75em;height:25.75em;"/></li>

</ul>

</li>

</ol>

<pre style="padding-left: 120px">// Get the jquery into the JavaScript console<br/>var scriptJs = document.createElement('scriptJs');<br/>scriptJs.src = "https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js";<br/>document.getElementsByTagName('head')[0].appendChild(scriptJs)</pre>

<ol start="4">

<li style="list-style-type: none">前面的屏幕截图显示了坑洞和非坑洞图像数据集的文件夹结构。<ul>

<li>The preceding line of code will pull the jQuery JavaScript library.  Now you can use a CSS selector to collect a list of URLs using the following lines of code:</li>

</ul>

</li>

</ol>

<pre style="padding-left: 120px">// Collect the selected URLs<br/>var urls_images = $('.rg_di .rg_meta').map(function() { return JSON.parse($(this).text()).ou; });</pre>

<ol start="4">

<li>为用例二收集数据</li>

</ol>

<pre style="padding-left: 60px">// write the URls to a file <br/>var text_url_Save = urls_images.toArray().join('\n');<br/>var hiddenComponents = document.createElement('a');<br/>hiddenComponents.href = 'data:attachment/text,' + encodeURI(text_url_Save);<br/>hiddenComponents.target = '_blank';<br/>hiddenComponents.download = 'imageurls.txt';<br/>hiddenComponents.click();</pre>

<p style="padding-left: 60px">与用例一的情况一样，我们可以通过数码相机或使用现有的开源软件或两者的结合来收集数据。我们使用现有的开源数据集来实现排序算法。数据集是从美国的城市环境中收集的。由于固体废物类型可能因国家而异，因此最好根据用例将用于的国家来更新数据集。数据集由六种固体废物组成:玻璃、纸张、纸板、塑料、金属和垃圾。该数据集由2，527幅图像组成，它们被注释并组织到以下文件夹中，如以下屏幕截图所示:</p>

<ol start="5">

<li><img src="img/688c0162-ec3e-4ecb-97e1-60c59f450432.png" style="width:23.00em;height:28.83em;"/></li>

</ol>

<pre style="padding-left: 60px">python download_images.py  imageurls.txt</pre>

<ol start="6">

<li>用例二的数据探索</li>

</ol>

<p class="CDPAlignCenter CDPAlign">下面展示了用例二的数据探索的快照。正如我们所见，玻璃和塑料图像可能会使分类算法变得混乱。在这种情况下，重量传感器数据有助于解决这一问题:</p>

<p><img src="img/1efe9aad-74f3-4cfe-879a-da94f0bf12da.png"/></p>

<p class="mce-root"/>





            



            

        

    






    

        <title>Collecting data for use case two </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">Collecting data for use case two </h1>

                

            

            

                

<p>As is the case with use case one, we can collect data through digital cameras or use an existing open source or a combination of both. We are using an existing and open source dataset for the implementation of the sorting algorithm. The dataset was collected from urban environments of the USA . As solid waste types may vary by country, it is better to update the dataset based on the country the use case will be used for. The dataset consists of six types of solid wastes: glass, paper, cardboard, plastic, metal, and trash. The dataset consists of 2,527 images, and they were annotated and organized into the following folders, as shown in the  following screenshot:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/688c0162-ec3e-4ecb-97e1-60c59f450432.png" style="width:23.00em;height:28.83em;"/></p>





            



            

        

    






    

        <title>Data exploration of use case two </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">数据预处理</h1>

                

            

            

                

<p>这是DL管道的一个重要步骤。用例中使用的坑洞图像和固体废物图像的现有数据集经过预处理，可用于训练、验证和测试。如下图所示，原始图像和修改后的图像(为pothole类下载的附加图像)都被组织为子文件夹，每个子文件夹都以五个类别中的一个命名，并且只包含该类别中的图像。在准备训练图像集期间，有几个问题需要注意:</p>

<p class="CDPAlignCenter CDPAlign"><strong>数据量</strong> : <em> </em>我们需要为每个类收集至少一百张图像来训练一个工作良好的模型。我们收集的信息越多，训练模型的准确性就可能越高。所用数据集中的五个类别都有超过1，000个样本图像。我们还确保这些图像很好地代表了我们的应用程序在实际实现中实际面临的情况。</p>

<p class="mce-root"><strong>数据异构</strong> : <em> </em>为训练收集的数据应该是异构的。例如，关于坑洞的图像需要在尽可能多的情况下，在不同的时间，用不同的设备拍摄。</p>

<p class="mce-root">模特培训</p>

<p class="mce-root">正如我们前面提到的，我们正在使用迁移学习，它不需要从头开始训练；在许多情况下，用新的数据集重新训练模型就足够了。我们在一台复制市议会服务器的台式电脑上重新训练了CNN的两个流行架构或模型，即Incentive V3和Mobilenet V1。在这两个模型中，重新训练模型花费的时间不到一个小时，这是迁移学习方法的一个优势。在运行<kbd>retrain.py</kbd>文件之前，我们需要理解关键参数的列表，该文件在code文件夹中。如果我们输入我们的终端(在Linux或macOS中)或命令提示符(Windows) <kbd>python retrain.py -h</kbd>，我们将会看到一个如下截图所示的窗口，其中包含额外的信息(即每个参数的概述)。必选参数是图像目录，它是数据集文件夹视图上前面的图中显示的数据集目录之一:</p>

<p class="mce-root"><img src="img/716599bf-9f65-4cd9-82e6-b46defa25ad4.png"/></p>





            



            

        

    






    

        <title>Data pre-processing </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">在下文中，我们将给出两个命令示例:一个用于重新训练激励模型V3，另一个用于在修改后的数据集(数据集修改)上保留Mobilenet V1。为了重新培训激励V3，我们没有传递架构参数值，因为它是<kbd>retrain.py</kbd>中包含的默认架构。对于其余的参数，包括训练、验证和测试之间的数据分割比率，我们使用默认值。在这个用例中，我们使用数据分割规则，将80%的图像放入主训练集中，保留10%单独用于训练期间的验证，最后10%的数据作为测试集。测试集用于测试分类器的真实分类性能:</h1>

                

            

            

                

<p>要运行Mobilenet V1模型的培训和验证，请使用以下命令:</p>

<ul>

<li>一旦我们运行了前面的命令，它将在给定的目录中生成重新训练模型(<kbd>retrained_graph.pb</kbd>)、标签文本(<kbd>retrained_labels.txt</kbd>)以及由模型的训练和验证摘要信息组成的摘要目录。TensorBoard可以使用具有默认值<kbd>retrain_logs</kbd>的摘要信息<kbd>(--summaries_dir</kbd>参数来可视化模型的不同方面，包括网络及其性能图。如果我们在终端或命令提示符下键入以下命令，它将运行TensorBoard:</li>

<li>TensorBoard运行后，将您的网络浏览器导航至<kbd>localhost:6006</kbd>查看TensorBoard并查看相应型号的网络。下图<strong> (a) </strong>和<strong> (b) </strong>分别显示了激励V3和Mobilenet V1的网络。该图展示了与V1移动网络相比，激励V3的复杂性:</li>

</ul>





            



            

        

    






    

        <title>Models training </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title"><img src="img/22d6ea4b-2faa-4ca9-8936-08c84bc6711f.png" style="width:86.67em;height:62.25em;"/></h1>

                

            

            

                

<p>在第二个用例中，我们只在固体废物数据集上重新训练了Mobilenet V1。如前所述，您可以通过仅提供图像或数据集目录来重新训练模型，如下所示:</p>

<p class="CDPAlignCenter CDPAlign">评估模型</p>

<p>首先，我们确定了再培训模型的规模。如下面的截图所示，Mobilenet V1仅需要17.1 MB(对于两种用例)，这是Incentive V3 (92.3 MB)的五分之一，并且该模型可以轻松部署在资源受限的物联网设备中，包括Raspberry Pi或智能手机。其次，我们评估了模型的性能。对用例进行了两个级别的性能评估:(I)在桌面PC平台/服务器上的再训练阶段进行了数据集范围的评估或测试，以及(ii)在Raspberry Pi 3环境中测试或评估了单个图像或样本(真实图像):</p>

<pre>python retrain.py \<br/>--output_graph=trained_model_incentive-modified-dataset/retrained_graph.pb \<br/>--output_labels=trained_model_incentive-modified-dataset/retrained_labels.txt \<br/>--image_dir=dataset-modified</pre>

<p><img src="img/89912302-26f7-4677-b93e-6f01168445dd.png" style="width:40.75em;height:24.67em;"/></p>

<pre>python retrain.py \<br/>--output_graph=trained_model_mobilenetv1-modified-dataset/retrained_graph.pb \<br/>--output_labels=trained_model_mobilenetv1-modified-dataset/retrained_labels.txt \<br/>--architecture mobilenet_1.0_224 \<br/>--image_dir=dataset-modified</pre>

<p>模型性能(用例一)</p>

<pre>tensorboard --logdir retrain_logs<br/> </pre>

<p>用例一的所有评估性能都呈现在下面的截图中。以下六个截图展示了激励V3和Mobilenet V1模型在两组数据上的训练、验证和测试性能。前三个截图呈现的是重新训练模型后在终端生成的结果，后三个截图是从TensorBoard生成的。</p>

<p class="CDPAlignCenter CDPAlign">下面的截图展示了在原始数据集上对激励V3的评估结果:</p>

<p><img src="img/f968d524-a60a-4c52-8817-65c4cf04c4f5.png" style="width:51.25em;height:10.08em;"/></p>

<pre>--image_dir=dataset-solidwaste</pre>





            



            

        

    






    

        <title>Evaluating models</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">下面的屏幕截图显示了在修改后的数据集上对激励V3的评估结果:</h1>

                

            

            

                

<p class="CDPAlignLeft CDPAlign">以下截图显示了Mobilenet V1在原始数据集上的评估结果:</p>

<p class="CDPAlignCenter CDPAlign">以下截图显示了Mobilenet V1在修改后的数据集上的评估结果:</p>





            



            

        

    






    

        <title>Model performance (use case one) </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title"><img src="img/eaa8056d-ed33-4e1e-9b9e-e57e812d82f1.png"/></h1>

                

            

            

                

<p>下面的截图展示了在TensorBoard生成的原始数据集上对Incentive V3的评估结果:</p>

<p>以下截图展示了Mobilenet V1在TensorBoard生成的原始数据集上的评估结果:</p>

<p class="CDPAlignCenter CDPAlign">从之前所有的模型性能截图来看，很明显，训练和验证准确率都远高于90%，这对于故障检测来说已经足够了。</p>

<p>下图显示了单个样本的分类或对象检测性能。对于这些，我们使用了两套不同的分类代码(可在本章的代码文件夹中找到)。</p>

<div><img src="img/5eaee1f3-7570-44ed-bd87-1d28aa93f7f7.png" style="width:52.92em;height:10.00em;"/></div>

<p>第一个屏幕截图显示了在两个样本上运行Mobilenet V1分类器的快照。正如我们从所有结果中可以看到的，测试或评估精度远高于94%，并且在这样的精度下，DL模型(CNN)具有检测物体的潜力，包括坑洞、检修孔和道路上的其他物体。然而，Pi 3上的物体检测时间在三到五秒的范围内，如果我们想在实时检测和驱动中使用它们，需要改进这一时间。此外，结果表明，在修改后的数据集上训练的模型有很好的机会在真实环境中提供高检测或测试准确性(如前面的屏幕截图所示)，特别是在检测坑洞方面，因为这类数据通过添加来自谷歌图像的不同图像而得到了改善:</p>

<div><img src="img/cfb61326-9ce7-4ded-97a7-8b0b1c20e223.png" style="width:52.50em;height:11.00em;"/></div>

<p class="mce-root"><img src="img/8256fc92-b359-45b0-8b38-4f76fc35cd19.png"/></p>

<p class="mce-root CDPAlignCenter CDPAlign">下面的截图展示了在原始数据集(Pi 3 B+)上训练的激励V3模型的坑洞检测的评估结果:</p>

<p>下图显示了在原始数据集(Pi 3 B+)上训练的激励V3模型的检查井检测的评估结果:</p>

<div><img src="img/943f8ef3-d31f-4d57-87e3-a52137a240c8.png" style="width:69.08em;height:33.42em;"/></div>

<p><img src="img/09716d1d-c32b-400c-a862-160475bb473c.png" style="font-size: 1em;width:36.83em;height:15.25em;"/></p>

<div><img src="img/2469605e-54d7-4ff8-8437-fa1e9a518650.png" style="width:66.75em;height:31.25em;"/></div>

<p class="CDPAlignLeft CDPAlign">下图显示了使用在原始数据集(Pi 3 B+)上训练的Mobilenet V1模型进行坑洞检测的评估结果:</p>

<p><img src="img/072508d6-ea2a-4eef-9d33-a287aaec601d.png" style="width:36.75em;height:15.17em;"/></p>

<p>下图显示了使用在原始数据集(Pi 3 B+)上训练的Mobilenet V1模型进行检查井检测的评估结果:</p>

<p class="CDPAlignCenter CDPAlign">模型性能(用例二)</p>

<p>用例二的所有评估性能都显示在下面的截图中。对于这个用例，我们只展示Mobilenet V1的结果。下图展示了Mobilenet V1模型在两个数据集上的训练、验证和测试性能。从下面的截图中我们可以看到，测试精度并不高(77.5%)，但对于固体废物检测和分类来说已经足够好了:</p>

<div><img src="img/4e62eb2e-fec0-4466-bfb5-c1296717e078.png" style="width:34.42em;height:14.50em;"/></div>

<p><img src="img/81309be7-f043-4486-ae9f-2243d26318db.png"/></p>

<div><p class="CDPAlignCenter CDPAlign">以下截图展示了Mobilenet V1在TensorBoard生成的数据集上的评测结果:</p>

</div>

<div><p><img src="img/3a8271f6-78a8-4302-8e08-c7c4f63c25ad.png" style="width:70.00em;height:32.42em;"/></p>

<p class="CDPAlignCenter CDPAlign">以下三个截图显示了单个样品的分类或物体(固体废物)检测性能。第一张截图呈现了玻璃检测的评测结果:</p>

<p><img src="img/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png"/></p>

<div><img src="img/d0460b70-2622-48f5-aa3e-8fee182a53f4.png" style="width:39.08em;height:15.92em;"/></div>





            



            

        

    






    

        <title>Model performance (use case two) </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">以下截图呈现了塑料检测的评价结果:</h1>

                

            

            

                

<p><img src="img/d037aa38-1eb6-40be-b709-4377b1bb1ee7.png"/></p>

<p class="CDPAlignCenter CDPAlign">以下屏幕截图显示了使用Mobilenet V1进行金属检测的评估结果:</p>

<p><img src="img/0a1d55f8-9114-4ba5-a4c8-39bd2a1cff87.png"/></p>

<p class="CDPAlignCenter CDPAlign">摘要</p>

<p>在本章的第一部分，我们简要描述了不同的物联网应用及其基于图像检测的决策。此外，我们简要讨论了两个用例:基于图像检测的道路故障检测和基于图像检测的固体废物分类。第一个应用程序可以使用智能手机摄像头或Raspberry Pi摄像头检测道路上的坑洼。第二个应用程序检测不同类型的固体废物，并根据智能回收进行分类。</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png"/></p>

<p>The following screenshot presents the evaluation results of plastic detection:</p>

<p class="CDPAlignCenter CDPAlign">在本章的第二部分，我们通过几个示例网络简要讨论了迁移学习，并检验了它在资源受限的物联网应用中的有用性。此外，我们讨论了选择CNN背后的基本原理，包括两个流行的实现，即Inception V3和Mobilenet V1。本章的其余部分描述了Inception V3和Mobilenet V1模型的DL管道的所有必要组件。</p>

<p>在许多物联网应用中，图像识别本身可能不足以检测物体和/或主体。在这种情况下，有时，音频/语音/声音识别会很有用。<a href="ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml">第3章</a>，物联网中的音频/语音/声音识别，将介绍物联网应用中基于DL的语音/声音数据分析和识别。</p>

<p class="CDPAlignCenter CDPAlign">参考</p>





            



            

        

    






    

        <title>Summary </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title"><em>智能巡逻:使用智能手机传感器和众包的高效路面监控</em>，Gurdit Singh，Divya Bansal，Sanjeev Sofat，Naveen Aggarwal，<em>普及和移动计算</em>，2017年第40卷，第71-88页</h1>

                

            

            

                

<p><em>使用深度神经网络通过智能手机捕获图像进行道路损坏检测</em>，Hiroya Maeda，Yoshihide Sekimoto，Toshikazu Seto，Takehiro Kashiyama，Hiroshi Omata，arXiv:1801.09454</p>

<p class="mce-root"><em>坑洞每年给英国司机造成17亿英镑的损失:如果你的车被损坏，如何索赔</em>，卢克·约翰·史密斯:<a href="https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK">https://www . express . co . UK/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK</a></p>

<p class="mce-root"><em>多浪费:固体废物管理全球回顾</em>，D Hoornweg和P Bhada-Tata，世界银行，美国DC华盛顿州，2012年</p>

<p><em>用于移动视觉应用的高效卷积神经网络</em><em/>，Andrew G Howard，朱梦龙，，Dmitry Kalenichenko，，Tobias Weyand，Marco Andreetto，Hartwig Adam，<em>MobileNets:</em>arXiv:1704.04861</p>

<p><em>深度卷积神经网络的Imagenet分类</em>，A . Krizhevsky，I . Sutskever，G . E . hint on，载于<em>神经信息处理系统的进展</em>，第1097-1105页，2012年。1, 6.</p>





            



            

        

    






    

        <title>References</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:86d202d3-ea3e-40e5-8f75-0218d47a52e1" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title"><em> MobileNetV2:反向残差和线性瓶颈</em>，Mark Sandler，，Menglong Zhu，Andrey Zhmoginov，Liang-Jie Chen，arXiv:1801.04381。</h1>

                

            

            

                

<ul>

<li>坑洞数据集:<a href="https://cord.cranfield.ac.uk/articles/PotDataset/5999699">https://cord.cranfield.ac.uk/articles/PotDataset/5999699</a></li>

<li>垃圾网:<a href="https://github.com/garythung/trashnet">https://github.com/garythung/trashnet</a></li>

<li><em>Potholes cost UK drivers £1.7 billion a year: Here's how to claim if you car is damaged</em>, Luke John Smith: <a href="https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK">https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK</a></li>

<li><em>What a Waste: A Global Review of Solid Waste Management</em>, D Hoornweg and P Bhada-Tata, World Bank, Washington, DC, USA, 2012 </li>

<li><em>Efficient Convolutional Neural Networks for Mobile Vision Applica</em><em>tions</em>, Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam, <em>MobileNets: </em>arXiv:1704.04861</li>

<li><em>Imagenet classification with deep convolutional neural networks</em>, A Krizhevsky, I Sutskever, G E Hinton, in <em>Advances in Neural Information Processing Systems</em>, pages 1,097–1,105, 2012. 1, 6.</li>

<li><em>MobileNetV2: Inverted Residuals and Linear Bottlenecks</em>, Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, arXiv:1801.04381.</li>

<li>Pothole dataset: <a href="https://cord.cranfield.ac.uk/articles/PotDataset/5999699">https://cord.cranfield.ac.uk/articles/PotDataset/5999699 </a></li>

<li>Trashnet: <a href="https://github.com/garythung/trashnet">https://github.com/garythung/trashnet</a></li>

</ul>





            



            

        

    



</div></body></html>