

# 六、使用 RNN 分析数据序列

概观

本章详述了循环神经网络的概念。你将了解到**循环神经网络** ( **RNNs** )的学习过程，以及它们如何存储记忆。本章将介绍**长短期记忆** ( **LSTM** )网络架构，它利用短期和长期记忆，通过数据序列来解决数据问题。在本章结束时，你将牢固掌握 RNNs 以及如何解决自然语言处理数据问题。

# 简介

在前面的章节中，我们解释了不同的网络架构——从可以解决分类和回归问题的传统 ann，到主要用于通过执行对象分类、定位、检测和分割任务来解决计算机视觉问题的 CNN。

在这最后一章中，我们将探讨 RNNs 的概念并解决顺序数据问题。这些网络架构能够处理上下文至关重要的顺序数据，这要归功于它们能够保存来自先前预测的信息，这被称为记忆。这意味着，例如，当逐字分析一个句子时，rnn 在处理最后一个单词时，有能力保存关于句子第一个单词的信息。

本章将探讨 LSTM 网络体系结构，这是一种 RNN，可以保存长期和短期记忆，尤其适用于处理长数据序列，如视频剪辑。

本章还将探讨自然语言处理的概念。NLP 指的是计算机与人类语言的交互，由于虚拟助手的兴起，这是当今的一个热门话题，虚拟助手提供定制的客户服务。本章将使用 NLP 进行情感分析，包括分析句子背后的含义。这有助于根据客户评论了解客户对产品或服务的看法。

注意

本章中出现的所有代码都可以在:[https://packt.live/2yn253K](https://packt.live/2yn253K)找到。

# 循环神经网络

正如人类不会每秒都重置自己的思维一样，旨在理解人类语言的神经网络也不应该这样做。这意味着，为了理解一个段落甚至整本书的每个单词，你或模型需要理解前面的单词，这可以帮助给可能有不同含义的单词提供上下文。

正如我们到目前为止所讨论的，传统的神经网络不能执行这样的任务——因此产生了 RNNs 的概念和网络架构。正如我们之前简要解释的那样，这些网络架构包含不同节点之间的环路。这允许信息在模型中保留更长的时间。由于这个原因，模型的输出变成了预测和记忆，当序列文本的下一位通过模型时，将使用这两个输出。

这一概念可以追溯到 20 世纪 80 年代，尽管它只是在最近才变得流行，这要归功于技术的进步，这些进步导致了机器计算能力的提高，并允许数据的回忆，以及 20 世纪 90 年代 LSTM rnn 概念的发展，这增加了它们的应用。rnn 是最有前途的网络架构之一，这要归功于它们存储内部存储器的能力，这允许它们有效地处理数据序列并解决各种各样的数据问题。

## RNNs 的应用

虽然我们已经非常清楚，RNNs 最适合处理数据序列，如文本、音频剪辑和视频，但仍然有必要解释 RNNs 在现实生活问题中的不同应用。

以下是通过使用 rnn 可以执行的不同任务的一些简要说明:

*   **NLP**: This refers to the ability of machines to represent human language. This is perhaps one of the most explored areas of deep learning and undoubtedly the preferred data problem when making use of RNNs. The idea is to train the network using text as input data, such as poems and books, among others, with the objective of creating a model that is capable of generating such texts.

    NLP 通常用于创建聊天机器人(虚拟助手)。通过从以前的人类对话中学习，NLP 模型能够帮助一个人解决常见的问题或查询。据此，他们的造句能力仅限于他们在训练过程中学到的东西，也就是说他们只能回答他们所学到的东西。

    当你试图通过在线聊天系统联系一家银行时，你可能会遇到这种情况，通常情况下，当查询超出常规范围时，你就会被转接到人工接线员那里。现实生活中聊天机器人的另一个常见例子是通过 Facebook Messenger 接受查询的餐馆:

![Figure 6.1: Facebook's Messenger chatbot
](img/B15778_06_01.jpg)

图 6.1:脸书的信使聊天机器人

*   **Speech recognition**: Similar to NLP, speech recognition attempts to understand and represent human language. However, the difference here is that the former (NLP) is trained on and produces the output in the form of text, while the latter (speech recognition) uses audio clips. With the proliferation of developments in this field and the interest of big companies, these models are capable of understanding different languages and even different accents and pronunciation.

    语音识别设备的一个流行例子是 Alexa——亚马逊的语音激活虚拟助理模型:

![Figure 6.2: Amazon's Alexa
](img/B15778_06_02.jpg)

图 6.2:亚马逊的 Alexa

*   **Machine translation**: This refers to a machine's ability to translate human languages effectively. According to this, the input is the source language (for instance, Spanish) and the output is the target language (for instance, English). The main difference between NLP and machine translation is that, in the latter, the output is built after the entire input has been fed to the model.

    随着全球化的兴起和如今休闲旅游的流行，人们需要掌握一种以上的语言。因此，能够在不同语言之间进行翻译的设备大量涌现。该领域的最新发明之一是谷歌的 Pixel Buds，它可以实时进行翻译:

![Figure 6.3: Google's Pixel Buds
](img/B15778_06_03.jpg)

图 6.3:谷歌的像素芽

*   **Time-series forecasting**: A less popular application of an RNN is the prediction of a sequence of data points in the future based on historical data. RNNs are particularly good at this task due to their ability to retain an internal memory, which allows time-series analysis to consider the different timesteps in the past to perform a prediction or a series of predictions in the future.

    这通常用于预测未来的收入或需求，这有助于公司为不同的情况做好准备。下图显示了月销售额的预测:

![Figure 6.4: The forecasting of monthly sales (quantity)
](img/B15778_06_04.jpg)

图 6.4:月销售额(数量)预测

例如，如果通过预测对几种保健产品的需求，确定对其中一种产品的需求将会增加，而对另一种产品的需求将会减少，那么公司可能会决定生产更多的这种产品，而生产更少的另一种产品。

*   **图像识别**:与 CNN 结合，RNNs 可以给一张图像加标题或描述。这种模型组合允许您检测图像中的所有对象，这决定了图像主要由什么组成。输出可以是图像中存在的对象的一组标签、图像的描述或图像中相关对象的标题，如下图所示:

![Figure 6.5: Image recognition using RNNs
](img/B15778_06_05.jpg)

图 6.5:使用 RNNs 的图像识别

## RNNs 是如何工作的？

简单地说，RNNs 接受一个输入(x)并返回一个输出(y)。这里，产出不仅受投入的影响，还受过去投入的整个历史的影响。这种输入历史通常被称为模型的内部状态或内存，它是遵循一个订单并且彼此相关的数据序列，例如时间序列，它是按订单(例如按月)列出的数据点序列(例如销售额)。

注意

请记住，根据手头的问题，RNN 的一般结构可能会有所不同。例如，它们可以是一对多类型或多对一类型，正如我们在第 2 章*、*神经网络的构建模块*中提到的。*

为了理解 RNNs 的概念，了解 RNNs 和传统神经网络之间的区别是很重要的。传统的神经网络通常被称为前馈神经网络，因为信息仅在一个方向上移动(即，从输入到输出)，而不经过节点两次来执行预测。这些网络对过去被喂食的食物没有任何记忆，这就是为什么它们不擅长预测序列中接下来会发生什么。

另一方面，在 RNNs 中，信息在循环中循环，因此每次预测都是通过考虑输入和来自先前预测的记忆来做出的。它的工作原理是复制每次预测的输出，并将其传递回网络，用于后续的预测。这样，rnn 有两个输入——现值和过去的信息:

![Figure 6.6: A graphical representation of a network, where A shows a 
feedforward neural network and B shows an RNN
](img/B15778_06_06.jpg)

图 6.6:网络的图形表示，其中 A 表示前馈神经网络，B 表示 RNN

注意

传统 RNNs 的内部记忆只是短期的。然而，我们稍后将探索一种能够存储长期和短期记忆的架构。

通过使用来自以前预测的信息，用有序数据序列训练网络，使其能够预测下一步。这是通过将当前信息和上一步的输出合并成一个操作来实现的。这可以从下图中看出。此操作的输出将成为预测，以及后续预测的部分输入:

![Figure 6.7: An RNN computation for each prediction
](img/B15778_06_07.jpg)

图 6.7:每次预测的 RNN 计算

如你所见，发生在节点内部的操作是任何其他神经网络的操作；最初，数据通过线性函数传递。权重和偏差是在训练过程中要更新的参数。接下来，使用激活函数破坏该输出的线性。在这种情况下，这就是 *tanh* 函数，因为几项研究表明，对于大多数有序数据问题，它可以获得更好的结果:

![Figure 6.8: A mathematical computation of traditional RNNs
](img/B15778_06_08.jpg)

图 6.8:传统 RNNs 的数学计算

这里， *M* t-1 指的是从前一次预测得到的记忆， *W* 和 *b* 是权重和偏差， *E* t 指的是当前事件。

记住这个学习过程，让我们考虑一个产品过去两年的销售数据。rnn 能够预测下个月的销售额，因为通过存储过去几个月的信息，它们能够检查销售额是增加还是减少。

使用*图 6.7* ，下个月的预测可以通过获取上个月的销售额(即当前事件)和短期记忆(代表过去几个月的数据)并将它们结合起来进行处理。该操作的输出将包含下个月的预测和过去几个月的一些相关信息，这些信息将依次成为后续预测的新的短期记忆。

此外，值得一提的是，一些 RNN 架构，如 LSTM 网络，也将能够考虑 2 年前甚至更早的数据(因为它存储长期记忆)。这将让网络知道在特定月份期间的减少是否可能继续减少或开始增加。稍后我们将更详细地探讨这个主题。

## 有序数据的输入和目标

考虑到目标是预测序列中的下一个元素，目标矩阵通常是与输入数据相同的信息，目标领先一步。

这意味着输入变量应该包含序列的所有数据点，除了最后一个值，而目标变量应该包含序列的所有数据点，除了第一个值，也就是说，目标变量的第一个值应该是输入变量的第二个值，依此类推，如下图所示:

![Figure 6.9: Input and target variables for a sequenced data problem
](img/B15778_06_09.jpg)

图 6.9:有序数据问题的输入和目标变量

## 练习 6.01:为有序数据问题创建输入和目标变量

在本练习中，您将使用虚拟数据集学习如何创建可用于解决有序数据问题的输入和目标变量。按照以下步骤完成本练习:

注意

对于本章中的练习和活动，您需要在本地计算机上安装 Python 3.7、Jupyter 6.0、Matplotlib 3.1、NumPy 1.17、Pandas 0.25 和 PyTorch 1.3+(最好是 PyTorch 1.4)。

1.  导入以下库:

    ```py
    import pandas as pd import numpy as np import torch
    ```

2.  Create a Pandas DataFrame that's 10 x 5 in size, filled with random numbers ranging from 0 to 100\. Name the five columns as follows: `["Week1", "Week2", "Week3", "Week4", "Week5"]`.

    确保将随机种子设置为`0`，以便能够重现本书中显示的结果:

    ```py
    np.random.seed(0)
    data = pd.DataFrame(np.random.randint(0,100,size=(10, 5)),
                        columns=['Week1','Week2','Week3',\
                                 'Week4','Week5'])
    data
    ```

    注意

    提醒一下，在 Jupyter 笔记本中，不需要打印功能也可以打印变量值。在其他编程平台中，您可能需要使用打印功能。

    产生的数据帧如下:

    ![Figure 6.10: The created DataFrame
    ](img/B15778_06_10.jpg)

    图 6.10:创建的数据帧

3.  创建一个输入变量和一个目标变量，考虑到输入变量应该包含所有实例的所有值，最后一列数据除外。目标变量应该包含所有实例的所有值，除了第一列:

    ```py
    inputs = data.iloc[:,:-1] targets = inputs.shift(-1, axis="columns", \                        fill_value=data.iloc[:,-1:])
    ```

4.  Print the input variable to verify its contents, as follows:

    ```py
    inputs
    ```

    输入变量应该如下所示:

    ![Figure 6.11: Input variable
    ](img/B15778_06_11.jpg)

    图 6.11:输入变量

5.  Print the resulting target variable using the following code:

    ```py
    targets
    ```

    运行上述代码将显示以下输出:

![Figure 6.12: Target variable
](img/B15778_06_12.jpg)

图 6.12:目标变量

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VQ5OjB](https://packt.live/2VQ5OjB)。

你也可以在 https://packt.live/31yCotG 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

## py torch 中的 RNNs

在 PyTorch 中，与任何其他层类似，递归层是在一行代码中定义的。这将在网络的转发函数中调用，如下面的代码所示:

```py
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super().__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, num_layers,\
                          batch_first=True)
        self.output = nn.Linear(hidden_size, 1)
    def forward(self, x, hidden):
        out, hidden = self.rnn(x, hidden)
        out = out.view(-1, self.hidden_size)
        out = self.output(out)
        return out, hidden
```

这里，递归层必须被定义为接受输入中预期特征数量的参数(`input_size`)；处于隐藏状态的特征数量，由用户定义(`hidden_size`)；以及循环层数(`num_layers`)。

注意

与任何其他神经网络类似，隐藏大小指的是该层中节点(神经元)的数量。

将`batch_first`参数设置为`True`,以定义输入和输出张量是批处理、序列和特征的形式。

在`forward`函数中，输入通过递归层传递，这些层的输出被展平，以便可以通过完全连接的层传递。值得一提的是，信息是通过 RNN 层传递的，同时还有一个隐藏状态(内存)。

此外，这种网络的训练可以如下处理:

```py
for i in range(1, epochs+1):
    hidden = None
    for inputs, targets in batches:
        pred, hidden = model(inputs, hidden)
        loss = loss_function(pred, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

对于每个时期，隐藏状态被初始化为`none`。这是因为，在每个时期，网络将试图将输入映射到目标(当给定一组参数时)。这种映射应该在没有任何偏差(隐藏状态)的情况下发生，这些偏差来自先前对数据集的运行。

接下来，使用一个`for`循环遍历不同批次的数据。在这个循环中，进行了一个预测并保存了一个隐藏状态，它将被用作下一批的输入。

最后计算损失函数，用来更新网络的参数。然后，该过程再次开始，直到达到期望的次数。

## 活动 6.01:使用简单的 RNN 进行时间序列预测

在本练习中，您将使用一个简单的 RNN 来解决一个时间序列问题。让我们考虑以下场景:您的公司希望能够提前预测其所有产品的需求。这是因为生产每一种产品需要相当长的时间，而且这个过程要花很多钱。因此，除非产品有可能卖出去，否则他们不愿意在生产上花费金钱和时间。为了预测未来的需求，他们为您提供了一个数据集，其中包含去年销售的所有产品的每周需求(在销售交易中)。按照以下步骤完成本活动:

注意

包含将在本练习中使用的数据集的 CSV 文件可在[https://packt.live/2K5pQQK](https://packt.live/2K5pQQK)找到。也可在[https://archive . ics . UCI . edu/ml/datasets/Sales _ Transactions _ Dataset _ Weekly](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly)上在线获取。

数据集和相关分析首次发表于此:Tan S.C .，Lau J.P.S. (2014)时间序列聚类:购物篮分析的一个优越替代方案。载于:Herawan T .、Deris M .、Abawajy J .(编辑)《第一届高级数据和信息工程国际会议论文集》(DaEng-2013)。电气工程讲义，第 285 卷。新加坡斯普林格。

1.  导入所需的库。
2.  加载数据集并对其进行切片，使其包含所有行，但只包含从索引 1 到 52 的列。
3.  绘制从整个数据集中随机选择的五种产品的每周销售交易。进行随机抽样时，使用随机种子`0`，以获得与当前活动相同的结果。
4.  Create the `inputs` and `targets` variables, which will be fed to the network to create the model. These variables should be of the same shape and be converted into PyTorch tensors.

    `inputs`变量应该包含所有周的所有产品的数据，除了最后一周，因为模型的想法是预测最后一周。

    `targets`变量应该比`inputs`变量领先一步；也就是说，`targets`变量的第一个值应该是输入变量的第二个值，依此类推，直到`targets`变量的最后一个值(应该是在`inputs`变量之外的最后一周)。

5.  创建一个包含网络架构的类；请注意，全连接层的输出大小应为 1。
6.  实例化包含模型的类函数。馈入输入大小，每个递归层的神经元数(10)，递归层数(1)。
7.  定义损失函数、优化算法和训练网络的历元数。使用均方误差损失函数、Adam 优化器和 10，000 个历元。
8.  使用`for`循环通过遍历所有时期来执行训练过程。在每个时期，必须进行预测，随后计算损失函数并优化网络参数。然后，保存每个历元的丢失。
9.  画出所有时代的损失。
10.  Using a scatter plot, display the predictions that were obtained in the last epoch of the training process against the ground truth values (that is, the sales transactions of the last week).

    注意

    这项活动的解决方案可在第 284 页找到。

# 长短期记忆网络

正如我们之前提到的，RNNs 只储存短期记忆。当处理长数据序列时，这是一个问题，在这种情况下，网络将很难将信息从前面的步骤传送到最后的步骤。

例如，著名诗人埃德加·爱伦·坡写的一千多字的诗《乌鸦》。试图使用传统的 RNN 来处理它，目的是创建一首类似的相关诗歌，这将导致模型忽略前几段中的关键信息。反过来，这可能导致与诗歌的初始主题无关的输出。例如，它可以忽略事件发生在晚上，因此使新诗不太可怕。

这种无法保持长期记忆的现象之所以发生，是因为传统的 rnn 遇到了一个叫做消失梯度的问题。当用于更新网络参数以最小化损失函数的梯度变得非常小时，就会发生这种情况，使得它们不再有助于网络的学习过程。这通常发生在网络的前几层，使网络忘记了不久前看到的内容。

正因为如此， **LSTM** 网络被开发出来。LSTM 网络可以长时间记忆信息，因为它们以类似于计算机的方式存储内存；也就是说，它们根据需要读取、写入和删除信息，这是通过使用门来实现的。

这些门帮助网络根据它分配给每一位信息的重要性来决定保留什么信息和从存储器中删除什么信息(是否打开门)。这是非常有用的，因为它不仅允许存储更多的信息(作为长期记忆)，而且还有助于丢弃可能改变预测结果的无用信息，如句子中的冠词。

## LSTM 网络的应用

除了我们之前解释的应用之外，LSTM 网络存储长期信息的能力使数据科学家能够利用大量数据序列作为输入来解决复杂的数据问题，其中一些将在这里解释:

*   **文本生成**:生成任何文本，比如你正在这里阅读的文本，都可以转化为一个 LSTM 网络的任务。这是通过基于所有前面的字母选择每个字母来实现的。执行这一任务的网络是用大型文本来训练的，比如那些名著。这是因为最终的模型将创建一个文本，该文本与它被训练的文本的写作风格相似。例如，一个在诗歌上训练的模型将会有一个不同于你在与邻居的对话中所期望的叙述。
*   **音乐生成**:正如一个文本序列可以输入到网络中以生成类似的新文本一样，一个音符序列也可以输入到网络中以生成新的音符序列。跟踪之前的音符将有助于实现和谐的旋律，而不仅仅是一系列随机的音符。例如，将甲壳虫乐队的一首流行歌曲输入音频文件，将产生一系列类似该乐队和声的音符。
*   **笔迹生成和识别**:在这里，每个字母也是所有先前字母的产物，这反过来将产生一组有意义的手写字母。同样，LSTM 网络也可以用于识别手写文本，其中一个字母的预测将取决于所有先前预测的字母。例如，当考虑前面的字母以及整个段落时，识别一个难看的手写字母会更容易，因为它有助于根据上下文缩小预测范围。

## LSTM 网络是如何工作的？

到目前为止，已经很清楚，LSTM 网络与传统 rnn 的区别在于它们具有长期记忆的能力。然而，值得一提的是，久而久之，非常旧的信息不太可能影响下一个输出。考虑到这一点，LSTM 网络还能够考虑数据位和底层上下文之间的距离，以便做出决定，忘记一些不再相关的信息。

那么，LSTM 网络是如何决定何时记住何时忘记的呢？与每个节点只执行一次计算的传统 RNNs 不同，LSTM 网络执行四次不同的计算，允许网络的不同输入(即，当前事件、短期记忆和长期记忆)之间的交互来得出结果。

要理解 LSTM 网络背后的过程，请考虑用于管理网络中信息的四个网关，如下图所示:

![Figure 6.13: LSTM network gates
](img/B15778_06_13.jpg)

图 6.13: LSTM 网络网关

上图中每个门的功能可以解释如下:

*   **学习门**:短时记忆(也称为隐藏状态)和当前事件都进入学习门，在这里信息被分析，任何不需要的信息都被忽略。从数学上来说，这是通过使用线性函数和激活函数( *tanh* )将短期记忆和当前事件结合起来实现的。这个输出乘以一个忽略因子，删除任何不相关的信息。为了计算忽略因子，短期记忆和当前事件通过一个线性函数传递。然后，它们被**s 形**激活函数挤压在一起:

![Figure 6.14: The mathematical computations that occur in the learn gate
](img/B15778_06_14.jpg)

图 6.14:发生在学习门的数学计算

这里， *STM* t-1 指的是从之前的预测中推导出来的短时记忆， *W* 和 *b* 是权重和偏差， *E* t 指的是当前事件。

*   **遗忘门**:长时记忆(也称为细胞状态)进入遗忘门，在那里一些信息被删除。这是通过长期记忆和遗忘因子相乘实现的。为了计算遗忘因子，短期记忆和当前事件通过一个线性函数和一个激活函数( **sigmoid** ):

![Figure 6.15: The mathematical computations that occur in the forget gate
](img/B15778_06_15.jpg)

图 6.15:发生在遗忘门中的数学计算

这里， *STM* t-1 指的是从上一次预测得到的短时记忆， *LTM* t-1 是从上一次预测得到的长时记忆， *W* 和 *b* 是权重和偏差， *E* t 指的是当前事件。

*   **记忆门**:在遗忘门没有被遗忘的长时记忆和从学习门保留下来的信息在记忆门汇合在一起，成为新的长时记忆。从数学上来说，这是通过对学习和遗忘门的输出求和来实现的:

![Figure 6.16: The mathematical computation that occurs in the remember gate
](img/B15778_06_16.jpg)

图 6.16:发生在记忆门中的数学计算

这里， *L* t 指的是学习门的输出，而 *F* t 指的是遗忘门的输出。

*   **Use gate**: This is also known as the output gate. Here, the information from both the learn and forget gates are joined together in the use gate. This gate makes use of all the relevant information to perform a prediction, which also becomes the new short-term memory.

    这分三步实现。首先，它将一个线性函数和一个激活函数( *tanh* )应用于遗忘门的输出。第二，它将线性和激活函数( *sigmoid* )应用于短期记忆和当前事件。第三，它将前面步骤的输出相乘。第三步的输出将是新的短期记忆和当前步骤的预测:

![Figure 6.17: The mathematical computations that occur in the use gate
](img/B15778_06_17.jpg)

图 6.17:在使用门中发生的数学计算

这里， *STM* t-1 指的是从之前的预测中推导出来的短时记忆， *W* 和 *b* 是权重和偏差， *E* t 指的是当前事件。

注意

虽然使用不同的激活函数和数学运算符似乎是任意的，但这样做是因为它已被证明适用于处理大量数据序列的大多数数据问题。

对模型执行的每个预测都执行上述过程。例如，对于为创建文学作品而构建的模型，学习、遗忘、记忆和使用信息的过程将针对模型生成的每个字母进行，如下图所示:

![Figure 6.18: An LSTM network process through time
](img/B15778_06_18.jpg)

图 6.18:随时间推移的 LSTM 网络过程

# py torch 中的 LSTM 网络

在 PyTorch 中定义 LSTM 网络结构的过程类似于我们到目前为止讨论过的任何其他神经网络。但是，重要的是要注意，当处理与数字序列不同的数据序列时，需要进行一些预处理，以便为网络提供它可以理解和处理的数据。

考虑到这一点，我们需要解释训练模型的一般步骤，以便能够将文本数据作为输入并检索新的文本数据。值得一提的是，并不是这里解释的所有步骤都是严格要求的，但是作为一个整体，它们为使用带有文本数据的 LSTMs 提供了干净且可重用的代码。

## 对输入数据进行预处理

第一步是将文本文件加载到代码中。这些数据将经过一系列转换，以便正确地输入到模型中。这是必要的，因为神经网络执行一系列数学计算来获得输出，这意味着所有的输入必须是数字的。此外，将数据分批而不是一次全部提供给模型也是一种很好的做法，因为这有助于减少训练时间，特别是对于长数据集。这些转换将在下面的小节中解释。

### 编号标签

首先，从输入数据中获得未复制字符的列表。这些字符中的每一个都被分配了一个号码。然后，考虑到相同的字母必须始终由相同的数字表示，通过用指定的数字替换每个字符来对输入数据进行编码。例如，给定以下字符和数字的映射，单词“hello”将被编码为 12334:

![Figure 6.19: The mapping of characters and numbers
](img/B15778_06_19.jpg)

图 6.19:字符和数字的映射

前面的输出可以通过下面的代码片段实现:

```py
text = "this is a test text!"
chars = list(set(text))
indexer = {char: index for (index, char) \
           in enumerate(chars)}
indexed_data = []
for c in text:
    indexed_data.append(indexer[c])
```

第二行代码创建一个包含文本字母表(即文本序列中的字母和字符)的列表。接下来，创建一个字典，使用每个字母或字符作为键，使用与之相关的数字表示作为值。最后，通过在文本中执行一个`for`循环，可以用数字表示替换每个字母或字符，从而将文本转换成一个数字矩阵。

### 生成批次

对于 RNNs，使用两个变量创建批次:每批次的序列数和每个序列的长度。这些值用于将数据划分为矩阵，这将有助于加快计算速度。

使用 24 个整数的数据集，每批的序列数设置为 2，序列长度等于 4，除法运算如下:

![Figure 6.20: Batch generation for RNNs
](img/B15778_06_20.jpg)

图 6.20:rnn 的批量生成

如上图所示，创建了三个批次，每个批次包含两个长度为 4 的序列。

应该为`x`和`y`完成这个批量生成过程，其中前者是网络的输入，后者代表目标。据此，考虑到`y`会比`x`领先一步，网络的想法是想办法映射`x`和`y`的关系。

按照上图中说明的方法创建`x`的批次。然后，将创建`y`的批次，使它们与`x`的批次长度相同。这是因为`y`的第一个元素将是`x`的第二个元素，依此类推，直到`y`的最后一个元素(将是`x`的第一个元素):

注意

有许多不同的方法可以用来填充最后一个元素`y`，这里提到的是最常用的一种。方法的选择往往是个人喜好的问题，尽管某些数据问题可能从某种方法中比从其他方法中获益更多。

![Figure 6.21: A representation of the batches for x and y
](img/B15778_06_21.jpg)

图 6.21:x 和 y 的批次表示

批处理的生成可以通过下面的代码片段来实现:

```py
x = np.array(indexed_data).reshape((2,-1))
for b in range(0, x.shape[1], 5):
    batch = x[:,b:b+5]
    print(batch)
```

首先，数字矩阵被分成多个序列(你想要多少就有多少)。接下来，使用`for`循环，可以将排序后的数据分成指定长度的批次。通过打印`batch`变量，可以观察结果。

注意

尽管生成批处理被认为是数据预处理的一部分，但它通常被编程在训练过程的`for`循环中。

## 一键编码

将所有字符转换成数字不足以将它们输入到模型中。这是因为这种近似会给模型带来一些偏差，因为转换为更高数值的字符将被视为更重要。为了避免这种情况，最好将不同批次编码为一键矩阵。这包括创建具有 0 和 1 的三维矩阵，其中 0 表示不存在事件，1 表示存在事件。矩阵的最终形状应该是 *one hot =【序列数，序列长度，字符数】*。

这意味着，对于批处理中的每个元素，它将创建一个长度等于整个文本中字符总数的值序列。对于每个字符，它将放置一个零，除了在那个位置出现的那个(它将放置一个 1)。

注意

你可以在[https://www . geeksforgeeks . org/ml-one-hot-encoding-of-datasets-in-python/](https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/)找到更多关于 one-hot 编码的信息。

这可以通过下面的代码片段来实现:

```py
batch = np.array([[2 4 7 6 5]
                  [2 1 6 2 5]])
batch_flatten = batch.flatten()
onehot_flat = np.zeros((batch.shape[0] \
                        * batch.shape[1],len(indexer)))
onehot_flat[range(len(batch_flatten)), batch_flatten] = 1
onehot = onehot_flat.reshape((batch.shape[0],\
                              batch.shape[1], -1))
```

首先，将二维批次扁平化。接下来，创建一个矩阵并用零填充。当我们需要在给定的位置表示正确的字符时，用 1 代替 0。最后，展平的维度再次展开。

## 练习 6.02:预处理输入数据并创建一个独热矩阵

在本练习中，您将预处理一个文本片段，然后将它转换成一个独热矩阵。按照以下步骤完成本练习:

1.  导入数量:

    ```py
    import numpy as np
    ```

2.  创建一个名为`text`的变量，它将包含文本示例`"Hello World!"` :

    ```py
    text = "Hello World!"
    ```

3.  Create a dictionary by mapping each letter to a number:

    ```py
    chars = list(set(text))
    indexer = {char: index for (index, char) \
               in enumerate(chars)}
    print(indexer)
    ```

    运行上述代码将产生以下输出:

    ```py
    {'d': 0, 'o': 1, 'H': 2, ' ': 3, 'e': 4, 'W': 5, '!': 6, 'l': 7, 'r': 8}
    ```

4.  用我们在上一步中定义的数字对你的文本样本进行编码:

    ```py
    encoded = [] for c in text:     encoded.append(indexer[c])
    ```

5.  Convert the encoded variable into a NumPy array and reshape it so that the sentence is divided into two sequences of the same size:

    ```py
    encoded = np.array(encoded).reshape(2,-1)
    encoded
    ```

    运行上述代码将产生以下输出:

    ```py
    array([[2, 4, 7, 7, 1, 3],
           [5, 1, 8, 7, 0, 6]])
    ```

6.  定义一个函数，它接受一个数字数组并创建一个热矩阵:

    ```py
    def index2onehot(batch):     batch_flatten = batch.flatten()     onehot_flat = np.zeros((batch.shape[0] \                             * batch.shape[1], len(indexer)))     onehot_flat[range(len(batch_flatten)), \                 batch_flatten] = 1     onehot = onehot_flat.reshape((batch.shape[0], \                                   batch.shape[1], -1))     return onehot
    ```

7.  Convert the encoded array into a one-hot matrix by passing it through the previously defined function:

    ```py
    one_hot = index2onehot(encoded)
    one_hot
    ```

    输出应该如下所示:

![Figure 6.22: One-hot representation of sample text
](img/B16118_06_22.jpg)

图 6.22:示例文本的一键显示

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ZpGvFJ](https://packt.live/2ZpGvFJ)。

你也可以在[https://packt.live/38foCxD](https://packt.live/38foCxD)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地将一些示例文本转换为一键矩阵。

## 构建架构

与其他神经网络类似，LSTM 层很容易在一行代码中定义。然而，包含网络架构的类必须包含一个函数，该函数允许初始化隐藏和单元状态(即网络的两个存储器)。LSTM 网络架构的示例如下:

```py
class LSTM(nn.Module):
    def __init__(self, char_length, hidden_size, n_layers):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.lstm = nn.LSTM(char_length, hidden_size, \
                            n_layers, batch_first=True)
        self.output = nn.Linear(hidden_size, char_length)
    def forward(self, x, states):
        out, states = self.lstm(x, states)
        out = out.contiguous().view(-1, self.hidden_size)
        out = self.output(out)
        return out, states
    def init_states(self, batch_size):
        hidden = next(self.parameters()).data.new(self.n_layers, \
                      batch_size, self.hidden_size).zero_()
        cell = next(self.parameters()).data.new(self.n_layers, \
                    batch_size, self.hidden_size).zero_()
        states = (hidden, cell)
        return states
```

注意

同样，当输入和输出张量是批处理、序列和特征的形式时,`batch_first`参数被设置为`True`。否则不需要定义它，因为它的默认值是`False`。

正如我们所看到的，LSTM 层是在一行中定义的，以下列内容作为参数:

*   输入数据中要素的数量(即非重复字符的数量)
*   隐藏维度(神经元)的数量
*   LSTM 层数

与任何其他网络一样，`forward`功能定义了数据在`forward`通道中通过各层移动的方式。

最后，定义一个函数在每个时期将隐藏和单元状态初始化为零。这是通过`next(self.parameters()).data.new()`实现的，它获取模型的第一个参数，并创建一个相同类型的新张量，在括号内指定维度，然后用零填充。`hidden`和`cell`状态都作为一个元组输入到模型中。

## 训练模型

一旦定义了损失函数和优化算法，就该训练模型了。这是通过遵循与用于其他神经网络体系结构的方法非常相似的方法来实现的，如下面的代码片段所示:

```py
# Step 1: for through epochs
for e in range(1, epochs+1):
    # Step 2: Memory initialized
    states = model.init_states(n_seq)
    # Step 3: for loop to split data in batches.
    for b in range(0, x.shape[1], seq_length):
        x_batch = x[:,b:b+seq_length]

        if b == x.shape[1] - seq_length:
            y_batch = x[:,b+1:b+seq_length]
            y_batch = np.hstack((y_batch, indexer["."] \
                      * np.ones((y_batch.shape[0],1))))
        else:
            y_batch = x[:,b+1:b+seq_length+1]
        """
        Step 4: input data is converted to one-hot matrix.
        Inputs and targets are converted to tensors.
        """
        x_onehot = torch.Tensor(index2onehot(x_batch))
        y = torch.Tensor(y_batch).view(n_seq * seq_length)
        """
        Step 5: get a prediction and perform the 
        backward propagation
        """
        pred, states = model(x_onehot, states)
        loss = loss_function(pred, y.long())
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()
```

如前面的代码所示，步骤如下:

1.  为了得到更好的模型，有必要对数据进行多次检查，因此，有必要设置多个时期。
2.  在每个时期，必须初始化隐藏和单元状态。这是通过调用先前在类中创建的函数来实现的。
3.  使用一个`for`循环将数据批量输入模型。一个`if`语句用来判断是否是最后一批，以便在句尾加一个点，用来表示一个句号。
4.  输入数据被转换成一个独热矩阵。输入和目标都被转换成 PyTorch 张量。
5.  网络的输出是通过对一批数据调用模型获得的。然后，计算损失函数，优化参数。

## 执行预测

为了执行具有某种目的的预测(例如，以单词“once upon a time”开头的段落)，向定型模型提供前几个字符是一种很好的做法。这些初始字符应该在不执行任何预测的情况下输入到模型中，但目的是产生记忆。接下来，前一个字符和存储器被输入网络，输出通过`softmax`函数，以计算每个字符成为序列中下一个字符的概率。最后从概率较高的人物中，随机选择一个。

这可以通过下面的代码片段来实现:

```py
# Step 1
starter = "This is the starter text"
states = None
# Step 2
for ch in starter:
    x = np.array([[indexer[ch]]])
    x = index2onehot(x)
    x = torch.Tensor(x)

    pred, states = model(x, states)
# Step 3
counter = 0
while starter[-1] != "." and counter < 50:
    counter += 1
    x = np.array([[indexer[starter[-1]]]])
    x = index2onehot(x)
    x = torch.Tensor(x)

    pred, states = model(x, states)
    pred = F.softmax(pred, dim=1)
    p, top = pred.topk(10)
    p = p.detach().numpy()[0]
    top = top.numpy()[0]
    index = np.random.choice(top, p=p/p.sum())
    # Step 4
    starter += chars[index]
    print(starter)
```

以下步骤出现在前面的代码片段中:

1.  开始的句子被定义。
2.  在进行预测之前，使用一个`for`循环将起始句子的每个字符输入到模型中，以便更新模型的记忆。
3.  一个`while`循环用于执行新字符的预测，只要字符数不超过 50，直到新字符是一个句号。
4.  每个新字符都被添加到起始句中以形成新的文本序列。

## 活动 6.02:与 LSTM 网络的文本生成

注意

本活动中使用的文本数据可以在互联网上免费获取，尽管您也可以在本书的 GitHub 资源库中找到它。在本章的介绍中提到了存储库的 URL。

在这个活动中，我们将使用书*爱丽丝梦游仙境*来训练一个 LSTM 网络，这样我们就可以将一个起始句输入到模型中，并让它完成这个句子。考虑下面的场景:你喜欢让生活变得更简单的东西，并决定在你写电子邮件时建立一个帮助你完成句子的模型。为此，你决定用一本流行的儿童读物来训练一个网络。按照以下步骤完成本活动:

注意

值得一提的是，虽然本活动中的网络经过了足够多次的训练，可以显示不错的结果，但它并没有经过训练和配置，以实现最佳性能。我们鼓励您使用它来提高性能。

1.  导入所需的库。
2.  打开并把*爱丽丝梦游仙境*中的文字读入笔记本。打印前 50 个字符的摘录和文本文件的总长度。
3.  创建一个包含数据集中未复制字符列表的变量。然后，创建一个字典，将每个字符映射到一个整数，其中字符是键，整数是值。
4.  将数据集的每个字母编码成成对的整数。打印前 50 个编码字符和数据集编码版本的总长度。
5.  创建一个函数，它接收一个批处理并将其编码为一个热矩阵。
6.  创建一个定义网络架构的类。该类应包含一个额外的功能，初始化 LSTM 层的状态。
7.  确定要从数据集创建的批次数量，记住每个批次应包含 100 个序列，并且每个序列的长度应为 50。接下来，将编码数据分割成 100 个序列。
8.  使用 256 作为总共两个循环层的隐藏单元数来实例化您的模型。
9.  Define the loss function and the optimization algorithms. Use the Adam optimizer and the cross-entropy loss. Train the network for 20 epochs.

    注意

    根据您的资源，训练过程将需要很长时间，这就是为什么建议只运行 20 个纪元。然而，可以在 GPU 上运行的代码的等效版本可以在本书的 GitHub 资源库中找到。这将允许您运行更多的纪元，并实现出色的性能。

10.  In each epoch, the data must be divided into batches with a sequence length of 50\. This means that each epoch will have 100 sequences, each with a length of 50.

    注意

    批次是为输入和目标创建的 ed，其中后者是前者的副本但领先一步。

11.  绘制一段时间内损失的进程。
12.  Feed the following sentence starter into the trained model and complete the sentence: "So she was considering in her own mind ".

    注意

    这项活动的解决方案可以在第 290 页找到。

最后的句子会有变化，因为在选择每个字符时有一个随机因素；然而，它应该是这样的:

“所以她在心里考虑着我们，”她说着看了看陛下。前面的句子没有意义，因为网络没有被训练足够长的时间(损失函数仍然可以被最小化),并且它一次选择每个字符，而没有对先前创建的单词的长期记忆。然而，我们可以看到，仅仅经过 20 个时代，网络已经能够形成一些有意义的单词。

# 自然语言处理

计算机擅长分析标准化数据，如财务记录或存储在表格中的数据库。事实上，它们在这方面比人类做得更好，因为它们有能力一次分析数百个变量。另一方面，人类擅长分析非结构化数据，例如语言，这是计算机不擅长做的事情，除非他们手头有一套规则来帮助他们理解它。

考虑到这一点，计算机在人类语言方面面临的最大挑战是，尽管计算机在非常大的数据集上经过长时间的训练后，可以很好地分析人类语言，但它们仍然无法理解句子背后的真正含义，因为它们既没有直觉，也没有能力读懂字里行间的意思。

这意味着，虽然人类能够理解说“他昨晚着火了”的句子。多棒的游戏啊！”指的是某项运动的运动员的表现，计算机会在字面上理解它——这意味着它会将其解释为某人昨晚真的着火了。

NLP 是人工智能**(**AI**)的一个子领域，它通过让计算机理解人类语言来工作。虽然人类可能总是更擅长这项任务，但 NLP 的主要目标是通过让计算机理解人类语言来拉近计算机与人类的距离。**

这个想法是创建专注于特定领域的模型，例如机器翻译和文本摘要。这种任务的专业化有助于计算机开发一种模型，能够解决现实生活中的数据问题，而不必一次性处理人类语言的所有复杂性。

人类语言理解的一个领域(目前非常流行的领域)是情感分析。

## 情感分析

一般来说，情感分析包括理解输入文本背后的情感。考虑到随着社交媒体平台的激增，公司每天收到的消息和评论数量呈指数级增长，它变得越来越受欢迎。这使得实时手动修改和回复每封邮件的任务变得不可能，这可能会损害公司的形象。

情感分析侧重于提取句子的本质成分，而忽略细节。这有助于解决两个主要需求:

1.  确定客户最关心的产品或服务的关键方面。
2.  提取这些方面背后的感觉，以确定哪些方面引起了积极和消极的反应，并相应地处理它们:

![Figure 6.23: An example of a tweet
](img/B16118_06_23.jpg)

图 6.23:一条推文的例子

根据前面截图中的文本，执行情感分析的模型可能会获得以下信息:

*   “AI”作为推文的对象
*   “快乐”是从它衍生出来的感觉
*   “十年”作为对对象情感的时间框架

正如你所看到的，情绪分析的概念对任何有在线业务的公司来说都是关键，因为它将有能力对那些需要立即关注的评论做出惊人的快速响应，并且精确度类似于人。

作为情绪分析的一个示例用例，一些公司可能会选择对他们每天收到的大量消息执行情绪分析，以便对那些包含投诉或负面情绪的消息进行优先响应。这不仅有助于减轻这些特定客户的负面情绪；这也将有助于公司迅速纠正错误，并与客户建立信任关系。

为情感分析执行 NLP 的过程将在下一节详细解释。我们将解释词嵌入的概念以及在 PyTorch 中开发这样一个模型可以执行的不同步骤，这将是本章最后一个活动的目标。

# py torch 中的情感分析

在 PyTorch 中构建一个模型来执行情感分析与我们目前所看到的 RNNs 非常相似。不同的是，在这种情况下，文本数据将被逐字处理。本节将提供构建这样一个模型所需的步骤。

## 对输入数据进行预处理

与任何其他数据问题一样，您需要将数据加载到代码中，记住不同的方法用于不同的数据类型。除了将整个单词集转换成小写字母之外，数据还要经过一些基本的转换，这样就可以将数据输入到网络中。最常见的转换如下:

*   `string`模块的`punctuation`预初始化字符串，它提供了一个标点符号列表，可用于在文本序列中识别它们，如下面的代码片段所示:

    ```py
    test = pd.Series(['Hey! This is example #1.', \                   'Hey! This is example #2.', \                   'Hey! This is example #3.']) for i in punctuation:     test = test.str.replace(i,"")
    ```

*   **编号标签**:类似于前面解释的映射字符的过程，词汇表中的每个单词被映射到一个整数，该整数将被用来替换输入文本的单词，以便将它们馈送到网络中:

![Figure 6.24: The mapping of words and numbers
](img/B16118_06_24.jpg)

图 6.24:单词和数字的映射

PyTorch 允许您在单行代码中嵌入单词，而不是执行一键编码，这些代码可以在包含网络架构的类中定义(接下来将对此进行解释)。

## 建造建筑

同样，定义网络架构的过程与我们到目前为止所学的非常相似。然而，正如我们前面提到的，网络还应该包括一个嵌入层，它将接受输入数据(已经转换为数字表示)并为每个单词分配一个相关度。也就是说，这些值将在训练过程中更新，直到最相关的单词被赋予更高的权重。

以下是一个架构示例:

```py
class LSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, \
                 hidden_size, n_layers):
        super().__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_size, n_layers)
        self.output = nn.Linear(hidden_size, 1)
    def forward(self, x, states):
        out = self.embedding(x)
        out, states = self.lstm(out, states)
        out = out.contiguous().view(-1, self.hidden_size)
        out = self.output(out)
        return out, states
```

如您所见，嵌入层将整个词汇表的长度作为参数，以及由用户设置的嵌入维度。该嵌入尺寸将是 LSTM 层的输入尺寸。架构的其余部分将保持不变。

## 训练模型

最后，在您定义了损失函数和优化算法之后，训练模型的过程与任何其他神经网络相同。根据研究的需要和目的，数据可以分成不同的组。接下来，您必须设置历元的数量以及将数据分成批次的方法。在处理每批数据时，网络的存储器通常被保留，但随后在每个时期被初始化为零。网络的输出是通过对一批数据调用模型获得的。然后，计算损失函数，优化参数。

## 活动 6.03:为情感分析执行 g NLP

您将在此活动中使用的数据集称为*情感标签句子数据集*，可在加州大学欧文分校机器学习资源库中获得。

注意

本次活动的数据集可在[https://packt.live/2z2LYc5](https://packt.live/2z2LYc5)找到。也可以在[https://archive . ics . UCI . edu/ml/datasets/sensation+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)上在线获取。

数据集和相关分析首次发表于此:使用深度特征从群体到个体标签。艾尔。https://doi.org/10.1145/2783258.2783380 2015 年 KDD 青奥会

在本活动中，将使用 LSTM 网络来分析一组评论，以确定其背后的情绪。让我们考虑以下场景:你在一家互联网提供商的公共关系部门工作，审查你在该公司社交媒体档案上收到的每个查询的过程需要很长时间。最大的问题是，对服务有问题的客户比没有问题的客户更没有耐心，所以你需要优先考虑你的回应，以便你可以首先解决它们。由于您喜欢在空闲时间编程，您决定尝试构建一个能够确定消息是积极还是消极的神经网络。按照以下步骤完成本活动:

注意

值得一提的是，本活动中的数据没有划分为不同的数据集，这些数据集允许对模型进行微调和测试。这是因为此活动的主要重点是实现创建能够执行情感分析的模型的过程。

1.  导入所需的库。
2.  加载包含来自 Amazon 的 1，000 条产品评论的数据集，这些评论的标签分别为 0(表示负面评论)或 1(表示正面评论)。将数据分成两个变量，一个包含评论，另一个包含标签。
3.  删除评论中的标点符号。
4.  创建一个包含所有评论词汇的变量。此外，创建一个字典，将每个单词映射到一个整数，其中单词是键，整数是值。
5.  通过用成对整数替换评论中的每个单词，对评论数据进行编码。
6.  Create a class containing the architecture of the network. Make sure that you include an embedding layer.

    注意

    由于数据不会在训练过程中批量输入，所以不需要在`forward`函数中返回状态。然而，这并不意味着模型没有记忆，而是说记忆用于单独处理每个检查，因为一个检查不依赖于下一个检查。

7.  对于 3 个 LSTM 层，使用 64 个嵌入维度和 128 个神经元来实例化该模型。
8.  定义损失函数、优化算法和要训练的时期数。例如，可以使用二进制交叉熵损失作为损失函数、Adam 优化器，并训练 10 个时期。
9.  创建一个`for`循环，遍历不同的时期，并单独遍历每一个回顾。对于每个检查，执行预测，计算损失函数，并更新网络的参数。此外，根据训练数据计算网络的准确性。
10.  绘制一段时间内的损失进度和准确度。

最终的精度图如下所示:

![Figure 6.25: Plot displaying the progress of the accuracy score
](img/B16118_06_25.jpg)

图 6.25:显示准确度分数进度的图

注意

这项活动的解决方案可以在第 298 页找到。

# 总结

在本章中，我们讨论了 RNNs。开发这种类型的神经网络是为了解决与序列中的数据相关的问题。这意味着单个实例不包含所有相关信息，因为这取决于来自先前实例的信息。

有几个应用程序符合这种类型的描述。例如，如果没有文本其余部分的上下文，文本(或语音)的特定部分可能没有多大意义。然而，尽管使用 RNNs 对 NLP 进行了最多的探索，但在其他应用中，文本的上下文也很重要，例如预测、视频处理或与音乐相关的问题。

一个 RNN 以一种非常聪明的方式工作；网络不仅输出结果，还输出一个或多个通常被称为记忆的值。该记忆值用作未来预测的输入。

当处理处理非常大的序列的数据问题时，传统的 RNNs 存在一个问题，称为消失梯度问题。在这种情况下，梯度变得非常小，从而不再有助于网络的学习过程，这通常发生在网络的早期层，导致网络不能具有长期记忆。

为了解决这个问题，开发了 LSTM 网络。这种网络架构能够存储两种类型的内存，因此得名。此外，这个网络中发生的数学计算允许它通过只存储过去的相关信息来忘记信息。

最后解释了一个非常新潮的 NLP 任务:情感分析。在这项任务中，理解文本提取背后的情感是很重要的。考虑到人类可以使用许多不同的词语和表达形式(例如，讽刺)来描述事件背后的情绪，这对机器来说是一个非常困难的问题。然而，由于社交媒体使用的增加，产生了更快地处理文本数据的需求，这个问题在大公司中变得非常流行，大公司已经投入时间和金钱来创建几个近似来解决它，如本章的最后活动所示。

现在，您已经阅读了本书的所有章节，对不同的深度神经网络架构有了广泛的了解，这些架构可用于使用 PyTorch 解决各种各样的数据问题。本书中解释的体系结构也可以用来解决其他数据问题。