<html><head/><body>


	
		<title>B15778_04_Final_SZ_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-91"><a id="_idTextAnchor112"/> 4。卷积神经网络</h1>
		</div>
		<div><p class="callout-heading">概观</p>
			<p class="callout">本章解释了训练<strong class="bold">卷积神经网络</strong>(<strong class="bold">CNN</strong>)的过程——也就是说，在CNN架构中通常可以找到的不同层中发生的计算以及它们在训练过程中的目的。您将学习如何通过对模型应用数据扩充和批量标准化来提高计算机视觉模型<a id="_idTextAnchor113"/>的性能。在本章结束时，你将能够使用CNN解决使用PyTorch的图像分类问题。这将是在计算机视觉领域实现其他解决方案的起点。</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor114"/>简介</h1>
			<p>在前一章中，解释了最传统的神经网络体系结构，并将其应用于现实生活中的数据问题。在本章中，我们将探讨细胞神经网络的不同概念，它主要用于解决计算机视觉问题(即图像处理)。</p>
			<p>尽管现在所有的神经网络领域都很流行，但CNN可能是所有神经网络架构中最流行的。这主要是因为，尽管他们在许多领域工作，但他们特别擅长处理图像，并且技术进步允许收集和存储大量图像，这使得使用图像作为输入数据来应对当今的各种挑战成为可能。</p>
			<p>从图像分类到物体检测，CNN正被用于诊断癌症患者和检测系统中的欺诈行为，以及构建深思熟虑的自动驾驶汽车，这将彻底改变未来。</p>
			<p>本章将重点解释CNN在处理图像时优于其他架构的原因，以及更详细地解释其架构的构建块。它将涵盖构建CNN的主要编码结构，以解决图像分类数据问题。</p>
			<p>此外，我们将探讨数据扩充和批处理规范化的概念，这将用于提高模型的性能。本章的最终目标是比较三种不同方法的结果，以便使用CNN解决图像分类问题。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">本章中的所有代码都可以在<a href="https://packt.live/3bg0KKP">https://packt.live/3bg0KKP</a>找到。</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor115"/>建立有线电视新闻网</h1>
			<p>在处理图像数据问题时，CNN是理想的架构。然而，尽管它们的能力扩展到图像处理领域的其他领域，但由于它们通常用于图像分类任务，所以它们经常未被充分使用。这一章不仅会解释CNN如此擅长理解图像的原因，还会确定可以处理的不同任务，并提供一些现实生活中的应用示例。</p>
			<p>此外，本章将探索CNN的不同构建模块及其使用PyTorch的应用，最终使用PyTorch的一个图像分类数据集构建一个解决数据问题的模型。</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor116"/>为什么用CNN做图像处理？</h2>
			<p>一幅图像是一个像素矩阵，那么为什么不干脆把矩阵展平成一个向量，用传统的神经网络架构来处理它呢？答案是，即使是最简单的图像，也存在一些改变图像含义的像素相关性。例如，一只猫的眼睛，一个汽车轮胎，甚至一个物体的边缘都是由几个以某种方式排列的像素构成的。如果我们将图像展平，这些依赖关系就会丢失，传统模型的准确性也会丢失:</p>
			<div><div><img src="img/B15778_04_01.jpg" alt="Figure 4.1: Representation of a flattened matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.1:展平矩阵的表示</p>
			<p>CNN能够捕捉图像的空间相关性，因为它将图像作为矩阵进行处理，并根据过滤器的大小一次分析图像的整个块。例如，使用大小为3 x 3的过滤器的卷积层将一次分析9个像素，直到它覆盖整个图像。</p>
			<p>图像的每个块都有一组参数(权重和偏差),这些参数是指该组像素与整个图像的相关性，具体取决于手边的过滤器。这意味着垂直边缘滤波器将为包含垂直边缘的图像块分配更大的权重。据此，通过减少参数的数量和通过分块分析图像，CNN能够更好地呈现图像。</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor117"/>图像作为输入</h2>
			<p>如前所述，CNN的典型输入是矩阵形式的图像。矩阵的每个值代表图像中的一个像素，其中的数值由颜色的强度决定，取值范围从0到255。</p>
			<p>在灰度图像中，白色像素用数字255表示，黑色像素用数字0表示。灰色像素是介于两者之间的任何数字，取决于颜色的强度；灰色越浅，数字越接近255。</p>
			<p>彩色图像通常使用RGB系统来表示，RGB系统将每种颜色表示为红色、绿色和蓝色的组合。这里，每个像素将有三个维度，每种颜色一个。每个维度中的值范围从0到255。在这里，颜色越强烈，数字越接近255。</p>
			<p>根据前面的段落，给定图像的矩阵是三维的。这里，第一维指的是图像的高度(以像素为单位)，第二维指的是图像的宽度(以像素为单位)，第三维称为通道，指的是图像的配色方案。</p>
			<p>彩色图像的通道数量为三个(RGB系统中每种颜色一个通道)。另一方面，灰度图像只有一个通道:</p>
			<div><div><img src="img/B15778_04_02.jpg" alt="Figure 4.2: Matrix representation of an image – to the left, a colored image; &#13;&#10;to the right, a gray-scaled image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.2:图像的矩阵表示——左边是彩色图像；右边是灰度图像</p>
			<p>与文本数据相比，输入CNN的图像不需要太多的预处理。图像通常按原样输入，最常见的变化如下:</p>
			<ul>
				<li>归一化像素值，以加快学习过程并提高性能</li>
				<li>缩小图像尺寸(即减小其宽度和长度)以加快学习过程</li>
			</ul>
			<p>归一化输入的最简单方法是取每个像素的值，然后除以255，这样我们得到的值在0和1之间。然而，不同的方法被用于归一化图像，例如均值居中技术。选择一个或另一个的决定，在大多数时候，是一个偏好的问题；但是，当使用预训练模型时，强烈建议您使用第一次训练模型时使用的相同技术。这些信息通常可以在预训练模型的文档中找到。</p>
			<h2 id="_idParaDest-96">细胞神经网络的应用</h2>
			<p>虽然CNN主要用于计算机视觉问题，但重要的是要提到它们解决其他学习问题的能力，主要是在分析数据序列方面。例如，已知CNN在文本、音频和视频序列上表现良好，有时与其他网络架构相结合，或者通过将序列转换成可由CNN处理的图像。使用CNN和数据序列可以解决的一些具体数据问题包括文本的机器翻译、自然语言处理和视频帧标记等。</p>
			<p>CNN可以执行适用于所有监督学习问题的不同任务；然而，这一章将集中讨论计算机视觉。以下是对每项任务的简要说明，以及每项任务的真实示例。</p>
			<h3 id="_idParaDest-97"><a id="_idTextAnchor119"/>分类</h3>
			<p>这是计算机视觉中最常见的任务。主要思想是将图像的一般内容分成一组类别，称为标签。</p>
			<p>例如，分类可以确定图像是狗、猫还是任何其他动物。这种分类是通过输出图像属于每个类别的概率来完成的，如下图所示:</p>
			<div><div><img src="img/B15778_04_03.jpg" alt="Figure 4.3: Classification task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.3:分类任务</p>
			<h3 id="_idParaDest-98"><a id="_idTextAnchor120"/>本地化</h3>
			<p>定位的主要目的是生成描述图像中对象位置的边界框。输出由一个类标签和一个边界框组成。</p>
			<p>该任务可在传感器中使用，以确定物体是在屏幕的左侧还是右侧:</p>
			<div><div><img src="img/B15778_04_04.jpg" alt="Figure 4.4: Localization task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.4:本地化任务</p>
			<h3 id="_idParaDest-99"><a id="_idTextAnchor121"/>检测</h3>
			<p>该任务包括对图像中的所有对象执行对象定位。输出由多个边界框和多个类标签(每个框一个)组成。</p>
			<p>该任务用于建造自动驾驶汽车，目的是能够定位交通标志、道路、其他汽车、行人和任何其他可能与确保安全驾驶体验相关的物体:</p>
			<div><div><img src="img/B15778_04_05.jpg" alt="Figure 4.5: Detection task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.5:检测任务</p>
			<h3 id="_idParaDest-100"><a id="_idTextAnchor122"/>分割</h3>
			<p>这里的任务是输出图像中每个对象的类别标签和轮廓。这主要用于标记图像的重要对象，以便进一步分析。</p>
			<p>例如，该任务可用于严格划定与患者肺部图像中的肿瘤相对应的区域。下图描述了如何勾画感兴趣的对象并为其分配标签:</p>
			<div><div><img src="img/B15778_04_06.jpg" alt="Figure 4.6: Segmentation task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.6:分段任务</p>
			<p>从这一节开始，本章将重点关注使用PyTorch的一个影像数据集训练一个模型来执行影像分类。</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor123"/>CNN的基石</h2>
			<p>深度卷积网络将图像作为输入，并通过一系列的<code>softmax</code>激活函数将图像分类到一个类别标签中。与人工神经网络一样，这种分类形式是通过给每个类别标签一个介于0和1之间的值来计算图像属于每个类别标签的概率。概率较高的类别标签被选为该图像的最终预测。</p>
			<p>下面是对每一层的详细解释，以及如何在PyTorch中定义这些层的代码示例。</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor124"/>卷积层</h3>
			<p>这是从图像中提取特征的第一步。目标是通过学习图像的小部分的特征来保持邻近像素之间的关系。</p>
			<p>在这一层进行数学运算，给出两个输入(图像和滤镜)并获得一个输出。正如我们之前解释的，该操作包括卷积滤波器和与滤波器大小相同的图像部分。对图像的所有子部分重复该操作。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">重温<em class="italic">第2章</em>、<em class="italic">神经网络构建模块</em>的<em class="italic">CNN简介</em>部分，了解输入和滤波器之间执行的精确计算。</p>
			<p>得到的矩阵将具有取决于输入形状的形状，其中大小为(<em class="italic">h</em>x<em class="italic">w</em>x<em class="italic">c</em>的图像矩阵和大小为(<em class="italic">f</em>h x<em class="italic">f</em>w x<em class="italic">c</em>的过滤器)将根据以下等式输出矩阵:</p>
			<div><div><img src="img/B15778_04_07.jpg" alt="Figure 4.7: Output height, width, and depth of a convolutional layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.7:卷积层的输出高度、宽度和深度</p>
			<p>这里，<em class="italic"> h </em>是指输入图像的高度，<em class="italic"> w </em>是宽度，<em class="italic"> c </em>是指深度(也称为通道)，并且<em class="italic"> f </em> h和<em class="italic"> f </em> w是由用户设置的关于滤波器大小的值。</p>
			<p>下图以矩阵的形式描述了这种尺寸转换，其中左边的矩阵表示彩色图像，中间的矩阵表示应用于图像所有通道的单个滤镜，右边的矩阵是图像和滤镜计算的输出:</p>
			<div><div><img src="img/B15778_04_08.jpg" alt="Figure 4.8: Dimensions of the input, filter, and output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.8:输入、滤波器和输出的尺寸</p>
			<p>值得一提的是，在单个卷积层中，几个滤波器可以应用于相同的图像，所有滤波器都具有相同的形状。考虑到这一点，就深度而言，对输入应用两个滤波器的卷积层的输出形状等于2，如下图所示:</p>
			<div><div><img src="img/B15778_04_09.jpg" alt="Figure 4.9: Convolutional layer with two filters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.9:带两个滤波器的卷积层</p>
			<p>这些滤波器中的每一个将执行不同的操作，以便发现图像的不同特征。例如，在具有两个滤波器的单个卷积层中，这些操作可以是垂直边缘检测和水平边缘检测。随着网络层数的增加，过滤器将执行更复杂的操作，利用以前检测到的特征，例如，通过使用来自边缘检测器的输入来检测人的轮廓。</p>
			<p>过滤器通常在每一层中增加。这意味着，虽然第一个卷积层有8个滤波器，但通常会创建第二个卷积层，使其具有两倍的数量(16)，第三个卷积层也具有两倍的数量(32)，依此类推。</p>
			<p>然而，值得一提的是，在PyTorch中，正如在许多其他框架中一样，您应该只定义要使用的过滤器的数量，而不是过滤器的类型(例如，垂直边缘检测器)。每个滤波器配置(它包含的检测特定特征的数字)是系统变量的一部分。</p>
			<p>关于卷积层，还有两个额外的概念要介绍，如下所述。</p>
			<p><strong class="bold">填充</strong></p>
			<p>顾名思义，padding参数用零填充图像。这意味着它向图像的每一侧添加了额外的像素(用零填充)。</p>
			<p>下图显示了一个图像的示例，该图像的每一侧都填充了一个像素:</p>
			<div><div><img src="img/B15778_04_10.jpg" alt="Figure 4.10: Graphical representation of an input image padded by one&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.10:用1填充的输入图像的图形表示</p>
			<p>这用于在输入矩阵通过过滤器后保持其形状。这是因为，尤其是在前几层，目标应该是尽可能多地保留原始输入的信息，以便从中提取最多的特征。</p>
			<p>为了更好地理解填充的概念，请考虑下面的场景。</p>
			<p>将3 x 3滤镜应用于形状为32 x 32 x 3的彩色图像会产生形状为30 x 30 x 1的矩阵。这意味着下一层的输入已经收缩。但是，通过向输入图像添加填充1，输入的形状将变为34 x 34 x 3，从而使用相同的滤波器产生32 x 32 x 1的输出。</p>
			<p>使用填充时，以下公式可用于计算输出宽度:</p>
			<div><div><img src="img/B15778_04_11.jpg" alt="Figure 4.11: Output width after applying a convolutional layer using padding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.11:使用填充应用卷积层后的输出宽度</p>
			<p>这里，<em class="italic"> W </em>是指输入矩阵的宽度，<em class="italic"> F </em>是指滤波器的宽度，<em class="italic"> P </em>是指填充。同样的等式可以适用于计算输出的高度。</p>
			<p>要获得与输入形状相同的输出矩阵，请使用以下等式来计算填充值(假设我们将在下一节中定义的步幅等于1):</p>
			<div><div><img src="img/B15778_04_12.jpg" alt="Figure 4.12: Padding the number to get an output matrix that's equal in shape to the input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.12:填充数字以获得与输入形状相同的输出矩阵</p>
			<p>请记住，输出通道的数量(深度)将始终等于已应用于输入的滤镜数量。</p>
			<p><strong class="bold">跨步</strong></p>
			<p>此参数指的是过滤器在输入矩阵上水平和垂直移动的像素数。正如我们到目前为止所看到的，过滤器通过图像的左上角，然后向右移动一个像素，以此类推，直到它垂直和水平地通过图像的所有部分。此示例是一个跨距等于1的卷积层，这是此参数的默认配置。</p>
			<p>当步幅等于2时，位移将改为两个像素，如下图所示:</p>
			<div><div><img src="img/B15778_04_13.jpg" alt="Figure 4.13: Graphical representation of a convolutional layer with a stride of two&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.13:步长为2的卷积层的图形表示</p>
			<p>可以看出，初始操作发生在左上角；然后，通过向右移动两个像素，在右上角进行第二次计算。接下来，该计算向下移动两个像素以在左下角执行计算，最后，通过再次向右移动两个像素，最终计算发生在右下角。</p>
			<p class="callout-heading">注意</p>
			<p class="callout"><em class="italic">图4.12 </em>中的数字是虚构的，并非实际计算。重点应该放在盒子上，它解释了当步幅等于2时的移动过程。</p>
			<p>使用stride时，以下公式可用于计算输出宽度:</p>
			<div><div><img src="img/B15778_04_14.jpg" alt="Figure 4.14: Output width of a convolutional layer using stride&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.14:使用步幅的卷积层的输出宽度</p>
			<p>这里，<em class="italic"> W </em>是指输入矩阵的宽度，<em class="italic"> F </em>是指滤波器的宽度，<em class="italic"> S </em>是指步幅。同样的等式可以适用于计算输出的高度。</p>
			<p>引入这些参数后，计算从卷积层获得的矩阵的输出形状(宽度和高度)的最终公式如下:</p>
			<div><div><img src="img/B15778_04_15.jpg" alt="Figure 4.15: Output width after a convolutional layer using padding and stride&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.15:使用填充和步幅的卷积层后的输出宽度</p>
			<p>每当值是一个浮点值时，它应该被向下舍入。这基本上意味着输入的某些区域会被忽略，并且不会从中提取任何特征。</p>
			<p>最后，一旦输入已经通过所有滤波器，输出被馈送到激活函数，以便打破线性，类似于传统神经网络的过程。虽然有几个激活函数可以在这个步骤中应用，但首选的是ReLU函数，因为它在CNN中显示了突出的结果。我们在这里获得的输出成为后续层的输入，这通常是一个池层。</p>
			<h2 id="_idParaDest-103">练习4.01:计算卷积层的输出形状</h2>
			<p>使用给定的等式，考虑以下场景并计算输出矩阵的形状:</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这个练习不需要编码，而是由基于我们前面提到的概念的计算组成。</p>
			<ol>
				<li>计算从卷积层导出的矩阵的输出形状，输入形状为64 x 64 x 3，滤波器形状为3 x 3 x 3: <pre>Output height = 64 -3 + 1 = 62 Output width = 64 - 3 + 1 = 62 Output depth = 1</pre></li>
				<li>计算从卷积层导出的矩阵的输出形状，输入形状为32 x 32 x 3，10个过滤器形状为5 x 5 x 3，填充为2: <pre>Output height = 32 - 5 + (2 * 2) + 1 = 32 Output width = 32-5 + (2 * 2) + 1 = 32 Output depth = 10</pre></li>
				<li>计算从卷积层导出的矩阵的输出形状，卷积层的输入形状为128 x 128 x 1，五个滤波器的形状为5 x 5 x 1，步长为3: <pre>Output height = (128 - 5)/ 3 + 1 = 42 Output width = (128 - 5)/ 3 + 1 = 42 Output depth = 5</pre></li>
				<li>计算从卷积层导出的矩阵的输出形状，输入形状为64 x 64 x 1，过滤器形状为8 x 8 x 1，填充为3，步长为3: <pre>Output height = ((64 - 8 + (2 * 3)) / 3) +1 = 21.6 ≈ 21 Output width = ((64 - 8 + (2 * 3)) / 3) +1 = 21.6 ≈ 21 Output depth = 1</pre></li>
			</ol>
			<p>这样，你就成功地计算出了从卷积层得到的矩阵的输出形状。</p>
			<p>在PyTorch中编码一个卷积层非常简单。使用定制模块，只需要创建<code>network</code>类。该类应该包含一个定义网络架构(即网络的各层)的<code>__init__</code>方法和一个定义信息通过各层时要执行的计算的<code>forward</code>方法，如下面的代码片段所示:</p>
			<pre>import torch.nn as nn
import torch.nn.functional as F
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        self.conv1 = nn.Conv2d(3, 18, 3, 1, 1)
    def forward(self, x):
        x = F.relu(self.conv1(x))
        return x</pre>
			<p>定义卷积层时，从左到右传递的参数是指输入通道、输出通道(滤波器数量)、内核大小(滤波器大小)、步幅和填充。</p>
			<p>前面的例子包括一个卷积层，它有三个输入通道，18个滤波器，每个滤波器的大小为3，跨距和填充等于1。</p>
			<p>另一种有效的方法，相当于前面的例子，包括自定义模块的语法和使用<code>Sequential</code>容器的组合，如下面的代码片段所示:</p>
			<pre>import torch.nn as nn
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(3, 18, 3, 1, 1), \
                                   nn.ReLU())
    def forward(self, x):
        x = self.conv1(x)
        return x</pre>
			<p>这里，层的定义发生在<code>Sequential</code>容器内部。通常，一个容器包括一个卷积层、一个激活函数和一个池层。一组新的层包含在它下面的不同容器中。</p>
			<p>在前面的例子中，卷积层和激活层都在<code>Sequential</code>容器中定义。因此，在<code>forward</code>方法中，不需要通过激活函数传递卷积层的输出，因为它已经使用容器进行了处理。</p>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor126"/>汇集层层</h3>
			<p>通常，汇集层是特征选择步骤的最后一部分，这就是为什么汇集层通常位于卷积层之后。正如我们在前面的章节中解释的，这个想法是从图像的子部分中提取最相关的信息。池层的大小通常是两个，跨距等于它的大小。</p>
			<p>池化图层通常会将输入的高度和宽度减半。这一点很重要，因为为了让卷积层找到图像中的所有特征，需要使用几个滤波器，并且该操作的输出可能会变得太大，这意味着需要考虑许多参数。池层旨在通过保留最相关的要素来减少网络中的参数数量。从图像的子部分中选择相关特征是通过获取最大数量或者通过平均该区域中的数量来实现的。</p>
			<p>对于影像分类任务，最常见的是使用最大池图层而不是平均池图层。这是因为前者在保留最相关的特征是关键的任务中表现出更好的结果，而后者已被证明在平滑图像等任务中工作得更好。</p>
			<p>要计算输出矩阵的形状，请使用以下公式:</p>
			<div><div><img src="img/B15778_04_16.jpg" alt="Figure 4.16: Output matrix width after a pooling layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.16:合并图层后的输出矩阵宽度</p>
			<p>这里，<em class="italic"> W </em>是指输入的宽度，<em class="italic"> F </em>是指滤波器的大小，<em class="italic"> S </em>是指步幅。同样的等式可以适用于计算输出高度。</p>
			<p>输入的通道或深度保持不变，因为池层将对图像的所有通道执行相同的操作。这意味着池化图层的结果仅影响宽度和长度方面的输入。</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor127"/>练习4.02:计算一组卷积层和汇集层的输出形状</h2>
			<p>以下练习将结合卷积层和池层。目标是在经过一组层之后确定输出矩阵的大小。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这个练习不需要编码，而是由基于我们前面提到的概念的计算组成。</p>
			<p>考虑以下图层组，并在所有变换结束时指定输出图层的形状，考虑大小为256 x 256 x 3的输入图像:</p>
			<ol>
				<li value="1">具有16个大小为3的滤波器以及步长和填充为1的卷积层。</li>
				<li>具有大小为2的过滤器和大小为2的步幅的池层。</li>
				<li>一个卷积层，有八个大小为7、步幅为1、填充为3的滤波器。</li>
				<li>A pooling layer with a filter of size two and a stride of two as well.<p>经过每一层后，矩阵的输出大小如下:</p></li>
			</ol>
			<ol>
				<li value="1">After the first convolutional layer:<p><code>output_width/height</code>=((256–3)+2 * 1)/1+1 = 256</p><p><code>output_channels</code> =应用了16个过滤器</p><p><code>output_matrix_size</code>= 256×256×16</p></li>
				<li>After the first pooling layer:<p><code>output_width/height</code>=(256–2)/2+1 = 128</p><p><code>output_channels</code> = 16作为合用不影响通道数</p><p><code>output_matrix_size</code>= 128×128×16</p></li>
				<li>After the second convolutional layer:<p><code>output_width/height</code>=((128–7)+2 = * 3)/1+1 = 128</p><p><code>output_channels</code> =应用了8个过滤器</p><p><code>output_matrix_size</code>= 128×128×8</p></li>
				<li>After the second pooling layer:<p><code>output_width/height</code>=(128–2)/2+1 = 64</p><p><code>output_channels</code> = 8，因为池化不影响通道数</p><p><code>output_matrix_size</code>= 64×64×8</p></li>
			</ol>
			<p>至此，您已经成功地计算出了从一系列卷积层和池层导出的矩阵的输出形状。</p>
			<p>使用与前面相同的编码示例，定义池层的PyTorch方法如下面的代码片段所示:</p>
			<pre>import torch.nn as nn
import torch.nn.functional as F
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        self.conv1 = nn.Conv2d(3, 18, 3, 1, 1)
        self.pool1 = nn.MaxPool2d(2, 2)
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        return x</pre>
			<p>可以看出，在<code>__init__</code>方法中，一个池层(<code>MaxPool2d</code>)被添加到网络架构中。这里，进入最大池层的参数从左到右是过滤器的大小(<code>2</code>)和步幅(<code>2</code>)。接下来，更新了<code>forward</code>方法，通过新的池层传递信息。</p>
			<p>同样，这里展示了一个同样有效的方法，使用定制模块和<code>Sequential</code>容器:</p>
			<pre>import torch.nn as nn
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(3, 18, 3, 1, 1),\
                                   nn.ReLU(),\
                                   nn.MaxPool2d(2, 2))
    def forward(self, x):
        x = self.conv1(x)
        return x</pre>
			<p>正如我们前面提到的，池层也包含在与卷积层相同的容器中，在激活函数下面。下面将在一个新的<code>Sequential</code>容器中定义一组后续层(卷积、激活和池化)。</p>
			<p>再次，<code>forward</code>方法不再需要单独调用每一层；相反，它通过容器传递信息，容器中保存了层和激活函数。</p>
			<h3 id="_idParaDest-106"><a id="_idTextAnchor128"/>完全连接的层</h3>
			<p>在输入通过一组卷积层和池层之后，在网络体系结构的末端定义FC层。来自第一FC层之前的层的输出数据被从矩阵展平成向量，该向量可以被馈送到FC层(与传统神经网络的隐藏层相同)。</p>
			<p>这些FC层的主要目的是考虑由先前层检测到的所有特征，以便对图像进行分类。</p>
			<p>不同的FC层通过激活函数传递，该函数通常是ReLU函数，除非它是最终层，该层将使用softmax函数来输出属于每个类标签的输入的概率。</p>
			<p>第一FC层的输入大小对应于前一层的展平输出矩阵的大小。输出大小是由用户定义的，同样，与人工神经网络一样，没有精确的科学来设置这个数字。最后一个FC层的输出大小应该等于类标签的数量。</p>
			<p>要在PyTorch中定义一组FC层，请考虑以下代码片段:</p>
			<pre>import torch.nn as nn
import torch.nn.functional as F
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 18, 3, 1, 1)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.linear1 = nn.Linear(32*32*16, 64)
        self.linear2 = nn.Linear(64, 10)
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = x.view(-1, 32 * 32 *16)
        x = F.relu(self.linear1(x))
        x = F.log_softmax(self.linear2(x), dim=1)
        return x</pre>
			<p>使用与上一节相同的编码示例，两个FC层被添加到网络的<code>__init__</code>方法中。接下来，在<code>forward</code>函数中，使用<code>view()</code>函数展平池层的输出。然后，它通过应用激活功能的第一个FC层。最后，数据连同其激活功能一起通过最终的FC层。</p>
			<p>同样，使用与前面相同的编码示例，可以使用自定义模块和<code>Sequential</code>容器向我们的模型添加FC层，如下所示:</p>
			<pre>import torch.nn as nn
class CNN_network(nn.Module):
    def __init__(self):
        super(CNN_network, self).__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 5, 1, 2,), \
                                   nn.ReLU(), \
                                   nn.MaxPool2d(2, 2))
        self.linear1 = nn.Linear(32*32*16, 64)
        self.linear2 = nn.Linear(64, 10)
    def forward(self, x):
        x = self.conv1(x)
        x = x.view(-1, 32 * 32 *16)
        x = F.relu(self.linear1(x))
        x = F.log_softmax(self.linear2(x), dim=1)
        return x</pre>
			<p>可以看到，<code>Sequential</code>容器保持不变，两个FC层被添加到下面的<code>__init__</code>方法中。接下来，<code>forward</code>函数通过整个容器传递信息，然后将输出扁平化，通过FC层传递。</p>
			<p>一旦确定了网络的结构，就可以像处理人工神经网络一样处理训练网络的下列步骤。</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor129"/>旁注–从PyTorch下载数据集</h2>
			<p>要从PyTorch加载数据集，请使用以下代码。除了下载数据集之外，以下代码还显示了如何使用数据加载器通过批量加载图像(而不是一次全部加载)来节省资源:</p>
			<pre>from torchvision import datasets
import torchvision.transforms as transforms
transform = \
transforms.Compose([transforms.ToTensor(), \
                    transforms.Normalize((0.5, 0.5, 0.5), \
                                         (0.5, 0.5, 0.5))])</pre>
			<p><code>transform</code>变量用于定义要在数据集上执行的一组转换。在这种情况下，数据集将被转换为张量，并在其所有维度上进行归一化。</p>
			<pre>train_data = datasets.MNIST(root='data', train=True,\
                            download=True, transform=transform)
test_data = datasets.MNIST(root='data', train=False,\
                           download=True, transform=transform)</pre>
			<p>在前面的代码中，要下载的数据集是MNIST。这是一个流行的数据集，包含从0到9的手写灰度数字图像。PyTorch数据集提供了训练集和测试集。</p>
			<p>从前面的代码片段中可以看出，为了下载数据集，有必要定义数据的根，默认情况下，应该定义为<code>data</code>。接下来，定义您下载的是定型数据集还是测试数据集。我们将参数<code>download</code>设置为<code>True</code>。最后，我们使用之前定义的<code>transform</code>变量对数据集执行转换:</p>
			<pre>dev_size = 0.2
idx = list(range(len(train_data)))
np.random.shuffle(idx)
split_size = int(np.floor(dev_size * len(train_data)))
train_idx, dev_idx = idx[split_size:], idx[:split_size]</pre>
			<p>考虑到我们需要第三组数据(验证集)，前面的代码片段用于将训练集分成两组。首先，定义验证集的大小，然后定义将用于每个数据集的索引列表(定型集和验证集):</p>
			<pre>train_sampler = SubsetRandomSampler(train_idx)
dev_sampler = SubsetRandomSampler(dev_idx)</pre>
			<p>在前面的代码片段中，PyTorch的<code>SubsetRandomSampler()</code>函数用于通过随机抽样索引将原始训练集分成训练集和验证集。这将在下面的步骤中使用，以生成将在每次迭代中输入到模型中的批:</p>
			<pre>batch_size = 20
train_loader = torch.utils.data.DataLoader(train_data, \
                                           batch_size=batch_size, \
                                           sampler=train_sampler)
dev_loader = torch.utils.data.DataLoader(train_data, \
                                         batch_size=batch_size, \
                                         sampler=dev_sampler)
test_loader = torch.utils.data.DataLoader(test_data, \
                                          batch_size=batch_size)</pre>
			<p><code>DataLoader()</code>功能用于批量加载每组数据的图像。首先，包含集合的变量作为参数传递，然后定义批量大小。最后，我们在上一步中创建的采样器用于确保每次迭代中使用的批次是随机创建的，这有助于提高模型的性能。该函数的结果变量(<code>train_loader</code>、<code>dev_loader</code>和<code>test_loader</code>)将分别包含特征和目标的值。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">问题越复杂，网络越深入，模型训练的时间就越长。考虑到这一点，本章中的活动可能会比前几章中的活动花费更长的时间。</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor130"/>活动4.01:为图像分类问题构建一个CNN</h2>
			<p>在本练习中，CNN将在PyTorch的图像数据集上进行训练(即，框架提供数据集)。将使用的数据集是CIFAR10，它包含总共60，000幅车辆和动物图像。有10种不同的类别标签(如“飞机”、“鸟”、“汽车”、“猫”等)。训练集包含50，000幅图像，而测试集包含剩余的10，000幅图像。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">要进一步探索这个数据集，请访问以下网址:<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>。</p>
			<p>让我们看一下我们的场景。你在一家人工智能公司工作，该公司根据客户的需求开发定制模型。您的团队目前正在创建一个模型，该模型可以区分车辆图片和动物图片，更具体地说，可以识别不同种类的动物和不同类型的车辆。他们为您提供了包含60，000幅图像的数据集来构建模型。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">本章中的活动可能需要很长时间才能在普通计算机(CPU)上完成。为了在GPU上运行代码，在本书的GitHub资源库中，每个活动都有一个等价的文件。</p>
			<ol>
				<li value="1">导入所需的库。</li>
				<li>设置要对数据执行的变换，即数据到张量的转换和像素值的归一化。</li>
				<li>设置100个图像的批量大小，并从<code>CIFAR10</code>数据集中下载训练和测试数据。</li>
				<li>使用20%的验证大小，定义将用于将数据集分成这两组的定型和验证采样器。</li>
				<li>使用<code>DataLoader()</code>功能定义用于每组数据的批次。</li>
				<li>Define the architecture of your network. Use the following information to do so:<p>Conv1:卷积层，将彩色图像作为输入，并通过10个大小为3的过滤器。填充和步幅都应设置为1。</p><p>Conv2:将输入数据通过20个大小为3的过滤器的卷积层。填充和步幅都应设置为1。</p><p>Conv3:将输入数据通过40个大小为3的过滤器的卷积层。填充和步幅都应设置为1。</p><p>在每个卷积层之后使用ReLU激活函数。</p><p>每个卷积层之后的池层，过滤器大小和步幅为2。</p><p>展平图像后设定为20%的漏失项。</p><p>Linear1:完全连接的层，接收来自前一层的展平矩阵作为输入，并生成100个单位的输出。对该层使用ReLU激活功能。这里，辍学项设置为20%。</p><p>Linear2:生成10个输出的全连接层，每个类别标签一个输出。使用输出层的<code>log_softmax</code>激活功能。</p></li>
				<li>定义训练模型所需的所有参数。将历元数设置为50。</li>
				<li>训练您的网络，并确保保存训练集和验证集的损失和准确性值。</li>
				<li>绘制两组的损耗和精度。</li>
				<li>Check the model's accuracy on the testing set—it should be around 72%.<p class="callout-heading">注意</p><p class="callout">这项活动的解决方案可在第262页找到。</p><p class="callout">由于数据在每个时期都被混洗，结果将无法精确再现。然而，你应该能够得到与本书中相似的结果。</p><p class="callout">这段代码可能需要时间来运行，这就是为什么本书的GitHub资源库中提供了等效的GPU版本解决方案。</p></li>
			</ol>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor131"/>数据增强</h1>
			<p>学习如何有效地对神经网络进行编码是开发性能良好的解决方案的步骤之一。此外，要开发出色的深度学习解决方案，找到一个我们可以为当前挑战提供解决方案的感兴趣领域至关重要。但一旦所有这些都完成了，我们通常会面临同样的问题:通过自我收集或从互联网和其他可用来源下载，获得一个适当大小的数据集，以从我们的模型中获得良好的性能。</p>
			<p>正如你可能想象的那样，尽管现在收集和存储大量数据是可能的，但由于相关的成本，这不是一项容易的任务。因此，大部分时间，我们都在处理包含数万个条目的数据集，而涉及到图像时就更少了。</p>
			<p>当开发计算机视觉问题的解决方案时，这成为一个相关的问题，主要是由于两个原因:</p>
			<ul>
				<li>The larger the dataset, the better the results, and larger datasets are crucial to arrive at decent enough models. This is true considering that training a model is a matter of tuning a bunch of parameters so that it is capable of mapping a relationship between an input and an output. This is achieved by minimizing the loss function to make the predicted value come as close as possible to the ground truth. Here, the more complex the model, the more parameters it requires.<p>考虑到这一点，有必要向模型提供相当数量的示例，以便它能够找到这样的模式，其中训练示例的数量应该与要调整的参数的数量成比例。</p></li>
				<li>计算机视觉问题中最大的挑战之一是让你的模型在图像的几种变化上表现良好。这意味着图像不需要遵循特定的对齐方式或具有固定的质量，而是可以以其原始格式进行馈送，包括不同的位置、角度、光照和其他失真。正因为如此，有必要找到一种方法来为模型提供这种变化。</li>
			</ul>
			<p>因此，设计了数据扩充技术。简单来说，就是通过对现有的例子稍加修改来增加训练例子数量的措施。例如，您可以复制当前可用的实例，并向这些副本添加一些噪声，以确保它们不完全相同。</p>
			<p>在计算机视觉问题中，这意味着通过改变现有图像来增加训练数据集中的图像数量，这可以通过稍微改变当前图像来创建稍微不同的复制版本来完成。</p>
			<p>这些对图像的微小调整可以是轻微旋转、改变对象在帧中的位置、水平或垂直翻转、不同的配色方案和扭曲等形式。这种技术是可行的，因为CNN会将这些图像中的每一个视为不同的图像。</p>
			<p>例如，下面的图像显示了一只狗的三个图像，虽然对人眼来说是相同的图像，但有某些变化，但对神经网络来说却完全不同:</p>
			<div><div><img src="img/B15778_04_17.jpg" alt="Figure 4.17: Augmented images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.17:增强图像</p>
			<p>不管变化如何，能够识别图像中的对象的CNN被认为具有不变性。事实上，CNN可以对每种类型的变异保持不变。</p>
			<h2 id="_idParaDest-110">使用PyTorch进行数据增强</h2>
			<p>使用<code>torchvision</code>包在PyTorch中执行数据扩充非常容易。这个包除了包含流行的数据集和模型架构之外，还包含可以在数据集上执行的常见图像转换函数。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">在这一节中，我们将提到一些图像变换。要获得可能的转变的完整列表，请访问<a href="https://pytorch.org/docs/stable/torchvision/transforms.html">https://pytorch.org/docs/stable/torchvision/transforms.html</a>。</p>
			<p>与我们在之前的活动中使用的将数据集归一化并转换为张量的过程一样，执行数据扩充需要我们定义所需的转换，然后将它们应用到数据集，如以下代码片段所示:</p>
			<pre>transform = transforms.Compose([\
            transforms.Ho<a id="_idTextAnchor133"/>rizontalFlip(probability_goes_here),\
            transforms.RandomGrayscale(probability_goes_here),\
            transforms.ToTensor(),\
            transforms.Normalize((0.5, 0.5, 0.5), \
                                 (0.5, 0.5, 0.5))])
train_data = datasets.CIFAR10('data', train=True, \
                              download=True, transform=transform)
test_data = datasets.CIFAR10('data', train=False, \
                             download=True, transform=transform)</pre>
			<p>这里，使用<code>HorizontalFlip</code>函数，要下载的数据将经历水平翻转(考虑一个概率值，该值由用户设置，并确定将经历该变换的图像的百分比)。通过使用<code>RandomGrayscale</code>功能，图像将被转换成灰度(也考虑概率)。然后，将数据转换为张量并归一化。</p>
			<p>考虑到模型是在迭代过程中训练的，其中训练数据被馈送多次，这些变换确保通过数据集的第二次运行不会向模型馈送完全相同的图像。</p>
			<p>而且，可以为不同的集合设置不同的变换。这很有用，因为数据扩充的目的是增加训练样本的数量，但用于测试模型的图像应该保持不变。然而，应该调整测试集的大小，以便为模型提供相同大小的图像。</p>
			<p>这可以通过以下方式实现:</p>
			<pre>transform = {"train": \
transforms.Compose([transforms.RandomHorizontalFlip\
                    (probability_goes_here),\
                    transforms.RandomGrayscale\
                    (probability_goes_here),\
                    transforms.ToTensor(),\
                    transforms.Normalize\
                    ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]), \
                    "test": transforms.Compose([transforms.ToTensor(),\
                                                transforms.Normalize\
                                                ((0.5, 0.5, 0.5), \
                                                 (0.5, 0.5, 0.5)),\
                    transforms.Resize(size_goes_here)])}
train_data = datasets.CIFAR10('data', train=True, download=True, \
             transform=transform["train"])
test_data = datasets.CIFAR10('data', train=False, download=True, \
            transform=transform["test"])</pre>
			<p>正如我们所看到的，定义了一个字典，其中包含一组用于训练集和测试集的转换。然后，可以调用字典来相应地将转换应用于每个集合。</p>
			<h2 id="_idParaDest-111">活动4.02: Im <a id="_idTextAnchor134"/>实施数据扩充</h2>
			<p>在本活动中，我们将在之前活动中创建的模型中引入数据扩充，以测试其准确性是否可以提高。让我们看看下面的场景。</p>
			<p>您创建的模型是好的，但是它的准确性没有达到期望的水平。你被要求想出一个可以提高模型性能的方法。按照以下步骤完成本活动:</p>
			<ol>
				<li value="1">复制上一个活动中的笔记本。</li>
				<li>Change the definition of the <code>transform</code> variable so that it includes, in addition to normalizing and converting the data into tensors, the following transformations:<p>对于训练/验证集，概率为50% (0.5)的<code>RandomHorizontalFlip</code>函数和概率为10% (0.1)的<code>RandomGrayscale</code>函数。</p><p>对于测试集，不要添加任何其他的转换。</p></li>
				<li>Train the model for 100 epochs. The resulting plots for loss and accuracy on the training and validation sets should be similar to the ones shown here:<div><img src="img/B15778_04_18.jpg" alt="Figure 4.18: Resulting plot showing the loss of the sets&#13;&#10;"/></div><p class="figure-caption">图4.18:显示器械包丢失的结果图</p><div><img src="img/B15778_04_19.jpg" alt="Figure 4.19: Resulting plot showing the accuracy of the sets&#13;&#10;"/></div><p class="figure-caption">图4.19:显示集合精确度的结果图</p><p class="callout-heading">注意</p><p class="callout">由于每个时期中数据的混洗，结果将不能精确再现。但是，您应该能够得到类似的结果。</p></li>
				<li>在测试集上计算结果模型的准确性。</li>
			</ol>
			<p>预期输出:模型在测试集上的性能应该在75%左右。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这项活动的解决方案可在第272页找到。</p>
			<p class="callout">这段代码可能需要时间来运行，这就是为什么本书的GitHub资源库中提供了等效的GPU版本解决方案。</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor135"/>批量归一化</h1>
			<p>通常会对输入图层进行归一化，以加快学习速度，并通过将所有要素重新缩放到相同比例来提高性能。因此，问题是，如果模型受益于输入层的归一化，为什么不归一化所有隐藏层的输出，以试图进一步提高训练速度？</p>
			<p><strong class="bold">批量归一化</strong>，顾名思义，对隐藏层的输出进行归一化，从而减少每层的方差，这也称为协方差偏移。协方差偏移的这种减少是有用的，因为它允许模型在遵循与用于训练它的图像不同的分布的图像上也很好地工作。</p>
			<p>举个例子，一个以检测动物是否是猫为目的的网络。当仅使用黑猫的图像来训练网络时，批量标准化还可以通过标准化数据来帮助网络对不同颜色的猫的新图像进行分类，使得黑色和彩色的猫图像都遵循相似的分布。下图显示了这样一个问题:</p>
			<div><div><img src="img/B15778_04_20.jpg" alt="Figure 4.20: Cat classifier – the model can recognize colored cats, &#13;&#10;even after being trained using only black cats&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图4.20:猫分类器——该模型可以识别有色猫，即使只使用黑猫进行训练</p>
			<p>此外，批处理规范化为模型定型过程带来了以下好处，最终有助于获得性能更好的模型:</p>
			<ul>
				<li>它允许设置更高的学习率，因为批量标准化有助于确保输出不会过高或过低。更高的学习率相当于更快的学习时间。</li>
				<li>It helps reduce overfitting because it has a regularization effect. This makes it possible to set the dropout probability to a lower value, which means that less information is ignored in each forward pass.<p class="callout-heading">注意</p><p class="callout">我们不应该主要依靠批量标准化来处理过度拟合。</p></li>
			</ul>
			<p>正如我们在前面的章节中所解释的，归一化一个隐藏层的输出是通过减去批平均值并除以批标准偏差来完成的。</p>
			<p>此外，批量标准化通常在卷积层以及FC层(不包括输出层)上执行。</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor136"/>使用PyTorch进行批量标准化</h2>
			<p>在PyTorch中，考虑到有两种不同的类型，添加批处理规范化就像向网络架构添加一个新层一样简单，如下所述:</p>
			<ul>
				<li><strong class="bold">batch normal 1d</strong>:该层用于对二维或三维输入进行批量归一化。它接收前一层的输出节点数作为参数。这通常用于FC层。</li>
				<li><strong class="bold"> BatchNorm2d </strong>:对四维输入进行批量归一化。同样，它采用的参数是前一层的输出节点数。它通常用于卷积层，这意味着它接受的参数应该等于前一层的通道数。</li>
			</ul>
			<p>据此，CNN中批量归一化的实现如下:</p>
			<pre>class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)
        self.norm1 = nn.BatchNorm2d(16)
        self.pool = nn.MaxPool2d(2, 2)
        self.linear1 = nn.Linear(16 * 16 * 16, 100)
        self.norm2 = nn.BatchNorm1d(100)
        self.linear2 = nn.Linear(100, 10)
    def forward(self, x):
        x = self.pool(self.norm1(F.relu(self.conv1(x))))
        x = x.view(-1, 16 * 16 * 16)
        x = self.norm2(F.relu(self.linear1(x)))
        x = F.log_softmax(self.linear2(x), dim=1)
        return x</pre>
			<p>正如我们所见，批处理规范化层最初是以类似于<code>__init__</code>方法中任何其他层的方式定义的。接下来，在<code>forward</code>方法内的激活函数之后，将每个批处理规范化层应用于其对应层的输出。</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor137"/>活动4.03:实现批量规范化</h2>
			<p>对于本活动，我们将对上一活动的架构实施批量标准化，以查看是否有可能在测试集上进一步提高模型的性能。让我们看看下面的场景。</p>
			<p>你上次在表现方面的进步给你的队友留下了深刻的印象，现在他们对你有了更高的期望。他们要求你最后一次尝试改进模型，使准确率达到80%。按照以下步骤完成本活动:</p>
			<ol>
				<li value="1">复制上一个活动中的笔记本。</li>
				<li>向每个卷积层以及第一个FC层添加批量归一化。</li>
				<li>Train the model for 100 epochs. The resulting plots of the loss and accuracy of the training and validation sets should be similar to the ones shown here:<div><img src="img/B15778_04_21.jpg" alt="Figure 4.21: Resulting plot showing the loss of the sets&#13;&#10;"/></div><p class="figure-caption">图4.21:显示器械包损失的结果图</p><div><img src="img/B15778_04_22.jpg" alt="Figure 4.22: Resulting plot showing the loss of the sets&#13;&#10;"/></div><p class="figure-caption">图4.22:显示器械包丢失的结果图</p></li>
				<li>Calculate the accuracy of the resulting model on the testing set—it should be around 78%.<p class="callout-heading">注意</p><p class="callout">这项活动的解决方案可在第274页找到。</p><p class="callout">由于在每个时期混洗数据，结果将不能精确再现。然而，你应该能够得到与本书中相似的结果。</p><p class="callout">这段代码可能需要时间来运行，这就是为什么本书的GitHub资源库中提供了等效的GPU版本解决方案。</p></li>
			</ol>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor139"/>总结</h1>
			<p>这一章集中讨论了CNN，它由一种在计算机视觉问题上表现出色的神经网络结构组成。我们首先解释了CNN被广泛用于处理图像数据集的主要原因，并介绍了通过使用它们可以解决的不同任务。</p>
			<p>本章通过解释卷积层、池层以及光纤通道层的性质，解释了网络架构的不同构建模块。在每一节中，都包含了对每一层的用途的解释，以及可用于在PyTorch中有效地对架构进行编码的代码片段。</p>
			<p>这导致了图像分类问题的引入，该问题集中于对车辆和动物的图像进行分类。这个问题的目的是将CNN的不同构件付诸实践，以解决图像分类数据问题。</p>
			<p>接下来，数据扩充作为一种工具被引入，通过增加训练样本的数量来提高网络的性能，而无需收集更多的图像。这种技术关注于创建现有图像的变体，以创建“新的”图像来提供给模型。</p>
			<p>通过实施数据扩充，本章的第二项活动旨在解决相同的图像分类问题，目的是比较结果。</p>
			<p>最后，本章解释了批处理规范化的概念。这包括标准化每个隐藏层的输出，以加速学习。在解释了在PyTorch中应用批量归一化的过程之后，本章的最后一个活动旨在使用批量归一化来解决相同的图像分类问题。</p>
			<p>现在，CNN的概念已经很清楚，并且已经被应用于解决一个计算机视觉问题，在下一章，我们将探索一个更复杂的应用CNN来创建图像，而不仅仅是对它们进行分类。</p>
		</div>
		<div><div/>
		</div>
	

</body></html>