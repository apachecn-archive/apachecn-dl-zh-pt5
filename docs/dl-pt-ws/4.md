

# 五、风格迁移

概观

本章解释了使用预训练模型来创建或利用性能良好的算法而无需收集大量数据的过程。在本章中，您将学习如何从 PyTorch 加载一个预先训练好的模型，以便创建一个风格迁移模型。本章结束时，你将能够通过使用预训练的模型进行风格转换。

# 简介

前一章解释了传统 CNN 的不同组成部分，以及提高其性能和减少训练时间的一些技术。那里解释的架构虽然典型，但并不是一成不变的，CNN 架构的扩散已经出现，以解决不同的数据问题，最常见的是在计算机视觉领域。

这些架构在配置和学习任务方面各不相同。现在非常流行的一个是由牛津机器人研究所的卡伦·西蒙扬和安德鲁·齐泽曼创建的**视觉几何组** ( **VGG** )架构。它是为对象识别而开发的，并由于网络所依赖的大量参数而实现了最先进的性能。它在数据科学家中受欢迎的一个主要原因是经过训练的模型的参数(权重和偏差)的可用性，这使得研究人员可以在没有训练的情况下使用它，以及模型的出色性能。

在这一章中，我们将使用这个预训练的模型来解决一个计算机视觉问题，这个问题由于专门分享图像的社交媒体渠道的流行而特别出名。它包括执行样式转换，以便创建一个具有一个图像的样式(颜色和纹理)和另一个图像的内容(形状和对象)的新图像。

当在社交媒体个人资料上发布时，在常规图像上应用过滤器以提高它们的质量和吸引力时，这项任务每天被执行数百万次。虽然这看起来是一个简单的任务，但本章将解释这些图像编辑应用程序背后的神奇之处。

注意

本章中的所有代码都可以在[https://packt.live/2yiR97z](https://packt.live/2yiR97z)找到。

# 风格迁移

简而言之，风格转换包括修改图像的风格，同时仍然保留其内容。例如，拍摄一幅动物图像，然后将该样式转换为类似莫奈的绘画，如下图所示:

![Figure 5.1: Style transfer inputs and output – the result of the final exercise of this chapter
](img/B15778_05_01.jpg)

图 5.1:风格迁移输入和输出——本章最后一个练习的结果

注意

这张图片可以在 https://packt.live/2XEykpL[的 GitHub 上找到。](https://packt.live/2XEykpL)

根据前面的图像，模型有两个输入:内容图像和样式图像。内容是指图像的对象，而风格是指颜色和纹理。因此，来自模型的输出应该是包含来自内容图像的对象和样式图像的艺术外观的图像。

## 它是如何工作的？

与解决传统的计算机视觉问题不同，风格转换需要不同的步骤来有效地将两幅图像作为输入，并创建一幅新图像作为输出。

以下是解决风格转换问题时所遵循的步骤的简要说明:

1.  **输入**:内容和样式图像都要输入到模型中，它们需要是相同的形状。这里的一个常见做法是调整样式图像的大小，使其与内容图像的形状相同。
2.  `models`子包执行风格迁移任务，不需要用大量图像训练网络。
3.  **确定图层的功能**:考虑到手头有两个主要任务(识别一幅图像的内容和区分另一幅图像的风格)，不同的图层将有不同的功能来提取不同的特征。对于风格图像，重点应该是颜色和纹理，而对于内容图像，重点应该是边缘和形式。在这个步骤中，不同的层被分成不同的任务。
4.  **Defining the optimization problem**: As with any other supervised problem, it is necessary to define a loss function, which will have the responsibility of measuring the difference between the output and inputs. Unlike other supervised problems, the task of style transfer requires you to define three different loss functions, all of which should be minimized during the training process. The three loss functions are explained here:

    **内容损失**:这测量内容图像和输出之间的距离，同时仅考虑与内容相关的特征。

    **风格损失**:仅考虑与风格相关的特征时，测量风格图像和输出之间的距离。

    **总损失**:这结合了内容和风格损失。这两种损失都有一个与之相关的权重，用于确定它们是否参与总损失的计算。

5.  **参数更新**:该步骤使用渐变来更新模型的不同参数。

# 使用 VGG-19 网络架构实现风格迁移

VGG-19 是由 19 层组成的 CNN。它使用 ImageNet 数据库中的数百万张图像进行训练。该网络能够将图像分为 1000 个不同的类别，包括大量的动物和不同的工具。

注意

要探索 ImageNet 数据库，请访问以下 URL:[http://www.image-net.org/](http://www.image-net.org/)。

考虑到其深度，该网络能够从各种各样的图像中识别复杂的特征，这使得它特别适合于风格迁移问题，其中特征提取在不同阶段和不同目的中是至关重要的。

这一节将集中讨论如何使用预训练的 VGG-19 模型进行风格转换。本章的最终目标是采用一幅动物或风景的图像(作为内容图像)和一幅著名艺术家的画作(作为风格图像)来创建一个具有艺术风格的常规对象的新图像。

但是，在深入研究这一过程之前，下面是一个导入列表及其用途的简要说明:

*   `NumPy`:这将用于转换要显示的图像。
*   `torch`、`torch.nn`和`torch.optim`:这些将实现神经网络并定义优化算法。
*   `PIL.Image`: This will load the images, as per the following code snippet:

    ```
    image = Image.open(image_name)
    image = transformation(image).unsqueeze(0)
    ```

    可以看出，第一步包括打开图像(这里，`image_name`应该替换为图像的路径)。接下来，先前定义的任何变换都可以应用于图像。

    注意

    为了提醒如何定义图像的变换，请重温*第 4 章*、*卷积神经网络*。

    `unsqueeze()`功能用于根据向 VGG-19 模型馈送图像的要求，向图像添加额外的维度。

*   `matplotlib.pyplot`:这将显示图像。
*   `torchvision.transforms`和`torchvision.models`:这将把图像转换成张量并加载预训练的模型。

## 输入–加载和显示

执行样式转换的第一步包括加载内容和样式图像。在该步骤中，处理基本的预处理，其中图像必须具有相等的大小(优选地，用于训练预训练模型的图像的大小)，这也将是输出图像的大小。此外，图像被转换为 PyTorch 张量，如果需要，可以进行归一化。

显示已经加载的图像，以确保它们符合要求，这始终是一个很好的做法。考虑到图像已经被转换成张量并在这一点上被归一化，张量应该被克隆，并且需要执行一组新的转换，以便我们可以使用 Matplotlib 显示它们。这意味着张量应转换回 **Python 图像库** ( **PIL** )图像，并且必须恢复归一化过程，如下例所示:

```
image = tensor.clone()
image = image.squeeze(0)
img_display = \
transforms.Compose([transforms.Normalize((-0.5/0.25, \
                                          -0.5/0.25, -0.5/0.25), \
                                         (1/0.25, 1/0.25, \
                                          1/0.25)), \
              transforms.ToPILImage()])
```

首先，张量被克隆，额外的维度被移除。接下来，定义转换。

为了理解恢复归一化的过程，考虑对于所有维度，使用 0.5 作为平均值，0.25 作为标准偏差来归一化的图像。恢复归一化的方法是使用平均值除以标准偏差的负值作为平均值(-0.5 除以 0.25)。新的标准差应该等于 1 除以标准差(1 除以 0.25)。定义加载和显示图像的函数有助于节省时间，并确保对内容和样式图像进行相同的处理。这个过程将在下面的练习中展开。

注意

本章的所有练习都将被编码在同一个笔记本中，因为组合的代码将一起执行风格转换任务。

## 练习 5.01:加载和显示图像

这是执行风格迁移的四个步骤中的第一步。本练习的目标是加载和显示将在后续练习中使用的图像(内容和样式图像)。按照以下步骤完成本练习:

注意

对于本章中的练习和活动，您需要安装 Python 3.7、Jupyter 6.0、Matplotlib 3.1、NumPy 1.17、Pillow 6.2 和 PyTorch 1.3+(最好是 PyTorch 1.4，有或没有 CUDA)(按照*前言*中的说明)。

在这本书的 GitHub 知识库([https://packt.live/2yiR97z](https://packt.live/2yiR97z))中，你将能够找到不同的图片，这些图片将在本章的不同练习和活动中使用。

1.  Import all the packages that will be required to perform style transfer:

    ```
    import numpy as np
    import torch
    from torch import nn, optim
    from PIL import Image
    import matplotlib.pyplot as plt
    from torchvision import transforms, models
    ```

    如果你有一个可用的 GPU，定义一个名为`device`等于`cuda`的变量，它将被用来分配一些变量给你机器的 GPU:

    ```
    device = "cuda"
    device
    ```

2.  Set the image size to be used for both images. Also, set the transformations to be performed over the images, which should include resizing the images, converting them into tensors, and normalizing them:

    ```
    imsize = 224
    loader = transforms.Compose([\
             transforms.Resize(imsize), \
             transforms.ToTensor(), \
             transforms.Normalize((0.485, 0.456, 0.406), \
                                  (0.229, 0.224, 0.225))])
    ```

    使用该代码，图像被调整到与最初用于训练 VGG-19 模型的图像相同的大小。使用用于归一化训练图像的相同值来完成归一化。

    注意

    使用归一化图像来训练 VGG 网络，其中每个通道分别具有 0.485、0.456 和 0.406 的平均值，以及分别为 0.229、0.224 和 0.225 的标准偏差。

3.  定义一个接收图像路径作为输入的函数，并使用`PIL`打开图像。接下来，它应该将变换应用到图像:

    ```
    def image_loader(image_name):     image = Image.open(image_name)     image = loader(image).unsqueeze(0)     return image
    ```

4.  Call the function to load the content and style images. Use the dog image as the content image and the Matisse image as the style image, both of which are available in this book's GitHub repository:

    ```
    content_img = image_loader("images/dog.jpg")
    style_img = image_loader("images/matisse.jpg")
    ```

    如果您的计算机有可用的 GPU，请使用以下代码片段来实现相同的结果:

    ```
    content_img = image_loader("images/dog.jpg").to(device)
    style_img = image_loader("images/matisse.jpg").to(device)
    ```

    前面的代码片段将保存图像的变量分配给 GPU，以便使用这些变量的所有操作都由 GPU 处理。

5.  要显示图像，请将它们转换回 PIL 图像，然后恢复正常化过程。在变量中定义这些转换:

    ```
    unloader = transforms.Compose([\            transforms.Normalize((-0.485/0.229, \                                  -0.456/0.224, \                                  -0.406/0.225), \                                 (1/0.229, 1/0.224, 1/0.225)),\            transforms.ToPILImage()])
    ```

6.  Create a function that clones the tensor, squeezes it, and applies the transformations defined in the previous step to the tensor:

    ```
    def tensor2image(tensor):
        image = tensor.clone() 
        image = image.squeeze(0)  
        image = unloader(image)
        return image
    ```

    如果您的计算机有可用的 GPU，请使用以下等效代码片段:

    ```
    def tensor2image(tensor):
        image = tensor.to('cpu').clone() 
        image = image.squeeze(0)  
        image = unloader(image)
        return image
    ```

    前面的代码片段将图像分配回 CPU，以便我们可以绘制它们。

7.  Call the function for both images and plot the results:

    ```
    plt.figure()
    plt.imshow(tensor2image(content_img))
    plt.title("Content Image")
    plt.show()
    plt.figure()
    plt.imshow(tensor2image(style_img))
    plt.title("Style Image")
    plt.show()
    ```

    生成的图像应该如下所示:

![Figure 5.2: Content image
](img/B15778_05_02.jpg)

图 5.2:内容图像

![Figure 5.3: Style image
](img/B15778_05_03.jpg)

图 5.3:样式图像

注意

要访问该特定部分的源代码，请参考 https://packt.live/2NQ6h0p 的。

你也可以在[https://packt.live/2BTreFi](https://packt.live/2BTreFi)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

要访问这个源代码的 GPU 版本，请参考[https://packt.live/31uctU5](https://packt.live/31uctU5)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

至此，您已经成功地加载并显示了用于样式转换的内容和样式图像。

## 加载模型

与许多其他框架一样，PyTorch 有一个包含不同模型的子包，这些模型之前已经过训练，可供公众使用。考虑到从头开始训练一个神经网络非常耗时，这一点非常重要；从预先训练好的模型开始可以帮助减少训练时间。这意味着可以加载预训练的模型，以便我们可以使用它们的最终参数(应该是那些最小化损失函数的参数)，而不需要经历迭代过程。

正如我们前面提到的，用于执行风格转换任务的架构是 19 层的 VGG 网络，也称为 VGG-19。预先训练好的模型可以在`torchvision`的`models`子包下获得。PyTorch 中保存的模型分为两部分:

1.  `vgg19.features`:这包括网络的所有卷积层和汇集层，以及参数。这些层负责从图像中提取特征；虽然一些层专门用于样式特征，例如颜色，但是其他层专门用于内容特征，例如边缘。
2.  `vgg19.classifier`: This refers to the linear layers (also known as fully connected layers) that are located at the end of the network, including their parameters. These layers are the ones that perform the classification of the image into one of the label classes, for instance, recognizing the type of animal in an image.

    注意

    如需了解 PyTorch 中的其他预培训模型，请访问[https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)。

根据前面的信息，为了提取内容和样式图像的必要特征，应该只加载模型的特征部分。加载模型包括调用`models`子包，后跟模型的名称，确保`pretrained`参数被设置为`True`(以便从之前的训练过程中加载参数)，并且只加载要素层，如以下代码片段所示:

```
model = models.vgg19(pretrained=True).features
```

每层中的参数(权重和偏差)应保持不变，因为这些参数将有助于检测所需的特征。这可以通过定义模型不需要计算任何这些层的梯度来实现，如下所示:

```
for param in model.parameters():
    param.requires_grad_(False)
```

这里，对于之前加载的模型的每个参数，`requires_grad_`方法被设置为`False`，以避免计算梯度，因为目的是利用预先训练的参数，而不更新它们。

## 练习 5.02:在 PyTorch 中加载预先训练好的模型

使用与上一个练习中相同的笔记本，本练习旨在加载预训练模型，该模型将在后续练习中使用，以使用我们之前加载的图像执行风格转换任务。

1.  打开上一个练习中的笔记本。
2.  Load the VGG-19 pre-trained model from PyTorch:

    ```
    model = models.vgg19(pretrained=True).features
    ```

    如前所述，选择模型的特征部分。这将使您能够访问模型的所有卷积层和池层，这些层将用于在本章的后续练习中提取特征。

3.  Perform a `for` loop through the parameters of the previously loaded model. Set each parameter so that it doesn't require gradients calculations:

    ```
    for param in model.parameters():
        param.requires_grad_(False)
    ```

    通过将梯度计算设置为`False`，我们确保在训练过程中不计算梯度。

    如果您的计算机有可用的 GPU，请将以下代码片段添加到前面的代码片段中，以便将模型分配给 GPU:

    ```
    model.to(device)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2VCYqIa](https://packt.live/2VCYqIa)。

    你也可以在 https://packt.live/2BXLXYE 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

    要访问该源代码的 GPU 版本，请参考[https://packt.live/2Vx2kC4](https://packt.live/2Vx2kC4)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

至此，您已经成功加载了一个预先训练好的模型。

## 提取特征

正如我们前面提到的，VGG-19 网络包含 19 个不同的层，包括卷积层、汇聚层和全连接层。卷积层出现在每个池层之前的堆栈中，整个架构中的堆栈数量为 5。

在风格转换领域，已经有不同的论文确定了对于识别内容和风格图像的相关特征至关重要的那些层。因此，通常认为每个堆栈的第一卷积层能够提取风格特征，而只有第四堆栈的第二卷积层应该用于提取内容特征。

从现在开始，我们将提取风格特征的层称为`conv1_1`、`conv2_1`、`conv3_1`、`conv4_1`和`conv5_1`，而负责提取内容特征的层称为`conv4_2`。

注意

用作本章指南的论文可通过以下网址获取:[https://www . cv-foundation . org/open access/content _ cvpr _ 2016/papers/Gatys _ Image _ Style _ Transfer _ CVPR _ 2016 _ paper . pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)。

这意味着风格图像的特征从五个不同的层获得，而内容图像的特征仅从一个层获得。这些层中的每一层的输出用于将输出图像与输入图像进行比较，其中目标是修改目标图像的参数，以便它们类似于内容图像的内容和样式图像的样式，这可以通过优化三个不同的损失函数来实现(这将在本章中进一步解释)。

要提取每一层的特征，可以使用下面的代码片段:

```
layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', \
          '19': 'conv4_1', '21': 'conv4_2', '28': 'conv5_1'}
features = {}
x = image
for index, layer in model._modules.items():
    x = layer(image)
    if index in layers:
        features[layers[index]] = x
```

在前面的代码片段中，`layers`是一个字典，它将所有相关层的位置(在网络中)映射到将用于识别它们的名称，而`model._modules`包含一个保存网络每一层的字典。

通过在不同的层中执行`for`循环，我们将图像传递到不同的层，并将感兴趣的层的输出(在我们之前创建的`layers`字典中的层)保存到`features`字典中。输出字典由包含图层名称的关键字和包含该图层输出要素的值组成。

为了确定目标图像是否包含与内容图像相同的内容，我们需要检查两个图像中是否存在某些特征。然而，为了检查目标图像和风格图像的风格表示，有必要检查相关性，而不是两个图像的特征的严格存在。这是因为两个图像的风格特征不会是精确的，而是近似的。

gram 矩阵用于检查这些相关性。它包括创建一个矩阵，查看给定层中不同风格特征的相关性。这是通过将卷积层的向量化输出乘以相同的转置向量化输出来实现的，如下图所示:

![Figure 5.4: Calculation of the gram matrix
](img/B15778_05_04.jpg)

图 5.4:gram 矩阵的计算

在上图中，A 表示 4x4 尺寸(高度和宽度)的输入样式图像，而 B 表示图像通过具有五个过滤器的卷积层后的输出。最后，C 是指 gram 矩阵的计算，其中左边的图像代表 B 的向量化版本，右边的图像是它的转置版本。从向量化输出的乘法中，创建 5x5 gram 矩阵，其值指示沿着不同通道(过滤器)的风格特征方面的相似性(相关性)。

这些相关性可用于确定与图像的风格表示相关的那些特征，然后可用于改变目标图像。考虑到风格特征是在五个不同的层中获得的，假定网络能够从风格图像中检测小的和大的特征是安全的，考虑到必须为每个层创建 gram 矩阵。

## 练习 5.03:设置特征提取流程

使用上一个练习中的网络架构和本章第一个练习中的图像，我们将创建几个能够从输入图像中提取特征并为样式特征创建 gram 矩阵的函数。

1.  打开上一个练习中的笔记本。
2.  打印我们在之前的练习中加载的模型的架构。这将帮助我们识别相关的层，以便我们可以执行样式转换任务:

    ```
    print(model)
    ```

3.  Create a dictionary for mapping the index of the relevant layers (keys) to a name (values). This will facilitate the process of calling relevant layers in the future:

    ```
    relevant_layers = {'0': 'conv1_1', '5': 'conv2_1', '10': \
                       'conv3_1', '19': 'conv4_1', '21': \
                       'conv4_2', '28': 'conv5_1'}
    ```

    为了创建字典，我们使用上一步的输出，它显示了网络中的每个层。在那里，可以观察到第一堆栈的第一层被标记为`0`，而第二堆栈的第一层被标记为`5`，以此类推。

4.  Create a function that will extract the relevant features (features extracted from the relevant layers only) from an input image. Name it `features_extractor` and make sure it takes the image, the model, and the dictionary we created previously as inputs:

    ```
    def features_extractor(x, model, layers):
        features = {}
        for index, layer in model._modules.items():
            x = layer(x)
            if index in layers:
                features[layers[index]] = x
        return features
    ```

    输出应该是一个字典，关键字是图层的名称，值是该图层的输出要素。

5.  对我们在本章第一个练习中加载的内容和样式图像调用`features_extractor`函数:

    ```
    content_features = features_extractor(content_img, model, \                                       relevant_layers) style_features = features_extractor(style_img, model, \                                     relevant_layers)
    ```

6.  Perform the gram matrix calculation over the style features. Consider that the style features were obtained from different layers, which is why different gram matrices should be created, one for each layer's output:

    ```
    style_grams = {}
    for i in style_features:
        layer = style_features[i]
        _, d1, d2, d3 = layer.shape
        features = layer.view(d1, d2 * d3)
        gram = torch.mm(features, features.t())
        style_grams[i] = gram
    ```

    对于每一层，获得风格特征矩阵的形状，以便对其进行向量化。接下来，通过将向量化输出乘以其转置版本来创建 gram 矩阵。

7.  Create an initial target image. This image will be compared against the content and style images later and be changed until the desired similarity is achieved:

    ```
    target_img = content_img.clone().requires_grad_(True)
    ```

    最好将初始目标映像创建为内容映像的副本。此外，考虑到我们希望能够在迭代过程中修改它，直到内容与内容图像的内容相似，并且样式与样式图像的样式相似，因此将它设置为需要计算梯度是很重要的。

    同样，如果您的计算机有可用的 GPU，请确保您也将目标映像分配给 GPU，而是使用以下代码片段:

    ```
    target_img = content_img.clone().requires_grad_(True).to(device)
    ```

8.  Using the `tensor2image` function we created during the first exercise of this chapter, plot the target image, which should look the same as the content image:

    ```
    plt.figure()
    plt.imshow(tensor2image(target_img))
    plt.title("Target Image")
    plt.show()
    ```

    输出图像如下所示:

![Figure 5.5: The target image
](img/B15778_05_05.jpg)

图 5.5:目标图像

注意

要访问该特定部分的源代码，请参考 https://packt.live/2ZtSoL7 的。

你也可以在 https://packt.live/2Vz7Cgm 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2Vz7Cgm)

要访问这个源代码的 GPU 版本，请参考[https://packt.live/3ePLxlA](https://packt.live/3ePLxlA)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

至此，您已经成功地执行了特征提取并计算了 gram 矩阵来执行风格转换。

## 优化算法、损耗和参数更新

尽管使用参数保持不变的预训练网络来执行风格迁移，但是创建目标图像包括迭代过程，其中计算三个不同的损失函数，并且通过仅更新与目标图像相关的参数来最小化这三个损失函数。

为了实现目标图像的创建，计算两个不同的损失函数(内容和风格损失)，然后将它们放在一起计算总损失函数，该总损失函数将被优化以达到适当的目标图像。然而，考虑到在内容和样式方面实现的测量精度非常不同，以下是对内容和样式损失函数的计算的解释，以及对如何计算总损失的描述。

### 内容丢失

这由一个函数组成，该函数基于由给定层获得的特征图，计算内容图像和目标图像之间的距离。在 VGG-19 网络的情况下，仅基于来自`conv4_2`层的输出来计算内容损失。

内容损失函数背后的主要思想是最小化内容图像和目标图像之间的距离，使得后者在内容方面与前者高度相似。

内容损失可以计算为相关层的内容和目标图像的特征图之间的均方差(`conv4_2`)，这可以使用以下等式来实现:

![Figure 5.6: The content loss function
](img/B15778_05_06.jpg)

图 5.6:内容损失函数

### 风格丧失

类似于内容损失，风格损失是通过计算均方差，根据风格特征(例如，颜色和纹理)来测量风格和目标图像之间的距离的函数。

与内容丢失的情况相反，它不是比较从不同层导出的特征图，而是比较基于样式和目标图像的特征图计算的 gram 矩阵。

必须使用`for`循环计算所有相关层(在本例中为五层)的样式损失。这将导致损失函数考虑来自两个图像的简单和复杂样式表示。

此外，最好在 0 到 1 之间对每个图层的样式表示进行加权，以便更重视提取较大和较简单要素的图层，而不是提取非常复杂要素的图层。这是通过给从风格图像中提取更多通用特征的早期层(`conv1_1`和`conv2_1`)更高的权重来实现的。

对于每个相关层，可以使用以下等式来计算风格损失:

![Figure 5.7: Style loss calculation
](img/B15778_05_07.jpg)

图 5.7:风格损失计算

### 全损

最后，总损失函数包括内容损失和风格损失的组合。它的值在通过更新目标图像的参数来创建目标图像的迭代过程中被最小化。

同样，建议您为内容和样式损失分配权重，以确定它们在最终输出中的参与程度。这有助于确定目标图像的风格化程度，同时保持内容仍然可见。将内容损失的权重设置为 1 是一种很好的做法，而样式损失的权重必须高得多，以达到您喜欢的比率。

分配给内容损失的权重通常被称为α，而分配给风格损失的权重被称为β。

计算总损耗的最终公式如下:

![Figure 5.8: Total loss calculation
](img/B15778_05_08.jpg)

图 5.8:总损失计算

一旦定义了损失的权重，就应该设置迭代步骤的数量以及优化算法，这应该只影响目标图像。这意味着，在每个迭代步骤中，将计算所有三个损失，以便我们可以使用梯度来优化与目标图像相关联的参数，直到损失函数被最小化并且获得具有期望外观的目标图像。

与之前的神经网络优化一样，以下是每次迭代中可以观察到的步骤:

1.  从目标图像中获取内容和风格方面的特征。在初始迭代中，这个图像将是内容图像的精确副本。
2.  计算内容损失。这是通过比较内容和目标图像的内容特征图来完成的。
3.  计算所有相关图层的平均样式损失。这是通过比较样式和目标图像的所有层的 gram 矩阵来实现的。
4.  计算总损失。
5.  计算总损失函数相对于目标图像的参数(权重和偏差)的偏导数。
6.  重复此操作，直到达到所需的迭代次数。

最终输出将是一个图像，其内容类似于内容图像的内容，并且其样式类似于样式图像的样式。

## 练习 5.04:创建目标图像

在本章的最后一个练习中，你将执行风格转换的任务。该练习包括对负责执行不同迭代的部分进行编码，同时优化损失函数，以达到理想的目标图像。要做到这一点，关键是要利用我们在本章前面的练习中编写的代码位:

注意

在 GPU 上运行这段代码时，需要做一些改动。请访问本书的 GitHub 资源库，修改这段代码的 GPU 版本。

1.  打开上一个练习中的笔记本。
2.  Define a dictionary containing the weights for each of the layers in charge of extracting style features:

    ```
    style_weights = {'conv1_1': 1., 'conv2_1': 0.8, 'conv3_1': 0.6, \
                     'conv4_1': 0.4, 'conv5_1': 0.2}
    ```

    请确保使用您在上一个练习中为层指定的相同名称作为关键字。

3.  定义与内容和风格损失相关的权重:

    ```
    alpha = 1 beta = 1e5
    ```

4.  Define the number of iteration steps, as well as the optimization algorithm. We can also set the number of iterations if we want to see a plot of the image that has been created at that point:

    ```
    print_statement = 200
    optimizer = torch.optim.Adam([target_img], lr=0.001)
    iterations = 2000
    ```

    这个优化算法要更新的参数应该是目标图像的参数。

    注意

    运行 2，000 次迭代，如本练习中的例子，将会花费相当长的时间，这取决于您的资源。然而，为了达到出色的目标图像，可能需要更多的迭代。

    为了让您体会到在每次迭代中目标图像发生的变化，几次迭代就足够了，但是鼓励您尝试更长时间的训练。

5.  定义`for`循环，计算所有三个损失函数，并执行优化过程:

    ```
    for i in range(1, iterations+1):     # Extract features for all relevant layers     target_features = features_extractor(target_img, model, \                                          relevant_layers)     # Calculate the content loss     content_loss = torch.mean((target_features['conv4_2'] \                                - content_features['conv4_2'])**2)     # Loop through all style layers     style_losses = 0     for layer in style_weights:         # Create gram matrix for that layer         target_feature = target_features[layer]         _, d1, d2, d3 = target_feature.shape         target_reshaped = target_feature.view(d1, d2 * d3)         target_gram = torch.mm(target_reshaped, \                                target_reshaped.t())         style_gram = style_grams[layer]         # Calculate style loss for that layer         style_loss = style_weights[layer] \                      * torch.mean((target_gram - style_gram)**2)         #Calculate style loss for all layers         style_losses += style_loss / (d1 * d2 * d3)     # Calculate the total loss     total_loss = alpha * content_loss + beta * style_losses     # Perform back propagation     optimizer.zero_grad()     total_loss.backward()     optimizer.step()     # Print target image     if  i % print_statement == 0 or i == 1:         print('Total loss: ', total_loss.item())         plt.imshow(tensor2image(target_img))         plt.show()
    ```

6.  Plot both the content and target images to compare the results. This can be achieved by using the `tensor2image` function, which we created in the previous exercises, in order to convert the tensors into PIL images that can be printed using `matplotlib`:

    ```
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))
    ax1.imshow(tensor2image(content_img))
    ax2.imshow(tensor2image(target_img))
    ax3.imshow(tensor2image(style_img))
    plt.show()
    ```

    最终图像应该类似于下图:

![Figure 5.9: Comparison between the content, style, and target image
](img/B15778_05_09.jpg)

图 5.9:内容、风格和目标图像之间的比较

注意

要查看高质量的彩色图像，请访问这本书在 https://packt.live/2VBZA5E[的 GitHub 知识库。](https://packt.live/2VBZA5E)

这样，你就成功地完成了风格的转换。

注意

要访问该特定部分的源代码，请参考 https://packt.live/2VyKJtK 的。

本节目前没有在线交互示例，需要在本地运行。

要访问这个源代码的 GPU 版本，请参考[https://packt.live/2YMcdhh](https://packt.live/2YMcdhh)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

## 活动 5.01:执行风格转换

在本次活动中，我们将进行一次风格迁移。为此，我们将对我们在本章中学到的所有概念进行编码。让我们看看下面的场景。

您想要改变一些图像，使它们具有艺术天赋，为了实现这一点，您决定创建一些代码，使用预训练的神经网络来执行风格转换。按照以下步骤完成本活动:

1.  导入所需的库。
2.  指定要对输入图像执行的变换。一定要把它们调整到相同的大小，转换成张量，然后归一化。
3.  定义一个图像加载函数。这将打开图像并转换它。调用图像加载器函数来加载两个输入图像。
4.  为了能够显示图像，定义一组新的变换来恢复图像的归一化，并将张量转换为 PIL 图像。
5.  创建一个函数(`tensor2image`)，它能够对张量执行前面的变换。为两幅图像调用该函数并绘制结果。
6.  装载 VGG-19 模型。
7.  创建一个字典，将相关层的索引(键)映射到名称(值)。然后，创建一个函数来提取相关图层的要素地图。使用它们来提取两个输入图像的特征。
8.  计算样式特征的 gram 矩阵。此外，创建初始目标图像。
9.  设置不同样式层的权重，以及内容和样式损失的权重。
10.  Run the model for 500 iterations. Define the Adam optimization algorithm before starting to train the model, using 0.001 as the learning rate.

    注意

    根据您的资源，培训过程可能需要几个小时。因此，为了取得优异的成绩，建议您进行数千次迭代训练。如果您希望看到培训过程的进度，添加打印报表是一个很好的做法。

    本章中显示的结果是通过运行大约 5000 次迭代获得的，如果没有 GPU，这将需要很长时间来运行(我们使用 GPU 的这个活动的解决方案也可以在本书的 GitHub 资源库中找到)。然而，要看到一些微小的变化，按照本活动(500)中的建议，运行几百次迭代就足够了。

11.  Plot the content, style, and target images to compare the results.

    5，000 次迭代后的输出应如下所示:

![Figure 5.10: Plotting of the content and target images
](img/B15778_05_10.jpg)

图 5.10:内容和目标图像的绘制

注意

这项活动的解决方案可在第 277 页找到。

要查看*图 5.10* 的高质量彩色图像，请访问[https://packt.live/2KcORcw](https://packt.live/2KcORcw)。

# 总结

这一章介绍了风格转换，这是一个当今流行的任务，可以使用 CNN 来执行。它将一个内容图像和一个样式图像作为输入，并将一个新创建的图像作为输出返回，该图像保留了一个图像的内容和另一个图像的样式。它通常用于通过将随机的常规图像与伟大艺术家的画作相结合来赋予图像艺术的外观。

虽然使用 CNN 来执行风格迁移，但是创建目标图像的过程不是通过传统地训练网络来实现的。本章解释了如何使用预训练网络来考虑某些特别擅长识别某些特征的相关图层的输出。

本章解释了开发能够执行样式转换任务的代码所需的每个步骤，其中第一步包括加载和显示输入。正如我们前面提到的，模型有两个输入(内容和样式图像)。每幅图像都要经过一系列的变换，目的是将图像的大小调整到相同的大小，将它们转换成张量，并对它们进行归一化，以便网络对它们进行适当的处理。

接下来，加载预训练的模型。正如我们在本章提到的，VGG-19 是解决这类任务最常用的体系结构之一。它由 19 层组成，包括卷积层、池层和全连接层，其中，对于所讨论的任务，只使用了一些卷积层。考虑到 PyTorch 提供了一个包含几个预训练网络架构的子包，加载预训练模型的过程相当简单。

一旦网络被加载，网络的某些层就被识别为在检测对风格迁移至关重要的某些特征方面表现优异。虽然五个不同的图层都能够提取与图像风格相关的特征，如颜色和纹理，但只有其中一个图层在提取内容特征方面表现尤为出色，如边缘和形状。因此，定义将用于从输入图像中提取信息的那些层以创建期望的目标图像是至关重要的。

最后，是时候对迭代过程进行编码，以创建具有所需特性的目标图像。为此，计算了三种不同的损失。一个用于比较内容图像和目标图像之间在内容方面的差异(内容损失)，另一个用于比较样式图像和目标图像之间在样式方面的差异(样式损失)，这通过计算 gram 矩阵来实现。最后，还有一个是结合了内容和风格损失(总损失)。

通过最小化总损失的值来创建目标图像，这可以通过更新与目标图像相关的参数来完成。虽然可以使用预先训练的网络，但是达到理想目标图像的过程可能需要数千次迭代和相当长的时间。

在下一章中，将解释一种不同的网络体系结构，以便使用文本数据序列来解决数据问题。rnn 是保存内存的神经网络架构，这允许它们处理顺序数据。它们通常用于解决与理解人类语言相关的问题。