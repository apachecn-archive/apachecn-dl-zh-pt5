

# 附录

# 1。深度学习和 PyTorch 简介

## 活动 1.01:创建一个单层神经网络

### 解决方案

1.  导入所需的库，包括熊猫，用于导入 CSV 文件:

    ```
    import pandas as pd import torch import torch.nn as nn import matplotlib.pyplot as plt
    ```

2.  读取包含数据集的 CSV 文件:

    ```
    data = pd.read_csv("SomervilleHappinessSurvey2015.csv")
    ```

3.  将输入要素与目标分开。注意，目标位于 CSV 文件的第一列。将值转换成张量，确保值转换成浮点数:

    ```
    x = torch.tensor(data.iloc[:,1:].values).float() y = torch.tensor(data.iloc[:,:1].values).float()
    ```

4.  定义模型的架构，并将其存储在一个名为`model`的变量中。记得创建一个单层模型:

    ```
    model = nn.Sequential(nn.Linear(6, 1),                       nn.Sigmoid())
    ```

5.  定义要使用的损失函数。使用 MSE 损失函数:

    ```
    loss_function = torch.nn.MSELoss()
    ```

6.  定义模型的优化器。使用 Adam 优化器和`0.01` :

    ```
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    ```

    的学习率
7.  Run the optimization for 100 iterations. Every 10 iterations, print and save the loss value:

    ```
    losses = []
    for i in range(100):
        y_pred = model(x)
        loss = loss_function(y_pred, y)
        losses.append(loss.item())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if i%10 == 0:
            print(loss.item())
    ```

    最终损失大概应该是`0.24`。

8.  Make a line plot to display the loss value for each iteration step:

    ```
    plt.plot(range(0,100), losses)
    plt.show()
    ```

    结果图应该如下所示:

![Figure 1.4: Loss function throughout the training process
](img/B15778_01_04.jpg)

图 1.4:整个训练过程中的损失函数

这意味着训练过程能够最小化损失函数，这意味着最终的模型将可能能够映射市民对城市服务的满意度与他们是否对政府满意之间的关系。

注意

要访问该特定部分的源代码，请参考[https://packt.live/2ZufWiI](https://packt.live/2ZufWiI)。

你也可以在 https://packt.live/2BZhyZF[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2BZhyZF)

# 2。神经网络的构建模块

## 活动 2.01:执行数据准备

### 解决方案

1.  导入所需的库:

    ```
    import pandas as pd
    ```

2.  Using pandas, load the `.csv` file:

    ```
    data = pd.read_csv("YearPredictionMSD.csv", nrows=50000)
    data.head()
    ```

    注意

    为了避免内存限制，在读取文本文件时使用`nrows`参数，以便读取整个数据集的较小部分。在前面的示例中，我们正在读取前 50，000 行。

    输出如下所示:

    ![Figure 2.33: YearPredictionMSD.csv
    ](img/B15778_02_33.jpg)

    图 2.33:年度预测 MSD.csv

3.  Verify whether any qualitative data is present in the dataset:

    ```
    cols = data.columns
    num_cols = data._get_numeric_data().columns
    list(set(cols) - set(num_cols))
    ```

    输出应该是一个空列表，这意味着没有定性特征。

4.  Check for missing values.

    如果您在先前用于此目的的代码行中添加一个额外的`sum()`函数，您将获得整个数据集中缺失值的总和，而不区分列:

    ```
    data.isnull().sum().sum()
    ```

    输出应该是`0`，这意味着没有任何特征包含缺失值。

5.  Check for outliers:

    ```
    outliers = {}
    for i in range(data.shape[1]):
        min_t = data[data.columns[i]].mean() \
                - (3 * data[data.columns[i]].std())
        max_t = data[data.columns[i]].mean() \
                + (3 * data[data.columns[i]].std())
        count = 0
        for j in data[data.columns[i]]:
            if j < min_t or j > max_t:
                count += 1
        percentage = count/data.shape[0]
        outliers[data.columns[i]] = "%.3f" % percentage
    print(outliers)
    ```

    输出字典应显示没有任何要素包含代表超过 5%数据的异常值。

6.  从目标数据中分离特征:

    ```
    X = data.iloc[:, 1:] Y = data.iloc[:, 0]
    ```

7.  Rescale the features data using the standardization methodology:

    ```
    X = (X - X.mean())/X.std()
    X.head()
    ```

    输出如下所示:

    ![Figure 2.34: Rescaled features data
    ](img/B15778_02_34.jpg)

    图 2.34:重新缩放的要素数据

8.  将数据分成三组:训练、验证和测试。使用您喜欢的方法:

    ```
    from sklearn.model_selection import train_test_split X_shuffle = X.sample(frac=1, random_state=0) Y_shuffle = Y.sample(frac=1, random_state=0) x_new, x_test, \ y_new, y_test = train_test_split(X_shuffle, \                                  Y_shuffle, \                                  test_size=0.2, \                                  random_state=0) dev_per = x_test.shape[0]/x_new.shape[0] x_train, x_dev, \ y_train, y_dev = train_test_split(x_new, \                                   y_new, \                                   test_size=dev_per, \                                   random_state=0)
    ```

9.  Print the resulting shapes as follows:

    ```
    print(x_train.shape, y_train.shape)
    print(x_dev.shape, y_dev.shape)
    print(x_test.shape, y_test.shape)
    ```

    输出应该如下所示:

    ```
    (30000, 90) (30000, )
    (10000, 90) (10000, )
    (10000, 90) (10000, )
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/31ukVTj](https://packt.live/31ukVTj)。

    你也可以在 https://packt.live/3dLWMdd 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

## 活动 2.02:为回归问题开发深度学习解决方案

### 解决方案

1.  导入所需的库:

    ```
    import torch import torch.nn as nn
    ```

2.  从我们在上一活动中创建的所有三组数据的目标中分割特征。将数据帧转换成张量:

    ```
    x_train = torch.tensor(x_train.values).float() y_train = torch.tensor(y_train.values).float() x_dev = torch.tensor(x_dev.values).float() y_dev = torch.tensor(y_dev.values).float() x_test = torch.tensor(x_test.values).float() y_test = torch.tensor(y_test.values).float()
    ```

3.  定义网络的架构。随意尝试层数和每层单元数的不同组合:

    ```
    model = nn.Sequential(nn.Linear(x_train.shape[1], 10), \                       nn.ReLU(), \                       nn.Linear(10, 7), \                       nn.ReLU(), \                       nn.Linear(7, 5), \                       nn.ReLU(), \                       nn.Linear(5, 1))
    ```

4.  定义损失函数和优化算法:

    ```
    loss_function = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    ```

5.  使用一个`for`循环训练网络 3000 个迭代步骤:

    ```
    for i in range(3000):     y_pred = model(x_train).squeeze()     loss = loss_function(y_pred, y_train)     optimizer.zero_grad()     loss.backward()     optimizer.step()     if i%250 == 0:         print(i, loss.item())
    ```

6.  Test your model by performing a prediction on the first instance of the test set and comparing it with the ground truth:

    ```
    pred = model(x_test[0])
    print("Ground truth:", y_test[0].item(), \
          "Prediction:", pred.item())
    ```

    您的输出应该如下所示:

    ```
    Ground truth: 1995.0 Prediction: 1998.0279541015625
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2CUDSnP](https://packt.live/2CUDSnP)。

    你也可以在[https://packt.live/3eQ1yI2](https://packt.live/3eQ1yI2)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 3。一个使用 DNNs 的分类问题

## 活动 3.01:构建一个 ANN

解决方案:

1.  导入以下库:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score import torch from torch import nn, optim import torch.nn.functional as F import matplotlib.pyplot as plt torch.manual_seed(0)
    ```

2.  Read the previously prepared dataset, which should have been named `dccc_prepared.csv`:

    ```
    data = pd.read_csv("dccc_prepared.csv")
    data.head()
    ```

    输出应该如下所示:

    ![Figure 3.14: dccc_prepared.csv
    ](img/B15778_03_14.jpg)

    图 3.14: dccc_prepared.csv

3.  从目标中分离特征:

    ```
    X = data.iloc[:,:-1] y = data["default payment next month"]
    ```

4.  Using scikit-learn's `train_test_split` function, split the dataset into training, validation, and testing sets. Use a 60:20:20 split ratio. Set `random_state` to 0:

    ```
    X_new, X_test, \
    y_new, y_test = train_test_split(X, y, test_size=0.2, \
                                     random_state=0)
    dev_per = X_test.shape[0]/X_new.shape[0]
    X_train, X_dev, \
    y_train, y_dev = train_test_split(X_new, y_new, \
                                      test_size=dev_per, \
                                      random_state=0)
    ```

    您可以使用以下代码打印每个集合的最终形状:

    ```
    print("Training sets:",X_train.shape, y_train.shape)
    print("Validation sets:",X_dev.shape, y_dev.shape)
    print("Testing sets:",X_test.shape, y_test.shape)
    ```

    每组的最终形状如下所示:

    ```
    Training sets: (28036, 22) (28036,)
    Validation sets: (9346, 22) (9346,)
    Testing sets: (9346, 22) (9346,)
    ```

5.  将验证集和测试集转换成张量，记住特征矩阵应该是浮点型的，而目标矩阵不应该是。暂时不对训练集进行转换，因为它们将进行进一步的转换:

    ```
    X_dev_torch = torch.tensor(X_dev.values).float() y_dev_torch = torch.tensor(y_dev.values) X_test_torch = torch.tensor(X_test.values).float() y_test_torch = torch.tensor(y_test.values)
    ```

6.  构建一个自定义模块类来定义网络层。包括指定将应用于每层输出的激活函数的转发函数。将`ReLU`用于所有层，除了输出，这里你应该使用`log_softmax` :

    ```
    class Classifier(nn.Module):     def __init__(self, input_size):         super().__init__()         self.hidden_1 = nn.Linear(input_size, 10)         self.hidden_2 = nn.Linear(10, 10)         self.hidden_3 = nn.Linear(10, 10)         self.output = nn.Linear(10, 2)     def forward(self, x):         z = F.relu(self.hidden_1(x))         z = F.relu(self.hidden_2(z))         z = F.relu(self.hidden_3(z))         out = F.log_softmax(self.output(z), dim=1)         return out
    ```

7.  实例化模型并定义训练模型所需的所有变量。将时期数设置为`50`，将批次大小设置为`128`。使用`0.001` :

    ```
    model = Classifier(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 50 batch_size = 128
    ```

    的学习率
8.  使用训练集的数据训练网络。使用验证集来衡量性能。为此，保存每个历元中训练集和验证集的损失和准确度:

    ```
    train_losses, dev_losses, \ train_acc, dev_acc = [], [], [], [] for e in range(epochs):     X_, y_ = shuffle(X_train, y_train)     running_loss = 0     running_acc = 0     iterations = 0     for i in range(0, len(X_), batch_size):         iterations += 1         b = i + batch_size         X_batch = torch.tensor(X_.iloc[i:b,:].values).float()         y_batch = torch.tensor(y_.iloc[i:b].values)         pred = model(X_batch)         loss = criterion(pred, y_batch)         optimizer.zero_grad()         loss.backward()         optimizer.step()         running_loss += loss.item()         ps = torch.exp(pred)         top_p, top_class = ps.topk(1, dim=1)         running_acc += accuracy_score(y_batch, top_class)     dev_loss = 0     acc = 0     with torch.no_grad():         pred_dev = model(X_dev_torch)         dev_loss = criterion(pred_dev, y_dev_torch)         ps_dev = torch.exp(pred_dev)         top_p, top_class_dev = ps_dev.topk(1, dim=1)         acc = accuracy_score(y_dev_torch, top_class_dev)     train_losses.append(running_loss/iterations)     dev_losses.append(dev_loss)     train_acc.append(running_acc/iterations)     dev_acc.append(acc)     print("Epoch: {}/{}.. ".format(e+1, epochs),\           "Training Loss: {:.3f}.. "\           .format(running_loss/iterations),\           "Validation Loss: {:.3f}.. ".format(dev_loss), \           "Training Accuracy: {:.3f}.. "\           .format(running_acc/iterations), \           "Validation Accuracy: {:.3f}".format(acc))
    ```

9.  Plot the loss of both sets:

    ```
    fig = plt.figure(figsize=(15, 5))
    plt.plot(train_losses, label='Training loss')
    plt.plot(dev_losses, label='Validation loss')
    plt.legend(frameon=False, fontsize=15)
    plt.show()
    ```

    生成的图应该与此处显示的图相似，尽管有一些差异，考虑到混合训练数据可能会得到略有不同的结果:

    ![Figure 3.15: A plot displaying the training and validation losses
    ](img/B15778_03_15.jpg)

    图 3.15:显示训练和验证损失的图

10.  Plot the accuracy of both sets:

    ```
    fig = plt.figure(figsize=(15, 5))
    plt.plot(train_acc, label="Training accuracy")
    plt.plot(dev_acc, label="Validation accuracy")
    plt.legend(frameon=False, fontsize=15)
    plt.show()
    ```

    下面是从这段代码中得出的图表:

![Figure 3.16: A plot displaying the accuracy of the sets
](img/B15778_03_16.jpg)

图 3.16:显示集合精确度的图

注意

要访问该特定部分的源代码，请参考 https://packt.live/2Vz6BoK 的。

你也可以在 https://packt.live/2NNBuRS 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2NNBuRS)

## 活动 3.02:提高模特的表现

解决方案:

1.  导入您在之前的活动中使用的相同库:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score import torch from torch import nn, optim import torch.nn.functional as F import matplotlib.pyplot as plt torch.manual_seed(0)
    ```

2.  加载数据并从目标中分割要素。接下来，使用 60:20:20 的拆分比率将数据拆分为三个子集(培训、验证和测试)。最后，将验证集和测试集转换成 PyTorch 张量，就像您在前面的练习中所做的一样:

    ```
    data = pd.read_csv("dccc_prepared.csv") X = data.iloc[:,:-1] y = data["default payment next month"] X_new, X_test, \ y_new, y_test = train_test_split(X, y, test_size=0.2, \                                  random_state=0) dev_per = X_test.shape[0]/X_new.shape[0] X_train, X_dev, \ y_train, y_dev = train_test_split(X_new, y_new, \                                   test_size=dev_per, \                                   random_state=0) X_dev_torch = torch.tensor(X_dev.values).float() y_dev_torch = torch.tensor(y_dev.values) X_test_torch = torch.tensor(X_test.values).float() y_test_torch = torch.tensor(y_test.values)
    ```

3.  Considering that the model is suffering from high bias, the focus should be on increasing the number of epochs or increasing the size of the network by adding additional layers or units to each layer. The aim should be to approximate the accuracy over the validation set to 80%.

    接下来，将显示性能最佳的模型，这是在多次微调尝试后实现的。首先，定义模型架构和向前传递，如下面的代码片段所示:

    ```
    class Classifier(nn.Module):
        def __init__(self, input_size):
            super().__init__()
            self.hidden_1 = nn.Linear(input_size, 100)
            self.hidden_2 = nn.Linear(100, 100)
            self.hidden_3 = nn.Linear(100, 50)
            self.hidden_4 = nn.Linear(50,50)
            self.output = nn.Linear(50, 2)
            self.dropout = nn.Dropout(p=0.1)
        def forward(self, x):
            z = self.dropout(F.relu(self.hidden_1(x)))
            z = self.dropout(F.relu(self.hidden_2(z)))
            z = self.dropout(F.relu(self.hidden_3(z)))
            z = self.dropout(F.relu(self.hidden_4(z)))
            out = F.log_softmax(self.output(z), dim=1)
            return out
    ```

    接下来，定义训练过程的不同参数。这包括损失函数、优化算法、批处理大小和历元数，如以下代码所示:

    ```
    model = Classifier(X_train.shape[1])
    criterion = nn.NLLLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    epochs = 4000
    batch_size = 128
    ```

    最后，按照下面的代码片段处理培训过程:

    ```
    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []
    x_axis = []
    for e in range(1, epochs + 1):
        X_, y_ = shuffle(X_train, y_train)
        running_loss = 0
        running_acc = 0
        iterations = 0
        for i in range(0, len(X_), batch_size):
            iterations += 1
            b = i + batch_size
            X_batch = torch.tensor(X_.iloc[i:b,:].values).float()
            y_batch = torch.tensor(y_.iloc[i:b].values)
            log_ps = model(X_batch)
            loss = criterion(log_ps, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            ps = torch.exp(log_ps)
            top_p, top_class = ps.topk(1, dim=1)
            running_acc += accuracy_score(y_batch, top_class)
        dev_loss = 0
        acc = 0
        with torch.no_grad():
            model.eval()
            log_dev = model(X_dev_torch)
            dev_loss = criterion(log_dev, y_dev_torch)
            ps_dev = torch.exp(log_dev)
            top_p, top_class_dev = ps_dev.topk(1, dim=1)
            acc = accuracy_score(y_dev_torch, top_class_dev)
        model.train()
        if e%50 == 0 or e == 1:
            x_axis.append(e)
            train_losses.append(running_loss/iterations)
            dev_losses.append(dev_loss)
            train_acc.append(running_acc/iterations)
            dev_acc.append(acc)
            print("Epoch: {}/{}.. ".format(e, epochs), \
                  "Training Loss: {:.3f}.. "\
                  .format(running_loss/iterations), \
                  "Validation Loss: {:.3f}.. ".format(dev_loss),\
                  "Training Accuracy: {:.3f}.. "\
                  .format(running_acc/iterations), \
                  "Validation Accuracy: {:.3f}".format(acc))
    ```

    注意

    本活动附带的 Jupyter 笔记本可以在之前共享的 GitHub 资源库中找到。在那里，您会发现对模型进行微调的不同尝试，以及它们的结果。表现最好的型号可以在笔记本的末尾找到。

4.  Plot the loss and accuracy for both sets of data:

    ```
    fig = plt.figure(figsize=(15, 5))
    plt.plot(x_axis,train_losses, label='Training loss')
    plt.plot(x_axis, dev_losses, label='Validation loss')
    plt.legend(frameon=False , fontsize=15)
    plt.show()
    ```

    运行上述代码会显示以下图形:

    ![Figure 3.17: A plot displaying the loss of the sets
    ](img/B15778_03_17.jpg)

    ```
    fig = plt.figure(figsize=(15, 5))
    plt.plot(x_axis, train_acc, label="Training accuracy")
    plt.plot(x_axis, dev_acc, label="Validation accuracy")
    plt.legend(frameon=False , fontsize=15)
    plt.show()
    ```

    运行上述代码会显示以下图形:

    ![Figure 3.18: A plot displaying the accuracy of the sets
    ](img/B15778_03_18.jpg)

    图 3.18:显示集合精确度的图

5.  Using the best-performing model, perform a prediction over the testing set (which should not have been used during the fine-tuning process). Compare the prediction with the ground truth by calculating the accuracy of the model over this set:

    ```
    model.eval()
    test_pred = model(X_test_torch)
    test_pred = torch.exp(test_pred)
    top_p, top_class_test = test_pred.topk(1, dim=1)
    acc_test = accuracy_score(y_test_torch, top_class_test)
    print(acc_test)
    ```

    通过模型架构和这里定义的参数获得的准确性应该在 80%左右。

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/2Bs42hh 的。

    本节目前没有在线交互示例，需要在本地运行。

## 活动 3.03:利用你的模型

### 解决方案

1.  打开您在之前的活动中使用的 Jupyter 笔记本。
2.  Copy the class containing the architecture of your best-performing model and save it in a Python file. Make sure that you import PyTorch's required libraries and modules. Name it `final_model.py`.

    该文件应该如下所示:

    ![Figure 3.19: A screenshot of final_model.py
    ](img/B15778_03_19.jpg)

    图 3.19:final _ model . py 的截图

3.  在 Jupyter 笔记本中，保存性能最好的型号。确保保存与输入单位相关的信息以及模型的参数。命名为`checkpoint.pth` :

    ```
    checkpoint = {"input": X_train.shape[1], \               "state_dict": model.state_dict()} torch.save(checkpoint, "checkpoint.pth")
    ```

4.  打开新的 Jupyter 笔记本。
5.  导入 PyTorch，以及我们在*步骤 2* :

    ```
    import torch import final_model
    ```

    中创建的 Python 文件
6.  创建一个加载模型的函数:

    ```
    def load_model_checkpoint(path):     checkpoint = torch.load(path)     model = final_model.Classifier(checkpoint["input"])     model.load_state_dict(checkpoint["state_dict"])     return model model = load_model_checkpoint("checkpoint.pth")
    ```

7.  Perform a prediction by inputting the following tensor into your model:

    ```
    example = torch.tensor([[0.0606, 0.5000, 0.3333, 0.4828, \
                             0.4000, 0.4000, 0.4000, 0.4000, \
                             0.4000, 0.4000, 0.1651, 0.0869, \
                             0.0980, 0.1825, 0.1054, 0.2807, \
                             0.0016, 0.0000, 0.0033, 0.0027, \
                             0.0031, 0.0021]]).float()
    pred = model(example)
    pred = torch.exp(pred)
    top_p, top_class_test = pred.topk(1, dim=1)
    ```

    通过打印`top_class_test`，我们获得模型的预测，在这种情况下，它等于`1`(是)。

8.  使用 JIT 模块转换模型:

    ```
    traced_script = torch.jit.trace(model, example, \                                 check_trace=False)
    ```

9.  Perform a prediction by inputting the same tensor as in *Step 7* to the traced script of your model:

    ```
    prediction = traced_script(example)
    prediction = torch.exp(prediction)
    top_p_2, top_class_test_2 = prediction.topk(1, dim=1)
    ```

    通过打印`top_class_test_2`，我们从您的模型的跟踪脚本表示中得到预测，它同样等于`1`(是)。

10.  打开一个新的 Jupyter 笔记本，导入所需的库，使用 Flask 创建一个 API，以及加载已保存模型的库:

    ```
    import flask from flask import request import torch import final_model
    ```

11.  初始化烧瓶应用程序:

    ```
    app = flask.Flask(__name__) app.config["DEBUG"] = True
    ```

12.  定义一个加载已保存模型的函数，然后实例化该模型:

    ```
    def load_model_checkpoint(path):     checkpoint = torch.load(path)     model = final_model.Classifier(checkpoint["input"])     model.load_state_dict(checkpoint["state_dict"])     return model model = load_model_checkpoint("checkpoint.pth")
    ```

13.  定义 API 到`/prediction`的路径，并将方法设置为`POST`。然后，定义将接收`POST`数据并将其馈送给模型以执行预测的函数:

    ```
    @app.route('/prediction', methods=['POST']) def prediction():     body = request.get_json()     example = torch.tensor(body['data']).float()     pred = model(example)     pred = torch.exp(pred)     _, top_class_test = pred.topk(1, dim=1)     top_class_test = top_class_test.numpy()          return {"status":"ok", "result":int(top_class_test[0][0])}
    ```

14.  Run the Flask app:

    ```
    app.run(debug=True, use_reloader=False)
    ```

    使用 Postman，一个为 API 开发而创建的平台，可以测试 API。要向 Postman 提交一个成功的请求，头部的`Content-Type`应该等于`application/json`。产生的输出应该如下所示:

![Figure 3.20: A screenshot of the app after running it
](img/B15778_03_13.jpg)

图 3.20:应用程序运行后的屏幕截图

注意

要访问该特定部分的源代码，请参考[https://packt.live/2NHkddn](https://packt.live/2NHkddn)。

本节目前没有在线交互示例，需要在本地运行。

# 4。卷积神经网络

## 活动 4.01:为图像分类问题构建 CNN

### 解决方案

1.  导入所需的库:

    ```
    import numpy as np import torch from torch import nn, optim import torch.nn.functional as F from torchvision import datasets import torchvision.transforms as transforms from torch.utils.data.sampler import SubsetRandomSampler from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt
    ```

2.  设置要对数据执行的转换，这将把数据转换成张量并归一化像素值:

    ```
    transform = \     transforms.Compose([transforms.ToTensor(), \                         transforms.Normalize((0.5, 0.5, 0.5),\                                              (0.5, 0.5, 0.5))])
    ```

3.  Set a batch size of 100 images and download both the training and testing data from the `CIFAR10` dataset:

    ```
    batch_size = 100
    train_data = datasets.CIFAR10('data', train=True, \
                                  download=True, \
                                  transform=transform)
    test_data = datasets.CIFAR10('data', train=False, \
                                 download=True, \
                                 transform=transform)
    ```

    前面的代码下载了 PyTorch 的`torchvision`包中的训练和测试数据集。数据集按照上一步中定义的转换进行转换。

4.  Using a validation size of 20%, define the training and validation sampler that will be used to divide the dataset into those two sets:

    ```
    dev_size = 0.2
    idx = list(range(len(train_data)))
    np.random.shuffle(idx)
    split_size = int(np.floor(dev_size * len(train_data)))
    train_idx, dev_idx = idx[split_size:], idx[:split_size]
    train_sampler = SubsetRandomSampler(train_idx)
    dev_sampler = SubsetRandomSampler(dev_idx)
    ```

    为了将训练集分成两个集合(训练和验证)，为每个集合定义了一个索引列表，然后可以使用`SubsetRandomSampler`函数对其进行随机采样。

5.  Use the `DataLoader()` function to define the batches of each set of data to be used:

    ```
    train_loader = \
    torch.utils.data.DataLoader(train_data, \
                                batch_size=batch_size, \
                                sampler=train_sampler)
    dev_loader = \
    torch.utils.data.DataLoader(train_data, \
                                batch_size=batch_size, \
                                sampler=dev_sampler)
    test_loader = \
    torch.utils.data.DataLoader(test_data, \
                                batch_size=batch_size)
    ```

    PyTorch 的`DataLoader`函数用于创建批次，这些批次将在开发过程的训练、验证和测试阶段被提供给模型。

6.  Define the architecture of your network. Use the following information to do so:

    Conv1:卷积层，将彩色图像作为输入，并通过 10 个大小为 3 的过滤器。填充和步幅都应设置为 1。

    Conv2:将输入数据通过 20 个大小为 3 的过滤器的卷积层。填充和步幅都应设置为 1。

    Conv3:将输入数据通过 40 个大小为 3 的过滤器的卷积层。填充和步幅都应设置为 1。

    在每个卷积层之后使用 ReLU 激活函数。

    在每个卷积层之后使用一个池层，过滤器大小和步幅为 2。

    展平图像后，使用设定为 20%的衰减项。

    Linear1:完全连接的层，接收来自前一层的展平矩阵作为输入，并生成 100 个单位的输出。对该层使用 ReLU 激活功能。这里的辍学项设置为 20%。

    Linear2:生成 10 个输出的全连接层，每个类别标签一个输出。使用输出层的`log_softmax`激活功能:

    ```
    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(3, 10, 3, 1, 1)
            self.conv2 = nn.Conv2d(10, 20, 3, 1, 1)
            self.conv3 = nn.Conv2d(20, 40, 3, 1, 1)
            self.pool = nn.MaxPool2d(2, 2) 
            self.linear1 = nn.Linear(40 * 4 * 4, 100)
            self.linear2 = nn.Linear(100, 10)
            self.dropout = nn.Dropout(0.2)
        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = self.pool(F.relu(self.conv3(x)))
            x = x.view(-1, 40 * 4 * 4)
            x = self.dropout(x)
            x = F.relu(self.linear1(x))
            x = self.dropout(x)
            x = F.log_softmax(self.linear2(x), dim=1)
            return x
    ```

    前面的代码片段包含一个定义网络架构的类(`__init__`方法)，以及信息转发过程中遵循的步骤(`forward`方法)。

7.  Define all of the parameters that are required to train your model. Set the number of epochs to `50`:

    ```
    model = CNN()
    loss_function = nn.NLLLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    epochs = 50
    ```

    我们为此练习选择的优化器是 Adam。此外，负对数似然被用作损失函数，如本书前一章所述。

    如果您的机器有可用的 GPU，模型的实例化应该按如下方式完成:

    ```
    model = CNN().to("cuda")
    ```

8.  Train your network and be sure to save the values for the loss and accuracy of both the training and validation sets:

    ```
    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []
    x_axis = []
    # For loop through the epochs
    for e in range(1, epochs+1):
        losses = 0
        acc = 0
        iterations = 0
        model.train()
        """
        For loop through the batches (created using 
        the train loader)
        """
        for data, target in train_loader:
            iterations += 1
            # Forward and backward pass of the training data
            pred = model(data)
            loss = loss_function(pred, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            losses += loss.item()
            p = torch.exp(pred)
            top_p, top_class = p.topk(1, dim=1)
            acc += accuracy_score(target, top_class)
        dev_losss = 0
        dev_accs = 0
        iter_2 = 0
        # Validation of model for given epoch
        if e%5 == 0 or e == 1:
            x_axis.append(e)
            with torch.no_grad():
                model.eval()
                """
                For loop through the batches of 
                the validation set
                """
                for data_dev, target_dev in dev_loader:
                    iter_2 += 1
                    dev_pred = model(data_dev)
                    dev_loss = loss_function(dev_pred, target_dev)
                    dev_losss += dev_loss.item()
                    dev_p = torch.exp(dev_pred)
                    top_p, dev_top_class = dev_p.topk(1, dim=1)
                    dev_accs += accuracy_score(target_dev, \
                                               dev_top_class)
            # Losses and accuracy are appended to be printed
            train_losses.append(losses/iterations)
            dev_losses.append(dev_losss/iter_2)
            train_acc.append(acc/iterations)
            dev_acc.append(dev_accs/iter_2)
            print("Epoch: {}/{}.. ".format(e, epochs), \
                  "Training Loss: {:.3f}.. "\
                  .format(losses/iterations), \
                  "Validation Loss: {:.3f}.. "\
                  .format(dev_losss/iter_2), \
                  "Training Accuracy: {:.3f}.. "\
                  .format(acc/iterations), \
                  "Validation Accuracy: {:.3f}"\
                  .format(dev_accs/iter_2))
    ```

    如果您的计算机有可用的 GPU，则对前面的代码进行一些修改，如下所示:

    ```
    train_losses, dev_losses, train_acc, dev_acc= [], [], [], []
    x_axis = []
    # For loop through the epochs
    for e in range(1, epochs+1):
        losses = 0
        acc = 0
        iterations = 0

        model.train()
        """
        For loop through the batches 
        (created using the train loader)
        """
        for data, target in train_loader:
            iterations += 1
            # Forward and backward pass of the training data
            pred = model(data.to("cuda"))
            loss = loss_function(pred, target.to("cuda"))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            losses += loss.item()
            p = torch.exp(pred)
            top_p, top_class = p.topk(1, dim=1)
            acc += accuracy_score(target.to("cpu"), \
                   top_class.to("cpu"))
        dev_losss = 0
        dev_accs = 0
        iter_2 = 0
        # Validation of model for given epoch
        if e%5 == 0 or e == 1:
            x_axis.append(e)
            with torch.no_grad():
                model.eval()
                """
                For loop through the batches of 
                the validation set
                """
                for data_dev, target_dev in dev_loader:
                    iter_2 += 1
                    dev_pred = model(data_dev.to("cuda"))
                    dev_loss = loss_function(dev_pred, \
                               target_dev.to("cuda"))
                    dev_losss += dev_loss.item()
                    dev_p = torch.exp(dev_pred)
                    top_p, dev_top_class = dev_p.topk(1, dim=1)
                    dev_accs += \
                    accuracy_score(target_dev.to("cpu"), \
                                   dev_top_class.to("cpu"))
            # Losses and accuracy are appended to be printed
            train_losses.append(losses/iterations)
            dev_losses.append(dev_losss/iter_2)
            train_acc.append(acc/iterations)
            dev_acc.append(dev_accs/iter_2)
            print("Epoch: {}/{}.. ".format(e, epochs), \
                  "Training Loss: {:.3f}.. "\
                  .format(losses/iterations), \
                  "Validation Loss: {:.3f}.. "\
                  .format(dev_losss/iter_2), \
                  "Training Accuracy: {:.3f}.. "\
                  .format(acc/iterations), \
                  "Validation Accuracy: {:.3f}"\
                  .format(dev_accs/iter_2))
    ```

9.  Plot the loss and accuracy of both sets. To plot the loss, use the following code:

    ```
    plt.plot(x_axis,train_losses, label='Training loss')
    plt.plot(x_axis, dev_losses, label='Validation loss')
    plt.legend(frameon=False)
    plt.show()
    ```

    结果图应类似于下图:

    ![Figure 4.23: Resulting plot showing the loss of the sets
    ](img/B15778_04_23.jpg)

    ```
    plt.plot(x_axis, train_acc, label="Training accuracy")
    plt.plot(x_axis, dev_acc, label="Validation accuracy")
    plt.legend(frameon=False)
    plt.show()
    ```

    该图应类似于下图:

    ![Figure 4.24: Resulting plot showing the accuracy of the sets
    ](img/B15778_04_24.jpg)

    图 4.24:显示集合精确度的结果图

    可以看出，在第 15 个时期之后，过度拟合开始影响模型。

10.  Check the model's accuracy on the testing set:

    ```
    model.eval()
    iter_3 = 0
    acc_test = 0
    for data_test, target_test in test_loader:
        iter_3 += 1
        test_pred = model(data_test)
        test_pred = torch.exp(test_pred)
        top_p, top_class_test = test_pred.topk(1, dim=1)
        acc_test += accuracy_score(target_test, top_class_test)
    print(acc_test/iter_3)
    ```

    使用我们之前创建的数据加载器，可以对测试集数据执行图像分类，以估计模型对未知数据的准确性。

    如果您的计算机有可用的 GPU，则对前面的代码进行一些修改，如下所示:

    ```
    model.eval()
    iter_3 = 0
    acc_test = 0
    for data_test, target_test in test_loader:
        iter_3 += 1
        test_pred = model(data_test.to("cuda"))
        test_pred = torch.exp(test_pred)
        top_p, top_class_test = test_pred.topk(1, dim=1)
        acc_test += accuracy_score(target_test .to("cpu"), \
                                   top_class_test .to("cpu"))
    print(acc_test/iter_3)
    ```

    测试集的准确性与其他两个集达到的准确性非常相似，这意味着该模型能够在看不见的数据上表现得同样好；应该在 72%左右。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3gjvWuV](https://packt.live/3gjvWuV)。

    本节目前没有在线交互示例，需要在本地运行。

    要访问这个源代码的 GPU 版本，请参考[https://packt.live/2BUGjGF](https://packt.live/2BUGjGF)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

## 活动 4.02:实施数据扩充

### 解决方案

1.  Duplicate the notebook from the previous activity.

    要完成此活动，除了修改`transforms`值之外，不会修改任何代码，如下步骤所示。

2.  Change the definition of the `transform` variable so that it includes, in addition to normalizing and converting the data into tensors, the following transformations:

    对于训练/验证集，使用概率为 50% ( `0.5`)的`RandomHorizontalFlip`函数和概率为 10% ( `0.1`)的`RandomGrayscale`函数。

    对于测试集，不要添加任何其他的转换:

    ```
    transform = \
    {"train": transforms.Compose([\
              transforms.RandomHorizontalFlip(0.5), \
              transforms.RandomGrayscale(0.1),\
              transforms.ToTensor(),\
              transforms.Normalize((0.5, 0.5, 0.5), \
                                   (0.5, 0.5, 0.5))]),\
    "test": transforms.Compose([\
            transforms.ToTensor(),\
            transforms.Normalize((0.5, 0.5, 0.5), \
                                 (0.5, 0.5, 0.5))])}
    ```

3.  Train the model for 100 epochs.

    如果您的机器有可用的 GPU，请确保使用代码的 GPU 版本来训练模型。

    训练集和验证集的损失和准确性的结果图应类似于此处所示:

    ![Figure 4.25: Resulting plot showing the loss of the sets
    ](img/B15778_04_25.jpg)

    图 4.25:显示器械包丢失的结果图

    ![Figure 4.26: Resulting plot showing the accuracy of the sets
    ](img/B15778_04_26.jpg)

    图 4.26:显示集合精确度的结果图

    通过添加数据扩充，可以提高模型的性能，并减少发生的过度拟合。

4.  Calculate the accuracy of the resulting model on the testing set.

    该模型在测试集上的性能提高了 75%左右。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3ePcAND](https://packt.live/3ePcAND)。

    本节目前没有在线交互示例，需要在本地运行。

    要访问该源代码的 GPU 版本，请参考[https://packt.live/38jpq4g](https://packt.live/38jpq4g)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

## 活动 4.03:实现批量标准化

### 解决方案

1.  Duplicate the notebook from the previous activity.

    要完成本练习，除了按照以下步骤在网络架构中添加一些层之外，不需要修改任何代码。

2.  Add batch normalization to each convolutional layer, as well as to the first fully connected layer.

    网络的最终架构应该如下所示:

    ```
    class CNN(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(3, 10, 3, 1, 1)
            self.norm1 = nn.BatchNorm2d(10)
            self.conv2 = nn.Conv2d(10, 20, 3, 1, 1)
            self.norm2 = nn.BatchNorm2d(20)
            self.conv3 = nn.Conv2d(20, 40, 3, 1, 1)
            self.norm3 = nn.BatchNorm2d(40)
            self.pool = nn.MaxPool2d(2, 2)
            self.linear1 = nn.Linear(40 * 4 * 4, 100)
            self.norm4 = nn.BatchNorm1d(100)
            self.linear2 = nn.Linear(100, 10)
            self.dropout = nn.Dropout(0.2)
        def forward(self, x):
            x = self.pool(self.norm1(F.relu(self.conv1(x))))
            x = self.pool(self.norm2(F.relu(self.conv2(x))))
            x = self.pool(self.norm3(F.relu(self.conv3(x))))
            x = x.view(-1, 40 * 4 * 4)
            x = self.dropout(x)
            x = self.norm4(F.relu(self.linear1(x)))
            x = self.dropout(x)
            x = F.log_softmax(self.linear2(x), dim=1)
            return x
    ```

3.  Train the model for 100 epochs.

    如果您的机器有可用的 GPU，请确保使用代码的 GPU 版本来训练模型。训练集和验证集的损失和准确性的结果图应类似于以下所示:

    ![Figure 4.27: Resulting plot showing the loss of the sets
    ](img/B15778_04_27.jpg)

    图 4.27:显示器械包丢失的结果图

    ![Figure 4.28: Resulting plot showing the loss of the sets
    ](img/B15778_04_28.jpg)

    图 4.28:显示器械包丢失的结果图

    虽然在模型中再次引入了过拟合，但是我们可以看到两组的性能都上升了。

    注意

    虽然本章没有探讨，但理想的做法是在网络架构中增加压差，以降低高方差。请随意尝试，看看您是否能够进一步提高性能。

4.  Calculate the accuracy of the resulting model on the testing set.

    该模型在测试集上的性能达到了 78%左右。

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/31sSR2G 的。

    本节目前没有在线交互示例，需要在本地运行。

    要访问这个源代码的 GPU 版本，请参考[https://packt.live/3eVgp4g](https://packt.live/3eVgp4g)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

# 5。风格转移

## 活动 5.01:进行风格转移

### 解决方案

1.  Import the required libraries:

    ```
    import numpy as np
    import torch
    from torch import nn, optim
    from PIL import Image
    import matplotlib.pyplot as plt
    from torchvision import transforms, models
    ```

    如果您的机器有可用的 GPU，请确保定义一个名为`device`的变量，这将有助于为 GPU 分配一些变量，如下所示:

    ```
    device = "cuda"
    ```

2.  指定要对输入图像执行的变换。一定要把它们调整到相同的大小，转换成张量，归一化:

    ```
    imsize = 224 loader = \ transforms.Compose([transforms.Resize(imsize), \                     transforms.ToTensor(),\                     transforms.Normalize((0.485, 0.456, 0.406), \                                          (0.229, 0.224, 0.225))])
    ```

3.  Define an image loader function. It should open the image and load it. Call the image loader function to load both input images:

    ```
    def image_loader(image_name):
        image = Image.open(image_name)
        image = loader(image).unsqueeze(0)
        return image
    content_img = image_loader("images/landscape.jpg")
    style_img = image_loader("images/monet.jpg")
    ```

    如果您的计算机有可用的 GPU，请使用以下代码片段:

    ```
    def image_loader(image_name):
        image = Image.open(image_name)
        image = loader(image).unsqueeze(0)
        return image
    content_img = image_loader("images/landscape.jpg").to(device)
    style_img = image_loader("images/monet.jpg").to(device)
    ```

4.  为了能够显示图像，设置转换以恢复图像的正常化，并将张量转换成`PIL`图像:

    ```
    unloader = transforms.Compose([\            transforms.Normalize((-0.485/0.229, \                                  -0.456/0.224, \                                  -0.406/0.225), \                                 (1/0.229, 1/0.224, 1/0.225)),\            transforms.ToPILImage()])
    ```

5.  Create a function (`tensor2image`) that's capable of performing the previous transformation over tensors. Call the function for both images and plot the results:

    ```
    def tensor2image(tensor):
        image = tensor.clone()
        image = image.squeeze(0)
        image = unloader(image)
        return image
    plt.figure()
    plt.imshow(tensor2image(content_img))
    plt.title("Content Image")
    plt.show()
    plt.figure()
    plt.imshow(tensor2image(style_img))
    plt.title("Style Image")
    plt.show()
    ```

    如果您的计算机有可用的 GPU，请使用以下代码片段:

    ```
    def tensor2image(tensor):
        image = tensor.to("cpu").clone()
        image = image.squeeze(0)
        image = unloader(image)
        return image
    plt.figure()
    plt.imshow(tensor2image(content_img))
    plt.title("Content Image")
    plt.show()
    plt.figure()
    plt.imshow(tensor2image(style_img))
    plt.title("Style Image")
    plt.show()
    ```

6.  Load the VGG-19 model:

    ```
    model = models.vgg19(pretrained=True).features
    for param in model.parameters():
        param.requires_grad_(False)
    ```

    如果您的机器有可用的 GPU，请确保将包含您的模型的变量分配给 GPU，如下所示:

    ```
    model.to(device)
    ```

7.  Create a dictionary for mapping the index of the relevant layers (keys) to a name (values). Then, create a function to extract the feature maps of the relevant layers. Use them to extract the features of both input images.

    下面的函数应该为每个相关层提取给定图像的特征:

    ```
    relevant_layers = {'0': 'conv1_1', '5': 'conv2_1', \
                       '10': 'conv3_1', '19': 'conv4_1', \
                       '21': 'conv4_2', '28': 'conv5_1'}
    def features_extractor(x, model, layers):
        features = {}
        for index, layer in model._modules.items():
            x = layer(x)
            if index in layers:
                features[layers[index]] = x
        return features
    ```

    接下来，应该为`content`和`style`图像调用该函数:

    ```
    content_features = features_extractor(content_img, \
                                          model, \
                                          relevant_layers)
    style_features = features_extractor(style_img, model, \
                                        relevant_layers)
    ```

8.  Calculate the gram matrix for the style features. Also, create the initial target image.

    以下代码片段为用于提取样式特征的每个图层创建了 gram 矩阵:

    ```
    style_grams = {}
    for i in style_features:
        layer = style_features[i]
        _, d1, d2, d3 = layer.shape
        features = layer.view(d1, d2 * d3)
        gram = torch.mm(features, features.t())
        style_grams[i] = gram
    ```

    接下来，创建初始目标映像作为内容映像的克隆:

    ```
    target_img = content_img.clone().requires_grad_(True)
    ```

    如果您的计算机有可用的 GPU，请使用以下代码片段:

    ```
    target_img = content_img.clone().\
                 requires_grad_(True).to(device)
    ```

9.  设置不同风格层的权重，以及内容和风格损失的权重:

    ```
    style_weights = {'conv1_1': 1., 'conv2_1': 0.8, \                  'conv3_1': 0.6, 'conv4_1': 0.4, \                  'conv5_1': 0.2} alpha = 1 beta = 1e5
    ```

10.  运行模型 500 次迭代。在开始训练模型之前定义 Adam 优化算法，使用`0.001`作为学习率:

    ```
    print_statement = 500 optimizer = torch.optim.Adam([target_img], lr=0.001) iterations = 5000 for i in range(1, iterations+1):     # Extract features for all relevant layers     target_features = features_extractor(target_img, model, \                                          relevant_layers)     # Calculate the content loss     content_loss = torch.mean((target_features['conv4_2'] \                                - content_features['conv4_2'])**2)     # Loop through all style layers     style_losses = 0     for layer in style_weights:         # Create gram matrix for that layer         target_feature = target_features[layer]         _, d1, d2, d3 = target_feature.shape         target_reshaped = target_feature.view(d1, d2 * d3)         target_gram = torch.mm(target_reshaped, \                                target_reshaped.t())         style_gram = style_grams[layer]         # Calculate style loss for that layer         style_loss = style_weights[layer] * \                      torch.mean((target_gram - \                                  style_gram)**2)         #Calculate style loss for all layers         style_losses += style_loss / (d1 * d2 * d3)     # Calculate the total loss     total_loss = alpha * content_loss + beta * style_losses     # Perform back propagation     optimizer.zero_grad()     total_loss.backward()     optimizer.step()     # Print the target image     if i % print_statement == 0 or i == 1:         print('Total loss: ', total_loss.item())         plt.imshow(tensor2image(target_img))         plt.show()
    ```

11.  Plot the `content`, `style`, and `target` images to compare the results:

    ```
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    ax1.imshow(tensor2image(content_img))
    ax2.imshow(tensor2image(target_img))
    ax3.imshow(tensor2image(style_img))
    plt.show()
    ```

    从该代码片段中得出的图应该类似于以下所示:

![Figure 5.11: Output plots
](img/B15778_05_11.jpg)

图 5.11:输出图

注意

要查看 高质量的彩色图像，请访问本书在 https://packt.live/2KcORcw.的 GitHub 知识库

要访问该特定部分的源代码，请参考[https://packt.live/2BZj91B](https://packt.live/2BZj91B)。

本节目前没有在线交互示例，需要在本地运行。

要访问该源代码的 GPU 版本，请参考[https://packt.live/3eNfvqc](https://packt.live/3eNfvqc)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

# 6。用 RNNs 分析数据序列

## 活动 6.01:使用简单的 RNN 进行时间序列预测

### 解决方案

1.  导入所需的库，如下:

    ```
    import pandas as pd import matplotlib.pyplot as plt import torch from torch import nn, optim
    ```

2.  Load the dataset and then slice it so that it contains all the rows but only the columns from index 1 to 52:

    ```
    data = pd.read_csv("Sales_Transactions_Dataset_Weekly.csv")
    data = data.iloc[:,1:53]
    data.head()
    ```

    输出如下所示:

    ![Figure 6.26: Displaying dataset for columns from index 1 to 52
    ](img/B15778_06_26.jpg)

    图 6.26:显示从索引 1 到 52 的列的数据集

3.  Plot the weekly sales transactions of five randomly chosen products from the entire dataset. Use a random seed of `0` when performing random sampling in order to achieve the same results as in the current activity:

    ```
    plot_data = data.sample(5, random_state=0)
    x = range(1,53)
    plt.figure(figsize=(10,5))
    for i,row in plot_data.iterrows():
        plt.plot(x,row)
    plt.legend(plot_data.index)
    plt.xlabel("Weeks")
    plt.ylabel("Sales transactions per product")
    plt.show()
    ```

    结果图应该如下所示:

    ![Figure 6.27: Plot of the output
    ](img/B15778_06_27.jpg)

    图 6.27:输出图

4.  Create the `inputs` and `targets` variables that will be fed to the network to create the model. These variables should be of the same shape and be converted into PyTorch tensors.

    `inputs`变量应该包含除上周之外的所有周的所有产品的数据，因为模型的想法是预测最后一周。

    `targets`变量应该比`inputs`变量领先一步；也就是说，`targets`变量的第一个值应该是输入变量的第二个值，依此类推，直到`targets`变量的最后一个值(应该是在`inputs`变量之外的最后一周):

    ```
    data_train = data.iloc[:,:-1]
    inputs = torch.Tensor(data_train.values).unsqueeze(1)
    targets = data_train.shift(-1, axis="columns", \
                               fill_value=data.iloc[:,-1])\
                               .astype(dtype = "float32")
    targets = torch.Tensor(targets.values) 
    ```

5.  Create a class containing the architecture of the network. Note that the output size of the fully connected layer should be `1`:

    ```
    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers):
            super().__init__()
            self.hidden_size = hidden_size
            self.rnn = nn.RNN(input_size, hidden_size, \
                              num_layers, batch_first=True)
            self.output = nn.Linear(hidden_size, 1)
        def forward(self, x, hidden):
            out, hidden = self.rnn(x, hidden)
            out = out.view(-1, self.hidden_size)
            out = self.output(out)
            return out, hidden
    ```

    和前面的活动一样，这个类包含一个`__init__`方法和网络架构，以及一个`forward`方法，用于确定信息在各层之间的流动。

6.  Instantiate the `class` function containing the model. Feed the input size, the number of neurons in each recurrent layer (`10`), and the number of recurrent layers (`1`):

    ```
    model = RNN(data_train.shape[1], 10, 1) 
    model
    ```

    运行上述代码会显示以下输出:

    ```
    RNN(
      (rnn): RNN(51, 10, batch_first=True)
      (output): Linear(in_features=10, out_features=1, bias=True)
    )
    ```

7.  定义损失函数、优化算法和训练网络的历元数。使用 MSE 损失函数、Adam 优化器和 10，000 个时期来完成此操作:

    ```
    loss_function = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 10000
    ```

8.  Use a `for` loop to perform the training process by going through all the epochs. In each epoch, a prediction must be made, along with the subsequent calculation of the loss function and the optimization of the parameters of the network. Save the loss of each of the epochs:

    ```
    losses = []
    for i in range(1, epochs+1):
        hidden = None
        pred, hidden = model(inputs, hidden)
        target = targets[:,-1].unsqueeze(1)
        loss = loss_function(targets, pred)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        if i%1000 == 0:
            print("epoch: ", i, "=... Loss function: ", losses[-1])
    ```

    输出应该如下所示:

    ```
    epoch:  1000 ... Loss function:  58.48879623413086
    epoch:  2000 ... Loss function:  24.934917449951172
    epoch:  3000 ... Loss function:  13.247632026672363
    epoch:  4000 ... Loss function:  9.884735107421875
    epoch:  5000 ... Loss function:  8.778228759765625
    epoch:  6000 ... Loss function:  8.025042533874512
    epoch:  7000 ... Loss function:  7.622503757476807
    epoch:  8000 ... Loss function:  7.4796295166015625
    epoch:  9000 ... Loss function:  7.351718902587891
    epoch:  10000 ... Loss function:  7.311776161193848
    ```

9.  Plot the losses of all epochs, as follows:

    ```
    x_range = range(len(losses))
    plt.plot(x_range, losses)
    plt.xlabel("epochs")
    plt.ylabel("Loss function")
    plt.show()
    ```

    结果图应该如下所示:

    ![Figure 6.28: Plot displaying the losses of all epochs
    ](img/B15778_06_28.jpg)

    图 6.28:显示所有时期损失的图

10.  Using a scatter plot, display the predictions that were obtained in the last epoch of the training process against the ground truth values (that is, the sales transactions of the last week):

    ```
    x_range = range(len(data))
    target = data.iloc[:,-1].values.reshape(len(data),1)
    plt.figure(figsize=(15,5))
    plt.scatter(x_range[:20], target[:20])
    plt.scatter(x_range[:20], pred.detach().numpy()[:20])
    plt.legend(["Ground truth", "Prediction"])
    plt.xlabel("Product")
    plt.ylabel("Sales Transactions")
    plt.xticks(range(0, 20))
    plt.show()
    ```

    最终的剧情应该是这样的:

![Figure 6.29: Scatter plot displaying predictions
](img/B15778_06_29.jpg)

图 6.29:显示预测的散点图

注意

要访问该特定部分的源代码，请参考[https://packt.live/2BqDWvg](https://packt.live/2BqDWvg)。

你也可以在[https://packt.live/3ihPgKB](https://packt.live/3ihPgKB)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

## 活动 6.02:利用 LSTM 网络生成文本

### 解

1.  导入所需的库，如下:

    ```
    import math import numpy as np import matplotlib.pyplot as plt import torch from torch import nn, optim import torch.nn.functional as F
    ```

2.  打开*爱丽丝梦游仙境*的文字，读到笔记本里。打印前 50 个字符的摘录和文本文件的总长度:

    ```
    with open('alice.txt', 'r', encoding='latin1') as f:     data = f.read() print("Extract: ", data[:50]) print("Length: ", len(data))
    ```

3.  Create a variable containing a list of the unduplicated characters in your dataset. Then, create a dictionary that maps each character to an integer, where the characters will be the keys and the integers will be the values:

    ```
    chars = list(set(data))
    indexer = {char: index for (index, char) in enumerate(chars)}
    ```

    输出应该如下所示:

    ```
    Extract:  ALICE was beginning to get very tired of sitting b
    Length:  145178
    ```

4.  Encode each letter of your dataset to its paired integer. Print the first 50 encoded characters and the total length of the encoded version of your dataset:

    ```
    indexed_data = []
    for c in data:
        indexed_data.append(indexer[c])
    print("Indexed extract: ", indexed_data[:50])
    print("Length: ", len(indexed_data))
    ```

    输出如下所示:

    ```
    Indexed extract:  [51, 52, 29, 38, 28, 25, 11, 59, 39, 25, 16, 53, 2, 1, 26, 26, 1, 26, 2, 25, 56, 60, 25, 2, 53, 56, 25, 23, 53, 7, 45, 25, 56, 1, 7, 53, 13, 25, 60, 14, 25, 39, 1, 56, 56, 1, 26, 2, 25, 16]
    Length:  145178
    ```

5.  Create a function that takes in a batch and encodes it as a one-hot matrix:

    ```
    def index2onehot(batch):
        batch_flatten = batch.flatten()
        onehot_flat = np.zeros((batch.shape[0] \
                                * batch.shape[1],len(indexer)))
        onehot_flat[range(len(batch_flatten)), batch_flatten] = 1
        onehot = onehot_flat.reshape((batch.shape[0], \
                                      batch.shape[1], -1))
        return onehot
    ```

    这个函数接受一个二维矩阵并将其展平。接下来，它创建一个填充零的矩阵，该矩阵具有展平矩阵的形状和包含字母表的字典的长度(在*步骤 3* 中创建)。接下来，它用 1 填充对应于该批中每个字符的字母。最后，它重塑矩阵，使之成为三维的。

6.  Create a class that defines the architecture of the network. This class should contain an additional function that initializes the states of the LSTM layers:

    ```
    class LSTM(nn.Module):
        def __init__(self, char_length, hidden_size, n_layers):
            super().__init__()
            self.hidden_size = hidden_size
            self.n_layers = n_layers
            self.lstm = nn.LSTM(char_length, hidden_size,\
                                n_layers, batch_first=True)
            self.output = nn.Linear(hidden_size, char_length)
        def forward(self, x, states):
            out, states = self.lstm(x, states)
            out = out.contiguous().view(-1, self.hidden_size)
            out = self.output(out)
            return out, states
        def init_states(self, batch_size):
            hidden = next(self.parameters())\
                          .data.new(self.n_layers, batch_size, \
                          self.hidden_size).zero_()
            cell = next(self.parameters())\
                   .data.new(self.n_layers,batch_size, \
                   self.hidden_size).zero_()
            states = (hidden, cell)
            return states
    ```

    这个类包含一个定义网络体系结构的`__init__`方法，一个确定通过各层的数据流的`forward`方法，以及一个用零初始化隐藏和单元状态的`init_state`方法。

7.  确定要从数据集创建的批次数量，记住每个批次应包含 100 个序列，每个序列的长度为 50。接下来，将编码数据拆分成 100 个序列:

    ```
    # Number of sequences per batch n_seq = 100  seq_length = 50 n_batches = math.floor(len(indexed_data) \             / n_seq / seq_length) total_length = n_seq * seq_length * n_batches x = indexed_data[:total_length] x = np.array(x).reshape((n_seq,-1))
    ```

8.  Instantiate your model by using `256` as the number of hidden units for a total of two recurrent layers:

    ```
    model = LSTM(len(chars), 256, 2)
    model
    ```

    运行上述代码会显示以下输出:

    ```
    LSTM(
      (lstm): LSTM(70, 256, num_layers=2, batch_first=True)
      (output): Linear(in_features=256, out_features=70, bias=True)
    )
    ```

    如果您的机器有可用的 GPU，请确保使用以下代码片段将模型分配给 GPU:

    ```
    model = LSTM(len(chars), 256, 2).to("cuda")
    ```

9.  Define the loss function and the optimization algorithms. Use the Adam optimizer and the cross-entropy loss to do this. Train the network for `20` epochs:

    ```
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    epochs = 20
    ```

    如果您的机器有可用的 GPU，请尝试运行`500`个时期的训练过程:

    ```
    epochs = 500
    ```

10.  In each epoch, the data must be divided into batches with a sequence length of 50\. This means that each epoch will have 100 batches, each with a sequence of 50:

    ```
    losses = []
    for e in range(1, epochs+1):
        states = model.init_states(n_seq)
        batch_loss = []
        for b in range(0, x.shape[1], seq_length):
            x_batch = x[:,b:b+seq_length]
            if b == x.shape[1] - seq_length:
                y_batch = x[:,b+1:b+seq_length]
                y_batch = np.hstack((y_batch, indexer["."] \
                          * np.ones((y_batch.shape[0],1))))
            else:
                y_batch = x[:,b+1:b+seq_length+1]
            x_onehot = torch.Tensor(index2onehot(x_batch))
            y = torch.Tensor(y_batch).view(n_seq * seq_length)
            pred, states = model(x_onehot, states)
            loss = loss_function(pred, y.long())
            optimizer.zero_grad()
            loss.backward(retain_graph=True)
            optimizer.step()
            batch_loss.append(loss.item())
        losses.append(np.mean(batch_loss))
        if e%2 == 0:
            print("epoch: ", e, "... Loss function: ", losses[-1])
    ```

    输出应该如下所示:

    ```
    epoch:  2 ... Loss function:  3.1667490992052802
    epoch:  4 ... Loss function:  3.1473221943296235
    epoch:  6 ... Loss function:  2.897721455014985
    epoch:  8 ... Loss function:  2.567064647016854
    epoch:  10 ... Loss function:  2.4197753791151375
    epoch:  12 ... Loss function:  2.314083896834275
    epoch:  14 ... Loss function:  2.2241266349266313
    epoch:  16 ... Loss function:  2.1459227183769487
    epoch:  18 ... Loss function:  2.0731402758894295
    epoch:  20 ... Loss function:  2.0148646708192497
    ```

    如果您的计算机有可用的 GPU，则用于训练网络的等效代码片段如下:

    ```
    losses = []
    for e in range(1, epochs+1):
        states = model.init_states(n_seq)
        batch_loss = []
        for b in range(0, x.shape[1], seq_length):
            x_batch = x[:,b:b+seq_length]
            if b == x.shape[1] - seq_length:
                y_batch = x[:,b+1:b+seq_length]
                y_batch = np.hstack((y_batch, indexer["."] \
                                     * np.ones((y_batch.shape[0],1))))
            else:
                y_batch = x[:,b+1:b+seq_length+1]
            x_onehot = torch.Tensor(index2onehot(x_batch))\
                       .to("cuda")
            y = torch.Tensor(y_batch).view(n_seq * \
                                           seq_length).to("cuda")
            pred, states = model(x_onehot, states)
            loss = loss_function(pred, y.long())
            optimizer.zero_grad()
            loss.backward(retain_graph=True)
            optimizer.step()
            batch_loss.append(loss.item())
        losses.append(np.mean(batch_loss))
        if e%50 == 0:
            print("epoch: ", e, "... Loss function: ", \
                  losses[-1])
    ```

    运行 500 个时期的训练过程的结果如下:

    ```
    epoch:  50 ... Loss function:  1.5207843986050835
    epoch:  100 ... Loss function:  1.006190665836992
    epoch:  150 ... Loss function:  0.5197970939093622
    epoch:  200 ... Loss function:  0.24446514968214364
    epoch:  250 ... Loss function:  0.0640328845073437
    epoch:  300 ... Loss function:  0.007852113484565553
    epoch:  350 ... Loss function:  0.003644719101681278
    epoch:  400 ... Loss function:  0.006955199634078248
    epoch:  450 ... Loss function:  0.0030021724242973945
    epoch:  500 ... Loss function:  0.0034294885518992768
    ```

    可以看出，通过运行更多时期的训练过程，损失函数达到更低的值。

11.  Plot the progress of the loss over time:

    ```
    x_range = range(len(losses))
    plt.plot(x_range, losses)
    plt.xlabel("epochs")
    plt.ylabel("Loss function")
    plt.show()
    ```

    该图表应如下所示:

    ![Figure 6.30: Chart displaying the progress of the loss function
    ](img/B15778_06_30.jpg)

    图 6.30:显示损失函数进度的图表

    正如我们所看到的，在 20 个时期之后，损失函数仍然可以减少，这就是为什么强烈建议训练更多的时期，以便从模型中获得良好的结果。

12.  Feed the following sentence `starter` into the trained model for it to complete the sentence: `"So she was considering in her own mind "`:

    ```
    starter = "So she was considering in her own mind "
    states = None
    ```

    如果您的机器有可用的 GPU，则将模型分配回 CPU 以执行预测:

    ```
    model    = model.to("cpu")
    ```

    首先，使用一个`for`循环将种子输入到模型中，以便生成内存。接下来，执行预测，如下面的代码片段所示:

    ```
    for ch in starter:
        x = np.array([[indexer[ch]]])
        x = index2onehot(x)
        x = torch.Tensor(x)
        pred, states = model(x, states)
    counter = 0
    while starter[-1] != "." and counter < 100:
        counter += 1
        x = np.array([[indexer[starter[-1]]]])
        x = index2onehot(x)
        x = torch.Tensor(x)
        pred, states = model(x, states)
        pred = F.softmax(pred, dim=1)
        p, top = pred.topk(10)
        p = p.detach().numpy()[0]
        top = top.numpy()[0]
        index = np.random.choice(top, p=p/p.sum())
        starter += chars[index]
    print(starter)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2Bs6dRZ](https://packt.live/2Bs6dRZ)。

    本节目前没有在线交互示例，需要在本地运行。

    要访问该源代码的 GPU 版本，请参考[https://packt.live/3g9X6UI](https://packt.live/3g9X6UI)。这个版本的源代码不能作为在线交互示例，需要在本地运行 GPU 设置。

## 活动 6.03:为情感分析执行自然语言处理

### 解决方案

1.  导入所需的库:

    ```
    import pandas as pd import numpy as np import matplotlib.pyplot as plt from string import punctuation from sklearn.metrics import accuracy_score import torch from torch import nn, optim import torch.nn.functional as F
    ```

2.  加载包含来自亚马逊的 1000 条产品评论的数据集，这些评论带有标签`0`(针对负面评论)或`1`(针对正面评论)。将数据分成两个变量——一个包含评论，另一个包含标签:

    ```
    data = pd.read_csv("amazon_cells_labelled.txt", sep="\t", \                    header=None) reviews = data.iloc[:,0].str.lower() sentiment = data.iloc[:,1].values
    ```

3.  删除评论中的标点:

    ```
    for i in punctuation:     reviews = reviews.str.replace(i,"")
    ```

4.  创建一个包含所有评论词汇的变量。此外，创建一个字典，将每个单词映射到一个整数，其中单词是键，整数是值:

    ```
    words = ' '.join(reviews) words = words.split() vocabulary = set(words) indexer = {word: index for (index, word) \            in enumerate(vocabulary)}
    ```

5.  通过用成对整数替换评论中的每个单词来对评论数据进行编码:

    ```
    indexed_reviews = [] for review in reviews:     indexed_reviews.append([indexer[word] \                             for word in review.split()])
    ```

6.  Create a class containing the architecture of the network. Make sure that you include an embedding layer:

    ```
    class LSTM(nn.Module): 
        def __init__(self, vocab_size, embed_dim, \
                     hidden_size, n_layers):
            super().__init__()
            self.hidden_size = hidden_size
            self.embedding = nn.Embedding(vocab_size, embed_dim)
            self.lstm = nn.LSTM(embed_dim, hidden_size, \
                                n_layers, batch_first=True)
            self.output = nn.Linear(hidden_size, 1)
        def forward(self, x):
            out = self.embedding(x)
            out, _ = self.lstm(out)
            out = out.contiguous().view(-1, self.hidden_size)
            out = self.output(out)
            out = out[-1,0]
            out = torch.sigmoid(out).unsqueeze(0)
            return out
    ```

    该类包含一个定义网络架构的`__init__`方法和一个决定数据流经不同层的方式的`forward`方法。

7.  Instantiate the model using 64 embedding dimensions and 128 neurons for three LSTM layers:

    ```
    model = LSTM(len(vocabulary), 64, 128, 3)
    model
    ```

    运行上述代码将显示以下输出:

    ```
    LSTM(
      (embedding): Embedding(1905, 64)
      (lstm): LSTM(64, 128, num_layers=3, batch_first=True)
      (output): Linear(in_features=128, out_features=1, bias=True)
    )
    ```

8.  定义损失函数、优化算法和要训练的时期数。例如，您可以使用二进制交叉熵损失作为损失函数、Adam 优化器，并训练 10 个时期:

    ```
    loss_function = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 10
    ```

9.  Create a `for` loop that goes through the different epochs and through every single review individually. For each review, perform a prediction, calculate the loss function, and update the parameters of the network. Additionally, calculate the accuracy of the network on that training data:

    ```
    losses = []
    acc = []
    for e in range(1, epochs+1):
        single_loss = []
        preds = []
        targets = []
        for i, r in enumerate(indexed_reviews):
            if len(r) <= 1:
                continue
            x = torch.Tensor([r]).long()
            y = torch.Tensor([sentiment[i]])
            pred = model(x)
            loss = loss_function(pred, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            final_pred = np.round(pred.detach().numpy())
            preds.append(final_pred)
            targets.append(y)
            single_loss.append(loss.item())
        losses.append(np.mean(single_loss))
        accuracy = accuracy_score(targets,preds)
        acc.append(accuracy)
        if e%1 == 0:
            print("Epoch: ", e, "... Loss function: ", losses[-1], \
                  "... Accuracy: ", acc[-1])
    ```

    与前面的活动一样，训练过程包括进行预测，将其与地面实况进行比较以计算损失函数，以及执行反向传递以最小化损失函数。

10.  Plot the progress of the loss and accuracy over time. The following code is used to plot the loss function:

    ```
    x_range = range(len(losses))
    plt.plot(x_range, losses)
    plt.xlabel("epochs")
    plt.ylabel("Loss function")
    plt.show()
    ```

    该图应如下所示:

![Figure 6.31: Plot displaying the progress of the loss function 
](img/B15778_06_31.jpg)

图 6.31:显示损失函数进程的图

以下代码用于绘制准确度分数:

```
x_range = range(len(acc))
plt.plot(x_range, acc)
plt.xlabel("epochs")
plt.ylabel("Accuracy score")
plt.show()
```

该图应如下所示:

![Figure 6.32: Plot displaying the progress of the accuracy score
](img/B15778_06_32.jpg)

图 6.32:显示准确度分数进度的图

注意

要访问该特定部分的源代码，请参考[https://packt.live/2VyX0ON](https://packt.live/2VyX0ON)。

本节目前没有在线交互示例，需要在本地运行。