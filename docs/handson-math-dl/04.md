# 概率与统计

在这一章中，我们将讨论数学中最重要的两个领域——概率和统计。这是两个你在日常生活中可能会多次遇到的术语。人们用它来证明几乎所有发生的事情，或者当他们试图证明一个观点时。一旦你读完了这一章，你就会对它们有一个牢固的掌握，并且会理解它们之间的联系和区别。

本章将涵盖以下主题:

*   理解概率中的概念
*   统计学中的基本概念

# 理解概率中的概念

概率论是最重要的数学领域之一，对于理解和创建深度神经网络至关重要。我们将在接下来的章节中探讨这个声明的细节。然而，现在我们将集中精力对这一领域有一个复杂的了解。

我们使用概率论来理解某一事件发生的可能性。一般来说，概率论是关于理解和处理不确定性的。

# 经典概率

假设我们有一个随机变量，它将随机实验的结果映射到我们感兴趣的属性。上述随机变量衡量一组或多组结果发生的可能性(概率)。我们称之为**概率分布**。将概率分布视为我们将在本章学习的概念的基础。

概率论中有三个非常重要的概念——概率空间、随机变量和概率分布。让我们从定义一些更基本但重要的概念开始。

样本空间是所有可能结果的集合。我们用ω表示。假设我们有 *n* 个可能的结果——那么，我们有 [![](img/03483c0d-c6a6-40c6-a11b-d549d6f04381.png)] ，其中 *w [ i ]* 是一个可能的结果。样本空间(ω)的子集被称为**事件**。

概率与集合有很大的关系，所以让我们看一下一些符号，这样我们可以更好地理解后面的概念和例子。

假设我们有两个事件， *A* 和 *B* ，⊆ω。我们有以下公理:

*   *A* 的补码是*A^C，所以 [![](img/14e34629-3242-4a47-bcb0-43522e1133e2.png)] 。*
*   如果出现 *A* 或 *B* 中的任何一个，则写为*A*∨*B*(读作 *A* 联管节 *B* )。
*   如果 *A* 和 *B* 都出现，则写成 *A* ∩ *B* (读作 *A* 相交 *B* )。
*   如果 *A* 和 *B* 互斥(或不相交)，那么我们写[T5。]
*   如果 *A* 的发生暗示了 *B* 的发生，这就写成 *A* ⊆ *B* (所以， [![](img/4b689d79-0135-4b8d-8948-f4bf83eccd1f.png)] )。

比方说我们有一个事件，*一个*∈ω， [![](img/b873dd7c-ea17-454c-91a4-a22a9af938b2.png)] 。在这种情况下，发生 *A* 的概率定义如下:

![](img/ed32339c-6ea3-4bb9-9942-dd74e74be8a8.png)

这是一个可能出现的次数除以样本空间中可能结果的总数。

让我们来看一个简单的抛硬币的例子。在这里，样本空间由抛硬币的所有可能结果组成。假设我们正在处理两次抛硬币，而不是一次，h 表示正面，t 表示反面。所以，样本空间为ω= {*hh*， *ht* ， *th* ， *tt* }。

所有可能的实验结果构成了事件空间， [![](img/7c6782f4-9b02-457e-85e9-4a5cb44dea0c.png)] 。完成实验后，我们观察结果ω∈ω是否在 *A* 中。

由于在每个事件 [![](img/8f4046d7-1601-4bff-942c-182af8351ed6.png)] 中，我们将 *P(A)* 表示为事件发生的概率，将 *P(A)* 表示为 *A* 发生的概率。

继续前面的公理，![](img/f153cd8f-ff75-4d39-a2e6-2a5ac0c57fa4.png)必须满足以下条件:

*   [![](img/ce7395f4-137c-41dd-85ee-d8cd61f4a8c6.png)] 为 [![](img/cd1f2c74-89cb-4f58-8bcf-52f941de60a6.png)的所有情况。]
*   [![](img/328ba247-2525-48ad-b8f8-cb892d6c9e77.png)。]
*   如果事件 *A [1] ，A [2] ，…* 是不相交且可数相加的——即 [![](img/52252828-6a1c-482a-b70f-fdfd4acdcbff.png)] 对于 *i，j—* 的所有情况，我们则有 [![](img/76bfbe42-185c-44c1-863e-8c6596d23690.png)] 。

这三个 [![](img/b1f980f2-fbe3-4210-baa0-d9054288c774.png)] 项被称为**概率空间**。

根据经验，当 [![](img/4cba0ddd-83cb-459c-b409-116f6e546a6a.png)] 时，那么事件 *A* 几乎肯定会发生，而当 [![](img/80a9ab87-9a8f-40ec-bb44-9da306b708d3.png)] 时，那么事件 *A* 几乎不会发生。

使用前面的公理，我们可以得出以下结论:

![](img/cce00473-b188-4254-8b79-23afa7b1d346.png)

所以， [![](img/8db9764d-1664-4563-b77b-309fa3c815b9.png)] 。

此外，如果我们有两个事件， *A* 和 *B* ，那么我们可以推导如下:

![](img/1fc2aa1f-e307-4543-85a9-2f5fb36d29c7.png)。

继续前面的公理，![](img/8371eae4-d87f-4036-b7d4-8a1029cd54ec.png)必须满足以下条件:

[![](img/26f1c3b3-f2c9-4272-8c72-8baf9b72ad71.png)] 为所有![](img/17dae569-967d-4756-8fa8-ae42e139d788.png)

为了找到任何事情的概率，我们通常要计数。假设我们有一个装满网球的桶，我们从桶中选择一个球 *r* 次；因此，第一次挑选有*n[1]种可能性，下一次挑选有*n[2]种可能性，依此类推。选择的总数最终是*n[1]*×*n[2]*×…×*n[r]*。**

# 替换或不替换的采样

现在让我们假设桶中总共有 *n* 件物品，我们必须从中挑选 *r* 件。然后，设 *R* = {1，2，…， *r* }为选取的物品列表，设 *N* = {1，2，…， *n* }为物品总数。这可以写成一个函数，如下所示:

![](img/cf6492fe-1af9-473f-b963-143026c23a44.png)

这里， *f(i)* 是第 *i ^(th)* 项。

替换抽样是指我们随机选取一个项目，然后将它放回原处，以便可以再次选取该项目。

但是，无替换抽样是指当我们选择了一个项目，没有把它放回去，所以我们不能再次挑选它。让我们看一个两者都有的例子。

假设我们需要打开办公室的门，我们有一个装有 n 把钥匙的包；它们看起来都一样，所以无法区分它们。

我们第一次试着挑选一把钥匙，在试了一次之后，我们替换了每一把，并且我们设法在第*次^次次*次试验中找到了正确的钥匙，这意味着我们错了*次 r-1 次*。那么概率如下:

![](img/7a8733ed-c9da-4b6d-b46a-6117990688f7.png)

现在，我们知道我们早期的策略不是最聪明的，所以这次我们再试一次，但没有替换，并消除每个不起作用的键。现在，概率如下:

![](img/78230e2f-b4fc-4135-9f22-1e5d89b51137.png)

# 多项式系数

我们从二项式定理(你可能在高中学过)中知道以下是正确的:

![](img/f2970ab3-938a-4597-b2b0-bc5681efece0.png)

那么，三项式如下:

![](img/bbdd5078-bca0-45bf-a2d7-6e3271b51324.png)

假设我们有 n 块糖果，有蓝色和红色的糖果。我们挑选糖果的不同方式定义为 [![](img/205923c8-e800-4e64-9691-b115fc61afaa.png)] ，读作 *n* choose *k* 。

多项式系数如下:

![](img/c57dc5e8-171c-4061-861b-bbcb5d65d7ba.png)

这样，我们将 *n* 物品分散到 *k* 位置，其中*I^(th)位置有*n[I]物品。**

例如，假设我们在玩牌，我们有四个玩家。一副牌有 52 张牌，我们给每位玩家发 13 张牌。因此，我们分发卡片的可能方式如下:

![](img/04468143-efde-422b-a276-f0a185113692.png)

这绝对是巨大的！

这就是斯特灵公式的用处。它让我们可以近似地得到答案。

# 斯特灵公式

为便于论证，姑且说 [![](img/969912b8-c618-420f-b5d6-f444094be46a.png)] 。

我们知道以下是真实的:

![](img/120d5df1-3fae-42d9-8977-14c3afc08e6f.png)

然而，我们现在声明如下:

![](img/77a36317-3913-431f-91b3-3338b59705f8.png)

这可以用下面的例子来说明:

![](img/c3bd9297-5033-4be2-90bc-c81f869c67a3.png)

现在，通过计算积分，我们得到如下结果:

![](img/5996c50a-6dc4-4fd5-9e31-7a79e24cd05a.png)

我们现在将两边除以 [![](img/65fcf4e9-46f2-44f1-86a3-6d79546a039a.png)] ，取极限为 *n* →∞。我们观察到双方都倾向于 1。因此，我们有以下内容:

![](img/02479d10-afcf-4829-b03c-59563dd2e27e.png)

斯特灵公式指出，当 *n* →∞时，以下为真:

![](img/ee3d7e9a-6bd8-41ff-91aa-81833dc1956c.png)

此外，我们还有以下内容:

![](img/4baec502-e54a-49c8-9fcf-f1c08b717816.png)

我们将避免研究斯特林公式的证明，但如果你有兴趣了解更多，那么我强烈建议你去查一下。

# 独立性ˌ自立性

当事件彼此不相关时，它们是独立的；也就是说，一方的结果对另一方的结果没有影响。

假设我们有两个独立的事件， *A* 和 *B* 。然后，我们可以测试以下内容:

![](img/cb614af3-1c4a-4f04-8e52-a5318d6cbc33.png)

如果这不是真的，那么事件是相关的。

想象你在一个赌场，你在玩骰子。你掷出两个骰子——它们的结果彼此独立。

独立性的一个有趣性质是，如果 *A* 和 *B* 是独立事件，那么 *A* 和*B^C也是独立事件。*

让我们来看看这是如何工作的:

![](img/cd31d53f-a97a-42f1-87e6-63b3e6c6ec8f.png)

当我们有多个事件， *A [1] ，A [2] ，…，A [n]* 时，我们称它们相互独立当 [![](img/695b94c5-04a8-4ff6-a77f-7c414be5023f.png)] 对于所有 n ≥ 2 的情况。

让我们假设我们在一个实验室里进行两个实验；我们将它们分别建模为 [![](img/eb5c64f6-516f-4b14-92ec-0b12c114fa20.png)] 和 [![](img/a6189c28-6064-4f67-8a8d-37a4d3fd4546.png)] ，并且每个的概率分别为 [![](img/c902723a-4564-43f4-8f0b-bcf14067b398.png)] 和 [![](img/2ee881bc-5e46-45ed-b2a6-73fdef48877f.png)] 。如果这两者是独立的，那么我们有以下内容:

![](img/6a48b451-9e46-4b58-bb9f-eebfda13b1bb.png)

这是针对 *i* 和 *j* 的所有情况，我们新的样本空间是ω=ω[1]×ω[2]。

现在，假设 *A* 和 *B* 分别是ω[1]和ω[2]实验中的事件。通过计算*A*×ω[2]和*B*×ω[1]，我们可以将它们视为新样本空间ω的子空间，这导致如下结果:

![](img/e820dfc2-bb78-4337-9629-b11834f636e2.png)

尽管我们通常将独立性定义为同一实验中不同的(不相关的)结果，但我们也可以将其扩展到任意数量的独立实验。

# 离散分布

离散指的是我们的样本空间是可数的，比如掷硬币或掷骰子。

在离散概率分布中，样本空间是 [![](img/1f0f9681-eaf3-4830-9acc-f33efb7c2a14.png)] 和 [![](img/4c061723-d8be-4c59-b3c4-9eee1a942865.png)] 。

下面是我们在概率论中经常遇到的六种不同的离散分布:

*   伯努利分布
*   二项分布
*   几何分布
*   超几何分布
*   泊松分布

让我们按顺序定义它们。

对于伯努利分布，我们以抛硬币为例，我们的样本空间为ω= {*H*， *T* }(其中 *H* 是正面， *T* 是反面，*p*∈【0，1】)(即 0 ≤ *p* ≤ 1)。我们将分布表示为 *B(1* ， *p)* ，因此适用以下情况:

![](img/38aeae95-1952-46f4-bc03-e0580c813b3e.png)和![](img/727c7a77-7f02-4e99-be6b-30cdbcf245d1.png)

但是现在，让我们假设硬币被抛 *n* 次，每次都有前面提到的概率 *p* 结果是正面。然后，表示为 *B(n，p)* 的二项式分布陈述如下:

![](img/58b3b7cd-d069-43c3-a738-26637fb1d32f.png)

因此，我们有以下内容:

![](img/14d4f623-e99a-4871-beae-e4cd9e66d2a1.png)

一般来说，二项分布的写法如下:

![](img/e9042b37-85f2-43e0-897d-126e311be650.png)

几何分布不保留对过去事件的任何记忆，因此是无记忆的。假设我们再掷一次硬币。这种分布并没有给我们任何迹象表明什么时候我们可以期待一个正面的结果，或者需要多长时间。所以，我们把得到反面后得到正面的概率写为 k 次:

![](img/3a7870f1-c865-41d3-a626-2925a1c3ad34.png)

假设我们有一个装满两种颜色的球的桶——红色和黑色(我们将分别表示为 *r* 和 *b* )。从桶中，我们挑出了 *n* 个球，我们想计算出这些球中 *k* 是黑色的概率。为此，我们使用超几何分布，如下所示:

![](img/7c3f3385-715f-4863-ac0e-87f255c86da8.png)

泊松分布与其他分布略有不同。它用于模拟发生率为λ的罕见事件。它表示为 *P* (λ),写为:

![](img/2cd9c44f-25d0-47b5-b14a-3f04d224a443.png)

对于![](img/7f86ec8f-cdfa-4954-9dbe-9ec39271d282.png)的所有情况都是如此。

# 条件概率

当一个事件的发生导致另一个事件的发生时，条件概率是有用的。如果我们有两个事件， *A* 和 *B* ，其中 *B* 已经发生，我们想要找出 *A* 发生的概率，我们写如下:

![](img/8a27e0f3-306e-40ba-8f96-20cf46c4eaf7.png)

这里， [![](img/a5c8df71-3eed-4f7a-9531-b04e228d1302.png)] 。

然而，如果两个事件 *A* 和 *B* 是独立的，那么我们得到如下结果:

![](img/9542fb47-8681-4348-a6fc-a3018fa81ad9.png)

另外，如果 [![](img/f199d6b1-6442-4d5c-be27-70b5a03cb636.png)] ，那么就说 *B* 吸引 *A* 。但是，如果 *A* 吸引*B^C，那么它排斥 *B* 。*

*A* 与 *B* 之间的吸引力是双向的，即 *A* 只有在 *B* 也吸引 *A* 的情况下，才能吸引 *B* 。

以下是条件概率的一些公理:

*   [T59。]
*   [![](img/1c9aace0-180f-4c10-8a56-bbdccf62233b.png)。]
*   [![](img/b4681247-54dd-4a50-989e-42c0301cbdb6.png)。]
*   [![](img/ddc98d3f-4690-4e64-b416-86d99c44a22b.png)] 是一个概率函数，只对 *B* 的子集有效。
*   [T1。]
*   如果 [![](img/0fac1325-7227-45d1-8259-7993ffdcd7e7.png)] ，那么 [![](img/78aa0939-baff-4621-be22-fda466c2b9b3.png)。]

下面的等式被称为**贝叶斯法则**:

![](img/eba7347c-2eba-4534-8e0f-5c4b6ceb6c11.png)

这也可以这样写:

![](img/08f22faa-5df1-4592-812d-927234cb80ee.png)

在这里，我们有以下内容:

*   [![](img/d40d8ca3-29b7-4eab-8bdb-fc91e3ac74ba.png)] 被称为先验。
*   [![](img/60a49fa8-3b1b-4c4f-b49c-5b275bb67e8a.png)] 是后路。

*   [![](img/b95e6129-fc56-45ca-adb4-e7f1708596ea.png)] 是可能性。
*   [![](img/e2485bc2-e169-43f7-ade0-1918bc0c93e3.png)] 充当归一化常数。

![](img/25435825-223f-4374-8b78-d0306c1b6f66.png)符号读作**与**成比例。

通常，我们最终不得不处理复杂的事件，为了有效地导航它们，我们需要将它们分解成更简单的事件。

这就引出了分区的概念。划分被定义为一起构成样本空间的事件的集合，使得对于 *B [i]* 、 [![](img/198293d4-0671-48fe-97ce-74737f46784f.png)] 的所有情况。

在抛硬币的例子中，样本空间被分成两个可能的事件——正面和反面。

如果 *A* 是一个事件，而*B[I]是ω的一个分区，那么我们有如下:*

![](img/1eabf79c-aa99-4b5b-9605-939e8729ae05.png)

我们也可以用分区重写贝叶斯公式，这样我们就有了:

![](img/0736c821-dc23-4e6a-be02-12f01db2d752.png)

在这里， [![](img/87bd5d8b-31bc-4dbe-a2fb-9d312e0d0478.png)] 。

# 随机变量

随机变量是具有概率分布的变量，概率分布决定了每个变量的值。我们把随机变量看成一个函数，*X*:ω→ω[X]，其中 [![](img/b8558ada-ef3c-49aa-a5a6-5e57caf2362c.png)] 。 *X* 功能的范围由 [![](img/4701c9e8-5093-4914-92fb-be7bab0db877.png)] 表示。

离散随机变量是可以取有限或可数无限值的随机变量。

假设我们有*S*∈ω[x]:

![](img/0bd78772-a702-46ed-b3cd-c3f07ff03847.png)

这是 *S* 是包含结果的集合的概率。

在随机变量的情况下，我们看的是随机变量有某个值的概率，而不是获得某个事件的概率。

如果我们的样本空间是可数的，那么我们有:

![](img/055c9c4f-a4ca-47f4-b353-8ead0bf17f68.png)

假设我们有一个骰子， *X* 是掷骰子后的结果。那么，我们对于 *X* 的样本空间为ω[X]= { 1，2，3，4，5，6}。假设这个骰子是公平的(无偏的)，那么我们有如下结果:

![](img/3ad1fa48-6b05-4bea-a9c6-b93cb0caecaa.png)

当我们有有限数量的可能结果，并且每个结果都有一个相等的概率，这样每个结果都和其他结果一样可能，我们称之为离散均匀分布。

就说*X∞B(n，p)* 吧。那么， *X* 取值为 *r* 的概率如下:

![](img/1e2dedc4-e425-4955-a88b-81f971c1b46a.png)

概率文献中有时会将 [![](img/c8735e61-805e-49cd-a060-f6218d412033.png)] 写成 [![](img/245fad37-79a6-4e5f-b78d-218e0a473baa.png)] 。

很多时候，我们可能需要找到一个随机变量的期望值(平均值)。我们使用以下公式来实现这一点:

![](img/004396e7-e662-4bdc-92e4-a63490b6f256.png)

我们也可以将前面的等式写成以下形式:

![](img/5e8c359d-bf77-4e4c-a64d-c5f36f01ef2b.png)

前面两个方程只在我们的样本空间是离散的(可数的)时才有效。

以下是 [![](img/5cf75ec8-3673-457e-a8fc-0c62f46a0936.png)] 的一些公理:

*   如果说 [![](img/d4b3ea81-83a1-4553-915c-8bce6eb92ffc.png)] ，那么 [![](img/fbc2a53b-b69d-4dae-9d19-85ce2fdb67a4.png)。]
*   如果 [![](img/03296dd8-2f4e-4e0c-9a7a-3b33676f2e30.png)] 和 [![](img/6bb14c02-8a1d-4a8f-9d4f-49ab4dbed99b.png)] ，那么 [![](img/4b0493af-740c-4f82-a7bb-dc7204f67705.png)。]
*   [![](img/6a2553b3-dbc8-4dd2-b18d-67ad22964f86.png)。]
*   [![](img/42579c09-73b2-4116-be94-97624bd1790f.png)] ，给定α和β为常数且 *X [ i ]* 不独立。
*   [![](img/ff887879-65ec-4106-8a62-c0fe59bb0152.png)] ，当 *X [ i ]* 独立时成立。
*   [![](img/6c6a0c03-1d5f-4c5a-a632-214648189d54.png)] 最小化 [![](img/aa32ed28-6eb5-4b9f-91e2-be23c9285d96.png)] 超过 *c.*

假设我们有 n 个随机变量。那么，它们的期望值如下:

![](img/d94a83f2-fdf3-406e-ba58-e943201395e0.png)

现在我们已经很好地理解了实值随机变量的期望，是时候继续定义两个重要的概念了——方差和标准变量。

# 差异

我们将 *X* 的方差定义如下:

![](img/4a3bb4a1-a56d-4d53-94d2-46cc66eba155.png)

*X* 的标准偏差是方差的平方根:

![](img/2f27bf3c-b2f4-47bb-88b2-5bc2bc53b470.png)

我们可以认为这是期望值(平均值)的展开或接近程度。如果它们高度分散，那么它们具有高方差，但是如果它们被组合在一起，那么它们具有低方差。

下面是一些需要记住的方差属性:

*   [![](img/19af9c52-e0b1-49f8-8bbe-93d03d0d730a.png)。]
*   如果 [![](img/3566903a-644f-4cec-a028-6a0123f750b7.png)] ，那么 [![](img/677ce2ec-3e15-4719-9d56-392e0d0633fe.png)。]
*   [T1。]
*   [![](img/dd3ff0ee-dd34-4695-96b5-d085c3fe207f.png)。]
*   [![](img/531e3907-2cb6-4aa6-a9bc-1a40cc31f921.png)] ，鉴于所有的 *X [ i ]* 值都是独立的。

让我们假设我们现在有![](img/516e70ce-60bc-44c4-acbd-94bc71b47ef0.png)个离散随机变量。那么，它们是独立的，如果我们采取以下方式:

![](img/0b1bf323-a745-4bf0-8b7a-4bb63342b3e0.png)

现在，让我们的 *n 个*随机变量相互独立并且**同分布** ( **iid** )。我们现在有以下内容:

![](img/551a1a1b-6731-4dcd-91db-7a56262636b6.png)

这个概念非常重要，尤其是在统计学中。这意味着，如果我们想减少实验结果的方差，那么我们可以多次重复实验，样本平均值将有一个小的方差。

例如，让我们想象两根长度未知的绳子——分别是 *a* 和 *b* 。因为物体是绳子——因此是非刚性的——我们可以测量绳子的长度，但我们的测量可能不准确。设 *A* 为绳索 *a* 的测量值， *B* 为绳索 *b* 的测量值，则得到:

![](img/4c083980-6c63-493c-ae42-32bef1eb65f7.png)

我们可以通过测量 *X = A + B* 和*Y = A–B*来提高测量的准确度。现在，我们可以使用以下公式估算出 *a* 和 *b* :

![](img/23416a25-98bc-4170-93bb-241446ffd02b.png)

现在，[![](img/76a6252d-ef6a-4d28-8d0f-e986ba6b3f66.png)][![](img/822a2a6a-8bdd-489c-aff6-70f4ad2be5d0.png)]，这两个都是不偏不倚的。此外，我们可以看到，通过使用以下方法，我们的测量中方差已经减小:

![](img/67d1ce11-28dc-4962-8acc-4efbcc8a300d.png)

由此，我们可以清楚地看到，一起测量绳索而不是分别测量，极大地提高了我们的准确度。

# 多维随机变量

很多时候，我们最终会处理不止一个随机变量。当我们有两个或更多的变量时，我们可以检查随机变量之间的线性关系。我们称之为协方差。

如果我们有两个随机变量， *X* 和 *Y* ，那么协方差定义如下:

![](img/bc6d132f-3893-4563-8932-b44b1582dc89.png)

以下是协方差的一些公理:

*   如果 *c* 是常数，那么 [![](img/ef4a1b71-62cd-4671-b335-ea90679559be.png)。]
*   [![](img/63f053f8-cc51-48de-9b9e-bd84f5356c62.png)。]
*   [![](img/23ce2b25-59d8-41f3-9233-d3ba3ec1d647.png)。]
*   [![](img/a2d96a5d-f76c-4a42-9361-1bc747db4af9.png)。]
*   [![](img/2f649eb8-737f-4b05-9496-774cc17f0354.png)。]
*   [![](img/eedd1cad-fc30-4c14-a2f6-b90a20e6f9aa.png)。]

*   [![](img/6d88bb9d-91f6-47cd-8259-573532a1a3fa.png)] ，鉴于 *X* 和 *Y* 是独立的(但并不暗示两者是独立的)。
*   [![](img/26014aa6-4f93-4a08-8d52-30cec07ee128.png)]

然而，有时，协方差不能给我们两个变量之间相关性的全貌。这可能是 *X* 和 *Y* 变化的结果。为此，我们将协方差归一化如下，并得到相关性:

![](img/a5140026-c84e-4c14-a7e2-21ea1ed1b64f.png)

结果值将始终位于[-1，1]区间内。

这就引出了条件分布的概念，这里我们有两个随机变量， *X* 和 *Y* ，它们不是独立的，我们有联合分布， [![](img/64a8a6ed-302d-4a67-9093-745dd54328c1.png)] ，从中我们可以得到概率， [![](img/21ffaa8e-d944-4ee7-9e1f-8e509f0afad5.png)] 和 [![](img/39351bdf-7e86-469a-b743-fe6340495765.png)] 。然后，我们的分布定义如下:

![](img/45c4ff9a-381d-4ebe-b8ee-bf9423196043.png)

从这个定义中，我们可以发现给定 *Y* 的 *X* 的条件分布如下:

![](img/796c562d-b757-40ed-81d2-46738adac170.png)

我们可能还想找到给定 *Y* 的 *X* 的条件期望，如下所示:

![](img/30f06fb6-c3c3-4bad-bc82-21ac9e43f0be.png)

现在，如果我们的随机变量是独立的，那么， [![](img/b1be2d62-c472-41a1-9bba-ebcd5eb08712.png)] ，我们知道这是真的，因为 *Y* 对 *X* 没有影响。

# 连续随机变量

到目前为止，我们已经研究了样本空间中的离散结果，我们可以找到某个结果的概率。但是现在，在连续空间中，我们会发现我们的结果的概率在一个特定的区间或范围内。

现在，为了求 *X* 的分布，我们需要定义一个函数， *f* ，使得 *X* 的概率一定位于区间 [![](img/5794cda3-9f1e-4c28-a0c2-0528c9e9d219.png)]

形式上，一个随机变量，![](img/a0f56366-bbe1-473a-932e-2f0dd2433fce.png)，是连续的，如果，在一个函数中， [![](img/5587b9ca-262a-4565-b0ae-050800428fdf.png)] 这样我们就有如下:

![](img/397c6a87-7c95-423f-be55-c89b2e07e011.png)

我们称函数为 *f* 、**概率密度函数** ( **PDF** )，它必须满足以下条件:

*   [T38]
*   [![](img/47a2c693-41bf-417a-b87e-07e887e2cffb.png)]

还有一个分布函数对我们来说很重要，叫做**累积分布函数**。如果我们有一个随机变量， *X* ，它可以是连续的，也可以是离散的，那么， [![](img/be3f65db-4bd9-4d17-862f-9f685064a4db.png)] ，其中 *F(x)* 增加，因此 x→∞，而 *F(x)→1* 。

当处理如下的连续随机变量时，我们知道 *F* 既是连续的又是可微的:

![](img/14e36303-5461-4190-97df-0161bfe46b72.png)

所以，当 *F* 可微时，那么*F’(x)= F(x)*。

需要注意的一个重要事实是 [![](img/8110eefa-b4e6-46ab-bce8-eb7a78d14921.png)] 。

这就引出了均匀分布的概念，一般来说，其 PDF 如下:

![](img/98c36358-0492-420d-bf4b-2cd22644eec1.png)

因此，我们有以下内容:

![](img/da6dfe92-c875-4bc5-a552-9f7065528072.png)

![](img/36e6ab23-5de9-4cc8-8bfd-363b3bdada5b.png)就是这种情况。

我们写 [![](img/f11dfa86-15fb-494d-ab79-46a81e7325d8.png)] 如果 *X* 在[ *a* ， *b* 区间上遵循均匀分布。

现在，假设我们的随机变量是一个指数随机变量，并且增加了λ参数。那么，它的 PDF 为 [![](img/eb50042a-970a-498a-abf9-35db9bbd73d0.png)] 和 [![](img/73d7aaf7-fe65-4ee3-b75f-daef267de5ec.png)] 为所有![](img/b8ba8fd6-7fa9-4c42-b141-35047f72bf49.png)。

我们把这个写成 [![](img/133a6020-8cb8-41aa-b915-cf44b7e8866d.png)] ，于是我们有了下面的:

![](img/67f53b45-88bc-4033-8617-c9e6c8c96206.png)

还有很重要的一点要注意，指数随机变量，比如几何随机变量，是无记忆的；也就是说，过去没有给我们任何关于未来的信息。

正如在离散的情况下，我们可以定义连续随机变量情况下的期望和方差。

连续随机变量的期望定义如下:

![](img/491ff6f1-6c48-43af-a140-5859c1eb5a37.png)

但是，说 [![](img/6865b917-c2dc-4e80-8bd6-a1820c209abb.png)] 。然后，我们有以下内容:

![](img/a6f9e16e-9bcd-43c3-9a6f-48892f6b07ea.png)

在连续随机变量的情况下，方差定义如下:

![](img/97c0330b-a838-4516-b36f-abe13945464b.png)

这为我们提供了以下信息:

![](img/bfa14771-c603-485a-9123-8c93635c78c0.png)

现在举个例子，我们就拿 [![](img/a0003821-f605-4b9a-86aa-ec551887e0a8.png)] 来说吧。我们可以找到 *X* 的期望值如下:

![](img/40507b44-b1d1-482b-aee3-b0b8767d27a6.png)

它的方差如下所示:

![](img/2d1dabf8-5441-4def-bf16-179da04a8de5.png)

现在，我们很好地处理了连续分布中的期望和方差。让我们熟悉两个适用于 pdf 的附加术语— **模式**和**中值**。

PDF 中的模式是出现最多的值；然而，该模式也可能出现不止一次。例如，在均匀分布中，所有的 *x* 值都可以被认为是众数。

假设我们有一个 PDF， *f(x)* 。然后，我们将模式表示为![](img/f5569622-b675-4c09-a442-6807f4feff3f.png)，因此 [![](img/faba697b-8583-4a38-bca9-c80725a5f714.png)] 适用于 *x* 的所有情况。

我们将中位数定义如下:

![](img/c72d59ef-4d49-4a8b-a1e1-2fee7c9f92e8.png)

然而，在离散情况下，中位数如下:

![](img/6647fff3-007c-4f71-8dff-c5de1cb8e908.png)

很多时候，在概率上，我们取样本均值而不是均值。假设我们有一个分布，它包含了 *X* 可以取的所有值。从中，我们随机抽取 *n* 个值，并对其进行平均，从而得到以下结果:

![](img/07b25103-74cf-473a-b795-f3ade978fcd8.png)

# 联合分布

到目前为止，我们已经处理和学习了与一个随机变量相关的分布；但是现在，假设我们有两个随机变量， *X* 和 *Y* 。然后，将它们的联合分布定义为 [![](img/02e5e753-145e-49c1-8dbf-50b3ec96ab65.png)] ，这样 [![](img/642b9988-79c5-41f2-972b-ff7c776ab2fa.png)] 。

在联合分布中，我们通常倾向于知道一组变量的分布，但有时，我们可能只想知道一个子集的分布。我们称之为边际分布。我们将 *X* 的边际分布定义如下:

![](img/264ab983-aaab-4166-93d7-52d4163cb71e.png)

假设我们的 *n* 连续随机变量在 *A* 中是联合分布的，并且有 *f* PDF。然后，我们有以下内容:

![](img/3cc0e9d1-769a-44ae-8206-d6e5dee70797.png)

在这里， [![](img/fc5aaac3-bedc-4b47-8aeb-bc804637c9c5.png)] 和 [![](img/10954a31-2c2c-40ff-9c51-cb0292cd9623.png)] 。

让我们回顾一下之前的例子，这里有两个变量， *X* 和 *Y* 。如果变量是连续的，那么它们的联合分布是 [![](img/856a0270-c44e-456c-a788-c3b218c6e345.png)] 和 [![](img/85602f52-e8c6-4f55-9900-929cc9caaf6d.png)] 。

如果随机变量是联合连续的，那么它们是单独连续的。

现在，假设我们的 *n 个*连续随机变量是独立的。于是， [![](img/087a0680-c7e3-4aea-9668-e49bbed64247.png)] 为一切 [![](img/5c1c0e0e-066a-4871-98b2-40086b5e44b9.png)] 的事例。

如果 [![](img/cee4f394-934d-462a-91d1-cbf325a20bde.png)] 是累积分布函数 [![](img/9ac7ae41-3c20-487a-82c7-1fc0756d0498.png)] 是 PDF，那么[![](img/28678354-b5ca-44e1-b2ba-e4989e8db5fe.png)][![](img/a6c3c3bd-f4a2-471f-bdce-d4fbd7320f90.png)]。

# 更多概率分布

在本章前面，我们在*随机变量*部分介绍了几种不同类型的分布。我敢肯定，在某些时候，你会想，连续随机变量肯定也有概率分布。

# 正态分布

下面的分布是相当重要的一个，被称为**正态分布**。它看起来如下:

![](img/7b730080-ba84-4519-af83-737c14403c67.png)

正态分布，写成 [![](img/c9515d39-e75d-48b0-b3b5-f7add3cd8a7d.png)] ，对于![](img/d8240eee-37e2-4651-9f72-0e708235db7e.png)的所有情况都有 [![](img/3266f88f-500d-4d4f-aa1c-1a3910ae3819.png)] PDF。

此外，我们还有以下内容:

![](img/b7f6d31a-c9ab-4ec9-9941-9b85ad4a14b8.png)

当正态分布有 [![](img/26958e7e-a86d-41d1-bc93-decf7b167b56.png)] 和 [![](img/43298051-a412-46a6-ac32-9b6b2169ca49.png)] 时，称为**标准正态**，我们将 [![](img/d92fbb28-181f-41d3-a34a-c47270d846a8.png)] 表示为其 PDF，将 [![](img/7333c5a0-2068-4f06-81aa-0a60962c816e.png)] 表示为其累积分布函数。

正态分布有一些相当有趣的性质，如下所示:

*   [![](img/58567752-2609-40f1-aa56-c2f78746af06.png)]
*   [![](img/96063efd-866b-46f5-8e96-cbab53db66cf.png)]

假设我们有两个独立的随机变量， [![](img/aab98e9d-b4ee-4ace-827e-72cac46491fa.png)] 和 [![](img/c4daaf9c-b4e1-419e-8d2d-1bf68cc8aca8.png)] ，那么我们有如下:

*   [![](img/125b6691-8eee-4e7b-8495-4dec478096f5.png)]
*   [![](img/d84498f6-bac9-4f46-b41e-ca2fe6788dcb.png)] ，其中*一*为常数

这个概率分布如此重要的原因是因为它与中心极限定理的关系，该定理指出，如果我们有大量独立且同分布的随机变量，那么它们的分布与正态分布大致相同。

# 多元正态分布

正态分布也可以扩展到多个随机变量，这就给出了多元正态分布。

假设我们有从 *N(0，1)* 采样的 *n* iid 个随机变量。然后，我们将它们的联合密度函数定义如下:

![](img/74c05128-6313-43d8-9335-32d7b6374910.png)

在这里， [![](img/e18d1a4d-fa10-4d61-93bb-b602d41fff07.png)] 。

让我们更进一步。现在，假设我们有一个可逆的 *n×n* 矩阵， *A* ，对 [![](img/fc7a3657-df9f-400c-90cc-c2ea98ebc7bc.png)] 感兴趣。然后， [![](img/0cf3cf14-81c9-4eb5-85bb-d5e2fc04ed9e.png)] 和 [![](img/a93c60cf-3e11-41c0-bdf7-bc1bea83cfe9.png)] 。因此，我们有以下内容:

![](img/6a4c4e92-f63b-42c1-8469-549356da815e.png)

这里， [![](img/ded03a25-2c8b-46e4-9343-f85f126ef864.png)] 。因此， *Z* 是多元正态，表示如下:

![](img/75bda44e-7c2b-4a8f-98b9-a655be6e1a58.png)

你可能想知道这个新矩阵![](img/362a6521-6110-43e2-b9e5-401be8c5ce07.png)代表什么。就是 *i ^第个*和 *j ^第个*条目为 [![](img/738b1a8c-7eed-4664-ade4-8d27f27cfb9c.png)] 的协方差矩阵。

在协方差为 0 的情况下，这意味着变量是独立的，那么 [![](img/99cb769c-ab24-46e6-aecd-f1fede83389e.png)] 。

# 二元正态分布

当多元正态分布中的 *n = 2* 时，这是一种特殊情况，称为**二元正态**。它的协方差矩阵写如下:

![](img/e12e12ad-1541-44fd-8e9b-d5bcb164b869.png)

与此相反的情况如下:

![](img/29b2ce8d-f9cd-4553-b232-901decd81a02.png)

在这种情况下，两个变量之间的相关性如下:

![](img/0ef43a72-df2e-4d22-b047-fc3ebb7186b1.png)

为简单起见，我们将假设平均值为 0，因此二元正态的联合 PDF 如下:

![](img/64738400-590a-4a68-b0cd-5eabede9aa71.png)

# 伽马分布

伽马分布是一种广泛使用的分布，用于模拟具有偏斜分布的正连续变量。

伽马分布由 [![](img/517d5ba1-6a78-43f1-9c50-259a3c6c63ef.png)] 表示，其 PDF 如下:

![](img/08cab1d3-8add-449d-9b48-94277d4304e1.png)

至此，我们结束了概率部分。我们现在将开始探索统计学。

# 统计学中的基本概念

虽然概率允许我们测量和计算事件或结果发生的几率，但统计学允许我们根据一些未知的概率模型生成的数据做出判断和决策。我们使用数据来学习潜在概率模型的属性。我们称这个过程为参数推断。

# 估计

在估计中，我们的目标是给定与 *X* (概率模型)具有相同分布的 *n* iid 个样本。如果 PDF 和**概率质量函数** ( **PMF** )是 [![](img/ae2cef0a-5beb-48e1-a5f1-b357c299efdd.png)] ，我们需要求θ。

形式上，我们将统计量定义为θ的估计值。

一个统计量是数据 [![](img/a159163d-ddda-4dcc-9526-8d8033b60adb.png)] 的一个函数 *T* ，这样我们的估计就是 [![](img/8a3e68cf-e6cb-43c1-806f-38b119a91fec.png)] 。因此， *T(x)* 是统计量的抽样分布，也是θ的一个估计量。

接下来， *X* 将表示随机变量， *x* 将表示观察值。

假设我们有 [![](img/daf3f330-a78a-4f17-b227-d2a95cf5ccc6.png)] ，分别是 iid [![](img/f518720d-8500-4abd-9a4a-a38158e29e02.png)] 。那么，μ的可能估计值如下:

![](img/62bf0b56-be84-44d0-b66f-3f13039485e8.png)

然而，我们对特定观察样本![](img/56509ab4-3bbd-48e5-8829-05aa83bd2c87.png)的估计如下:

![](img/ea2ae08f-b460-4fcc-9f51-008a2e5460df.png)

我们用来确定我们的估计量是否好的一个方法是偏差。偏差定义为真实值和期望值之间的差值，记为 [![](img/bf858bc5-6b59-4958-8d56-71d76ada0425.png)] 。如果 [![](img/c8594a74-3ed4-4b1f-a404-5a020d51b76f.png)] ，估计量是无偏的。

# 均方误差

**均方误差** ( **MSE** )是衡量一个估计量有多好的指标，是比偏差更好的指标。我们这样写:

![](img/4c9bb100-93e9-4e4f-bd5b-1bf00e13898e.png)

然而，有时，我们使用根 MSE，它是 MSE 的平方根。

我们也可以用偏差和方差来表示 MSE，如下所示:

![](img/0c5ec3d2-a3ff-4d65-ba22-cedc58161600.png)

有时，当我们试图得到一个低的 MSE 时，最好是有一个低方差的有偏估计量。我们称之为**偏差-方差权衡**。

# 自满

很多时候，我们进行实验的目的是找到θ的值，并找到更大的图景。一个充分的统计量是能给出我们想要的关于θ的所有信息的统计量。

幸运的是，因式分解定理使我们有能力找到足够的统计数据。它指出，如果我们有以下等式，T 对于θ就足够了:

![](img/79a178a9-82b0-4df0-ac68-a81db320635f.png)

这里， *g* 和 *h* 是任意函数。

一般来说，如果 *T* 是一个充分的统计量，那么它不会丢失关于θ的任何信息，并且最佳统计量是给我们最大缩减的统计量。我们称之为最小充分统计量；在其定义中， *T(X)* 是极小的，当且仅当它是其他统计量的函数。所以，如果 *T'(X)* 足够，那么*T '(X)= T '(Y)T(X)= T(Y)*。

假设 *T = T(X)* 是满足 [![](img/5917f1c3-e8b3-416f-bd2f-1aae73357098.png)] 的统计量，不依赖于![](img/0d2b7a06-a42c-42d8-b01a-d6739fd0df24.png)如果(且仅如果) [![](img/ee76df88-3853-4587-bd53-cd079be5662e.png)] 。那么， *T* 对于θ是最小充分的。

再往下，假设我们有 [![](img/96b42b02-2996-49c5-8dda-1246af243e19.png)] ，分别是 iid [![](img/b4cd8933-cd49-43b9-b1b7-bf12021c9ea3.png)] 。然后，我们可以推断如下:

![](img/133755f0-1aac-4542-9864-7e5039f719d2.png)

这是一个常量函数，告诉我们[![](img/783f2304-eb07-484a-98c9-24de5561fd53.png)][![](img/948fb818-3420-4e69-8358-c836dd80365d.png)]。因此， [![](img/a02a5ed6-6067-4dcf-b6ae-14ffa5e4068f.png)] 是最低限度的充足。

最小充分统计的优点是，它们使我们能够以最有效的方式存储我们的实验结果，并且我们可以使用它们来改进我们的估计量。

这就引出了 Rao-Blackwell 定理，该定理指出，如果![](img/db6ddaf1-2663-4333-a87d-637de9c065a9.png)是 *θ* 的充分统计量，如果[![](img/a108d990-0094-4925-a68f-a37bdcf669a8.png)是θ的估计量——其中对于所有θ， [![](img/3bdfcaa3-a418-430c-a307-40296fe59e89.png)] 。设[![](img/35a206e6-4978-406d-bb04-bbc1490146e6.png)]—那么，对于θ的所有情况，我们有 [![](img/7311f9c9-5d24-416b-9796-a6cc5ca7f449.png)] 。]

# 可能性

通常，在实践中，当我们想要确定我们的估计量是否良好时，我们通常使用**最大似然估计量** ( **MLE** )。

给定 *n 个*随机变量(其中 [![](img/cd38b80b-487a-484f-ada8-1fddf247ef63.png)] 为联合 PDF)，那么如果 *X* = *x* ，则θ的可能性定义为 [![](img/c9f407f0-3452-40cf-a527-7a4cb7f6a840.png)] 。所以θ的 MLE 是最大化 [![](img/0ee0e32d-136d-4f00-925e-b1171a35aaed.png)] 的θ值的估计。

然而，在实践中，我们最大化对数似然，而不是简单的似然。

回到具有 [![](img/39ab6250-e262-4cb7-851d-ba7c61c7dd5c.png)] PDF 的 *n* iid 随机变量的例子，似然性和对数似然性如下:

![](img/4199dc56-b225-46c1-9ddd-c97bbb09fe0f.png)

假设我们的 *n* 变量是伯努利 *(p)* 。然后， [![](img/3625b2c6-7b7b-43fb-a470-118a60af38eb.png)] 。因此， [![](img/ad2a5be1-766b-4e0a-8685-ad6e2e08df1d.png)] 当 [![](img/f3e522ff-9961-433c-a58a-a09743fa40d4.png)] 等于 0，是无偏 MLE。

到目前为止，您可能想知道 MLE 与充足性到底有什么关系。如果 *T* 对于θ是充分的，那么它的可能性是 [![](img/36758fea-8e6d-4b75-b8d2-3ac7cc1c1477.png)] 并且为了最大化我们的估计，我们必须最大化 *g* 。因此，最大似然估计是充分统计量的函数——瞧！

# 置信区间

置信区间使我们能够确定某些区间包含θ的概率。我们将其正式定义如下。

θ的 [![](img/681ddc42-f012-4a68-b1af-69eebfb41dcf.png)] 置信区间是一个随机区间 [![](img/35662868-9e85-4d24-91bb-ce235a60c5a1.png)] ，这样 [![](img/9be406c6-7923-4112-9901-14fcd25dc2f4.png)] ，与θ的真实值无关。

假设我们对多个样本计算 [![](img/b7b6d010-e660-4ac0-beeb-ba22b45b0df1.png)] ， *x* 。那么，其中的 100γ%就覆盖了我们的θ真值。

假设我们有 [![](img/336d3b2a-848c-433a-91f2-72e188d88ea2.png)] ，分别是 iid [![](img/95b73743-00c8-4468-8e6c-08ee0d06a391.png)] ，我们想为θ找到一个 95%的置信区间。我们知道 [![](img/afe448f0-f0cb-44cf-b5f6-e5527e674e45.png)] 所以说 [![](img/3816503b-9384-4341-a0dd-152fd78e9b48.png)] 。然后，我们选择 *z [1] ，z [ 2 ]* ，这样 [![](img/635c0c2d-2cf1-4047-b029-5ef99a37eed4.png)] ，其中*φ*为正态分布。所以，![](img/14036c42-8a8e-4d7f-9328-3adc2a948ff4.png)，从中我们得到如下置信区间:

![](img/fe4b4fd6-51a1-4fb2-9a17-c446d83809e0.png)

下面是一个常用于寻找置信区间的三步法:

1.  找到 [![](img/35755383-e8d4-4592-be2c-6b4a1408934e.png)] ，使得 [![](img/dd9a5964-7bb7-4ad3-a478-09e17182bd07.png)] 的 [![](img/8a4308b7-adaf-4688-9fd2-cc383d29121f.png)] 不依赖于θ。我们称之为**支点**。
2.  在 [![](img/e296bbe4-70ea-4fc5-be23-06bd2f89bdd4.png)] 上方，将概率语句写在 [![](img/ebc4503c-bac3-42a3-8e73-3b90d065f43d.png)] 表格中。
3.  重新排列不等式，找出区间。

通常， *c [1]* 和 *c [2]* 是来自已知分布的百分点；例如，对于 95%的置信区间，我们将有 2.5%和 97.5%的点。

# 贝叶斯估计

在关于统计的这一节中，我们讨论了所谓的频率主义方法。然而，现在我们将看看所谓的贝叶斯方法，其中我们将θ视为随机变量，我们倾向于具有关于分布的先验知识，并且在收集一些额外数据后，我们找到后验分布。

在形式上，我们在收集任何额外数据之前，将先验分布定义为θ的概率分布；我们将其表示为π(θ)。后验分布是θ的概率分布，取决于我们进行的实验的结果；我们将此表示为π(θ| **x** )。

先验分布和后验分布之间的关系如下:

![](img/e762a4cc-1ac5-4755-8824-d3ab137a5ea9.png)

通常，我们避免计算 [![](img/9395c0c8-e981-4be2-8458-e38143bb8c98.png)] ，我们只观察关系:

![](img/5d66e796-d136-45ce-a578-fbf4afaadb36.png)

我们可以把这个读成 [![](img/74de7aaa-f594-410d-bbc6-9b2767b8a3b3.png)] 。

在进行实验并得出后验概率后，我们需要确定一个估计量，但要找到最佳估计量，我们需要一个损失函数，如二次损失或绝对误差损失，以了解θ的真实值与参数的估计值相差多远。

假设我们正在估计的参数是 *b* 。然后，贝叶斯估计器 [![](img/6353d454-a2aa-4bfa-9a35-713cfa57d6ec.png)] 使期望后验损失最小化，如下所示:

![](img/c064708e-a605-48fe-b9ff-759a32df49c5.png)

如果我们选择损失函数为二次损失，那么我们有如下结果:

![](img/5b834307-84b1-417d-8e55-b4e1f71c5741.png)

然而，如果我们选择绝对误差损失，则我们得到:

![](img/49fb6cf7-2784-4171-abc0-e23b383eab1b.png)

由于后验分布是我们的真实分布，我们知道通过对其积分，我们的结果如下:

![](img/4ee0ddd1-21fd-472d-afbd-237329a5c0cf.png)

如果你想知道这两个统计学派是如何比较的，可以把频率主义者和贝叶斯分别看作是绝对和相对的。

# 假设检验

在统计学中，我们通常需要检验假设，而且最有可能的是，我们会比较两种不同的假设——零假设和替代假设。零假设告诉我们，我们的实验包含统计意义；也就是说，变量之间没有关系。另一个假设告诉我们，变量之间有关系。

一般来说，我们从假设零假设为真开始，为了拒绝这个假设，我们需要通过我们的实验找到与之相反的证据。

# 简单的假设

简单假设 *H* 是其中分布的参数被完全指定的假设，否则它被称为复合假设。

当针对替代假设( *H [1]* )测试零假设( *H [0]* )时，我们使用我们的测试将![](img/82552f43-137b-4265-be68-844864a4cff1.png)分成两个区域 *C* 和 [![](img/abe39872-54f7-4e0b-a2aa-ca18ce448641.png)] 。如果 [![](img/4c2ec251-ab81-483f-b8d0-07a756bfdcaa.png)] ，那么我们拒绝零假设，但是如果 [![](img/e919c215-066e-4b12-82e8-3b8c450dc3e8.png)] 那么我们不拒绝零假设。我们称 C 为临界区域。

当我们执行测试时，我们希望得到正确的结论，但是我们可能会犯以下两个错误中的任何一个:

*   **错误 1** :当*H[0]为真时拒绝*H[0]**
*   **错误 2** :当*H[0]为假时不剔除*H[0]**

如果*H[0]和*H[1]都是简单假设，那么我们有如下:**

![](img/11f0c189-4377-408a-bc9d-4b168590bb5a.png)

这里，α是我们测试的大小，1-β是测试寻找*H[1]的能力。*

如果我们有一个简单的假设，![](img/0c4f64dc-79ba-4632-b984-4d312944fd9e.png)，那么我们也想找出给定的 *x* 的可能性。我们的做法如下:

![](img/7bc50d6b-9756-4804-ace3-7df858919bd4.png)

我们还可以求出 *H [0]* 和 *H [1]* 给定 *x* 的似然比如下:

![](img/d198ad53-94d2-45e0-bf88-7c431c81f7ea.png)

给定 *k* ，似然比测试的关键区域如下:

![](img/c22a7637-2624-4853-9740-48b821e72e96.png)

让我们看一个例子，并对此有一点直觉。说我们有 [![](img/cd11ba88-9c52-4908-96c9-9949498a0168.png)] ，分别是 iid [![](img/63ca4aac-27dc-4692-8c12-66f6e44b807f.png)] ， [![](img/60b63337-d86d-4251-943c-46a0f8759010.png)] 是已知量。现在，我们要找出零假设 [![](img/c9937c97-b93b-4cfa-a006-c6e329820742.png)] 与备选假设 [![](img/ccc6f54f-ce88-4734-8f97-aea1ab769306.png)] 的最佳检验大小。让我们假设![](img/f6ca7aa5-a778-45b8-88c7-fafde0072f14.png)和![](img/53716cc1-40ca-4a63-ae09-698a9405961b.png)对我们来说也是已知的，比如![](img/1375019f-9b75-43f2-9e1b-014403651c7c.png)。因此，我们有以下内容:

![](img/0a7735fe-30ac-464f-a9e9-f01a1d32cfd8.png)

我们知道这个函数是递增的，所以 [![](img/b13e47c4-7554-40c9-ba8e-fe34d8fd0648.png)] 对于任意情况下的 *k* ，这告诉我们对于任意值的 *c* ，![](img/02ddd37a-721a-4352-8334-3075514fb4a9.png)。

我们选择![](img/dc98ae99-744e-4ee4-9ecd-6b2848a6840c.png)的值，使得 [![](img/3eb7536f-5213-40e6-b75c-550aa4a81aaf.png)] ，如果![](img/dd522a17-5624-43da-8a0d-3b76c397c0cc.png)，我们拒绝零假设。

在零假设下，[![](img/fe127bd0-6b64-476d-946e-1193deeca1b4.png)]；所以， [![](img/e668e5a7-9178-45ce-9290-5dcb3353df9d.png)] 。现在，从![](img/660eb9eb-f940-450e-9762-256b5cea13b3.png)开始，如果我们有以下情况，测试规模拒绝零假设:

![](img/57823923-6850-4e07-89e5-dac1414e4e70.png)

这就是所谓的*z*-测试，我们用它来测试一个假设，而 z 得分告诉我们数据点距离平均值有多少标准差。

这里，如果 *z > k* ，似然比拒绝零假设。测试尺寸为 [![](img/6e3a330d-7de5-4e1a-92f8-9b2c1a2fccea.png)] ，随着 *k* 的增大而减小。如果 [![](img/b8b93e75-b736-48ca-be20-4f0e33113171.png)] ，则 *z* 的值在拒绝区域。

在上式中， *p ^** ，称为*p*-数据的值，*x*；换句话说，就是观测数据(证据)反对零假设的概率。

# 复合假设

现在，如果我们有一个复合假设，比如 [![](img/8b35701f-e0c6-4d19-bcd2-6e80c38f4738.png)] ，错误概率不是单值的。

因此，我们定义幂函数如下:

![](img/c198677c-2b7f-407c-877a-fff0277edb72.png)。

理想情况下，我们希望 *W* ( *θ* )在零假设下较小，在替代假设下较大。

测试的尺寸是 [![](img/0629f5ba-0683-4dfe-b103-05452e24bfab.png)] ，这不是一个理想的尺寸。鉴于 [![](img/9ab50740-e94c-4137-99c0-5b6b5a09709e.png)] ， [![](img/34421f0b-e3c2-4fb8-abcf-d1e494f84515.png)] 。

之前，我们看到了由以下临界区域给出的*H[0]对*H[1]的测试的最佳尺寸:**

![](img/4ca10f44-d854-46ef-9802-96a41a72af55.png)

这个取决于 [![](img/f3b75cd5-4cdc-468d-9483-e7302993d0f5.png)] 和 [![](img/3e6bfced-2e42-4c0c-944e-022e7ffe5854.png)] ，而不是 [![](img/c7abc6a3-460c-4012-b9f1-3656ed0db4cc.png)] 的值。

我们称之为测试，它由 [![](img/54025d5d-5f05-4d8d-987e-47649a29d910.png)] 测试对 [![](img/2e70d22d-6b4f-43ad-be0a-e15606eaf717.png)] 测试的最有效尺寸![](img/87a6c215-5a0b-4030-9883-a9d67f2def82.png)的 *C* 指定，但前提是以下条件成立:

*   [![](img/9e9c392e-ebc9-42f2-a4a2-9fb47bc5c02a.png)]
*   [![](img/7ffe3029-4743-4483-bd17-9e26f41e4102.png)] 为所有情况下的 [![](img/ce01ff2b-1efa-4023-bb0e-796d2194679d.png)] 如果 [![](img/fa862a2f-e488-44b1-88a8-9d5de3f95f75.png)]

现在，和以前一样，我们要找出一个复合假设的可能性， [![](img/2363d223-0bc5-4c72-80fc-48469a2e188e.png)] ，给定一些数据， *x* 。我们的做法如下:

![](img/f08d4b63-4866-4f79-9055-ee74b3358819.png)

# 多元常态理论

在这一节中，我们已经讨论了随机变量或 iid 随机变量的向量。现在，让我们假设我们有一个随机向量， [![](img/34165953-146a-4a7e-bdbc-cb54b61fffaa.png)] ，其中的 *X [ i ]* 值都是相关的。

现在，如果我们想找到 *X* 的平均值，我们可以这样做:

![](img/91724f93-d33f-4252-a0b4-d7c2c6a83a1e.png)

如果存在，协方差矩阵如下:

![](img/a03239c7-fe1b-4243-9c94-1c3456f01951.png)

另外，如果我们有 [![](img/8a3d1ee9-b059-4ba1-b14b-e3e0e77cd8f8.png)] ，那么就有 [![](img/b9a771ad-d7ed-4b25-893d-02668aad631b.png)] 和 [![](img/bb633a7b-f2db-4e49-a496-530c64d1eeec.png)] 。

如果我们在处理两个随机向量，那么我们有如下结果:

*   [![](img/6999b581-2c03-486c-8233-c1438a8ffb3d.png)]
*   [![](img/03569a0c-49a8-412b-a383-dd37bd97cc35.png)]

现在，让我们定义什么是多元正态分布。

假设我们有一个随机向量， *X* 。它具有多元正态分布如果对于 [![](img/221ff3e8-3a7e-4d6a-8722-a585a321c39b.png)] ， [![](img/ae41ef31-efdc-4ab9-8897-adbc11c19a7f.png)] 具有正态分布。

如果[![](img/9b329d48-78e6-43ca-a217-39089a82f43f.png)][![](img/eb305975-3426-4745-ab7a-3e0916025c61.png)]，那么 [![](img/6fa9183f-6f70-4e8c-bfa6-0754ba3e399c.png)] ，其中 [![](img/abf8c575-074b-4690-a372-058e15ca0f1d.png)] 是对称正半定矩阵，因为 [![](img/9d1a222f-08b8-44c4-afd3-a78c3ae2f1f1.png)] 。

你们中的一些人可能想知道多元正态分布的 PDF 是什么样子的。我们很快就会到达那里。

现在，假设 [![](img/47df59d1-ee7a-432e-b11b-284d529339c9.png)] 和我们把![](img/bd94950c-d5d9-447f-be25-bc21830b3c3d.png)分成两个更小的随机向量，比如 [![](img/ebad2aef-2228-4204-ad90-31553a6e01d7.png)] ，其中 [![](img/241be2f3-74a7-41d1-9e4f-03ea72fcbe9c.png)] 和 [![](img/e56a6648-0fe9-4d0b-969c-ce63cb6b7462.png)] 。

同理，[![](img/b4ae1084-2fda-42bf-8e4c-e52cfb87c9c8.png)][![](img/b1eaf2cd-ccfc-4fd8-b5c4-f14731f259cc.png)]。

现在，我们有以下内容:

*   [![](img/05f56368-5db1-43cd-b78e-7e4aeecdba89.png)]
*   **X** [1] 和 **X** [2] 如果 [![](img/0bd91a15-be9c-459b-b263-e3c80451049d.png)] 是独立的

当 [![](img/90d12d00-c683-4797-b74b-b345320de67c.png)] 为半正定时，则 *X* 有如下 PDF:

![](img/c8fbf7b2-8fdb-4ccc-92cf-4160b0855de4.png)

这里， *n* 是 *x* 的尺寸。

假设我们有 [![](img/aa0ff384-401a-43ce-9ffd-1404a285509e.png)] ，分别是 iid [ ![](img/eacccc89-e573-47ed-93f6-728c9cdd6dbf.png)、]和 [![](img/5bee595f-7f18-4fff-b5b6-8cea70a4d729.png)] 和 [![](img/cfaf9d3e-ea1e-453d-8cd2-2462cb4ee042.png)] 。然后，我们有以下内容:

*   [![](img/80e49a1d-e173-4898-909c-10488ade217c.png)]
*   [![](img/16f5fedf-b5cf-4ddf-b009-47d2e9b4324b.png)]
*   *X* 和 *S [xx] 和*是独立的

# 线性模型

在统计学中，我们使用线性模型来模拟因变量和一个或多个预测变量之间的关系。

举个例子，假设我们有 *n* 个观测值*Y[I]和 *p* 个预测值*x[j]，其中 *n > p* 。我们可以将每个观察值写成如下:**

![](img/df12e5e9-4892-4246-88d5-557b45da72a4.png)

对于 [![](img/207301fd-7f26-455f-a11c-cc34a23f14a2.png)] 的所有情况，我们可以假设如下:

*   [![](img/bb2ff12d-a21e-43aa-ba6e-c4c8020438f5.png)] 是我们想要弄清楚的未知的、固定的参数
*   [![](img/95ce8668-c2fa-444f-9c45-5f50812501e6.png)] 是 *p 的值*预测值为 *i ^( th )* 响应
*   [![](img/e4172a8a-7db1-4370-85ce-560dbdaf26a6.png)] 是均值为 0、方差为σ ² 的独立随机变量

我们一般认为 [![](img/ade791f9-ea58-47c9-9d4f-42c2280954c4.png)] 为*x[ij]**ε[I]*为随机误差项。所以[![](img/faf454cf-3be7-4619-99c1-cc9f90255594.png)][![](img/ad690f9b-55d0-4bb7-8330-f76049c10f54.png)][![](img/c8fe0611-b6b0-4965-95eb-5f56d07ee718.png)]都是独立的。

给定所有数据，我们可能希望在数据上绘制一条直线，因此可能的模型如下:

![](img/ff72d58f-5925-4d99-a129-1e12d3f0ef51.png)

这里， *a* 和 *b* 为常数。

我们可以将前面的模型重写如下:

![](img/85985358-f49c-42bf-8c96-476705fa4364.png)

这里，展开的形式如下:

![](img/9db67e29-19cb-437e-ad70-7bf1d51cc39f.png)

还有，[![](img/1ea56a87-6168-4feb-9809-fbc4b6577d39.png)][![](img/561e8418-62a2-41e1-891c-acf5d40aec5b.png)]。

[![](img/a54cfaf6-bf6a-430c-9113-6ddd5ad39ded.png)] 的最小二乘估计器 [![](img/f4cabe0c-ce00-4585-a0fe-26ed010b1157.png)] 通过最小化直线和点之间的垂直距离的平方来最小化我们的线性模型，如下所示:

![](img/a41ba500-0f5d-42c5-827d-047520831ed2.png)

为了使其最小化，我们对所有情况下的 *k* 应用以下公式:

![](img/0901e9c0-ac52-4b26-824a-256b6cf425cb.png)

所以，我们有 [![](img/c17c7155-da0b-402d-a277-8f7636c5b07a.png)] 所以， [![](img/2818372d-b08c-410b-bbf6-fd58a08db06c.png)] 为 *k* 的所有情况。

通过把前面的函数放入矩阵形式，就像我们之前做的那样，我们得到 [![](img/4ede1bec-bd6a-4a52-8f53-a2624335127b.png)] 。

我们知道 [![](img/3d4a8efc-89f0-45bb-adcb-5f87718c4f3c.png)] 是正的，半定的，有逆的。因此， [![](img/c0249ae1-e0e5-484b-a157-06fa293a0dbb.png)] 。

在正常假设下，我们的最小二乘估计与最大似然估计相同。

我们现在有以下内容:

![](img/49994f38-c0e3-4edf-93e2-ff48207a1418.png)

这告诉我们，我们的估计量是无偏的 [![](img/d9b090a4-a01a-4f10-90cb-86ea17972736.png)] 。

我知道你在想什么——太激烈了！干得好，走到了这一步；我们很快就要完成这一章了，所以坚持下去。

# 假设检验

在假设检验中，我们的目标是确定某些变量是否会影响结果。

让我们检验一个一般线性模型的假设。假设我们有 [![](img/ad70ebb2-7402-4603-b8ce-ab6805402f97.png)] 和 [![](img/d081a397-ef2f-4689-a2af-79870d4c9eac.png)] 。我们愿考 [![](img/b4e69d79-5f1b-499c-ab9a-d1a0b0214fcb.png)] 对阵[![](img/3f8e9abf-d6f6-4974-8290-932ff3883f53.png)][![](img/8301d233-ae79-4496-81cb-6690dd9885c7.png)]，自*下 H[0]*[![](img/e1efcc5e-4253-4549-b74f-63ec5ba0fdd9.png)]消失。

在零假设下， *β [0]* 和 *σ ²* 的最大似然分别是 [![](img/ff2c91c9-3f27-443a-9768-0a39ba305872.png)] 和 [![](img/f09971f6-e977-4270-babe-63267e0689a5.png)，]这两个我们前面知道的，是相互独立的。

原假设的估计者戴着两顶帽子，而不是一顶，另一个假设只有一顶。

恭喜你！你已经正式完成了这一章，现在你已经对概率和统计有了一个坚实的直觉。

# 摘要

在这一章中，我们学习了许多概念。如果需要的话，我建议再看一遍这一章，因为这一章的主题对于深入理解深度学习非常重要。你们中的许多人可能想知道到目前为止你们所学的章节与神经网络有什么关系；我们将在接下来的几章中把它们联系起来。

下一章着重于凸和非凸优化方法，并为理解用于训练神经网络的优化算法打下基础。