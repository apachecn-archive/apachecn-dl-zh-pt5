<html><head/><body>









<title>Chapter 5: Multiple Search Modalities</title>







<div><div><h1 class="chapter-number" id="_idParaDest-64"><a id="_idTextAnchor068"/> 5</h1>

<h1 id="_idParaDest-65"><a id="_idTextAnchor069"/>多种搜索方式</h1>

<p>借助深度学习和人工智能的优势，我们可以将任何类型的数据编码成<strong class="bold">向量</strong>。这允许我们创建一个搜索系统，使用任何类型的数据作为查询，并返回任何类型的数据作为搜索结果。</p>

<p>在这一章中，我们将介绍正在兴起的话题<strong class="bold">多模态搜索问题</strong>。您将看到不同的数据形式以及如何使用它们。您将看到如何将文本、图像和音频文档转换成向量，以及如何独立于数据形式实现搜索系统。您还将看到<strong class="bold">多模态</strong>和<strong class="bold">跨模态</strong>概念之间的差异。</p>

<p>在本章中，我们将讨论以下主要话题:</p>

<ul>

<li>如何表示不同数据类型的文档</li>

<li>如何编码多式联运单据</li>

<li>跨模态和多模态搜索</li>

</ul>

<p>在本章结束时，你将对跨模态和多模态搜索的工作原理有一个坚实的理解，以及在纪娜处理不同模态的数据是多么容易。</p>

<h1 id="_idParaDest-66"><a id="_idTextAnchor070"/>技术要求</h1>

<p>本章有以下技术要求:</p>

<ul>

<li>最低内存为4 GB的笔记本电脑(最好是8 GB或更多)</li>

<li>Python在类Unix操作系统上安装了<em class="italic"> 3.7 </em>、<em class="italic"> 3.8 </em>或<em class="italic"> 3.9 </em>，如macOS或Ubuntu</li>

</ul>

<h1 id="_idParaDest-67"><a id="_idTextAnchor071"/>引入多式联运单据</h1>

<p>在过去的十年里，各种类型的数据，如文本、T21、图像和音频在互联网上迅速增长。通常，不同类型的数据与一条内容相关联。例如，图像通常也有文本标签和标题来描述内容。因此，内容有两种形式:图像和文本。带字幕的电影剪辑有三种形式:图像、音频和文本。</p>

<p>纪娜<a id="_idIndexMarker332"/>是一个<strong class="bold">数据类型不可知的框架</strong>，让<a id="_idIndexMarker333"/>你可以处理任何类型的数据，开发跨模态和多模态的搜索系统。为了更好地理解这意味着什么，有必要首先展示如何表示不同数据类型的文档，然后展示如何在纪娜中表示多式联运文档。</p>

<h2 id="_idParaDest-68"><a id="_idTextAnchor072"/>文本文档</h2>

<p>要在纪娜表示一个文本文档是相当容易的。只需使用下面的代码就可以做到这一点:</p>

<pre class="source-code">from docarray import Document

doc = Document(text='Hello World.')</pre>

<p>在某些情况下，一个文档可能包含数千个单词。但是一个几千字的长文档很难搜索；更精细的粒度会更好。你可以通过将一个长文档分割成更小的块来做到这一点。例如，让我们通过使用<code>!</code>标记来分割这个简单的文档:</p>

<pre class="source-code">from jina import Document

d = Document(text='नमस्ते दुनिया!你好世界!こんにちは世界!Привет мир!')

d.chunks.extend([Document(text=c) for c in d.text.split('!')])</pre>

<p>这将在原始文档下创建五个子文档，并将它们存储在<code>.chunks</code>下。为了看得更清楚，您可以通过<code>d.display()</code>将其可视化，其输出如下图所示:</p>

<div><div><img alt="Figure 5.1 – An example of a text document with chunk-level subdocuments&#10;&#10;" height="916" src="img/Figure_5.01_B17488.jpg" width="1250"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.1–一个包含块级子文档的文本文档示例</p>

<p>您还可以使用<code>.texts</code> sugar属性<a id="_idIndexMarker335"/>打印出每个子文档的文本属性:</p>

<pre class="source-code">print(d.chunks.texts)</pre>

<p>这将输出以下内容:</p>

<pre>['नमस्ते दुनिया', '你好世界', 'こんにちは世界', 'Привет мир', '']</pre>

<p>关于在纪娜表示文本文档，您只需要知道这些！</p>

<h2 id="_idParaDest-69"><a id="_idTextAnchor073"/>图像文档</h2>

<p>与文本数据相比，图像数据更通用，更容易理解。图像数据往往<a id="_idIndexMarker337"/>只是一个<code>ndim=2</code>或<code>ndim=3</code>和<code>dtype=uint8</code>。该ndarray中的每个元素表示某个通道上某个位置的<code>0</code>和<code>255</code>之间的一个像素值。例如，256x300的彩色JPG图像可以表示为<code>[256, 300, 3]</code>n阵列。你可能会问为什么<code>3</code>在最后一个维度。因为它代表了每个像素的<code>R</code>、<code>G</code>和<code>B</code>通道。一些图像有不同数量的通道。例如，具有透明背景的PNG有四个通道，其中额外的通道表示不透明。灰度图像只有一个通道，它表示亮度(一种表示黑白比例的度量)。</p>

<p>在纪娜，您<a id="_idIndexMarker338"/>可以通过指定图像URI来加载图像数据，然后使用文档API将其转换为<code>.tensor</code>。例如，我们将使用以下代码加载一个PNG苹果图像(如图<em class="italic">图5.2 </em>所示):</p>

<div><div><img alt="Figure 5.2 – An example PNG image located in the apple.png local file&#10;&#10;" height="432" src="img/Figure_5.02_B17488.jpg" width="472"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.2–位于apple.png本地文件中的示例PNG图像</p>

<pre class="source-code">from docarray import Document

d = Document(uri='apple.png')

d.load_uri_to_image_tensor()

print(d.content_type, d.tensor)</pre>

<p>您将获得以下输出:</p>

<pre>tensor [[[255 255 255]
  [255 255 255]
  [255 255 255]
  ...</pre>

<p>现在，图像内容被转换成文档的<code>.tensor</code>字段，然后可以用于进一步的处理。一些帮助功能可用于处理图像数据。您可以调整它的大小(即向下采样/向上采样)并将其正常化。您可以切换<code>.tensor</code>的通道轴，以满足某些框架的某些要求，最后，您可以将所有这些处理步骤链接在一行中。例如，可以对图像进行归一化处理，颜色轴应该放在最前面，而不是最后。您可以使用以下代码执行这样的图像转换:</p>

<pre class="source-code">from docarray import Document

d = (

    Document(uri='apple.png')

    .load_uri_to_image_tensor()

    .set_image_tensor_shape(shape=(224, 224))

    .set_image_tensor_normalization()

    .set_image_tensor_channel_axis(-1, 0)

)

print(d.content_type, d.tensor.shape)</pre>

<p>您也可以使用以下命令将<code>.tensor</code>转储回PNG图像文件:</p>

<pre class="source-code">d.save_image_tensor_to_file('apple-proc.png', channel_axis=0)</pre>

<p>请注意，由于我们刚刚执行的加工步骤，通道轴现在切换到<code>0</code>。最后，您将得到如图5.3 所示的结果图像:</p>

<div><div><img alt="Figure 5.3 – The resulting image after resizing and normalizing&#10;&#10;" height="224" src="img/Figure_5.03_B17488.jpg" width="224"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.3-调整大小和标准化后的结果图像</p>

<h2 id="_idParaDest-70"><a id="_idTextAnchor074"/>音频文档</h2>

<p>作为存储信息的一种重要格式，数字音频数据可以是一段音乐、音乐、铃声或背景噪音。它通常以<code>.wav</code>和<code>.mp3</code>格式出现，在这两种格式中，声波通过以离散的间隔采样而被数字化。要在纪娜加载一个<code>.wav</code>文件作为文档，只需使用下面的代码:</p>

<pre class="source-code">from docarray import Document

d = Document(uri='hello.wav')

d.load_uri_to_audio_tensor()

print(d.content_type, d.tensor.shape)</pre>

<p>您将看到以下输出:</p>

<pre>tensor [-0.00061035 -0.00061035 -0.00082397 ...  0.00653076  0.00595093 0.00631714]</pre>

<p>如前面的示例所示，来自<code>.wav</code>文件的数据被转换为一维(mono)n array，其中每个元素通常位于[-1.0，+1.0]范围内。您决不会局限于使用纪娜本地的方法进行音频处理。以下是一些命令行工具、程序和资源库，您可以使用它们对音频数据进行更高级的处理:</p>

<ul>

<li>FFmpeg(<a href="https://ffmpeg.org/">https://ffmpeg.org/</a>):这是一个免费的开源项目，用于处理<a id="_idIndexMarker341"/>多媒体文件和流。</li>

<li><strong class="bold">py dub</strong>(<a href="https://github.com/jiaaro/pydub">https://github.com/jiaaro/pydub</a>):这款<a id="_idIndexMarker342"/>用简单易用的高级界面操控音频。</li>

<li><strong class="bold">Librosa</strong>(<a href="https://librosa.github.io/librosa/">https://librosa.github.io/librosa/</a>):这个<a id="_idIndexMarker343"/>是一个用于<a id="_idIndexMarker344"/>音乐和音频分析的Python包。</li>

</ul>

<h2 id="_idParaDest-71"><a id="_idTextAnchor075"/>多式联运单据</h2>

<p>到目前为止，你已经学会了如何在纪娜中表现不同的数据形态。然而，在现实世界中，数据通常以结合多种形式的形式出现，例如视频，它通常至少包括<em class="italic">图像</em>和<em class="italic">音频</em>，以及字幕形式的<em class="italic">文本</em>。现在，知道如何表示多模态数据是非常有趣的。</p>

<p>纪娜文档可以通过块垂直嵌套。将不同模态的数据分块放入子文档是很直观的。例如，您可以创建一个时尚产品文档<a id="_idIndexMarker346"/>，它有两个模态，包括一个服装图片和一个产品描述。</p>

<div><div><img alt="Figure 5.4 – An example of a fashion product document with two modalities&#10;&#10;" height="276" src="img/Figure_5.04_B17488.jpg" width="231"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.4-具有两种形态的时尚产品文档示例</p>

<p>只需使用下面的代码就可以做到这一点:</p>

<pre class="source-code">from jina import Document 

text_doc = Document(text='Men Town Round Red Neck T-Shirts') 

image_doc = Document(uri='tshirt.jpg').load_uri_to_image_tensor()

fashion_doc = Document(chunks=[text_doc, image_doc])</pre>

<p>现在，示例时尚产品(如图<em class="italic">图5.4 </em>所示)被表示为一个纪娜文档，它有两个块级文档，分别表示产品的描述和服装图像。您也可以使用<code>fashion_doc.display()</code>来产生可视化效果，如图<em class="italic">图5.5 </em>所示:</p>

<div><div><img alt="Figure 5.5 – An example of a fashion product document with two chunk-level documents&#10;&#10;" height="556" src="img/Figure_5.05_B17488.jpg" width="1232"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.5–带有两个块级文档的时尚产品文档示例</p>

<p class="callout-heading">重要说明</p>

<p class="callout">您可能认为不同的模态对应不同种类的数据(在这种情况下是图像和文本)。然而，这并不准确。例如，您可以通过搜索不同视角的图像或搜索给定段落文本的匹配标题来进行跨模态搜索。因此，我们可以认为一个模态与数据可能来自的给定数据分布相关。</p>

<p>到目前为止，我们已经学习了如何表示单个文本、图像和音频数据，以及将多模态数据表示为纪娜文档。在下一节中，我们将展示如何获得每个文档的嵌入。</p>

<h1 id="_idParaDest-72"><a id="_idTextAnchor076"/>如何对多式联运单据进行编码</h1>

<p>在<a id="_idIndexMarker348"/>为不同类型的数据定义文档之后，下一步是使用模型将文档编码成矢量嵌入。形式上，嵌入是文档的多维度(通常是一个<code>[1, D]</code>向量)，旨在包含文档的内容信息。随着所有深度学习方法的性能的当前进步，甚至通用模型(例如，在ImageNet上训练的CNN模型)也可以用于提取有意义的特征向量。在下面的章节中，我们将展示如何为不同模态的文档编码嵌入<a id="_idIndexMarker349"/>。</p>

<h2 id="_idParaDest-73"><a id="_idTextAnchor077"/>编码文本文档</h2>

<p>为了将<a id="_idIndexMarker350"/>文本文档转换成向量，我们可以使用语句<a id="_idIndexMarker352"/>转换器(<a href="https://www.sbert.net/">https://www.sbert.net/</a>)提供的<a id="_idIndexMarker351"/>预训练的伯特模型(<a href="https://www.sbert.net/docs/pretrained_models.xhtml">https://www.sbert.net/docs/pretrained_models.xhtml</a>)，如下例所示:</p>

<pre class="source-code">from docarray import DocumentArray

from sentence_transformers import SentenceTransformer

da = DocumentArray(...)

model = SentenceTransformer('all-MiniLM-L6-v2')

da.embeddings = model.encode(da.texts)

print(da.embeddings.shape)</pre>

<p>结果，在完成<code>.encode(...)</code>之后，输入<code>DocumentArray</code>中的每个文档将具有384维密集向量空间的嵌入。</p>

<h2 id="_idParaDest-74"><a id="_idTextAnchor078"/>编码图像文件</h2>

<p>对于编码<a id="_idIndexMarker353"/>图像文档，我们可以使用Pytorch的预训练模型进行嵌入。作为一个例子，我们将使用<a id="_idIndexMarker354"/><strong class="bold">resnet 50</strong>网络(<a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>)对<strong class="bold">火炬视觉</strong>(<a href="https://pytorch.org/vision/stable/models.xhtml">https://pytorch.org/vision/stable/models.xhtml</a>)提供的<a id="_idIndexMarker355"/>图像进行物体分类:</p>

<pre class="source-code">from docarray import DocumentArray

import torchvision

da = DocumentArray(...)

model = torchvision.models.resnet50(pretrained=True)

da.embed(model)

print(da.embeddings.shape)</pre>

<p>这样，我们已经成功地将一个图像文档编码成它的特征向量表示。生成的特征向量是神经网络的输出激活(1000个分量的向量)。</p>

<p class="callout-heading">重要说明</p>

<p class="callout">您可能已经注意到，在前面的例子中，我们使用<code>.embed()</code>进行嵌入。通常，当<code>DocumentArray</code>设置了<code>.tensors</code>时，可以使用这个API对文档进行编码。使用GPU时可以指定<code>.embed(..., device='cuda')</code>。设备名称标识符取决于您使用的模型框架。</p>

<h2 id="_idParaDest-75"><a id="_idTextAnchor079"/>编码音频文件</h2>

<p>为了<a id="_idIndexMarker357"/>将声音片段编码成矢量，我们选择了来自Google Research的<a id="_idIndexMarker358"/>VG GISH模型(【https://arxiv.org/abs/1609.09430】T21)。我们将使用来自<strong class="bold">torchvgish</strong>(<a href="https://github.com/harritaylor/torchvggish">https://github.com/harritaylor/torchvggish</a>)的预训练模型<a id="_idIndexMarker359"/>来获得音频数据的特征嵌入:</p>

<pre class="source-code">import torch

from docarray import DocumentArray

model = torch.hub.load('harritaylor/torchvggish', 'vggish')

model.eval()

for d in da:

    d.embedding = model(d.uri)[0]

print(da.embeddings.shape)</pre>

<p>每个声音剪辑的<a id="_idIndexMarker360"/>返回嵌入是一个大小为<em class="italic"> K </em> x 128的矩阵，其中<em class="italic"> K </em>是对数mel声谱图中的样本数，大致对应于以秒为单位的音频长度。因此，组块中的每个4秒音频片段由四个128维向量表示。</p>

<p>我们现在已经了解了如何为不同形式的文档编码嵌入。在下一节中，我们将向您展示如何使用多种设备搜索数据。当试图寻找不容易用单一模态表示的数据时，这可能是有用的。例如，您可以使用图像搜索来查找本质上是文本的数据。</p>

<h1 id="_idParaDest-76"><a id="_idTextAnchor080"/>跨模态和多模态搜索</h1>

<p>现在我们知道了如何使用多模态数据，我们可以描述跨模态搜索(T2)和多模态搜索(T4)。在此之前，我想先描述一下<a id="_idIndexMarker362"/>的<strong class="bold">单峰</strong>(单模态)搜索。一般来说，单峰搜索意味着在索引和查询时处理单一形式的数据。例如，在图像<a id="_idIndexMarker363"/>搜索检索中，返回的搜索<a id="_idIndexMarker364"/>结果也是基于给定图像查询的图像。</p>

<p>到目前为止，我们已经知道如何将文档内容编码成特征向量来创建嵌入。在索引中，包含图像、文本或音频内容的每个文档都可以表示为嵌入向量，并存储在索引中。在查询中，查询文档也可以表示为嵌入，然后可以使用该嵌入通过一些相似性得分(例如余弦、欧几里德距离等)来识别相似的文档。<em class="italic">图5.6 </em>展示了搜索问题的统一匹配视图:</p>

<div><div><img alt="Figure 5.6 – An illustration of the unified matching view for the search problem&#10;&#10;" height="345" src="img/Figure_5.06_B17488.jpg" width="339"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.6–搜索问题的统一匹配视图的图示</p>

<p>更正式地说，搜索<a id="_idIndexMarker365"/>可以认为是构建一个匹配模型，计算输入查询文档与搜索中文档的匹配程度。有了这个统一的匹配视图，用于<strong class="bold">单峰</strong>、<strong class="bold">多峰</strong>和<strong class="bold">跨峰</strong>搜索<a id="_idIndexMarker366"/>的匹配模型在架构和方法学方面彼此更加相似，如技术中所反映的:将输入(查询和文档)嵌入为分布式表示，组合神经网络组件来表示不同模态的数据之间的关系，以及以端到端的方式训练模型参数。</p>

<h2 id="_idParaDest-77"><a id="_idTextAnchor081"/>跨模态搜索</h2>

<p>在<a id="_idIndexMarker367"/>单峰搜索中，搜索被设计成处理单一数据类型，这使得它在不同数据类型的输入方面不太灵活，也更脆弱。除了单峰搜索，<strong class="bold">跨峰搜索</strong>旨在将一种类型的数据作为查询来检索另一种类型的相关数据，如图像-文本、视频-文本、音频-文本跨峰搜索。例如，如图<em class="italic">图5.7 </em>所示，我们可以设计一个文本到图像的搜索系统，根据短文本描述检索图像作为查询:</p>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>

<div><div><img alt="Figure 5.7 – A cross-modal search system to look for images from captions&#10;&#10;" height="577" src="img/Figure_5.07_B17488.jpg" width="299"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.7-从标题中寻找图像的跨模态搜索系统</p>

<p>最近，由于多模态数据的快速增长，跨模态搜索<a id="_idIndexMarker368"/>吸引了相当多的关注。随着多模态数据的增长，用户很难有效且高效地搜索感兴趣的信息。迄今为止，已经提出了用于搜索多模态数据的各种搜索方法。然而，这些搜索技术大多是基于单一模态的，这将跨模态搜索转换为基于关键字的搜索。这可能是昂贵的，因为你需要一个人来写这些关键字，而且，关于多模态内容的信息并不总是可用的。我们需要寻找另一种解决方案！<strong class="bold">C</strong>T3】Ross-modal搜索旨在识别跨不同模态的相关数据。跨模态搜索的主要挑战是如何度量不同模态数据之间的内容相似性。已经提出了各种方法来处理这样的问题。一种常见的方式是从同一潜在空间中的不同模态生成特征向量，使得新生成的特征可以应用于距离度量的计算。</p>

<p>为了实现这一目标，通常可以利用两种<a id="_idIndexMarker369"/>非常常见的深度度量学习架构(暹罗和三元网络)。它们都有一个共同的想法，即不同的子网络(可以共享也可以不共享权重)同时接收不同的输入(对于连体网络是正的和负的对，对于三元组是正的、负的和锚文档)，并试图将它们自己的特征向量投射到一个公共的潜在空间，在该空间中计算对比损失并将其误差传播到所有的子网络。</p>

<p>正对是语义上相关的并且期望在投影空间中保持接近的对象对(图像、文本或任何文档)。另一方面，否定对是应该分开的文档对。</p>

<div><div><img alt="Figure 5.8 – The schema of the deep metric learning process with a triplet network and anchor&#10;&#10;" height="406" src="img/Figure_5.08_B17488.jpg" width="1028"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.8–具有三元网络和锚点的深度度量学习流程的模式</p>

<p>如图<em class="italic">图5.8 </em>所示，一个图像和文本之间的跨模态搜索的例子，用于提取图像特征<a id="_idIndexMarker370"/>的子网是一个<strong class="bold"> ResNet50 </strong>架构，其权重在ImageNet上预先训练，而对于文本嵌入，使用来自<a id="_idIndexMarker371"/>预先训练<strong class="bold"> Bert </strong>模型的隐藏层的输出。最近，一种新的深度度量学习预训练模型被提出，<strong class="bold">对比语言-图像预训练</strong> ( <strong class="bold">剪辑</strong>)，这是一种在各种图像-文本对上训练<a id="_idIndexMarker372"/>的神经网络。它被训练在互联网上的文本片段和图像对的帮助下，从自然语言中学习视觉概念。它可以通过在相同的<a id="_idIndexMarker373"/>语义空间中对文本标签和图像进行编码并为两种模态创建标准嵌入来执行零镜头学习。使用CLIP-style模型，图像和查询文本都可以映射到相同的潜在空间，以便可以使用相似性度量来比较它们。</p>

<h2 id="_idParaDest-78"><a id="_idTextAnchor082"/>多模式搜索</h2>

<p>与单峰和跨峰搜索相比，<strong class="bold">多峰搜索</strong>旨在启用多峰数据作为查询输入。搜索查询可以由文本输入、图像输入和其他输入形式的组合组成。组合不同形式的信息来提高搜索性能是很直观的。设想一个电子商务搜索场景，它采用两种类型的查询信息:一个图像和一个文本。例如，如果您正在搜索裤子，图像将是裤子的图片，文本将是类似“紧身”和“蓝色”的内容在这种情况下，搜索查询由两种模态(文本和图像)组成。我们可以将这种搜索场景称为多模态搜索。</p>

<p>为了允许多模态搜索，在实践中广泛使用两种方法来融合搜索中的多个模态:早期融合(融合来自多个模态的特征作为查询输入)和晚期融合(在最后融合来自不同模态的搜索结果)。</p>

<p>具体地，早期融合方法融合从不同模态的数据中提取的特征。如图<em class="italic">图5.9 </em>所示，来自不同模型的两种不同模态(图像和文本)的特征被送入融合算子。为了简单地组合特征，我们可以使用特征串联作为融合操作符来产生多模态数据的特征。另一个融合选项包括将不同的模态投影到公共嵌入空间中。然后，我们可以直接添加来自不同模态的数据的特征。</p>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>

<div><div><img alt="Figure 5.9 – Early fusion, the fusion of multimodal features as the query input&#10;&#10;" height="437" src="img/Figure_5.09_B17488.jpg" width="786"/>

</div>

</div>

<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图5.9-早期融合，多模态特征的融合作为查询输入</p>

<p>在组合了这些特征之后，我们可以使用为单峰搜索设计的相同方法来解决多峰搜索问题。这种特征融合方法有一个明显的局限性:很难在用户查询的上下文中定义各种模态的重要性之间的正确平衡。为了克服这个限制，可以训练端到端神经网络来模拟联合多模态空间。然而，建模这种联合多模态空间需要复杂的训练策略和彻底注释的数据集。</p>

<p>在实践中，为了解决上述缺点，我们可以简单地使用后融合方法来分离每个模态的搜索，然后融合来自不同模态的搜索结果，例如，利用每个文档的所有模态的检索分数的线性组合。虽然晚期融合已被证明是稳健的，但它有几个问题:模态的适当权重不是一个小问题，并且有一个主要模态问题。例如，在文本-图像多模态搜索中，当用户仅基于视觉相似性来评估结果时，文本分数的影响可能恶化最终结果的视觉质量。</p>

<p>这两种搜索模式之间的主要区别在于，对于跨模态，单个文档和嵌入空间中的向量之间存在直接映射，而对于多模态，这并不成立，因为两个或多个文档可能被组合成单个向量。</p>

<h1 id="_idParaDest-79"><a id="_idTextAnchor083"/>总结</h1>

<p>本章描述了多模态数据的概念，以及跨模态和多模态搜索问题。首先，我们介绍了多模态数据以及如何在纪娜中表示它。然后，我们学习了如何使用深度神经网络从不同模态的数据中获取向量特征。最后，我们介绍了跨模态和多模态搜索系统。这开启了许多强大的搜索模式，并使理解如何用纪娜实现跨模态和多模态搜索应用变得更加容易。</p>

<p>在下一章，我们将介绍一些基本的实际例子来解释如何使用纪娜来实现搜索应用程序。</p>

</div>

</div>



</body></html>