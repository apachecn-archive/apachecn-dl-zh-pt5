<title>Chapter 7: Exploring Advanced Use Cases of Jina</title>

# 7

# 探索纪娜的高级用例

在这一章中，我们将讨论纪娜神经搜索框架的更高级的应用。基于我们在前几章中学到的概念，我们现在来看看我们还能通过纪娜实现什么。我们将研究多级粒度匹配、索引时的查询以及一个跨模态示例。这些是神经搜索中具有挑战性的概念，并且是实现复杂的现实生活应用所必需的。特别是，我们将在本章中讨论这些主题:

*   引入多级粒度
*   使用图像和文本的跨模式搜索
*   并发查询和索引数据

这些涵盖了神经搜索应用程序的各种实际需求。使用这些例子，以及第 6 章 、*纪娜*中的基本例子，您可以扩展和改进您的纪娜应用程序，以涵盖更高级的使用模式。

# 技术要求

在本章中，我们将构建并执行 GitHub 资源库中提供的高级示例。代码可从 https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with-纪娜/tree/main/src/Chapter07 获得。在遵循如何重现用例的说明时，确保下载并导航到每个示例各自的文件夹。

要运行此代码，您需要以下内容:

*   安装了 WSL2 的 macOS、Linux 或 Windows。纪娜不能在本机 Windows 上运行。
*   Python 3.7 或 3.8
*   或者，为每个示例创建一个全新的虚拟环境
*   码头工人

# 引入多级粒度

在本节中，我们将讨论纪娜如何捕捉和利用真实数据的层次结构。为了遵循现有的代码，检查一个名为`multires-lyrics-search`的文件夹的章节代码。这是我们将在本节中引用的示例。

这个例子依赖于`Document`类型保存块(子文档)和引用特定父文档的能力。使用这种结构，您可以在文档中构建高级的任意级别的文档层次结构。这模拟了各种现实生活中与数据相关的问题。例子可以是图像的补丁、段落的句子、较长电影的视频剪辑等等。

关于如何用纪娜的`Document` API 来执行这个操作，请看下面的代码:

```
from jina import Document

 document = Document() 

chunk1 = Document(text='this is the first chunk') 

chunk2 = Document(text='this is the second chunk') 

document.chunks = [chunk1, chunk2]
```

然后，这可以被链接起来，具有多个粒度级别，每个块都有自己的块。这在处理分层数据结构时很有帮助。有关`Document`数据类型的更多信息，您可以查看第四章[](B17488_04.xhtml#_idTextAnchor054)*、*学习纪娜基础知识*中的*了解纪娜组件*部分。*

 *在这个例子中，数据集将由各种流行歌曲的歌词组成。在这种情况下，粒度是基于语言概念的。顶层将是歌曲歌词主体的全部内容。下一级将是从顶级正文中提取的单个句子。这种拆分是使用`Sentencizer`执行器完成的，它通过寻找特定的分隔符文本标记来拆分长文本，比如`.`或`,`。

这个应用程序有助于展示**分块**的概念及其在搜索系统中的重要性。这很重要，因为为了在神经搜索系统中获得最佳结果，最好使用相同长度的文本输入进行搜索。否则，您正在搜索的数据与您用来训练模型的数据之间的上下文与内容的比率将会不同。一旦我们构建了这个例子，我们就可以想象系统是如何通过一个定制的前端来匹配输入和输出的。

## 浏览代码

我们现在来看一下app 的逻辑和各个组件的功能。你可以跟随资源库中的代码，网址是[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with-纪娜/tree/main/src/chapter 07/multires-lyrics-Search](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search)。我将解释文件夹中主要文件的用途和设计。

## app.py

这个是例子的主入口点。用户可以使用这个脚本来索引(添加)新数据或者使用他们想要的查询进行搜索。对于索引数据，这是从命令行完成的，如下所示:

```
python -t app.py index
```

除了提供`index`参数，您还可以提供`query`或`query_text`作为参数。`query`启动外部 REST API 使用的流。然后，您可以使用存储库中提供的自定义前端来连接到这个。`query_text`允许用户直接从命令行搜索。

编制索引时，从`CSV`文件中顺序读取数据。我们还附加了相关的标签信息，比如作者、歌曲名、专辑名和语言，用于在界面中显示元数据。用户也可以以任何他们需要的方式使用标签。在第 4 章 、*学习纪娜基础知识*的*理解纪娜组件*一节的*从标签访问嵌套属性*小节中讨论了它们。

## index.yml

该文件定义了索引数据(添加数据)时使用的流程结构。以下是文件中提供的不同配置选项:

*   `jtype`通知 YAML 解析器这个对象的类类型。在本例中，它是`Flow`类。然后，YAML 解析器将使用各自的配置参数实例化该类。
*   `workspace`定义每个执行器可能想要存储数据的默认位置。不是所有的遗嘱执行人都需要工作空间。这可以被每个执行器的`workspace`参数覆盖。
*   `executors`是定义该流程中的处理步骤的列表。这些步骤由特定的类定义，所有这些类都是`Executor`类的子类。

索引流程如下图所示:

![ Figure 7.1 – Index Flow showing document chunking

](img/Figure_7.1_B17488.jpg)

图 7.1-显示文档分块的索引流

请注意数据流在网关处是如何被分割的。原始文档按原样保存在`root_indexer`中，以备将来检索。在另一条路径上，对文档进行处理，以便提取它的块，对它们进行编码，然后将它们存储在索引器中。

以下是本例中使用的不同执行者:

1.  第一个是`segmenter`，用的是`Sentencizer`级，来自纪娜 Hub。我们使用默认配置。它使用一组通常用来分隔句子的标点符号，如`.`、`,`、`;`、`!`，将歌词的主体分割成句子。这是创建块并分配给它们的父文档的地方，基于这些标记在文本中的位置。
2.  接下来是`encoder`。这是流程中将句子从文本转换成数字格式的组件。该组件使用了`TransformerTorchEncoder`类。它从`Huggingface` API 下载`distilbert-base-cased`模型，并使用它将文本本身编码成向量，然后可用于向量相似性计算。我们还将在这里定义一些配置选项:
    *   `pooling_strategy: 'cls'`:这是编码器使用的池策略。
    *   `pretrained_model_name_or_path: distilbert-base-cased`:这是使用的深度学习模型。它是预先训练好的，由执行者在开始时下载。
    *   `max_length: 96`:表示句子中要编码的最大字符数。超过这个限制的句子会被修剪掉(多余的字符会被删除)。
    *   `device: 'cpu'`:该配置指示执行器在 CPU 上运行。执行器也可以在 GPU 上运行(用`'gpu'`)。
    *   `default_traversal_paths: ['c']`:这在块级别上计算嵌入。这表示由`segmenter`提取的句子的层级。我们只对这些进行编码，因为我们将只在这个级别执行搜索匹配。由于模型需要编码的数据量，匹配一首歌的整个歌词将不会执行得很好。
3.  我们现在将深入研究实际的存储引擎`indexer`。为此，我们使用名为`SimpleIndexer`的执行程序，同样来自纪娜中心。这使用了纪娜的`DocumentArrayMemmap`类，将数据存储在磁盘上，但同时，根据需要将其加载到内存中进行读写，而不会消耗太多内存。我们为它定义了以下配置选项:
    *   `default_traversal_paths: ['c']`:这些选项配置组件来存储文档块。这与`default_traversal_paths`之前的用法目的相同。
4.  接下来是另一个索引器，`root_indexer`。这是这个例子的特定需求的一部分。以前，在`indexer`，我们只存储文档的块。但是，在搜索时，我们还需要检索父文档本身，以便获得与之相关联的标签(艺术家姓名、歌曲名称等等)。因此，我们需要将这些文档存储在某个地方。这就是为什么我们需要这个额外的遗嘱执行人。通常，这不是必需的，取决于您的应用程序中的用例。我们定义了以下配置选项:
    *   `default_traversal_paths: ['r']`:我们定义我们将索引文档的根级别(即，不是块级别)
    *   `needs: [gateway]`:这告诉流程将请求并行发送到两个独立的路径:一个发送到`segmenter`和`encoder`路径，另一个直接发送到`root_indexer`，因为这一个不依赖于另一个路径中的任何执行器

你会注意到一个额外的论点，它在一些遗嘱执行人身上重复出现。这符合在 Docker 容器中挂载本地目录的 Docker 语法，以便在运行的 Docker 容器中挂载工作区。

## query.yml

该文件定义了查询数据(搜索数据)时使用的流程结构。这与索引时使用的流配置不同，因为操作的顺序不同。查看下图，我们注意到主要的变化是查询时的操作是严格顺序的:

![Figure 7.2 – Query Flow showing document chunking

](img/Figure_7.2_B17488.jpg)

图 7.2–显示文档分块的查询流

匹配是从`indexer`中检索的，正如我们之前定义的，它在块级别上操作。`ranker`然后为块中的每个父 ID 创建一个匹配。最后，根据其 ID 从`root_indexer`中检索该父匹配文档的原始元数据。为了获得块的完整上下文(父块的完整文本内容以及艺术家和歌曲的名称)，这是必需的。

就像`index.yml`文件一样，`query.yml`文件也定义了一个带有执行者的流程。我们将讨论它们的配置选择，但是我们将只讨论它们与`index.yml`文件中的等价配置的不同之处。如果本节中没有涉及某个参数，请查看上一节。以下是查询流中定义的执行者:

*   `segmenter`也是一样。
*   `encoder`也是一样。
*   `indexer`也是一样。

1.  第一个新的执行人是`ranker`。这将对搜索结果进行自定义排序和分类。我们用的是`SimpleRanker`，来自纪娜枢纽。这里唯一的参数是`metric: 'cosine'`。这将该类配置为使用`cosine`度量作为其排名的基础。它的工作原理是将父文档块(子文档)的分数聚合成父文档的总分数。这是确保对客户端(前端、REST API 客户端或命令行界面)来说，匹配以有意义的方式排序所必需的。
2.  最后一跳是`root_indexer`。这里，我们将`default_traversal_paths`改为`['m']`。这意味着我们想要检索文档匹配的元数据，而不是请求文档本身的元数据。这将获取文档的 ID 并执行元数据查找。如前所述，`indexer`只存储文档的块。我们需要检索组块的父文档的完整元数据。

## 安装并运行示例

我现在将指导您安装并运行这个示例应用程序:

1.  确保满足本章开头定义的要求。
2.  从[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with-纪娜](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)中克隆 Git 存储库，并在`src/Chapter07/multires-lyrics-search`打开示例文件夹中的终端。
3.  安装要求:

    ```
    pip install -r requirements.txt
    ```

4.  下载完整的数据集。这一步是可选的；您可以跳过这一步，使用提供的示例数据:

    ```
    bash script should perform all the steps needed to download the full dataset:
    ```

    1.  如果您还没有安装 Kaggle 库，请从安装 ka ggle 库开始。你还需要设置你的 API 键，如下所述:[https://github.com/Kaggle/kaggle-api#api-credentials:](https://github.com/Kaggle/kaggle-api#api-credentials%0D)

```
bash get_data.sh
```

*   下一步是索引数据。这个步骤处理您的数据，并将其存储在流的执行者的工作区:

    ```
    python app.py -t index
    ```

    *   搜索您的数据。这里你有两个选择:
    *   `python app.py -t query_text`:该选项启动命令行应用程序。在某些时候，它会要求输入一个短语。该短语将被处理，然后用作搜索查询。结果将显示在终端中。
    *   `python app.py -t query`:以服务器模式启动应用程序。它监听 REST API 上的传入请求，并以最佳匹配响应客户机。

在第二种模式下，您可以使用我们构建的自定义前端来探索结果。您可以通过在终端中运行以下命令来启动前端:

```
cd static
python -m http.server --bind localhost
```

现在，您可以在浏览器中打开 [http://127.0.0.1:8000/](http://127.0.0.1:8000/) ，您应该会看到一个 web 界面。在此界面中，您可以在左侧的框中键入您的文本。然后你会在右边得到结果。匹配的组块将在歌词正文中高亮显示。

以下是界面截图:

![Figure 7.3 – Lyrics search engine example showing matching songs

](img/Figure_7.3_B17488.jpg)

图 7.3-显示匹配歌曲的歌词搜索引擎示例

比如你加上句子`I am very happy today`，应该会看到类似的结果。你在右手边看到的每个方框都是数据集中的一首歌。每个高亮显示的句子都是一个*匹配*。匹配是一个相似的句子，由嵌入空间中两个向量的接近程度决定。

可以使用左侧的分解滑块来调整相似性。当您向右移动滑块时，您会看到更多匹配项出现。这是因为我们增加了向量空间的半径来寻找相似的匹配。

您在歌曲框底部看到的相关性分数总结了一首歌曲中的所有匹配项。每个匹配都有一个介于 0 和 1 之间的数值，确定它与向量空间中的原始输入有多接近。这些匹配值的平均值就是相关性分数。这意味着只有好的匹配的歌曲将被排名为高度相关。

该示例还允许更复杂的多句子查询。如果在查询时输入两到三个句子，查询流会将全部输入分解成单独的“块”本例中的这些块是句子，但是在构建纪娜时，您可以为自己的数据确定块是什么。

在本节中，我们已经介绍了如何在纪娜框架中对现实生活中的数据的层次结构进行建模。我们使用`Document`类及其保存数据块的能力来表示这些数据。然后，我们构建了一个示例应用程序，我们可以用它在句子级别上搜索歌词。这种方法可以推广到任何文本(或其他形式)数据应用程序。在下一节中，我们将看到如何利用文档的模态来搜索带有文本的图像。

# 使用图像和文本进行跨模态搜索

在本节中，我们将介绍一个展示**跨模态搜索**的高级示例。跨模态搜索是神经搜索的一个子类型，其中我们索引的数据和我们搜索的数据属于不同的模态。这是神经搜索所独有的，因为没有一种传统的搜索技术可以轻松实现这一点。这是由于中央神经搜索技术而成为可能的:所有深度学习模型从根本上将所有数据类型转换为向量的相同共享数字表示(从网络的特定层提取的嵌入)。

这些模态可以用不同的数据类型来表示:音频、文本、视频和图像。同时，它们也可以是相同的类型，但是具有不同的分布。这方面的一个例子可能是搜索论文摘要并希望获得论文标题。它们都是文本，但底层数据分布不同。因此，在这种情况下，分布也是一种模态。

本节中示例的目的是展示纪娜框架如何帮助我们轻松执行这种搜索。我们强调如何使用流将数据处理分成两个执行程序的管道，这取决于模态。这是通过`needs`字段完成的，它定义了执行程序之前需要的步骤。将这些`needs`链接起来，我们可以获得不同的路径。

现在让我们来看看应用程序的逻辑和每个文件的用途。代码可以在[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with 纪娜](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)的文件夹`src/Chapter07/cross-modal-search`中找到。

## app.py

这是示例的主入口点。用户可以调用**索引**或**搜索**。然后，它创建流，或者索引数据，或者使用来自用户的查询进行搜索。

## 流量指数. yml

该文件定义了索引数据(添加数据)时使用的流的结构。我将解释不同的步骤。

流程本身具有以下参数:

*   `prefetch`定义从客户端请求中预取的文档数量。
*   `workspace`定义数据存储的默认位置。这可以被每个执行器的`workspace`参数覆盖。

然后，`executors`列表定义了这个流程中使用的执行者。该列表中的每一项都是一个执行器及其配置。

下图显示了索引流程。注意路径如何从网关分叉，这取决于数据是图像还是文本:

![Figure 7.4 – Index Flow showing cross-modal features

](img/Figure_7.4_B17488.jpg)

图 7.4-显示跨模态特征的索引流

我们将描述每个执行者的目的，按路径分组。第一个路径是图像数据的路径:

1.  第一个执行者是`image_loader`。这使用了本地定义在`flows/executors.py`文件中的`ImageReader`类。这将从特定的文件夹中加载图像文件，并将它们进一步传递到流程中进行处理。当一个文档被创建时，我们可以给它分配一个`mime`类型。然后可以在特定的执行器中使用它来执行定制逻辑。在这里，我们使用它来限制哪些文档属于哪些执行者。

这些参数如下:

*   `py_modules`:这告诉 Python 进程在哪里找到可以在`uses`参数中使用的额外类。
*   `needs`:这创建了一个从网关(总是流程的第一个和最后一个跳跃点)到这个执行器的直接连接。它让这个组件等待来自网关的请求。这在这里是必需的，因为我们希望文本和图像有两个独立的路径。

1.  下一个是`image_encoder`。这是主要工作完成的地方。编码器是将数据转换成数字表示的执行器。它使用的是`CLIPImageEncoder`，版本 0.1。这些参数如下:
    *   `needs`:定义图像路径上的数据路径
2.  `image_indexer`是包含图像的文档的嵌入和元数据的存储器。它用的是`SimpleIndexer`。这些参数如下:
    *   `index_file_name`:定义存储数据的文件夹
    *   `needs`:这使执行器成为图像处理路径的一部分，通过明确使其依赖于`image_encoder`
3.  接下来的元素将是文本路径的一部分。`text_filter`类似于`image_filter`。它可以读取数据，但只能读取基于文本的文档。这里使用的参数如下:
    *   `py_modules`:该参数再次定义了定义了`TextFilterExecutor`的文件。
    *   `needs: gateway`定义了执行者之间的依赖路径。在这种情况下，这个执行器位于路径的开始，因此依赖于`gateway`。
4.  接下来，类似于图像路径的，我们有编码器`text_encoder`。这将处理文本并使用`CLIPTextEncoder`对其进行编码。这里使用的参数如下:
    *   `needs: text_filter`:该参数指定该执行者是文本 pat 的一部分。
5.  `text_indexer`存储执行者的嵌入。
6.  最后，我们连接两条路径。`join_all`将两条路径的结果合并成一条。这里的`needs`参数给出了一个执行者名称列表。

你会注意到一些遗嘱执行人反复提到的一个论点:

`volumes`:这是将本地目录装入 Docker 容器的 Docker 语法。

## query.yml

在这一节中，我们将涵盖查询(搜索)流程。这指定了使用上述索引流搜索您已索引(存储)的数据的过程。在个人层面上，执行者的配置是相同的。

从下图可以看出，流程也是类似的。根据数据类型，它在开始时也会分叉:

![Figure 7.5 – Query Flow showing cross-modal features

](img/Figure_7.5_B17488.jpg)

图 7.5–显示跨模态特性的查询流

不同的是现在我们可以跨两种模态搜索文档。因此，`text_loader`发送带有要由`text_encoder`编码的文本的文档，但是实际的相似性匹配是通过索引流中已经存储在`image_indexer`中的图像文档来完成的。在这个例子中，这是允许我们实现跨模态搜索的核心方面。

## 安装并运行示例

若要运行该示例，请执行以下操作:

1.  确保满足本章开头定义的要求。
2.  从位于[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with-纪娜](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)的资源库中克隆代码，并在`src/Chapter07/cross-modal-search`文件夹中打开一个终端。
3.  请注意，该示例仅包括两幅图像作为样本数据集。为了下载整个数据集并研究结果，您需要从 Kaggle 下载它。你可以注册一个免费的 Kaggle 账户。然后，设置您的 API 令牌。最后，为了下载`flickr 8k`数据集，在终端中运行以下命令:

    ```
    bash get_data.sh 
    ```

4.  要索引整个数据集，请运行以下命令:

    ```
    python app.py -t index -d f8k -n 8000
    ```

5.  从命令行启动索引流和索引样本数据，就像这样:

    ```
    python app.py -t index
    ```

这将创建索引流，处理特定文件夹中的数据，并将其存储在本地文件夹`workspace`中。

1.  然后，为了启动搜索流程并允许用户执行搜索查询，您可以运行以下命令:

让我们从运行一个小的测试查询开始。这个测试查询实际上包含一个图像和一个文本文档。正文是句子`a black dog and a spotted dog are fighting.`图像是`toy-data/images/1000268201_693b08cb0e.jpg`。然后，系统以跨模态的方式搜索图像和文本。这意味着图像用于搜索文本数据，而文本用于搜索图像数据。

使用图像搜索的文本结果将在您的终端中打印出来，如下所示:

![Figure 7.6 – Cross-modal search terminal output

](img/Figure_7.6_B17488.jpg)

图 7.6-跨模式搜索终端输出

图像结果将显示在如下的`matplotlib`图中:

![Figure 7.7 – Cross-modal search plot output

](img/Figure_7.7_B17488.jpg)

图 7.7-跨模态搜索图输出

在这种情况下，分数越低越好，因为它测量的是向量之间的距离。

您可以使用以下内容传递您自己的图像查询:

```
python app.py -t query --query-image path_to_your_image
```

`path_to_your_image`变量可以从终端的当前工作目录路径作为绝对或相对路径提供。

或者，对于文本，您可以这样做:

```
python app.py -t query --query-text "your text"
```

在这一节中，我们介绍了纪娜框架如何让我们轻松地构建跨模态搜索应用程序。这是可能的，因为纪娜的通用和可概括的数据类型，主要是文件，和灵活的管道建设过程。我们看到`needs`参数允许我们根据 *mime* 类型将处理管道分成两条路径。在下一节中，我们将看到如何在修改数据的同时提供数据。

# 并发查询和索引数据

在这一部分，我们将介绍如何在能够更新、删除或向数据库添加新数据的同时，持续满足客户请求的方法。这是业内的普遍要求，但要实现却并非易事。这里的挑战是维护用最新数据实现的向量索引，同时还能够以原子方式更新数据，而且还要在可扩展的容器化环境中执行所有这些操作。有了纪娜框架，所有这些挑战都可以轻松应对和克服。

默认情况下，在纪娜流中，不能同时索引数据和搜索。这是由网络协议的性质决定的。本质上，每个执行器都是一个单线程应用程序。您可以使用分片来扩展组成执行器组的执行器副本的数量。然而，这仅对于纯粹的并行操作是安全的，例如编码数据。这些种类的操作不会影响执行器的状态。另一方面， **CRUD** ( **创建/读取/更新/删除**)是影响状态的操作。通常，在可扩展的系统中，这些很难并行化。因此，如果您向应用程序发送大量要索引(添加)的数据，这将阻塞来自客户端的所有搜索请求。当然，这是非常有限的。在这个解决方案中，我将展示如何在纪娜解决这个问题。

该解决方案的关键组件是 **HNSWPostgresIndexer** 执行器。这是纪娜框架的执行者。它将内存中的 HNSW 矢量数据库与到 PostgreSQL 数据库的连接结合起来。文档的元数据存储在 SQL 数据库中，而嵌入存储在 RAM 中。与前面示例中的应用程序不同，它不需要两个不同的流。所有 CRUD 操作都在一个流生命周期内执行。这是可能的，因为执行器能够在 SQL 数据库和它的内存向量数据库之间同步状态。这可以配置为自动完成，也可以在所需时间手动触发。

现在让我们深入研究这个例子的每个组件在做什么。代码可以在[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with 纪娜](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)的文件夹`/src/Chapter07/wikipedia-sentences-query-while-indexing`中找到。

## app.py

这是示例的主入口点。用户可以调用它来启动索引和搜索流程，或者搜索文档。为了启动流程，您可以如下运行`app.py`:

```
python app.py -t flow
```

这将初始化纪娜应用程序的流程及其执行者。然后，它将向 **HNSWPostgreSQL** 执行器添加新数据，一次添加五个文档。这些数据最初只是被插入到 SQL 数据库中。这是因为 SQL 数据库被认为是主要的数据源。 **HNSW** 矢量索引将根据 SQL 数据库中的数据逐步更新。一旦有数据存在，执行器将自动将其同步到 HNSW 向量索引中。这个过程一直持续到数据被完全插入。一旦完成一轮，将有可供用户搜索的数据。然后，用户可以使用以下命令查询数据:

```
python app.py -t client
```

然后，将提示用户输入文本进行查询。然后，将对该文本进行编码，并与现有数据集进行比较，以获得最佳匹配。这些将被打印回终端。

## flow.yml

该文件定义了索引数据(添加数据)和搜索时使用的流程的结构。我将解释不同的选项。

下面是索引流程图。请注意，这非常简单:我们只是对编码后的数据进行编码和存储。这个示例应用程序的复杂性源于 **HNSWPostgreSQL** 执行器的内部行为。

![Figure 7.8 – Query Flow showing concurrency

](img/Figure_7.8_B17488.jpg)

图 7.8–显示并发性的查询流

流程本身具有以下参数:

*   `protocol`:定义流应该对外开放其 HTTP 协议
*   `port_expose`:定义监听端口

然后，执行者定义流程中的步骤:

*   第一个是`storage_encoder`。这用的是来自纪娜枢纽的`FlairTextEncoder`。这将文本编码成向量，用于机器学习中所需的线性代数运算。
*   第二个是`indexer`。这个用的`HNSWPostgresIndexer`，也是来自纪娜枢纽。这里使用的参数如下:
    *   `install_requirements`:将此项设置为`True`将安装该执行器所需的库
    *   `sync_interval`:自动将 SQL 数据库中的数据同步到矢量数据库中需要等待多少秒
    *   `dim`:嵌入的维度

你会注意到一些遗嘱执行人反复提到的另一个论点:

*   `timeout_ready`:这定义了在取消之前等待一个执行程序变得可用的秒数。我们将它设置为`–1`,所以只要需要，我们就等待。根据您的情况，应该对此进行调整。例如，如果您想要安全地终止一个长时间运行的下载请求，您可以将它设置为您想要等待执行程序启动的任意秒数。

## 安装和运行示例

在运行这个示例之前，确保您理解了上一章 chatbot 示例中的基本文本搜索。此外，您需要在计算机上安装 Docker:

1.  从[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with-纪娜/tree/main/src/chapter 07/Wikipedia-sentences-query-while-indexing](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing)中克隆 Git 存储库，并在示例的文件夹中打开一个终端。
2.  创建一个新的 Python 3.7 环境。虽然不是必需的，但强烈建议这样做。
3.  安装要求:

    ```
    pip install -r requirements.txt
    ```

知识库包括维基百科数据集的一个小子集，用于快速测试。你可以用这个。如果你想使用整个数据集，运行`bash` `get_data.sh`，然后修改`DATA_FILE`常量(在`app.py`中)指向那个文件。

1.  然后使用以下命令启动流程:

    ```
    python app.py -t flow
    ```

这创建了流，并建立了数据同步循环，如前面的`app.py`所述。

1.  为了查询数据，运行以下:

    ```
    python app.py -t client
    ```

然后会提示您输入一些文本。输入您想要的任何查询。然后，您将获得与您的查询最匹配的结果。

由于流公开了一个 HTTP 协议，所以您可以使用纪娜客户端、cURL、Postman 或纪娜内置的定制 Swagger UI 来查询 REST API。在终端中，可以在流通知的 URL 处到达 Swagger UI。通常是`http://localhost:45678/docs`，不过要看你配置的系统。

在本节中，我们已经了解了如何使用`HNSWPostgreSQLIndexer` Executor 在我们的实时系统中同时索引和搜索数据。在前面的例子中，为了在两种模式之间切换，需要重新定义和重新启动流。由于这个执行器结合了元数据存储(通过连接到 SQL 数据库)和嵌入索引(通过内存中的 HNSW 索引)，因此可以在一个流生命周期内执行所有 CRUD 操作。使用这些技术，我们可以拥有一个真正的面向客户端的应用程序，它不会因为需要更新索引中的底层数据库而受阻。

# 摘要

在这一章中，我们分析并练习了如何使用纪娜的高级功能，如组块、模态和高级`HNSWPostgreSQL`执行器，以解决神经搜索中最困难的目标。我们实现了任意层次深度数据表示、跨模态搜索和非阻塞数据更新的解决方案。组块让我们能够思考一些数据具有多层次语义的特性，如段落中的句子或较长电影中的视频剪辑。跨模态搜索开辟了神经搜索的一个主要优势——它的数据普遍性。这意味着您可以用任何数据搜索任何类型的数据，只要您对数据类型使用正确的模型。最后，`HNSWPostgreSQL` Executor 允许我们构建一个实时系统，用户可以同时搜索和索引，数据保持同步。*