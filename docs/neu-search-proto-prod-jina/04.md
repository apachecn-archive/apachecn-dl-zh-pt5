

# 三、系统设计和工程挑战

理解**机器学习** ( **ML** )和深度学习概念是必不可少的，但如果你想建立一个由**人工智能** ( **AI** )和深度学习支持的有效搜索解决方案，你还需要生产工程能力。有效地部署 ML 模型需要在技术领域更常见的能力，如软件工程和开发运维。这些能力称为 **MLOps** 。对于需要高可用性和低等待时间的搜索系统来说尤其如此。

在这一章中，你将学习设计搜索系统的基础知识。您将了解核心概念，如**索引**和**查询**以及如何使用它们来保存和检索信息。

在本章中，我们将特别介绍以下主要话题:

*   索引和查询
*   评估神经搜索系统
*   构建神经搜索系统的工程挑战

在本章结束时，你将会完全理解将神经搜索投入生产时的能力和可能要克服的困难。您将能够评估什么时候使用神经搜索是有用的，以及哪种方法最适合您自己的搜索系统。

# 技术要求

本章有以下技术要求:

*   最低内存为 4 GB 的笔记本电脑；建议 8 GB。
*   在类似 Unix 的操作系统(如 macOS 或 Ubuntu)上安装了 Python，版本为 3.7、3.8 或 3.9。

本章的代码文件可以在[https://github . com/packt publishing/Neural-Search-From-Prototype-to-Production-with Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)获得。

# 引入索引和查询

在本节中，您将完成两个重要的高级任务来构建一个搜索系统:

*   **索引**:这是收集、解析和存储数据以促进快速准确的信息检索的过程。这包括添加、更新、删除和读取要索引的文档。
*   **查询**:查询是对用户查询进行解析、匹配、排序，并将相关信息返回给用户的过程。

在神经搜索系统中，索引和查询都由一系列任务组成。让我们深入了解一下索引和查询组件。

## 索引

索引是搜索系统中一个重要的过程。它构成了核心功能，因为它有助于有效地检索信息。索引把文件归纳成其中包含的有用信息。它将术语映射到包含信息的各个文档。在搜索系统中查找相关文档的过程与查看字典的过程基本相同，在字典中，索引可以帮助您有效地查找单词。

在介绍细节之前，我们先问以下问题，以了解我们所处的位置:

*   索引管道的主要组成部分是什么？
*   哪些内容可以被索引？
*   我们如何进行增量索引，如何快速索引？

如果你不知道这些问题的答案，不要担心。继续读吧！

在索引管道中，我们通常有三个主要组件:

*   `text/plain`，我们可能需要一个分词器和词干分析器，正如在 [*第一章*](B17488_01.xhtml#_idTextAnchor014) ，*神经搜索的神经网络*中所介绍的。如果我们想要索引模态`image/jpeg`的图像，我们可能想要一个组件来调整输入图像的大小或将输入图像转换成神经网络的预期格式。它高度依赖于你的任务和输入数据。
*   **编码器**:在神经搜索系统中，编码器等同于神经网络。这个神经网络将你的预处理输入作为一个向量表示(嵌入)。在这一步之后，每个由文本、图像、视频甚至 DNA 信息组成的原始文档都应该表示为一个数值向量。
*   **索引器(用于存储)**:一个索引器，更好地称为**存储索引器**，在索引阶段将编码器产生的向量存储到存储器中，如内存或数据库。这包括关系数据库(PostgresSQL)、NoSQL (MongoDB)，或者更好的向量数据库，比如 Elasticsearch。

应该注意的是每个步进任务都是独立的。从不同的角度来看可能会有所不同。例如，如果您正在一个电子商务环境中开发一个多模型搜索引擎，那么您的目标是创建一个可以将文本和图像作为查询并找到最相关产品的搜索系统。在这种情况下，您的索引可能有两种途径:

*   应该使用基于文本的预处理程序和编码器对文本信息进行预处理和编码。
*   同样，应该使用基于图像的预处理器和编码器对图像数据进行预处理和编码。

您可能想知道什么可以被索引。什么都可以，只要你有编码器，你的数据可以被编码。可以索引的一些常见数据类型包括文本、图像、视频和音频。正如我们在前面章节中所讨论的，你可以编码源代码来建立一个源代码搜索系统，或者编码基因信息来建立一个围绕它的搜索系统。下图说明了一个索引管道:

![Figure 3.1 – A simple indexing pipeline takes documents as input and applies preprocessing and encoding. In the end, save the encoded features into storage

](img/Figure_3.1_B17488.jpg)

图 3.1–一个简单的索引管道将文档作为输入，并应用预处理和编码。最后，将编码后的特征保存到存储器中

现在，我们转到一个重要的话题:增量索引。首先，让我们讨论什么是增量索引。

### 了解增量索引

**增量索引**对于任何搜索系统来说都是一个至关重要的特性。考虑到我们要索引的数据集合每天都可能发生巨大的变化，我们无法承受每次有小的变化时就索引整个数据集合。

通常，有两种执行索引任务的常见做法，如下所示:

*   **实时索引**:给定发送到集合的任何数据，索引器立即将文档添加到索引中。
*   **计划索引**:假设任何数据被发送到集合，调度器触发索引任务并执行索引作业。

前面的做法各有利弊。在实时索引中，用户可以立即获得新添加的文档(如果匹配的话)，同时也会消耗更多的系统资源，并可能导致数据不一致。然而，在计划索引的情况下，用户不能实时访问新添加的结果，但是它不容易出错并且更容易管理。

您选择的索引策略取决于您的任务。如果任务是时间敏感的，最好使用实时索引。否则，最好设置一个 cron 作业，并在某个时间增量索引您的数据。

### 加速索引

在神经搜索中执行索引任务时，另一个关键的问题是索引的速度。虽然符号搜索系统仅在文本数据上工作，但是神经搜索系统的输入可以是三维的(*高度*宽度*颜色通道*)，例如 RGB 图像，或者四维的(*帧*高度*宽度*颜色通道*)，例如视频。这种数据可以用不同的模态进行索引，这会大大降低数据预处理和编码过程的速度。

一般来说，有几种策略可以用来提高索引速度。其中一些如下:

*   预处理器:对某些数据集应用某些预处理可以大大提高你的索引速度。例如，如果您想要索引高分辨率图像，最好调整它们的大小，使它们变小。
*   **GPU 推断**:在一个神经搜索系统中，编码占用了大部分索引时间。更具体地说，给定预处理的文档，使用深度神经网络将文档编码成向量需要时间。通过使用 GPU 实例进行编码，可以极大地提高性能。由于 GPU 有更高带宽的内存和 L1 缓存，GPU 适合 ML 任务。
*   **水平伸缩**:在一台机器上索引大量数据会使过程变慢，但是如果我们将数据分布在多台机器上并并行执行索引，速度会快得多。例如，下图演示了如何将更多编码器分配给管道:

![Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference in parallel

](img/Figure_3.2_B17488.jpg)

图 3.2–三个编码器并行利用 GPU 推理进行高速索引

值得一提的是，如果您来自文本检索背景，那么在为符号搜索构建倒排索引时，**索引压缩**也很重要。这在神经搜索系统中不再完全相同:

*   首先，编码器将文档作为输入，并将文档的内容编码成一个 N 维向量(嵌入)。因此，我们可以认为编码器本身是一种压缩功能。
*   第二，压缩密集向量最终会牺牲向量的质量。通常，更大维度的向量会带来更好的搜索结果，因为它们可以更好地表示被编码的文档。

在实践中，我们需要在维度和内存使用之间找到一个平衡点，以便将所有向量加载到内存中来执行大规模的相似性搜索。在下一节中，我们将深入到查询部分，这将使您理解如何进行大规模的相似性搜索。

## 查询

说到查询管道，它有很多与索引管道重叠的组件，但是有一些修改和附加组件，比如一个 ranker。在这个阶段，管道的输入是单个用户查询。典型的查询任务有四个主要组成部分:

*   **预处理器**:该组件类似于索引管道中的预处理器。它将查询文档作为输入，并将与索引管道相同的预处理程序应用于输入。
*   **编码器**:编码器将预处理后的查询文档作为输入，产生向量作为输出。应该注意，在跨模态搜索系统中，索引中的编码器可能与查询步骤中的编码器不同。这将在 [*第七章*](B17488_07.xhtml#_idTextAnchor101) 、*探索 Jina*的高级用例中解释。
*   **索引器**:这个索引器，更好的是，命名为 **SearchIndexer** ，将编码器产生的向量作为输入，对所有索引的文档进行大规模的相似性搜索。这就是所谓的**近似最近邻** ( **安**)搜索。我们将在下一节详细阐述这个概念。
*   **排序器**:排序器针对集合中的每一个项目获取查询向量和相似性分数，以降序产生排序列表，并将结果返回给用户。

索引和查询的一个主要区别是索引(在大多数情况下)是一个离线任务，而查询是一个在线任务。更具体地说，当我们引导一个神经搜索系统并创建一个查询时，系统将返回一个空列表，因为此时没有任何内容被索引。在我们向用户展示搜索系统之前，我们应该预先索引数据集合中的所有文档。该索引是脱机执行的。

另一方面，在查询任务中，用户向系统发送一个查询，并期望立即得到匹配。所有的预处理、编码、索引搜索和排序都应该在等待时间内完成。因此，这是一项在线任务。

重要说明

实时索引可以被认为是一项在线任务。

与索引不同，在查询时，每个用户将单个文档作为查询发送到系统。预处理和编码需要很短的时间。另一方面，在索引存储器中寻找相似的项目成为影响神经搜索系统性能的关键工程挑战。这是为什么呢？

例如，您已经预先索引了 10 亿个文档，在查询时，用户向系统发送一个查询，然后文档被预处理并编码成一个向量(嵌入)。给定查询向量，现在需要从一百万个向量中找出前 N 个相似向量。你是怎么做到的？通过逐个计算向量之间的距离来进行相似性搜索可能需要很长时间。相反，我们进行人工神经网络搜索。

重要说明

当我们谈论人工神经网络搜索时，我们正在考虑百万/十亿规模的搜索。如果您想构建一个玩具示例并在成百上千的文档中搜索，普通的线性扫描已经足够快了。在生产环境中，请遵循下一节将介绍的选择策略。

### 人工神经网络搜索

顾名思义，人工神经网络搜索是不同因素之间的权衡:准确性、运行时间和内存消耗。与强力搜索相比，它在一定程度上牺牲了查准率和查全率的同时，保证了用户可以接受的运行时间。能达到多快？给定 10 亿个 100 维向量，它可以装入一个 32 GB 内存的服务器，响应速度为 10 ms。在深入研究 ANN search 的细节之前，让我们先来看看下图:

![Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor Search, Yusuke Matsui)

](img/Figure_3.3_B17488.jpg)

图 3.3-ANN 备忘单(来源:十亿级近似最近邻搜索，Yusuke Matsui)

前面的图说明了*如何根据你的搜索系统*选择人工神经网络库。在图中， *N* 表示您的*存储索引*中的文档数量。不同数量的 N 可以使用不同的 ANN 搜索库进行优化，如`FAISS`([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))或`NMSLIB`([https://github.com/nmslib/nmslib](https://github.com/nmslib/nmslib))。同时，由于你很可能是一个 Python 用户，【https://github.com/spotify/annoy】库([和](https://github.com/spotify/annoy))提供了一个性能合理的用户友好界面，对于百万级的向量搜索来说，它已经足够好了。

前面提到的库是基于不同的算法实现的，最流行的库是 **KD-Trees** 、**本地敏感哈希** ( **LSH** )，以及**产品量化** ( **PQ** )。

KD-Trees 遵循迭代过程来构建树。为了便于可视化，我们假设数据只包含两个特征， *f1* ( *x* 轴)和 *f2* ( *y* 轴)，如下所示:

![Figure 3.4 – KD-Trees, sample dataset to index

](img/Figure_3.4_B17488.jpg)

图 3.4–KD 树，要索引的样本数据集

构建一个 KD 树始于选择一个实用特征并为该特征设置一个阈值。为了说明这个想法，我们从手动选择 f1 和特征阈值 0.5 开始。为此，我们得到这样一个边界:

![Figure 3.5 – KD-Trees construction iteration 1

](img/Figure_3.5_B17488.jpg)

图 3.5–KD 树构建迭代 1

从*图 3.5* 可以看出，特征空间已经被我们第一次选择的阈值为 0.5 的 f1 分割成两部分。对于树是如何体现的？构建指数时，我们实际上是在创建一个二叉查找树。阈值为 0.5 的 f1 的第一个选择成为我们的根节点。给定每个数据点，如果 f1 大于 0.5，它将被放置在节点的右侧。否则，如图*图 3.6* 所示，我们把它放到节点的左边:

![Figure 3.6 – KD-Trees construction iteration 2

](img/Figure_3.6_B17488.jpg)

图 3.6–KD 树构建迭代 2

我们从前面的树继续。在第二次迭代中，让我们将我们的规则定义为:给定 f1 > 0.5，选择阈值为 0.5 的 f2。如上图所示，现在我们根据新的规则再次分割特征空间，这也反映在我们的树上:我们在图中创建了一个新节点 **f2-0.5** (图中的 **none** 节点仅用于可视化目的；我们还没有创建这个节点)。如下图所示:

![Figure 3.7 – KD-Tree construction iteration N (final iteration)

](img/Figure_3.7_B17488.jpg)

图 3.7–KD 树构建迭代 N(最终迭代)

如*图 3.7* 所示，整个特征空间被分成六个箱。与之前相比，我们添加了三个新节点，包括两个叶节点:

*   之前的**无**被一个实际节点 **f2-0.65** 代替；这个节点基于阈值 0.65 分割 f2 的空间，只有在 f1 < 0.5 时才会发生。
*   当 f2 <0.65, we further split f1 by a threshold of 0.2.
*   When f2> 0.65 时，我们进一步将 f1 除以 0.3 的阈值。

为此，我们的树有三个叶节点，每个叶节点可以构造两个箱(小于/大于阈值)，我们总共有六个箱。此外，每个数据点可以放入其中一个箱中。然后，我们完成了 KD 树的构建。应该注意的是，构建一个 KD 树可能并不简单，因为您需要考虑一些超参数，比如如何设置阈值或者我们应该创建多少个箱(或者停止标准)。实际上，没有金科玉律。通常，平均值或中值可用于设置阈值。箱的数量可能高度依赖于结果的评估和微调。

在搜索时，给定一个用户查询，它可以被放入特征空间内的一个箱中。我们能够计算查询和作为最近邻候选的箱内所有项目之间的距离。我们还需要计算查询和所有其他箱之间的最小距离。如果查询向量和其他箱之间的距离大于查询向量和最近邻居候选之间的距离，我们可以通过修剪树的叶节点来忽略该箱内的所有数据点。否则，我们也将该区间内的数据点视为最近邻的候选。

通过构建一个 KD 树，我们不再需要计算查询向量和每个文档之间的相似性。只有一定数量的箱应该被认为是候选。因此，可以大大减少搜索时间。

在实践中，KD 树遭受维数灾难。将它们应用于高维数据是很棘手的，因为有如此多的箱要搜索，仅仅因为对于每个特征，我们总是创建几个阈值。**位置敏感散列法** ( **LSH** )可能是一个很好的替代算法。

LSH 背后的基本思想是相似的文档共享相同的散列码，它被设计成最大化冲突。更具体地说:给定一组向量，我们希望有一个哈希函数，能够将相似的文档编码到同一个哈希桶中。然后，我们只需要在桶内找到相似的向量(不需要扫描所有的数据)。

先说 LSH 指数建设。在索引时，我们首先需要创建随机平面(超平面)来将特征空间分成*个面元*。

![Figure 3.8 – LSH index construction with random hyperplanes

](img/Figure_3.8_B17488.jpg)

图 3.8–用随机超平面构建 LSH 指数

在*图 3.8* 中，我们已经创建了六个超平面。每个超平面能够将我们的特征空间分成两个面元，或者左/右或者上/下，可以用二进制代码(或符号)表示:0 或 1。这被称为容器的索引。

让我们试着得到右下 bin 的 bin 索引(它在 bin 中有四个点)。料仓位于以下位置:

*   **平面 1** 的右侧，所以位置 0 的符号为 1
*   **平面 2** 的右侧，所以位置 1 的符号为 1
*   **平面 3** 的右侧，所以位置 2 的符号为 1
*   **平面 4** 的右侧，所以位置 3 的符号为 1
*   **平面 5** 的底部，所以位置 4 的符号为 0
*   **平面 6** 的底部，所以位置 5 的符号为 0

因此，我们可以将右下角的 bin 表示为 111100。如果我们重复这个过程并用 bin 索引注释每个 bin，我们将得到一个 hash map。哈希映射的关键字是 bin 索引，而哈希映射的值是 bin 内数据点的 id。

![Figure 3.9 – LSH index construction with bin index

](img/Figure_3.9_B17488.jpg)

图 3.9–利用 bin 指数构建 LSH 指数

在 LSH 山顶搜索很容易。直观地说，给定一个查询，您可以只搜索它自己的箱内的所有数据点，也可以搜索它的相邻箱。

你如何在它的邻近箱子中搜索？看看*图 3.9* 。bin 索引表示为二进制代码；相邻的容器与其自己的容器索引相比只有 1 比特的差异。显然，您可以将 bin 索引之间的差异视为一个超参数，并搜索更多的相邻 bin。例如，如果您将 hyper 参数设置为 2，意味着您允许 LSH 搜索 2 个邻居箱。

为了更好地理解这一点，我们将看看 LSH 的恼人的实现，即 LSH 与随机投影。给定由深度神经网络产生的向量列表，我们首先进行以下操作:

1.  随机初始化一个超平面。
2.  向量与法线(垂直于超平面的向量)的点积。对于每个向量，如果值为正，我们生成二进制代码 1，否则为 0。
3.  我们生成 N 个超平面，并将该过程迭代 N 次。最后，每个向量由 0 和 1 的二进制向量表示。
4.  我们将每个二进制代码视为一个桶，并将所有具有相同二进制代码的文档保存到同一个桶中。

以下代码块演示了随机投影 LSH 的简单实现:

```py
pip install numpy

pip install spacy

spacy download en_core_web_md
```

我们将两段句子预处理成桶:

```py
from collections import defaultdict

import numpy as np

import spacy

n_hyperplanes = 10

nlp = spacy.load('en_core_web_md')

# process 2 sentences using the model

docs = [

    nlp('What a nice day today!'),

    nlp('Hi how are you'),

]

# Get the mean vector for the entire sentence

assert docs[0].vector.shape == (300,)

# Random initialize 10 hyperplanes, dimension identical to embedding shape

hyperplanes = np.random.uniform(-10, 10, (n_hyperplanes, docs[0].vector.shape[0]))

def encode(doc, hyperplanes):

    code = np.dot(doc.vector, hyperplanes.T)  # dot product vector with norm vector

    binary_code = np.where(code > 0, 1, 0)

    return binary_code

def create_buckets(docs, hyperplanes):

    buckets = defaultdict()

    for doc in docs:

        binary_code = encode(doc, hyperplanes)

        binary_code = ''.join(map(str, binary_code))

        buckets[binary_code] = doc.text

    return buckets

if __name__ == '__main__':

    buckets = create_buckets(docs, hyperplanes)

    print(buckets)
```

这样，我们将数百万个文档映射到多个桶中。在搜索时，我们使用相同的超平面对搜索文档进行编码，获得二进制代码，并在相同的桶内找到相似的文档。

在 Annoy 实现中，搜索速度取决于两个参数:

*   `search_k`:这个参数是你想从索引中获取的最上面的`k`个元素。
*   `N_trees`:该参数表示您要搜索的桶数。

很明显，搜索运行时间高度依赖于这两个参数，用户需要根据他们的用例来微调这些参数。

另一个流行的人工神经网络搜索算法是 PQ。在我们深入 PQ 之前，理解什么是*量化*很重要。假设您有一百万个文档要索引，并且您为所有文档创建了 100 个*质心*。一个**量化器**是一个功能，可以将一个向量映射到一个质心。你可能会觉得这个想法很熟悉。实际上，K-means 算法就是一个可以帮助你生成这样的质心的函数。如果您不记得了，K-means 的工作方式如下:

1.  随机初始化`k`形心。
2.  将每个向量指定给其最近的质心。每个质心代表一个聚类。
3.  基于所有赋值的平均值计算新的形心，直到收敛。

一旦 K-均值收敛，给定所有要索引的向量，我们得到 K 个聚类。对于每个要索引的文档，我们在文档 ID 和集群索引之间创建一个映射。在搜索时，我们根据质心计算距离查询向量，得到最近的聚类，然后在这些聚类中找到最近的向量。

这种量化算法具有相对较好的压缩比。你不必为了得到最接近的向量而线性扫描所有的向量；你只需要扫描量化器产生的某些簇。另一方面，如果质心的数量很少，搜索时的召回率可能很低。这是因为有太多的边缘情况不能被正确地分配到正确的集群。此外，如果我们将质心的集合数量简化为一个大的数字，我们的 K-means 运算将需要很长时间才能收敛。这成为离线索引时间和在线搜索时间的瓶颈。

PQ 背后的基本思想是将高维向量分割成子向量，如以下步骤所示:

1.  我们将每个向量分成 *m* 个子向量。
2.  对于每个子向量，我们应用量子化。为此，对于每个子向量，我们有一个唯一的簇 ID(子向量与其质心最近的簇)。
3.  对于全向量，我们有一个簇 id 列表，可以作为全向量的码本。码本的维数与子向量的数量相同。

下图说明了 PQ 算法:给定一个向量，我们将其切割成更低维度的子向量，并应用量化。为此，每个量子化的子向量得到一个代码:

![Figure 3.10 – Product quantization

](img/Figure_3.10_B17488.jpg)

图 3.10-产品量化

在搜索时，我们再次将高维查询向量分割成子向量，并生成码本(桶)。我们针对集合中的每个向量计算子向量级余弦相似性，并对子向量级相似性分数求和。我们根据向量级余弦相似性对最终结果进行排序。

实际上，FAISS 有一个高性能的 PQ 实现(并且超越了 PQ)。更多信息，请参考文档(【https://github.com/facebookresearch/faiss/wiki】T4)。

现在我们已经学习了神经搜索的两个基本任务，索引和查询。在下一节中，我们将介绍神经搜索系统评估，以使您的神经搜索系统完整并可投入生产。

# 评估神经搜索系统

一旦你建立了一些基线，评估一个神经搜索系统的有效性是至关重要的。通过监控评估指标，您可以立即知道您的系统性能如何。通过深入查询，您还可以进行故障分析，并了解如何改进您的系统。

在本节中，我们将简要概述最常用的评估指标。如果你想对这个话题有一个更详细的数学理解，我们强烈建议你通过信息检索(【https://nlp.stanford.edu/IR-book/pdf/08eval.pdf】)中的*评测。*

一般来说，考虑到搜索任务之间的差异，我们通常可以将搜索评估分为两类:

*   **未分级结果的评估**:这些指标广泛应用于一些检索或分类任务，包括精度、召回率和 F 值。
*   **排名结果的评估**:这些指标主要用于典型的搜索应用，给出的结果是有序的(排名)。

首先，让我们从精确度、召回率和 F 值开始:

*   在典型的搜索场景中，精度定义如下:

*精度=(检索到的相关文档数)/(检索到的文档数)*

这个想法很简单。假设我们的搜索系统返回 10 个文档，其中 7 个是相关的，那么精度将是 0.7。

应该注意的是，在评估过程中，我们关心顶部的`k`检索结果。就像前面提到的例子一样，我们评估前 10 个结果中的相关文档。这通常被称为精度，例如**精度@10** 。它也适用于我们将在本节稍后介绍的其他指标，例如 Recall@10、mAP@10 和 nDCG@10。

*   同样，召回定义如下:

*召回=(检索到的相关文档数)/(相关文档数)*

例如，如果我们在我们的系统中搜索`cat`，我们已经知道有 100 张与猫相关的图片被索引，并且返回了 80 张图片，那么召回率是 0.8。这是一个评估指标，用于衡量搜索系统表现的*完整性*。

重要说明

召回率是评估人工神经网络算法性能的最重要的评估指标，因为它描述了所有查询平均找到的真正最近邻的比例。

重要说明

对于典型的 ML 任务，例如分类，准确性可以是一个很好的度量。但是对于搜索任务来说情况并非如此，因为搜索任务中的大多数数据集是偏斜的/不平衡的。

作为一个搜索系统的设计者，你可能已经注意到这两个数字是相互权衡的:随着 K 数的增加，我们总是可以期待更低的精确度和更高的召回率，反之亦然。这是你的决定，优化精度或召回或优化这两个数字作为一个评估指标，即 F1 分数。

*   f1-分数为，定义如下:

*F1-得分= (2 *精度*召回)/(精度+召回)*

它是精确度和召回率的加权调和平均值。事实上，较高的召回率往往与较低的准确率相关联。假设您正在评估一个排序列表，并且您关心要检索的前 10 个项目(并且在整个集合中有 10 个相关文档):

| **文件** | **标签** | **精度** | **召回** |
| 文档 1 | 相关的 | 1/1 | 1/10 |
| 文档 2 | 相关的 | 2/2 | 2/10 |
| 文档 3 | 不相干的 | 2/3 | 2/10 |
| 文档 4 | 不相干的 | 2/4 | 2/10 |
| 文档 5 | 相关的 | 3/5 | 3/10 |
| 文档 6 | 不相干的 | 3/6 | 3/10 |
| 文档 7 | 不相干的 | 3/7 | 3/10 |
| 文档 8 | 相关的 | 4/8 | 4/10 |
| 文档 9 | 不相干的 | 4/9 | 4/10 |
| Doc10 | 不相干的 | 4/10 | 4/10 |

表 3.1-前 10 个文档中 10 个的精确召回

*表 3.1* 显示了给定二进制标签时不同级别的精度和召回率。

熟悉了精度之后，我们现在可以继续计算平均精度。这个指标将让我们更好地理解我们的搜索系统对查询结果进行排序的能力。

具体地，给定前面排序的列表，`aP@10`如下:

`aP@10 = (1/1 + 2/2 + 3/5 + 4/8) / 10 = 0.31`

请注意，在计算 aP 时，只考虑相关文档的精度。

现在，aP 已经针对一个特定的用户查询进行了计算。然而，为了给出更健壮的搜索系统评估，我们想要评估作为测试集的用户查询集合的性能。这就是所谓的`aP@k`的，然后我们对一组查询中的所有接入点进行平均，得到地图得分。

mAP 是给定排序列表的最重要的搜索系统评估指标之一。要对您的搜索系统进行地图评估，通常您必须遵循以下步骤:

1.  编写一个查询列表，很好地代表用户的信息需求。这个数字取决于你的情况，比如 50、100 或 200。
2.  如果您的文档已经有了指示相关度的标签，那么直接使用这些标签来计算每个查询的 aP。如果您的文档不包含任何与每个查询相关的信息，我们需要专家注释或汇集来访问相关学位。
3.  通过取 AP 的平均值来计算查询列表上的 mAP。如前所述，如果您没有对排名文档的相关评估，一种常见的技术称为 **pooling** 。它要求我们建立多个搜索系统(比如三个)进行测试。给定每个查询，我们收集这三个搜索系统返回的前 K 个文档。人类注释者判断所有 3 * K 文档的相关程度。之后，我们认为该库中的所有文档都是不相关的，而该库中的所有文档都是相关的。然后，可以在池的顶部评估搜索结果。

在这一点上，即使 mAP 正在评估一个排序列表，精度定义的本质仍然忽略了搜索任务的一些本质:精度是基于相关或不相关的二进制标签来评估的。它不反映针对文档的查询的*相关度*。**归一化折扣累积增益** ( **nDCG** )用于评估搜索系统在相关度上的性能。

nDCG 可以为每个文档设置多个等级，例如*不相关*、*相关*或*高度相关*。在这种情况下，映射不再起作用。

例如，给定三种相关度(不相关、相关和高度相关)，这些相关度的差异可以表示为用户通过获取每个文档可以获得的信息增益。对于高度相关的文档，增益可以赋值为 *3* ，相关可以为 *1* ，不相关可以设置为 *0* 。然后，如果高度相关的文档的排名高于不相关的文档，则用户可以累积更多的*增益*，这被称为**累积增益** ( **CG** )。下面的表格显示了给定由搜索系统产生的排名前 10 的文档，我们从每个文档获得的信息增益:

| **文件** | **标签** | **增益** |
| 文档 1 | 高度相关 | 3 |
| 文档 2 | 相关的 | 一 |
| 文档 3 | 不相干的 | 0 |
| 文档 4 | 不相干的 | 0 |
| 文档 5 | 相关的 | 一 |
| 文档 6 | 不相干的 | 0 |
| 文档 7 | 不相干的 | 0 |
| 文档 8 | 高度相关 | 3 |
| 文档 9 | 不相干的 | 0 |
| Doc10 | 不相干的 | 0 |

表 3.2–获得信息最多的 10 份文件

在前面的表中，系统将排名前 10 的文档返回给用户。基于相关度，我们将 3 指定为高度相关文档的增益，将 1 指定为相关文档的增益，将 0 指定为不相关文档的增益。CG 是前 10 个文档中所有收益的总和，如下所示:

`CG@10 = 3 + 1 + 1 + 3 = 8`

但是想想搜索引擎的本质:用户从上到下浏览排名列表。因此，本质上，排名靠前的文档应该比排名靠后的文档有更多的收益，这样我们的搜索系统就会尝试将高度相关的文档排在更高的位置。因此，在实践中，我们将通过头寸来惩罚收益。请参见以下示例:

| **文件** | **标签** | **增益** | **贴现收益** |
| 文档 1 | 高度相关 | 3 | 3 |
| 文档 2 | 相关的 | 一 | 1/log2 |
| 文档 3 | 不相干的 | 0 | 0 |
| 文档 4 | 不相干的 | 0 | 0 |
| 文档 5 | 相关的 | 一 | 1/log5 |
| 文档 6 | 不相干的 | 0 | 0 |
| 文档 7 | 不相干的 | 0 | 0 |
| 文档 8 | 高度相关 | 3 | 3/log8 |
| 文档 9 | 不相干的 | 0 | 0 |
| Doc10 | 不相干的 | 0 | 0 |

表 3.3–收益和贴现收益的前 10 份文件

在前面的表格中，给定了一个文档的增益及其排名位置，我们通过将增益除以一个因子来惩罚增益。在这种情况下，它是排名位置的对数。该收益的总和称为**贴现累计收益** ( **DCG** ):

`DCG@10 = 3 + 1/log2 + 1/log5 + 3/log8 = 6.51`

在我们开始计算 nDCG 之前，理解理想 DCG 的概念很重要。它仅仅意味着我们能达到的最好的排名结果。在前一种情况下，如果我们查看前 10 个位置，理想情况下，排序列表应该包含所有增益为 3 的高度相关的文档。因此，iDCG 应该如下所示:

`iDCG@10 = 3 + 3/log2 + 3/log3 + 3/log4 + 3/log5 + 3/log6 + 3/log7 + 3/log8 + 3/log9 + 3/log10 = 21.41`

最终，最终的 nDCG 如下:

`nDCG = DCG/iDCG`

在前面的示例中，我们有以下内容:

`nDCG@10 = 6.51/21.41 = 0.304`

值得一提的是，尽管 nDCG 非常适合评估反映相关性程度的搜索系统，但*相关性*本身偏向于不同的因素，如搜索上下文和用户偏好。在现实世界的场景中执行这样的评估是很重要的。在下一章中，我们将深入探讨这些挑战的细节，并简要介绍如何解决它们。

# 构建神经搜索系统的工程挑战

现在，你会注意到神经搜索系统最重要的组成部分是编码器和索引器。编码帖子的质量直接影响最终的搜索结果，而索引器的速度决定了你的神经搜索系统的可扩展性。

同时，这仍然不足以让你的神经搜索系统准备就绪。还需要考虑许多其他问题。第一个问题是:你的编码器(神经模型)和你的数据有相同的分布吗？对于进入神经搜索系统世界的新人来说，他们正在使用预先训练的深度神经网络，例如在 ImageNet 上训练的 ResNet，快速建立搜索系统是微不足道的。然而，如果你的目标是建立一个特定领域的神经搜索系统，比如说一个时尚产品图像搜索，它不会产生令人满意的结果。

在我们真正开始创建编码器和设置我们的搜索系统之前，一个重要的主题是将迁移学习应用到您的数据集并评估匹配结果。这意味着采用一个预训练的深度学习模型，如 ResNet，砍掉头部层，冻结预训练模型的权重，并在模型的末尾附加一个新的嵌入层，然后在您的域上的新数据集上训练它。这将大大提高搜索性能。

除此之外，在一些基于视觉的搜索系统中，仅仅依靠编码器可能是不够的。例如，许多基于视觉的搜索系统严重依赖于对象检测器。在将完整图像发送到编码器之前，应首先将其发送到对象检测器，并提取图像中有意义的部分(并去除背景噪声)。这可能会提高嵌入质量。同时，一些基于视觉的分类模型也可以作为硬过滤器来丰富搜索上下文。例如，如果您正在构建一个神经搜索系统，允许人们在给定图像作为查询的情况下搜索类似的汽车，那么预先训练的品牌分类器可能会很有用。更具体地说，你预先训练一个汽车品牌分类器，让*根据图像识别*不同的汽车品牌，并将识别应用于索引和搜索管道。一旦基于视觉的搜索完成，您可以将识别的品牌作为硬过滤器来过滤掉其他品牌的汽车。

类似地，对于基于文本的搜索，当用户提供关键字作为查询时，直接应用基于嵌入的相似性搜索可能是不够的。例如，您可以在索引和查询管道中创建一个**命名实体识别** ( **NER** )模块来丰富元数据。

对于基于 web 的搜索引擎，如 Google、Bing 或百度，很容易看到查询自动完成。向您的索引和搜索管道中添加一个基于深度神经网络的关键字提取组件，以使用类似的用户体验，这可能也非常有趣。

总之，要构建一个生产就绪的神经搜索系统，设计一个功能完整的索引和查询管道是非常具有挑战性的，因为搜索是一项如此复杂的任务。设计这样一个系统已经很有挑战性了，更不用说设计基础设施了。幸运的是，Jina 已经可以帮你完成大多数最具挑战性的任务。

# 总结

在这一章中，我们已经讨论了构建神经搜索系统的基本任务，即索引和查询管道。我们研究了这两个方面，并介绍了最具挑战性的部分，比如编码和索引。

您应该对索引和查询的基本构件有基本的了解，比如预处理、编码和索引。您还应该注意到，搜索结果的质量高度依赖于编码器，而神经搜索系统的可伸缩性高度依赖于索引器和索引器背后最流行的算法。

当您需要构建一个生产就绪的搜索系统时，您会意识到仅仅依靠基本的构建模块是不够的。由于搜索系统实现起来很复杂，所以为了获得更好的搜索结果，总是需要设计自己的构建模块并将其添加到索引和查询管道中。

在下一章，我们将开始介绍 Jina，这是帮助你设计神经搜索系统的最流行的框架。你会意识到，Jina 已经为你解决了最困难的问题，并可能使你作为神经搜索系统工程师/科学家的生活变得容易得多。