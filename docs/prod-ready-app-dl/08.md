

# 六、高效的模型训练

类似于我们在上一章中如何扩大数据处理管道，我们可以通过分配更多的计算资源来减少训练**深度学习** ( **DL** )模型所需的时间。在本章中，我们将学习如何配置 **TensorFlow** ( **TF** )和 **PyTorch** 训练逻辑，以利用不同机器上的多个 CPU 和 GPU 设备。首先，我们将了解 TF 和 PyTorch 如何在没有任何外部工具的情况下支持分布式训练。接下来，我们将描述如何利用 SageMaker，因为它是为了端到端地处理云上的 DL 管道而构建的。最后，我们将看看专门为分布式培训开发的工具:Horovod、Ray 和 Kubeflow。

在本章中，我们将讨论以下主要话题:

*   在单台计算机上训练模型
*   在群集上训练模型
*   使用 SageMaker 训练模型
*   使用 Horovod 训练模型
*   使用光线训练模型
*   使用 Kubeflow 训练模型

# 技术要求

你可以从本书的 GitHub 资源库下载本章的补充资料:https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 6。

# 在单台机器上训练模型

如 [*第三章*](B18522_03.xhtml#_idTextAnchor062) ，*开发一个强大的深度学习模型*，训练一个 DL 模型涉及从数据集中提取有意义的模式。当数据集的大小很小时，模型很少有参数需要调整，一个**中央处理单元** ( **CPU** )可能足以训练该模型。然而，当 DL 模型用更大的训练集进行训练并且由更多数量的神经元组成时，它们表现出更好的性能。因此，使用**图形处理单元** ( **GPU** )进行训练已经成为标准，因为你可以利用它在矩阵乘法方面的巨大并行性。

## 在 TensorFlow 中利用多种设备进行训练

TF 提供了`tf.distribute.Strategy`模块，通过非常简单的代码修改[阳离子(https://www.tensorflow.org/guide/distributed](https://www.tensorflow.org/guide/distributed_training)_ training)，允许你使用多个 GPU 或 CPU 设备进行训练。`tf.distribute.Strategy`完全兼容`tf.keras.Model.fit`，以及自定义训练循环，详见 [*第三章*](B18522_03.xhtml#_idTextAnchor062)*开发强大的深度学习模型*的*在 TensorFlow* 部分实现并训练模型。Keras 的各种组件，包括变量、层、模型、优化器、指标、摘要和检查点，旨在支持各种`tf.distribute.Strategy`类，尽可能简单地过渡到分布式培训。让我们来看看`tf.distribute.Strategy`模块是如何让你快速修改一套为单台设备设计的代码到单台机器上的多台设备的:

```

import tensorflow as tf

mirrored_strategy = tf.distribute.MirroredStrategy()

# or 

# mirrored_strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:3"])

# if you want to use only specific devices 

with mirrored_strategy.scope():

    # define your model 

    # …

model.compile(... )

model.fit(... ) 
```

一旦模型被保存，它可以加载带或不带`tf.distribute.Strategy`范围。要用定制的训练循环实现分布式训练，你可以效仿 https://www.tensorflow.org/tutorials/distribute/cus 汤姆训练的例子。说到这里，我们来回顾一下使用最多的策略。我们将讨论最常见的方法，其中一些方法超出了训练单个实例的范围。它们将在接下来的几个部分中使用，这些部分将涵盖多台机器上的培训:

*   为`tf.keras.Model.fit`和定制训练循环提供全面支持的策略:
    *   `MirroredStrategy`:在一台机器上使用多个 GPU 的同步分布式训练
    *   `MultiWorkerMirroredStrategy`:多台机器上的同步分布式训练(可能每台机器使用多个 GPU)。这个策略类需要一个已经使用`TF_CONFIG`环境变量([https://www . tensor flow . org/guide/distributed _ training # TF _ CONFIG](https://www.tensorflow.org/guide/distributed_training#TF_CONFIG))进行了配置的 TF 集群
    *   `TPUStrategy`:在多个**张量处理单元** ( **TPUs** )上训练
*   用于`tf.keras.Model.fit`和定制训练循环的具有实验特征的策略(意味着类和方法仍处于开发阶段):
    *   `ParameterServerStrategy`:模型参数在多个工作器之间共享(集群由工作器和参数服务器组成)。在每次迭代之后，工人读取并更新在参数服务器上创建的变量。
    *   `CentralStorageStrategy`:变量存储在中央存储器中，在每个 GPU 之间复制。
*   最后要提到的策略是`tf.distribute.OneDev`[`iceStrategy`(https://www . tensor flow . org/API _ docs/python/TF/distribute/One](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy)device strategy)。它在单个 GPU 设备上运行训练代码，如下所示:

    ```
    strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
    ```

在前面的例子中，我们选择了第一个 GPU ( `"/gpu:0"`)。

另外值得一提的是，`tf.distribute.get_strategy`函数可以用来获取当前的`tf.distribute.Strategy`对象。您可以使用这个函数为您的训练代码动态地更改`tf.distribute.Strategy`对象，如下面的代码片段所示:

```

if tf.config.list_physical_devices('GPU'):

    strategy = tf.distribute.MirroredStrategy()

else:  # Use the Default Strategy

    strategy = tf.distribute.get_strategy()
```

在前面的代码中，当 GPU 设备可用时，我们使用`tf.distribute.MirroredStrategy`，当 GPU 设备不可用时，我们退回到默认策略。接下来，我们来看看 PyTorch 提供的特性。

## 利用多种设备进行 PyTorch 培训

为了成功训练 PyTorch 模型，需要为同一设备配置模型和输入张量。如果您想要使用 GPU 设备，则需要在训练之前使用`to(device=torch.device('cuda'))`或`cuda()`函数将它们明确加载到目标 GPU 设备上:

```

cpu = torch.device(cpu')

cuda = torch.device('cuda')     # Default CUDA device

cuda0 = torch.device('cuda:0')

x = torch.tensor([1., 2.], device=cuda0)

# x.device is device(type='cuda', index=0)

y = torch.tensor([1., 2.]).cuda()

# y.device is device(type='cuda', index=0)

# transfers a tensor from CPU to GPU 1

a = torch.tensor([1., 2.]).cuda()

# a.device are device(type='cuda', index=1)

# to function of a Tensor instance can be used to move the tensor to different devices

b = torch.tensor([1., 2.]).to(device=cuda)

# b.device are device(type='cuda', index=1)
```

前面的例子展示了使用 GPU 设备时应该注意的一些关键操作。这是 PyTorch 官方文档中的一个子集:https://pytorch.org/docs/stable/notes/cuda.html.

然而，为培训设置单个组件可能会令人厌倦。因此，`Trainer`的`gpus`参数:

```

# Train using CPU

Trainer()

# Specify how many GPUs to use

Trainer(gpus=k)

# Specify which GPUs to use

Trainer(gpus=[0, 1])

# To use all available GPUs put -1 or '-1'

Trainer(gpus=-1)
```

在前面的示例中，我们描述了单台计算机的各种训练设置:仅使用 CPU 设备的训练、使用一组 GPU 设备的训练以及使用所有 GPU 设备的训练。

要记住的事情

a.TF 和 PyTorch 为使用 CPU 和 GPU 设备训练模型提供了内置支持。

b.可以使用 TF 中的`tf.distribute.Strategy`类来控制训练。单机训练模型时，可以使用`MirroredStrategy`或`OneDeviceStrategy`。

c.要使用 GPU 设备训练 PyTorch 模型，模型和相关张量需要手动加载到同一个 GPU 设备上。PL 通过将位置作为`Trainer`类的一部分来处理，隐藏了大部分样板代码。

在这一部分，我们学习了如何在一台机器上使用多个设备。然而，由于单台机器的计算能力是有限的，所以人们一直在努力利用机器集群进行训练。

# 在集群上训练模型

尽管在一台机器上使用多个 GPU 已经大大减少了培训时间，但一些模型非常庞大，仍然需要多天进行培训。添加更多的 GPU 仍然是一种选择，但物理限制往往存在，阻止您充分利用多 GPU 设置的潜力:主板可以支持有限数量的 GPU 设备。

幸运的是，许多 DL 框架已经支持在分布式系统上训练模型。虽然在实际实现中有微小的差异，但是大多数框架都采用了**模型并行**和**数据并行**的思想。如下图所示，模型并行性将模型的组件分发到多台计算机上，而数据并行性则分发训练集的样本:

![Figure 6.1 – The difference between model parallelism and data parallelism
](img/B18522_06_01.jpg)

图 6.1–模型并行性和数据并行性的区别

在为模型训练设置分布式系统时，有几个细节是您必须了解的。首先，集群中的机器需要稳定地连接到互联网，因为它们通过网络进行通信。如果稳定性得不到保证，集群必须有办法从连接问题中恢复。理想情况下，分布式系统应该不知道可用的机器，并且能够在不影响整体进度的情况下添加或删除机器。这种功能将允许用户动态地增加或减少机器的数量，以最具成本效益的方式实现模型训练。AWS 通过**Elastic MapReduce**(**EMR**)和**弹性容器服务** ( **ECS** )提供上述开箱即用的功能。

在接下来的两节中，我们将深入探讨模型并行性和数据并行性。

## 模型并行性

在模型并行的情况下，分布式系统中的每台机器获取模型的一部分，并为分配的组件管理计算。当网络太大而不适合单个 GPU 时，通常会考虑这种方法。然而，这在现实中并不常见，因为 GPU 设备通常有足够的内存来适应该模型，并且设置它相当复杂。在本节中，我们将描述模型并行的两种最基本的方法:**模型分片**和**模型流水线**。

### 模型分片

模型分割只不过是将模型分割成跨越多个设备的多个计算子图。让我们假设一个基本单层**深度神经网络** ( **DNN** )模型的简单场景(没有并行路径)。该模型可以被分割成几个连续的子图，并且分片轮廓可以用图形表示如下。数据将从具有第一个子图的器件开始按顺序流动。每个设备将把计算的值传递给下一个子图的设备。在必要的数据到达之前，设备将保持空闲状态。在这个例子中，我们有四个子图:

![Figure 6.2 – A sample distribution of a model in model sharding; each arrow indicates a mini-batch
](img/B18522_06_02.jpg)

图 6.2–模型分割中模型的样本分布；每个箭头表示一个小批量

正如你所看到的，模型分片并没有利用全部的计算资源；一个设备正在等待另一个设备处理它的子图。为了解决这个问题，提出了流水线方法。

### 模型流水线

在模型流水线的情况下，一个小批量被分割成小批量，并以链的形式提供给系统，如下图所示:

![Figure 6.3 – A diagram of model pipeline logic; each arrow indicates a mini-batch
](img/B18522_06_03.jpg)

图 6.3–模型管道逻辑图；每个箭头表示一个小批量

然而，模型流水线需要反向传播的修改版本。让我们看看如何在模型流水线设置中实现单个前向和后向传播。在某种程度上，每个设备不仅需要执行其子图的正向计算，还需要执行梯度计算。单一的向前和向后传播可以这样实现:

![Figure 6.4 – A single forward and backward propagation in model pipelining
](img/B18522_06_04.jpg)

图 6.4–模型管道中的单一前向和后向传播

在前面的图中，我们可以看到每个设备依次向前传播和反向传播，将计算值传递给下一个设备。把所有的东西放在一起，我们得到下面的图表，它总结了模型流水线的逻辑:

![Figure 6.5 – Model parallelism based on model pipelining
](img/B18522_06_05.jpg)

图 6.5–基于模型流水线的模型并行性

为了进一步改善训练时间，每个设备存储其先前计算的值，并在随后的计算中使用它们。

### TensorFlow 中的模型并行

以下代码片段显示了在定义模型架构时，如何将一组层分配给 TF 中的特定设备:

```

with tf.device('GPU:0'): 

    layer1 = layers.Dense(16, input_dim=8) 

with tf.device('GPU:1'): 

    layer2 = layers.Dense(4, input_dim=16)
```

如果您想进一步探索 TF 中的模型并行性，我们建议查看 Mesh TF 资源库(https://github . com/tensor flow/Mesh)。

### PyTorch 中的模型并行性

模型并行性仅在 PyTorch 上可用，尚未在 PL 中实现。虽然使用 PyTorch 实现模型并行有很多方法，但最标准的方法是使用`torch.distributed.rpc`模块，该模块使用**远程过程调用** ( **RPC** )实现机器间的通信。基于 RPC 的方法的三个主要特征是远程触发功能或网络(远程执行)，访问和引用远程数据对象(远程引用)，以及跨机器边界扩展 PyTorch 的梯度更新功能(分布式梯度更新)。我们把细节委托给官方文件:https://pytorch.org/docs/stable/rpc.html.

## 数据并行

与模型并行不同，数据并行旨在通过将数据集分割到集群中的机器来加速训练。每台机器都获得模型的副本，并使用它所分配到的数据集计算梯度。然后，聚集梯度,并且立即全局更新模型。

### TensorFlow 中的数据并行

利用`tf.distribute.MultiWorkerMirroredStrategy`、`tf.distribute.ParameterServerStrategy`和`tf.distribute.CentralStorageStrategy`可以在 TF 中实现数据并行。

我们在 TensorFlow 部分的*中介绍了利用多台设备进行训练的策略，因为特定的`tf.distributed`策略也用于在一台机器内的多台设备上设置训练。*

要使用这些策略，您需要建立一个 TF 集群，在这个集群中，每台机器都可以相互通信。

通常，使用一个`TF_CONFIG`环境变量来定义一个 TF 集群。`TF_CONFIG`只是一个`JSON`字符串，通过定义两个组件:`cluster`和`task`来指定集群配置。以下 Python 代码显示了如何从 Python 字典为`TF_CONFIG`生成一个`.json`文件:

```

tf_config = {

    'cluster': {

        'worker': ['localhost:12345', 'localhost:23456']

    },

    'task': {'type': 'worker', 'index': 0}

}

js_tf = json.dumps(tf_config)

with open("tf_config.json", "w") as outfile:

    outfile.write(js_tf)
```

[`TF_CONFIG`字段和格式在 https://cloud.google.com](https://cloud.google.com/ai-platform/training/docs/distributed-training-details)/ai-platform/training/docs/distributed-training-details 中描述。

如*在 TensorFlow* 部分使用多种设备进行训练所演示的，您需要将训练代码放在`tf.distribute.Strategy`范围下。在下面的例子中，我们将展示一个`tf.distribute.MultiWorkerMirroredStrategy`类的使用示例。

首先，您必须将您的模型实例放在`tf.distribute.MultiWorkerMirroredStrategy`的范围内，如下面的代码片段所示:

```

strategy = tf.distribute.MultiWorkerMirroredStrategy()

with strategy.scope():

    model = … 
```

接下来，您需要确保已经为集群中的每台机器正确设置了`TF_CONFIG`环境变量，并运行训练脚本，如下所示:

```
# On the first node
TF_CONFIG='{"cluster": {"worker": ['localhost:12345', 'localhost:23456']}, "task": {"index": 0, "type": "worker"}}' python training.py
# On the second node
TF_CONFIG='{"cluster": {"worker": ['localhost:12345', 'localhost:23456']}, "task": {"index": 1, "type": "worker"}}' python training.py
```

为了正确保存你的[模型，请看一下官方文档:https://www . t](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)ensorflow.org/tutorials/distribute/multi_worker_with_keras.

在[自定义训练循环的情况下，可以按照 https://ww](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)w . tensor flow . org/tutorials/distribute/multi _ worker _ with _ CTL 上的说明进行操作。

### PyTorch 中的数据并行性

与模型并行不同，数据并行可用于 PyTorch 和 PL。在各种实现中，最标准的特性是`torch.nn.parallel.DistributedDataParallel` (DDP)。在本节中，我们将主要讨论 PL，因为它的主要优势来自于使用数据并行性的训练模型的简单性。

为了使用数据并行性训练一个模型，你需要修改训练代码以利用底层的分布式系统，并在每台机器上用`torch.distributed.run`模块([https://pytorch.org/docs/stable/distributed.html](https://pytorch.org/docs/stable/distributed.html))生成一个进程。

以下代码片段描述了您需要为 ddp 进行哪些更改。你只需为`Trainer`的`accelerator`参数提供`ddp`。`num_nodes`是集群中有多台机器时需要调整的参数:

```

# train on 8 GPUs (same machine)

trainer = Trainer(gpus=8, accelerator='ddp')

# train on 32 GPUs (4 nodes)

trainer = Trainer(gpus=8, accelerator='ddp', num_nodes=4)
```

一旦设置好脚本，您需要在每台机器上运行以下命令。请记住，`MASTER_ADDR`和`MASTER_PORT`必须一致，因为它们被每个处理器用来通信。另一方面，`NODE_RANK`表示机器的索引。换句话说，每台机器必须不同，而且必须从零开始:

```
python -m torch.distributed.run
    --nnodes=2 # number of nodes you'd like to run with
    --master_addr <MASTER_ADDR>
    --master_port <MASTER_PORT>
    --node_rank <NODE_RANK>
    train.py (--arg1 ... train script args...)
```

根据官方文档，DDP 的工作方式如下:

1.  每个节点上的每个 GPU 都会加速一个进程。
2.  每个进程获得训练集的一个子集。
3.  每个进程初始化模型。
4.  每个进程并行执行向前和向后传播。
5.  梯度在所有过程中被同步和平均。
6.  每个进程更新它所拥有的模型的权重。

要记住的事情

a.TF 和 PyTorch 提供了使用模型并行性和数据并行性在多台机器上训练 DL 模型的选项。

b.模型并行性将模型分割成多个组件，并将它们分布在不同的机器上。要在 TF 和 PyTorch 中设置模型并行性，您可以分别使用`Mesh TensorFlow`库和`torch.distributed.rpc`包。

c.数据并行将模型复制到每台机器上，并在机器间分发小批量数据用于训练。在 TF 中，可以使用`MultiWorkerMirroredStrategy`、`ParameterServerStrategy`或`CentralStorageStrategy`来实现数据并行。PyTorch 中为数据并行设计的主包叫做`torch.nn.parallel.DistributedDataParallel`。

在本节中，我们学习了如何在集群的生命周期被明确管理的情况下实现模型训练。然而，一些工具也管理用于模型训练的聚类。由于每种工具都有不同的优点，所以您应该了解它们之间的差异，以便为您的开发选择正确的工具。

首先，我们将看看 SageMaker 的内置特性，这些特性以分布式方式训练 DL 模型。

# 使用 SageMaker 训练模型

正如在 [*第 5 章*](B18522_05.xhtml#_idTextAnchor106) 、*云中的数据准备*的*利用 SageMaker 进行 ETL* 一节中提到的，SageMaker 的动机是帮助工程师和研究人员专注于开发高质量的 DL 管道，而无需担心基础设施管理。SageMaker 为您管理数据存储和计算资源，允许您以最小的努力利用分布式系统进行模型训练。此外，SageMaker 支持将数据流式传输到您的模型，用于推理、超参数调整以及跟踪实验和工件。

SageMaker Studio 是您定义模型逻辑的地方。SageMaker Studio 笔记本允许您快速浏览可用数据并设置模型训练逻辑。当模型训练花费太长时间时，通过对基础设施的配置进行一些修改，可以有效地扩大规模以使用多种计算资源并找到最佳超参数集。此外，SageMaker 支持在分布式系统上进行超参数调优，以利用并行性。

尽管 SageMaker 听起来像是 DL 管道的神奇钥匙，但它也有缺点。首先是它的成本。分配给 SageMaker 的实例比同等的 EC2 实例贵 40%左右。接下来，你可能会发现笔记本里并不是所有的库都有。换句话说，您可能需要花费一些额外的时间来构建和安装您需要的库。

## 为 SageMaker 设置模型培训

现在，您应该能够启动笔记本并为您的项目选择预定义的开发环境，因为我们已经在 [*第 5 章*](B18522_05.xhtml#_idTextAnchor106) 、*云中的数据准备*的*利用 SageMaker 进行 ETL* 部分中介绍了这些。假设您已经处理了原始数据并将处理后的数据存储在数据存储中，我们将在这一部分重点讨论模型训练。SageMaker 的模型训练可以总结为以下三个步骤:

1.  如果存储中已处理的数据尚未拆分为定型集、验证集和测试集，则必须先将其拆分。
2.  您需要定义模型定型逻辑并指定群集配置。
3.  最后，您需要训练您的模型，并将工件保存回数据存储中。训练完成后，分配的实例将自动终止。

用 SageMaker 进行模型训练的关键是`sagemaker.estimator.Estimator`。它允许您配置训练设置，包括 I[n 基础结构设置、要使用的 Docker 图像类型以及超参数](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)([https://sagemaker . readthedocs . io/en/stable/API/training/estimators . html](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html))。以下是您通常会配置的主要参数:

*   `role` ( `str`):一个 AWS IAM 角色
*   `instance_count` ( `int`):用于训练的 SageMaker EC2 实例的数量
*   `instance_type` ( `str`):用于训练的 SageMaker EC2 实例的类型
*   `volume_size`(`int`):Amazon**Elastic Block Store**(**EBS**)卷的大小(以千兆字节为单位)，将用于临时下载输入数据进行训练
*   `output_path` ( `str`):存储训练结果的 S3 对象
*   `use_spot_instances` ( `bool`):指定是否使用 SageMaker 管理的 AWS Spot 实例进行训练的标志
*   `checkpoint_s3_uri` ( `str`):一个 S3 URI，训练时将在这里存放检查点
*   `hyperparameters` ( `dict`):包含初始超参数集的字典
*   `entry_point` ( `str`):运行 Python 文件的路径
*   `dependencies` ( `list[str]`):将要加载到作业中的目录列表

只要你从亚马逊**弹性容器注册表** ( **ECR** )中选择正确的容器，你就可以为 SageMaker 设置任何训练配置。也存在为 CPU 和 GPU 设备配置不同的容器。你可以在 https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD 找到这些。

此外，还有一些开源工具包的存储库，旨在帮助亚马逊 SageMaker 上的 TF 和 PyTorch 模型培训。这些库还包含 Docker 文件，这些文件已经安装了必要的库，例如 TF、PyTo [rch 以及构建 SageMaker ima](https://github.com/aws/sagemaker-tensorflow-training-toolkit) ges 所需的其他依赖项:

*   TF[:https://github.com/aws/sagemaker-tensorflow-traini](https://github.com/aws/sagemaker-pytorch-training-toolkit)ng-工具包
*   https://github.com/aws/sagemaker-pytorch-training-toolkit

最后，我们想提一下，您可以在本地机器上构建和运行容器。如果需要，还可以更新已安装的库。如果做了任何修改，需要将修改后的容器上传到 Amazon ECR，然后才能和`sagemaker.estimator.Estimator`一起使用。

在下面的两个部分中，我们将描述训练 TF 和 PyTorch 模型所需的一组变化。

## 使用 SageMaker 训练 TensorFlow 模型

SageMaker 提供了一个为 TF 构建的`sagemaker.estimator.Estimator`类:`sagemaker.tensorflow.estimator.TensorFlow`([https://SageMaker . readthe docs . io/en/stable/frameworks/tensor flow/SageMaker . tensor flow . html](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html))。

下面的例子展示了使用`sagemaker.tensorflow.estimator.TensorFlow`类在 SageMaker 上训练一个 TF 模型需要编写的包装器脚本:

```

import sagemaker

from sagemaker.tensorflow import TensorFlow

# Initializes SageMaker session

sagemaker_session = sagemaker.Session()

bucket = 's3://dataset/'

tf_estimator = TensorFlow(entry_point='training_script.py', 

              source_dir='.',

              role=sagemaker.get_execution_role(),

              instance_count=1, 

              instance_type='ml.c5.18xlarge',

              framework_version=tf_version, 

              py_version='py3',

              script_mode=True,

              hyperparameters={'epochs': 30} )
```

请记住在`hyperparameters`参数中的每个键必须有一个在训练脚本`train_script.py`的`ArgumentParser`中定义的对应条目。在前面的例子中，我们只定义了时期(`'epochs': 30`)。

要触发训练，需要调用`fit`函数。您需要为训练和验证提供数据集。如果您将它们放在 S3 桶上，`fit`函数将如下所示:

```

tf_estimator.fit({'training': 's3://bucket/training',

                  'validation': 's3://bucket/validation'})   
```

前面的例子将运行在`entry_point`参数中指定的`training_script.py`，在`source_dir`提供的目录中找到它。实例的细节可以在`instance_count`和`instance_type`参数中找到。训练脚本将使用在`fit`函数中定义的训练和验证数据集上为`tf_estimator`的`hyperparameters`定义的参数运行。

## 使用 SageMaker 训练 PyTorch 模型

与`sagemaker.tensorflow.estimator.TensorFlow`类似的[，还有`sagemaker.pytorch.PyTorch` (](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html) )。您可以为您的 PyTorch(或 PL)模型设置训练，如*云中的数据准备*的*实现和训练 PyTorch 中的模型*一节中的所述，并集成`sagemaker.pytorch.PyTorch`，如下面的代码片段所示:

```

import sagemaker

from sagemaker.pytorch import PyTorch

# Initializes SageMaker session

sagemaker_session = sagemaker.Session()

bucket = 's3://dataset/'

pytorch_estimator = PyTorch(

                      entry_point='train.py',

                      source_dir='.',

                      role=sagemaker.get_execution_role(),

                      framework_version='1.10.0',

                      train_instance_count=1,

                      train_instance_type='ml.c5.18xlarge',

                      hyperparameters={'epochs': 6})

…

pytorch_estimator.fit({

                        'training': bucket+'/training',

                        'validation': bucket+'/validation'})   
```

PyTorch 估计量的使用与上一节描述的 TF 估计量相同。

这就是 SageMaker 用于模型训练的基本用法。接下来，我们将学习如何扩大 SageMaker 的培训工作。我们将讨论使用分布式策略的分布式培训。我们还将介绍如何利用其他延迟更低的数据存储服务来加快培训速度。

## 使用 SageMaker 以分布式方式训练模型

SageMaker 中的 Da [ta 并行可以使用分布式数据并行](https://sagemaker.readthedocs.io/en/stable/api/training/smd_data_parallel.html) l 库(https://SageMaker . readthe docs . io/en/stable/API/training/SMD _ data _ parallel . html)来实现。

您需要做的就是在创建`sagemaker.estimator.Estimator`实例时启用`dataparallel`，如下所示:

```

distribution = {"smdistributed": {"dataparallel": { "enabled": True}} 
```

下面的代码片段 sho [是一个用`dataparallel`创建的 TF 估算器。完整的细节可以在 https://docs . AWS . Amazon . com/en _ jp/sagemaker/latest/DG/data-parallel-use-API . html 找到:](https://docs.aws.amazon.com/en_jp/sagemaker/latest/dg/data-parallel-use-api.html)

```

tf_estimator = TensorFlow(

                 entry_point='training_script.py', 

                 source_dir='.',

                 role=sagemaker.get_execution_role(),

                 instance_count=4, 

                 instance_type='ml.c5.18xlarge',

                 framework_version=tf_version, 

                 py_version='py3',

                 script_mode=True,

                 hyperparameters={'epochs': 30}

                 distributions={'smdistributed':

                 "dataparallel": {"enabled": True}})
```

PyTorch 估计器也需要同样的修改。

SageMaker 支持两种不同的机制来将输入数据传输到底层算法:文件模式和管道模式。默认情况下，SageMaker 使用文件模式，将输入数据下载到 EBS 卷进行训练。但是，如果数据量很大，这会降低训练速度。在这种情况下，您可以使用管道模式，该模式从 S3(使用 Linux FIFO)传输数据，而无需制作额外的副本。

在 TF 的[情况下，你可以简单地从`sagemaker-tensorflow`扩展(https://github.com/aws/sagemaker-tensorflow-extensions)中使用`PipeModeDataset` fr](https://github.com/aws/sagemaker-tensorflow-extensions) ，如下所示:

```

from sagemaker_tensorflow import PipeModeDataset

ds = PipeModeDataset(channel='training', record_format='TFRecord') 
```

然而，使用管道模式训练 PyTorch 模型需要更多的工程努力。因此，我们将为您提供一个深入描述每个步骤的笔记本示例:https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced _ functional/pipe _ bring _ your _ own/pipe _ bring _ your _ own . ipynb

分布式策略和管道模式应该通过扩大底层计算资源和增加数据传输吞吐量来加速训练。然而，如果它们还不够，你可以尝试利用另外两个与 SageMaker 兼容的更高效的数据存储服务:亚马逊**弹性文件系统** ( **EFS** )和亚马逊**完全托管共享存储** ( **FSx** )，后者[是为 Lustre f](https://aws.amazon.com/efs/) ilesy [stem 构建的。更多细节，你可以分别参考他们在 https://aws.amazon.com/efs/和 https://aws.amazon.com/fsx/lustre/,的官方网页。](https://aws.amazon.com/fsx/lustre/)

## 带 Horovod 的 SageMaker

SageMaker 分布式培训的另一个选择是使用 *Horovod，*一个基于**消息传递接口** ( **MPI** )原则的免费开源分布式 DL 培训框架。MPI 是一个标准的消息传递库，广泛应用于并行计算架构中。Horovod 假设 MPI 可用于工人发现和缩减协调。Horovod 还可以利用 Gloo 代替 MPI，这是一个开源的集体通信库。以下是为 Horovod 配置的分布参数示例:

```

distribution={"mpi": {"enabled":True, 

                        "processes_per_host":2 }}
```

在前面的代码片段中，我们使用 MPI 实现了机器之间的协调。`processes_per_host`定义在每个实例上运行的进程数量。这相当于在`mpirun`或`horovodrun`命令中使用`-H`参数来定义进程的数量，这两个命令分别控制程序在 MPI 和 Horovod 中的执行。

在下面的代码片段中，我们选择了控制训练脚本执行数量的并行进程的数量(`-np`参数)。然后，使用`-H`参数的指定值将该数字分割到特定的机器中。使用以下命令，每台机器将运行`train.py`两次。当您有四台机器，每台机器有两个 GPU 时，这将是一个典型的设置。分配的`-H`进程的总和不能超过`-np`值:

```
mpirun -np 8 -H server1:2,server2:2,server3:2,server4:2 … (other parameters) python train.py  
```

我们将在下一节深入讨论 Horovod，因为我们将介绍如何在由 EC2 实例组成的独立 Horovod 集群上训练 DL 模型。

要记住的事情

a.SageMaker 提供了一个优秀的工具，SageMaker Studio，它允许您快速执行初始数据探索和训练基线模型。

b.`sagemaker.estimator.Estimator`对象是使用 SageMaker 训练模型的重要组件。它还支持在一组具有各种 CPU 和 GPU 配置的机器上进行分布式培训。

c.利用 SageMaker 进行 TF 和 PyTorch 模型训练可以实现为每个框架专门设计的估计器。

现在，让我们看看如何在没有 SageMaker 的情况下使用 Horovod 进行分布式模型训练。

# 使用 Horovod 训练模型

即使我们像介绍 SageMaker 一样介绍了 Horovod，Horovod 也被设计为单独支持分布式训练(https://horovod.ai/)。它旨在通过为流行的 DL 框架(包括 TensorFlow 和 PyTorch)提供良好的集成，提供一种以分布式方式训练模型的简单方法。

正如前面在 *SageMaker with Horovod* 一节中提到的，Horovod 的核心原则基于 MPI 概念，如大小、等级、本地 ra nk、allreduce、allgather、broadcast 和 all toall(https://horo VOD . readthedocs . io/en/stable/concepts . html)。

在本节中，我们将了解如何使用 EC2 实例来设置 Horovod 集群。然后，我们将描述您需要在 TF 和 PyTorch 脚本中进行的修改，以便在 Horovod 集群上训练您的模型。

## 建立一个 Horovod 集群

要使用 EC2 实例设置 Horovod 集群,您必须遵循以下步骤:

1.  去 https://console.aws.amazon.com/ec2/.的 EC2 实例控制台
2.  点击右上角的**启动实例**按钮。
3.  选择安装了 TF、PyTorch、Horovod 的**深度学习 AMI** (亚马逊机器映像的缩写)。点击右下角的**下一步…** 按钮。
4.  为您的培训选择正确的**实例类型**。您可以选择符合您需求的 CPU 或 GPU 实例类型。点击右下角的**下一步…** 按钮:

![Figure 6.6 – Instance type selection in the EC2 Instance console
](img/B18522_06_06.jpg)

图 6.6–EC2 实例控制台中的实例类型选择

1.  选择组成 Horovod 集群的所需实例数量。在这里，您还可以请求 AWS Spot 实例(基于可以中断的稀疏 EC2 容量的更便宜的实例，使它们只适用于容错任务)。然而，为了简单起见，让我们使用按需资源。
2.  选择正确的网络和子网设置。在现实生活中，这类信息将由 DevOps 部门提供。
3.  在同一个页面上，选择**将实例添加到放置组**和**添加到新的放置组**，键入要用于该组的名称，并为**放置组策略**选择**集群**。
4.  在同一页面上，提供您的**身份和访问管理** ( **IAM** )角色，以便您可以访问 S3 存储桶。点击右下角的**下一步…** 按钮。
5.  为您的实例选择合适的存储大小。点击右下角的**下一步……**按钮。
6.  为您的实例选择唯一的标签/标记(https://docs . AWS . Amazon . com/general/latest/gr/AWS _ tagging . html)。在现实生活中，这些可能被用作额外的安全措施，比如用特定的标签终止实例。点击右下角的**下一步…** 按钮。
7.  创建一个安全组或选择一个现有的安全组。同样，您必须与 DevOps 部门联系以获得适当的信息。点击右下角的**下一步…** 按钮。
8.  查看所有信息并启动。您将被要求提供一个用于认证的**隐私增强邮件** ( **PEM** )密钥。

完成这些步骤后，所需数量的实例将启动。如果您没有在*步骤 10* 中添加**名称**标签，您的实例将没有任何名称。在这种情况下，您可以导航到 EC2 实例控制台并手动更新名称。在编写本文时，您可以请求名为 Elastic IPs 的静态 IPv4 地址，并将其分配给实例(https://docs . AWS . Amazon . com/AWS ec2/latest/user guide/Elastic-IP-addresses-EIP . html)。

最后，您需要确保实例可以顺利地相互通信。您应该检查**安全组**设置，并在必要时为 SSH 和其他流量添加入站规则。

此时，您只需要将您的 PEM 密钥从本地机器复制到主 EC2 实例。对于 Ubuntu AMI，您可以运行以下命令:

```
scp -i <your_pem_key_path> ubuntu@<IPv4_Public_IP>:/home/ubuntu/.ssh/ 
```

现在，您可以使用 SSH 连接到主 EC2 实例。接下来您需要做的是通过使用以下命令在 SSH 命令中提供您的 PEM 密钥来设置 EC2 实例之间的无密码连接:

```
eval 'ssh-agent'
ssh-add <your_pem_key>
```

在前面的代码片段中，`eval`命令设置由`ssh-agent`命令提供的环境变量，而`ssh-add`命令向身份验证代理添加 PEM 身份。

现在，集群已经准备好支持 Horovod 了！完成后，必须在 web 控制台上停止或终止集群。否则，它会不断向你收取资源费用。

在接下来的两节中，我们将学习如何为 Horovod 更改 TF 和 PyTorch 训练脚本。

## 为 Horovod 配置 TensorFlow 培训脚本

使用 Horovod 训练一个 TF 模型，你需要`horovod.tensorflow.keras`模块。首先，你需要导入`tensorflow`和`horovod.tensorflow.keras`模块。我们将把`horovod.tensorflow.keras`称为`hvd`。然后，您需要初始化 Horovod 集群，如下所示:

```

import tensorflow as tf

import horovod.tensorflow.keras as hvd

# Initialize Horovod

hvd.init()
```

此时，您可以使用`hvd.size`功能检查集群的大小。Horovod 中的每个进程将被分配一个等级(根据您想要运行的进程或想要使用的设备，从 0 到集群大小的一个数字)，您可以通过`hvd.rank`函数访问该等级。在每个实例上，每个进程都有一个不同的编号，从 0 到该实例上的进程数，称为本地等级(每个实例的编号是唯一的，但在实例之间是重复的)。使用`hvd.local_rank`函数可以访问当前进程的本地等级。

可以使用 local rank 为每个进程固定一个特定的 GPU 设备，如下所示。这个例子还展示了如何使用`tf.config.experimental.set_memory_growth`为您的 GPU 设置内存增长:

```

gpus = tf.config.experimental.list_physical_devices('GPU')

for gpu in gpus:

    tf.config.experimental.set_memory_growth(gpu, True)

if gpus:

    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
```

在下面的代码中，我们根据等级分割数据，以便每个流程训练不同的示例集:

```

dataset = np.array_split(dataset, hvd.size())[hvd.rank()]
```

对于模型架构，可以按照第三章[](B18522_03.xhtml#_idTextAnchor062)*、*开发强大的深度学习模型*中*tensor flow*部分的说明进行:*

```

model = …
```

接下来，您需要配置优化器。在下面的例子中，学习率是由 Horovod 的大小决定的。此外，优化器需要用 Horovod 优化器包装:

```

opt = tf.optimizers.Adam(0.001 * hvd.size())

opt = hvd.DistributedOptimizer(opt)
```

下一步是编译您的模型，并将网络架构定义和优化器放在一起。当您使用比 v2.2 更早的 TF 版本调用`compile`函数时，您需要禁用`experimental_run_tf_function`，以便 TF 使用`hvd.DistributedOptimizer`来计算梯度:

```

model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),

              optimizer=opt,

              metrics=['accuracy'],

              experimental_run_tf_function=False)
```

您需要配置的另一个组件是回调函数。你需要加上`hvd.callbacks.BroadcastGlobalVariablesCallback(0)`。这将把权重的初始值和从等级 0 到所有其他机器和进程的偏差。这是确保一致初始化或从检查点正确恢复训练所必需的:

```

callbacks=[

    hvd.callbacks.BroadcastGlobalVariablesCallback(0)

]
```

您可以使用`rank`在特定的实例上执行特定的操作。例如，在主节点上记录和保存工件可以通过检查`rank`是否为 0 ( `hvd.rank()==0`)来实现，如下面的代码片段所示:

```

# Save checkpoints only on the instance with rank 0 to prevent other workers from corrupting them.

If hvd.rank()==0:

    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))
```

现在，您已经准备好触发`fit`功能。以下示例显示了如何使用 Horovod 集群的大小来缩放每个历元的步数。来自`fit`功能的消息仅在主节点上可见:

```

if hvd.rank()==0:

    ver = 1

else:

    ver = 0

model.fit(dataset,

          steps_per_epoch=hvd.size(),

          callbacks=callbacks,

          epochs=num_epochs,

          verbose=ver)
```

这就是使用 Horovod 以分布式方式训练一个 TF 模型所需要做的全部改变。你可以在 https://horo VOD . readthedocs.io/en/stable/tensorflow.html.找到完整的示例。Keras 版本可以在 https://horo VOD . rea[dthedocs.io/en/stable/keras.html.找到。此外，你可以修改你的训练脚本](https://horovod.readthedocs.io/en/stable/elastic_include.html)以便它以容错方式运行:https://horo VOD . readthedocs . io/en/stable/elastic _ include . html。通过这一更改，你应该能够使用 AWS Spot 实例并显著降低成本

## 为 Horovod 配置 PyTorch 培训脚本

不幸的是，PL 还没有合适的文档来支持 Horovod。因此，本节我们将重点介绍 PyTorch。类似于我们在上一节中所描述的，我们将演示您需要为 PyTorch 培训脚本所做的代码更改。对于 PyTorch，您需要`horovod.torch`模块，我们将再次将其称为`hvd`。在下面的代码片段中，我们导入必要的模块并初始化集群:

```

import torch

import horovod.torch as hvd

# Initialize Horovod

hvd.init()
```

如 TF 示例中所述，您需要使用本地秩为当前进程绑定一个 GPU 设备:

```

torch.cuda.set_device(hvd.local_rank())
```

培训脚本的其他部分需要类似的修改。数据集需要使用`torch.utils.data.distributed.DistributedSampler`跨实例分布，优化器必须围绕`hvd.DistributedOptimizer`包装。主要的区别来自于`hvd.broadcast_parameters(model.state_dict(), root_rank=0)`，它广播模型权重。你可以在下面的代码片段中找到细节:

```

# Define dataset...

train_dataset = ...

# Partition dataset among workers using DistributedSampler

train_sampler = torch.utils.data.distributed.DistributedSampler(

    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)

# Build model...

model = ...

model.cuda()

optimizer = optim.SGD(model.parameters())

# Add Horovod Distributed Optimizer

optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())

# Broadcast parameters from rank 0 to all other processes.

hvd.broadcast_parameters(model.state_dict(), root_rank=0)
```

现在，您已经准备好训练模型了。训练循环不需要任何修改。您只需将输入张量传递给模型，并通过触发`loss`上的`backward`函数和`optimizer`的`step`函数来触发反向传播。下面的代码片段描述了训练逻辑的主要部分:

```

for epoch in range(num_ephos):

   for batch_idx, (data, target) in enumerate(train_loader):

       optimizer.zero_grad()

       output = model(data)

       loss = F.nll_loss(output, target)

 loss.backward()

 optimizer.step()
```

c 的完整描述可以在官方的 Horovod 文档页面上找到:https://horovod.readthedocs.io/en/stable/pytorch.html.

作为使用 Horovod 部分的*训练模型的最后一个部分，下一个部分将解释如何使用`horovodrun`和`mpirun`命令启动模型训练过程。*

## 在 Horovod 集群上训练 DL 模型

Horovod 使用 MPI 原理来协调进程之间的工作。要在一台机器上运行四个进程，可以使用以下命令之一:

```
horovodrun -np 4 -H localhost:4 python train.py
mpirun -np 4 python train.py
```

在这两种情况下，`-np`参数定义了`train.py`脚本并行运行的次数。`-H`参数可用于定义每台机器的加工数量(见上例中的`horovodrun`命令)。当我们学习如何在单台机器上运行时，`-H`可以被删除，如`mpirun`命令所示。其他`mpirun`参数在 https://www.open-mpi.org/doc/v4.0/man1/mpirun.1.php#sect6.描述

如果没有安装 MPI，可以使用 Gloo 运行`horovodrun`命令。要使用 Gloo 对`localhost`运行相同的脚本四次(四个进程),只需添加`--gloo`标志:

```
horovodrun --gloo -np 4 -H localhost:4 python train.py
```

扩展到多个实例非常简单。以下命令显示了如何使用`horovodrun`在四台机器上运行训练脚本:

```
horovodrun -np 4 -H server1:1,server2:1,server3:1,server4:1 python train.py 
```

以下命令显示了如何使用`mpirun`在四台机器上运行训练脚本:

```
mpirun -np 4 -H server1:1,server2:1,server3:1,server4:1 python train.py
```

一旦前面的命令之一从主节点被触发，您将会看到每个实例运行一个用于训练的进程。

要记住的事情

a.要使用 Horovod，您需要一个在节点间开放交叉通信的集群。

b.Horovod 为 TF 和 PyTorch 提供了一种简单有效的实现数据并行的方法。

c.可以使用`horovodrun`或`mpirun`命令在 Horovod 集群上执行训练脚本。

在下一节中，我们将描述 Ray，另一个流行的分布式训练框架。

# 使用光线训练模型

Ray 是一个开源的执行框架，用于跨机器扩展 Python 工作负载(https://www.ray.io)。Ray 支持以下 Python 工作负载:

*   用 PyTorch 或 TF 实现的 DL 模型训练
*   超参数通过射线调优进行调优(https://docs.ray.io/en/latest/tune/index.html)
*   **强化学习** ( **RL** )通过 RL lib(https://docs . ray . io/en/latest/RL lib/index . html)，RL 的开源库
*   数据处理利用光线数据集(【https://docs.ray.io/en/latest/data/dataset.html】T21)
*   模特发球通过雷发球[https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html)
*   利用 Ray Core 的通用 Python 应用程序([https://docs.ray.io/en/latest/ray-core/walkthrough.html](https://docs.ray.io/en/latest/ray-core/walkthrough.html))

Ray 的关键优势来自其集群定义的简单性；您可以用不同类型和不同来源的机器定义一个集群。例如，Ray 允许您通过混合使用 AWS EC2 按需实例和具有不同 CPU 和 GPU 配置的 EC2 Spot 实例来构建实例群(基于各种 EC2 实例的集群，每个节点具有灵活和弹性的资源分配策略)。Ray 简化了集群创建和与 DL 框架的集成，使其成为分布式 DL 模型训练过程的有效工具。

首先，我们将学习如何建立一个光线簇。

## 设置光线簇

你可以用两种方法设置光线簇:

*   **Ray Cluster Launcher**:Ray 提供的工具，帮助使用云服务上的实例构建集群，包括 AWS、GCP 和 Azure
*   **手动构建簇**:所有节点需要手动连接到光线簇

光线簇由头节点(主节点)和工作节点组成。构成集群的实例应该配置为通过网络相互通信。Ray 实例之间的通信是基于**传输控制协议** ( **TCP** )连接的，您必须打开相应的端口。在接下来的两节中，我们将带仔细看看射线集群发射器和手动集群构造。

### 使用射线簇发射器设置射线簇

使用射线集群发射器时，YAML 文件用于[配置集群。你可以](https://github.com/ray-project/ray/tree/master/python/ray/autoscaler)在 Ray 的 GitHub 资源库上找到很多不同配置的示例`YAML`文件:[https://GitHub . com/Ray-project/Ray/tree/master/python/Ray/auto scaler](https://github.com/ray-project/ray/tree/master/python/ray/autoscaler)。

本节我们将介绍最基础的一个。`YAML`文件以关于集群的一些基本信息开始，例如集群的名称、最大工作线程数和扩展速度，如下所示:

```

cluster_name: BookDL

max_workers: 5

upscaling_speed: 1.0
```

接下来，它配置云服务提供商:

```

provider:

    type: aws

    region: us-east-1

    availability_zone: us-east-1c, us-east-1b, us-east-1a

    cache_stopped_nodes: True 

    ssh_user: ubuntu

    ssh_private_key: /Users/BookDL/.ssh/BookDL.pem
```

在前面的例子中，我们指定了提供者类型(`type: aws`)并选择了将提供实例的地区和可用性区域(`region: us-east-1`和`availability_zone: us-east-1c, us-east-1b, us-east-1a`)。然后，我们定义节点在未来是否可以重用(`cache_stopped_nodes: True`)。最后的配置用于用户认证(`ssh_user:ubuntu`和`ssh_private_key:/Users/BookDL/.ssh/BookDL.pem`)。

接下来，需要指定节点配置。首先，我们从头部节点开始:

```

available_node_types:

    ray.head.default:

        node_config:

            KeyName:"BookDL.pem"
```

接下来，我们必须设置安全设置。详细设置必须咨询 DevOps，devo PS 负责监控和保护实例:

```

            SecurityGroupIds:

                - sg-XXXXX

                - sg-XXXXX

            SubnetIds: [subnet-XXXXX]
```

以下配置适用于应该使用的实例类型和 AMI:

```

            InstanceType: m5.8xlarge

            ImageId: ami-09ac68f361e5f4a13
```

在以下代码片段中，我们提供了存储配置:

```

            BlockDeviceMappings:

                  - DeviceName: /dev/sda1

                    Ebs:

                    VolumeSize: 580
```

你可以很容易地定义`Tags`如下:

```

            TagSpecifications:

                - ResourceType:"instance"

                  Tags:

                      - Key:"Developer"

                        Value:"BookDL"
```

如果需要，可以提供 IAM 例程配置文件来访问特定的 S3 时段:

```

            IamInstanceProfile:

                Arn:arn:aws:iam::XXXXX
```

在`YAML`文件的下一部分，我们需要为工作者节点提供一个配置:

```

    ray.worker.default:

            min_workers: 2

            max_workers: 4
```

首先，我们必须指定工人的数量(`min_workers`和`max_workers`)。然后，我们可以像定义主节点配置一样定义节点配置:

```

        node_config:

            KeyName: "BookDL.pem"

            SecurityGroupIds:

                - sg-XXXXX

                - sg-XXXXX

            SubnetIds: [subnet-XXXXX]

            InstanceType: p2.8xlarge

            ImageId: ami-09ac68f361e5f4a13

            TagSpecifications:

                - ResourceType: "instance"

                  Tags:

                      - Key: "Developer"

                        Value: "BookDL"

            IamInstanceProfile:

                Arn: arn:aws:iam::XXXXX

            BlockDeviceMappings:

              - DeviceName: /dev/sda1

                Ebs:

                  VolumeSize: 120
```

此外，您可以在`YAML`文件中指定在每个节点上运行的 shell 命令列表:

```

setup_commands:

    - (stat $HOME/anaconda3/envs/tensorflow2_p38/ &> /dev/null && echo 'export PATH="$HOME/anaconda3/envs/tensorflow2_p38/bin:$PATH"' >> ~/.bashrc) || true

    - source activate tensorflow2_p38 && pip install --upgrade pip

    - pip install awscli

    - pip install Cython

    - pip install -U ray

    - pip install -U ray[rllib] ray[tune] ray

    - pip install mlflow

    - pip install dvc
```

在这个例子中，我们将为`conda`环境添加`tensorflow2_p38`到路径中，激活环境，并使用`pip`安装一些其他模块。如果您想只在 head 或 worker 节点上运行一些其他命令，您可以分别在`head_setup_commands`和`worker_setup_commands`中指定它们。它们将在执行完`setup_commands`中定义的命令后执行。

最后，`YAML`文件以启动光线簇的命令结束:

```

head_start_ray_commands:

    - ray stop

    - source activate tensorflow2_p38 && ray stop

    - ulimit -n 65536; source activate tensorflow2_p38 && ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

worker_start_ray_commands:      

    - ray stop

    - source activate tensorflow2_p38 && ray stop

    - ulimit -n 65536; source activate tensorflow2_p38 && ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
```

起初，用一个`YAML`文件设置光线簇可能看起来很复杂。但是，一旦您习惯了，您会注意到为将来的项目调整集群设置变得相当简单。此外，由于您可以重用以前项目中有关安全组、子网、标记和 IAM 配置文件的信息，因此它大大减少了正确定义集群所需的时间。

如果你需要其他细节，我们建议你花点时间看看官方文档:https://docs . ray . io/en/latest/cluster/config . html # cluster-config。

值得一提的是，Ray Cluster Launcher 既支持自动缩放，也支持使用带有或不带有 EC2 Spot 实例的实例车队。我们在前面的例子中使用了 AMI，但是您也可以为您的实例提供一个特定的 Docker 映像。通过利用 YAML 配置文件的灵活性，您可以使用单个文件构建任何集群配置。

正如我们在本节开始时提到的，您也可以通过手动添加单个实例来设置光线簇。我们接下来将研究这个选项。

### 手动设置光线簇

假设您有一组带有网络连接的机器，第一步是在每台机器上安装 Ray。接下来，您需要更改每台机器的安全设置，以便机器可以相互通信。之后，您需要选择一个节点作为头节点，并运行以下命令:

```
ray start --head --redis-port=6379  
```

前面的命令建立光线簇；Redis 服务器(用于集中控制平面)启动，其 IP 地址打印在终端上(例如，`123.45.67.89:6379`)。

接下来，您需要在所有其他节点上运行以下命令:

```
ray start --address=<redis server ip address>
```

您需要提供的地址是 head 节点上的命令输出的地址。

现在，您的机器可以支持 Ray 应用程序了。在手动设置的情况下，需要手动完成以下步骤:启动机器、连接到头节点终端、将训练文件复制到所有节点以及停止机器。让我们看看如何利用 Ray Cluster Launcher 来帮助完成这些任务。

在这个阶段，您应该能够使用 YAML 文件指定所需的光线簇设置。准备就绪后，可以使用以下命令启动第一个光线簇:

```
ray up your_cluster_setting_file.yaml
```

要获得头节点上的远程终端，您可以运行以下命令:

```
ray attach your_cluster_setting_file.yaml
```

要终止集群，可以使用以下命令:

```
ray down your_cluster_setting_file.yaml
```

现在，是时候学习如何在光线簇上执行 DL 模型训练了。

## 使用光线以分布式方式训练模型

Ray 提供了 Ray Train 库，这个库允许你通过在幕后处理分布式训练来专注于定义训练逻辑。射线列车支持 TF 和 PyTorch。它还提供了与 Horovod 的简单集成。此外，光线数据集的存在，通过分布式数据转换提供了分布式数据加载。最后，Ray 通过 Ray Tune 库提供超参数调整。

为 Ray 调整 TF 训练逻辑类似于我们在 TensorFlow 部分的*数据并行性中描述的内容。主要的区别来自于光线训练库，它帮助我们设置`TF_CONFIG`。*

调整后的训练逻辑如下所示:

```

def train_func_distributed():

    per_worker_batch_size = 64

    tf_config = json.loads(os.environ['TF_CONFIG'])

    num_workers = len(tf_config['cluster']['worker'])

    strategy = tf.distribute.MultiWorkerMirroredStrategy()

    global_batch_size = per_worker_batch_size * num_workers

    multi_worker_dataset = dataset(global_batch_size)

    with strategy.scope():

        multi_worker_model = build_and_compile_your_model()

    multi_worker_model.fit(multi_worker_dataset, epochs=20, steps_per_epoch=50)
```

然后，您可以使用 Ray Trainer 运行训练，如下所示:

```

import ray

from ray.train import Trainer

ray.init()

trainer = Trainer(backend="tensorflow", num_workers=4, use_gpu=True)

trainer.start()

trainer.run(train_func_distributed)

trainer.shutdown()
```

在前面的例子中，模型定义类似于单个设备的情况，除了它应该使用特定的策略`MultiWorkerMirroredStrategy`进行编译。数据集在`dataset`函数中被分割，为每个工作者节点提供不同的样本集。最后，`Trainer` [实例处理分布式训练。](https://docs.ray.io/en/latest/train/examples.html)

使用 Ray 的 Trai ning PyTorch 模型也可以通过一组最小的修改来实现。在[https://docs.ray.io/en/latest/train/examples.html#pytorch](https://docs.ray.io/en/latest/train/examples.html#pytorch)上展示了几个例子。

此外，您可以将 Ray 与 Horovod 一起使用，这样您可以利用弹性 Horovod 以容错的方式进行训练。Ray [将通过简化主机的发现和 orc](https://docs.ray.io/en/latest/train/examples/horovod/horovod_example.html) 记录来自动调整培训流程。我们不会涉及细节，但可以在[https://docs . ray . io/en/latest/train/examples/horo VOD/horo VOD _ example . html](https://docs.ray.io/en/latest/train/examples/horovod/horovod_example.html)找到一个很好的起点。

要记住的事情

a.Ray 的主要优势来自于它对集群定义的简单性。

b.通过连接每台机器或使用一个名为射线簇发射器的内置工具，可以手动创建一个射线簇。

c.Ray 为自动缩放训练过程提供了很好的支持。它简化了主机的发现和编排。

最后，我们来学习如何使用 Kubeflow 进行分布式训练。

# 使用 Kubeflow 训练模型

kube flow([https://www.kubeflow.org](https://www.kubeflow.org))涵盖了模型开发的每一个步骤，包括数据探索、预处理、特征提取、模型训练、模型服务、推理和版本化。Kubeflow 允许您通过利用容器和 Kubernetes(一个用于容器化应用程序的管理系统)轻松地从本地开发环境扩展到生产集群。

如果您的组织已经在使用 Kubernetes 生态系统，Kubeflow 可能是您分布式培训的首选。

## 【Kubernetes 简介

Kubernetes 是一个开源编排平台，用于管理容器化的工作负载和服务( [https://kubernetes.io](https://kubernetes.io) ):

*   Kubernetes 有助于持续的交付、集成和部署。
*   它将开发环境与部署环境分开。您可以构造一个容器映像并并行开发应用程序。
*   基于容器的方法确保了开发、测试以及生产环境的一致性。环境将在桌面计算机或云中保持一致，这将最大限度地减少从一个步骤到另一个步骤所需的修改。

我们假设您已经安装了 Kubeflow 及其所有依赖项，以及一个正在运行的 Kubernetes 集群。我们将在本节中描述的步骤足够通用，它们可以用于任何集群设置—**Minikube**(Kubernetes 的本地版本)、AWS **Elastic Kubernetes 服务** ( **EKS** )或许多节点的集群。这就是容器化工作负载和服务的美妙之处。在[https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)可以在线找到本地 Minikube 安装步骤。对于 EKS，我们将引导您查看 AWS 用户指南:[https://docs . AWS . Amazon . com/eks/latest/user guide/getting-started . html](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html)。

## 为 Kubeflow 设置模型训练

第一步是将你的[训练代码打包到一个容器中。这可以通过 Docker 文件来实现。根据您的起点 g 点，您可以使用 NVIDIA 容器映像空间的容器(位于 HTT](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html)PS://docs . NVIDIA . com/deep learning/frameworks/tensor[flow-release-notes/running . html 或位于 https://docs . NV](https://hub.docker.com/r/tensorflow/tensorflow/)[idia.com/deeplearning/frameworks/pytorch](https://hub.docker.com/r/pytorch/pytorch)-release-notes/index . html 的 PyTorch)或直接来自 DL 框架的容器(位于 https://hub.docker.com/r/tensorflow/tensorflow 的 TF 或位于 https://hub.docker 的 PyTorch)

让我们来看一个 TF docker 文件(`kubeflow/tf_example_job`)的例子:

```

FROM tensorflow/tensorflow:latest-gpu-jupyter

RUN pip install minio –upgrade

RUN pip install –upgrade pip 

RUN pip install pandas –upgrade 

… 

RUN mkdir -p /opt/kubeflow

COPY train.py /opt/kubeflow

ENTRYPOINT ["python", "/opt/kubeflow/train.py"]
```

在前面的 Docker 定义中，`train.py`脚本是一个典型的 TF 训练脚本。

现在，我们假设一台机器将用于训练。换句话说，这将是一个单一的集装箱作业。假设您准备了一个 Docker 文件和一个训练脚本，您可以使用以下命令构建您的容器并将其推送到存储库:

```
docker build -t kubeflow/tf_example_job:1.0
docker push kubeflow/tf_example_job:1.0
```

我们将使用`TFJob`，Kubeflow 的一个定制组件，它包含一个被表示为的 YAML 文件的`TFJob`，该文件描述了容器映像、训练脚本和执行参数。让我们来看一个 YAML 文件，`tf_example_job.yaml`，它包含一个运行在单台机器上的 Kubeflow 模型训练作业:

```

apiVersion: "kubeflow.org/v1" 

kind: "TFJob"

metadata:

    name: "tf_example_job"

spec:

    tfReplicaSpecs:

        Worker:

            replicas: 1

        restartPolicy: Never

        template:

            specs:

                containers:

                    - name: tensorflow 

                      image: kubeflow/tf_example_job:1.0
```

API 版本在第一行中定义。然后，会列出您的自定义资源的类型，`kind: "TFJob"`。`metadata`字段用于通过自定义名称来识别您的作业。该集群在`tfReplicaSpecs`字段中定义。如前面的例子所示，脚本(`tf_example_job:1.0)`将只执行一次(`replicas: 1`)。

要将定义的`TFJob`部署到您的集群，您可以使用`kubectl`命令，如下所示:

```
kubectl apply -f tf_example_job.yaml
```

您可以使用以下命令监视作业(使用元数据中定义的名称):

```
kubectl describe tfjob tf_example_job 
```

要执行分布式训练，可以使用带有特定`tf.distribute.Strategy`的 TF 代码，创建一个新的容器，并修改`TFJob`。我们将在下一节课中了解`TFJob`的必要变化。

## 使用 Kubeflow 以分布式方式训练 TensorFlow 模型

让我们假设我们已经从`MultiWorkerMirroredStrategy`获得了 TF 训练代码。为了使`TFJob`支持该策略，您需要调整`spec`字段中的`tfReplicaSpecs`。我们可以通过 YAML 文件定义以下类型的副本:

*   **首席(主)**:编排计算任务
*   **工作者**:运行计算
*   **参数服务器**:管理模型参数的存储
*   **评估员**:在模型训练期间运行评估

作为一个最简单的例子，我们将把一个工作者定义为那些可以充当主要节点的人之一。参数`server`和`evaluator`不是必须的。

让我们来看看针对分布式 TF 训练的调整后的 YAML 文件`tf_example_job_dist.yaml`:

```

apiVersion: "kubeflow.org/v1"

kind: "TFJob"

metadata:

    name: "tf_example_job_dist"

spec:

    cleanPodPolicy: None

    tfReplicaSpecs:

        Worker:

            replicas: 4

            restartPolicy: Never

            template:

                specs:

                    containers:

                        - name: tensorflow 

                          image: kubeflow/tf_example_job:1.1
```

前面的`YAML`文件将基于新容器`kubeflow/tf_example_job:1.1`上的`MultiWorkerMirroredStrategy`运行训练作业。我们可以使用相同的命令将`TFJob`部署到集群:

```
kubectl apply -f tf_example_job_dist.yaml
```

在下一节中，我们将学习如何使用 PyTorch 和 Ray。

## 使用 Kubeflow 以分布式方式训练 PyTorch 模型

对于 PyTorch，我们只需要将`TFJob`改为`PyTorchJob`并为提供一个 PyTorch 训练脚本。关于训练脚本本身，请参考 PyTorch 章节中的*数据并行。YAML 文件需要相同的组修改，如下面的代码片段中的所示:*

```

apiVersion: "kubeflow.org/v1 

kind: "PyTorchJob"

metadata:

    name: "pt_example_job_dist"

spec:

    pytorchReplicaSpecs:

        Master:

            replicas: 1

            restartPolicy: Never

            template:

                specs:

                    containers:

                        - name: pytorch 

                          image: kubeflow/pt_example_job:1.0

        Worker:

            replicas: 5

            restartPolicy: OnFailure

            template:

                specs:

                    containers:

                        - name: pytorch 

                          image: kubeflow/pt_example_job:1.0
```

在本例中，我们有一个主节点和五个工作节点的副本。完整的细节可以在 https://www.kubeflow.org/docs/components/training/pytorch 的[找到。](https://www.kubeflow.org/docs/components/training/pytorch)

要记住的事情

a.Kubeflow 允许您利用容器和 Kubernetes 轻松地从本地开发环境扩展到大型集群。

b.`TFJob`和`PyTorchJob`允许您分别使用 Kubeflow 以分布式方式运行 TF 和 PyTorch 培训作业。

在本节中，我们描述了如何利用 Kubeflow 以分布式方式训练 TF 和 PyTorch 模型。

# 总结

通过认识到来自多个设备和机器的并行性的好处，我们已经了解了训练 DL 模型的各种方法。首先，我们学习了如何在一台机器上使用多个 CPU 和 GPU 设备。然后，我们介绍了如何利用 TF 和 PyTorch 的内置特性来以分布式方式实现训练，其中底层集群是显式管理的。之后，我们学习了如何使用 SageMaker 进行分布式培训和扩展。最后，最后三节描述了为分布式培训设计的框架:Horovod、Ray 和 Kubeflow。

在下一章，我们将讨论模型理解。我们将学习流行的模型理解技术，这些技术提供了在整个训练过程中模型内部发生的事情的一些见解。*