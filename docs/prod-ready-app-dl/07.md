<title>B18522_05</title>

# 5

# 云中的数据准备

在本章中，我们将了解如何利用各种 AWS 云服务在云中设置数据准备。考虑到**提取、转换和加载** ( **ETL** )操作在数据准备中的重要性，我们将更深入地研究如何以经济高效的方式设置和调度 ETL 作业。我们将介绍四种不同的设置:在单节点 EC2 实例和 EMR 集群上运行的 ETL，然后利用 Glue 和 SageMaker 进行 ETL 作业。本章还将介绍 Apache Spark，这是最流行的 ETL 框架。通过完成本章，您将能够利用所介绍的设置的不同优势，并为您的项目选择正确的工具集。

在本章中，我们将讨论以下主要话题:

*   云中的数据处理
*   Apache Spark 简介
*   为 ETL 设置单节点 EC2 实例
*   为 ETL 设置 EMR 集群
*   为 ETL 创建粘合作业
*   利用 SageMaker 进行 ETL

# 技术要求

你可以从本书的 GitHub 资源库下载本章的补充资料:[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 5](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5)。

# 云中的数据处理

**深度学习** ( **DL** )项目的成功取决于数据的质量和数量。因此，用于数据准备的系统必须足够稳定和可伸缩，以便高效地处理数 TB 和数 Pb 的数据。这通常需要不止一台机器；必须为数据处理建立一个运行强大 ETL 引擎的机器集群，以便它能够存储和处理大量数据。

首先，我们想介绍一下 ETL，这是云中数据处理的核心概念。接下来，我们将概述用于数据处理的分布式系统设置。

## ETL 简介

在整个 ETL 过程中，数据将从一个或多个来源收集，根据需要转换成不同的形式，并保存在数据存储中。简而言之，ETL 本身覆盖了整个数据处理管道。ETL 始终与三种不同类型的数据交互:**结构化**、**非结构化**和**半结构化**。虽然结构化数据表示一组具有模式的数据(例如，表格)，但非结构化数据没有明确定义的模式(例如，文本、图像或 PDF 文件)。半结构化数据在数据本身中具有部分结构(例如，HTML 或电子邮件)。

流行的 ETL 框架有**Apache Hadoop**([https://hadoop.apache.org](https://hadoop.apache.org/))**Presto**([https://Presto db . io](https://prestodb.io/))**Apache Flink**([https://flink.apache.org](https://flink.apache.org/))**Apache Spark**([https://spark.apache.org](https://spark.apache.org/))。Hadoop 是最早利用分布式处理优势的数据处理引擎之一。Presto 专门处理 SQL 中的数据， Apache Flink 则是为了处理流数据而打造的。在这四个框架中，Apache Spark 是最受欢迎的工具，因为它可以处理每种数据类型。 *Apache Spark 利用内存中的数据处理来提高其吞吐量*，并提供比 Hadoop 更具可扩展性的数据处理解决方案。此外，它可以很容易地与其他 ML 和 DL 工具集成。出于这些原因，我们将在本书中主要关注 Spark。

## 数据处理系统架构

为数据处理设置一个系统不是一件简单的工作，因为它涉及到定期采购高端机器，正确连接各种数据处理软件，并确保在出现故障时数据不会丢失。因此，许多公司利用云服务，即通过互联网按需交付的各种软件服务。虽然许多公司提供各种各样的云服务，但亚马逊网络服务 ( **AWS** )凭借其稳定且易于使用的服务脱颖而出。

为了让您更全面地了解现实生活中的数据处理系统有多复杂，我们来看一个基于 AWS 服务的示例系统架构。这个系统的*核心组件*是开源的 *Apache Spark* 执行主要的 ETL 逻辑。典型的系统还包含用于调度单个作业、存储数据和可视化已处理数据的组件:

![Figure 5.1 – A generic architecture for data processing pipelines 
along with visualization and experimentation platforms
](img/B18522_05_01.jpg)

图 5.1–数据处理管道以及可视化和实验平台的通用架构

让我们来看看这些组件:

*   **数据存储**:数据存储负责保存数据和相关元数据；
    *   **Hadoop 分布式文件系统**(**https://hadoop.apache.org**):开源 HDFS 是一个*分布式文件系统，可以按需伸缩*([T16)。HDFS 一直是数据存储的传统选择，因为 Apache Spark 和 Apache Hadoop 在 HDFS 表现最佳。](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)
    *   **亚马逊简单存储服务(S3)** :这是 AWS【https://aws.amazon.com/s3】()提供的*数据存储服务。S3 使用了对象和桶的概念，其中一个对象指的是单个文件，一个桶指的是对象的容器。对于每个项目或子模块，您可以创建一个存储桶，并为读写操作配置不同的权限。存储桶还可以对数据应用版本控制，跟踪变化。*
*   `ml`，成本比其他 EC2 实例([https://aws.amazon.com/sagemaker/pricing](https://aws.amazon.com/sagemaker/pricing/))高 30%到 40%左右。在*利用 SageMaker 进行 ETL* 部分，我们将描述如何在 EC2 实例上为 ETL 过程设置 SageMaker。

考虑到需要处理的数据量，正确选择的 ETL 服务以及适当的数据存储选择可以显著提高管道的效率。要考虑的关键因素包括数据源、数据量、可用的硬件资源和可伸缩性等等。

*   **调度**:通常，ETL 作业必须定期运行(例如，每天、每周或每月)，因此需要一个调度器:
    *   **AWS Lambda 函数** : Lambda 函数([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda/))被设计为在 EMR 上运行作业，而无需供应或管理基础设施。执行时间可以动态配置；该作业可以立即运行，也可以安排在不同的时间运行。*AWS Lambda 函数以无服务器方式运行代码，因此不需要维护*。如果在执行过程中出现任何错误，EMR 集群将自动关闭。
    *   **气流**:调度器在自动化 ETL 过程中扮演重要角色。air flow([https://airflow.apache.org](https://airflow.apache.org/))*是数据工程师*使用的最流行的调度框架之一。气流的**有向无环图** ( **DAG** )可用于定期调度流水线。Airflow 比 AWS Lambda 函数更常用于定期运行 Spark 作业，因为 Airflow 使得在前面的任何执行失败时回填数据变得容易。
*   **构建**:构建是将代码包部署到 AWS 计算资源(如 EMR 或 EC2)或基于预定义规范建立一组 AWS 服务的过程；
    *   **cloud formation**:cloud formation模板([https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation/))*帮助提供云基础设施作为代码*。CloudFormation 通常在设置系统时执行特定的任务，例如创建一个 EMR 集群，准备一个具有特定规范的 S3 桶，或者终止一个正在运行的 EMR 集群。它有助于标准化重复性任务。
    *   **Jenkins**:Jenkins([https://www . Jenkins . io](https://www.jenkins.io/))构建用 Java 和 Scala 编写的可执行文件。我们使用*詹金斯构建 Spark 管道工件(例如。jar 文件)并将它们部署到 EMR 节点*。Jenkins 还利用 CloudFormation 模板以标准化的方式执行任务。
*   **数据库**:数据存储和数据库的关键区别在于，数据库是用来存储结构化数据的。在这里，我们将讨论两种流行的数据库类型:*关系数据库*和*键值存储数据库*。我们将描述它们的不同之处，并解释适当的使用案例:
    *   **关系型数据库** : *关系型数据库以表的格式用模式存储结构化数据*。以结构化方式存储数据的主要好处来自于数据管理；存储的数据值受到严格控制，以一致的格式保存这些值。这允许数据库在存储和检索特定数据集时进行额外的优化。ETL 作业一般从一个或多个数据存储服务中读取数据，处理数据，并将处理后的数据存储在关系数据库中，如**MySQL**([https://www.mysql.com](https://www.mysql.com/))，以及**PostgreSQL**([https://www.postgresql.org](https://www.postgresql.org/))。AWS 也提供关系数据库服务:**亚马逊 RDS**([https://aws.amazon.com/rds](https://aws.amazon.com/rds/))。
    *   **键值存储数据库**:与传统的关系数据库不同，这些数据库*针对大量读写操作进行了优化*。这种数据库以独特的键值对方式存储数据。通常，数据由一组键和一组保存每个键属性的值组成。许多数据库支持模式，但是它们的主要优势来自于它们也支持非结构化数据。换句话说，你可以存储任何数据，即使每个数据都有不同的结构。这类的热门数据库有**Cassandra**([https://cassandra.apache.org](https://cassandra.apache.org/))和**MongoDB**([https://www.mongodb.com](https://www.mongodb.com/))。有趣的是，AWS 提供了一个键值存储数据库，称为 **DynamoDB** 作为服务([https://aws.amazon.com/dynamodb](https://aws.amazon.com/dynamodb))。
*   **Metastore** :在某些情况下，收集并在数据存储中可用的初始数据集可能不包含任何关于自身的信息:例如，它可能缺少列类型或关于源的细节。当工程师管理和处理数据时，这些信息通常会有所帮助。因此，工程师们引入了 *metastore* 的概念，metastore*是元数据*的存储库。存储为表的元数据提供了它所指向的数据的位置、模式和更新历史。

在 AWS 的例子中， **Glue Data Catalog** 扮演 metastore 的角色，为 S3 提供内置支持。另一方面，hive([https://hive.apache.org](https://hive.apache.org/))是 HDFS 的开源 metastore。Hive 的主要优势来自数据查询、汇总和分析，这是自然而然的，因为它提供了基于类似 SQL 语言的交互。

*   **应用编程接口** ( **API** ) **服务** : *API 端点允许数据科学家和工程师高效地与数据交互*。例如，可以设置 API 端点，以便轻松访问存储在 S3 存储桶中的数据。许多框架都是为 API 服务设计的。以为例，**Flask API**([https://flask.palletsprojects.com](https://flask.palletsprojects.com/))和**Django**([https://www.djangoproject.com](https://www.djangoproject.com/))框架都是基于 Python 的，而Play 框架([https://www.playframework.com](https://www.playframework.com/))常用于 Scala 中的项目。
*   **实验平台**:评估系统在生产中的性能通常是通过一种流行的用户体验研究方法来实现的，这种方法被称为 A/B 测试。*通过部署两个不同版本的系统并比较用户体验，A/B 测试使我们能够了解最近的更改是否对系统产生了积极影响*。一般来说，设置 A/B 测试包括两个部分:
    *   **Rest API**:*Rest API 在处理带有不同参数的请求和以经过处理的方式返回数据方面提供了更大的灵活性*。因此，建立一个 Rest API 服务是很常见的，它从数据库或数据存储中聚集必要的数据用于分析目的，并以 JSON 格式向 A/B 实验平台提供数据。
    *   **A/B 实验平台**:数据科学家经常使用一个带有**图形用户界面** ( **GUI** )的应用程序来安排各种 A/B 测试实验，并直观地将汇总的数据可视化以供分析。growth book([https://www . growth book . io](https://www.growthbook.io/))就是这样一个平台的开源例子。
*   **数据可视化工具**:一个公司内有几个不同的团队(例如，营销、销售和高管)，他们可以从直观地可视化数据中受益。数据可视化工具通常支持定制仪表板的创建，这有助于数据分析过程。tableau(【https://www.tableau.com】)是项目负责人的常用工具，但它是专有软件。另一方面，Apache Superset([https://superset.apache.org](https://superset.apache.org/))是一个支持大多数标准数据库的开源数据可视化工具。如果管理成本是一个问题，Apache Superset 可以配置为使用存储在 AWS Athena([https://aws.amazon.com/athena](https://aws.amazon.com/athena/))等无服务器数据库中的数据来读取和绘制可视化。
*   **身份访问管理** ( **IAM** ): IAM 是一个权限系统，规范对 AWS 资源的访问。通过 IAM，可以控制用户可以访问的一组资源以及他们可以对所提供的资源进行的一组操作。更多关于 IAM 的细节可以在[https://aws.amazon.com/iam](https://aws.amazon.com/iam)找到。

要记住的事情

a.在整个 ETL 过程中，数据将从一个或多个来源收集，根据需要转换成不同的形式，并保存到数据存储或数据库中。

b.Apache Spark 是一个开源 ETL 引擎，广泛用于处理各种类型的大量数据:结构化、非结构化和半结构化。

c.为数据处理作业设置的典型系统由各种组件组成，包括数据存储、数据库、ETL 引擎、数据可视化工具和实验平台。

d.ETL 引擎可以在多种环境下运行——单台机器、集群、云中完全托管的 ETL 服务，以及为 DL 项目设计的端到端服务。

在下一节中，我们将介绍 Apache Spark 中的关键编程概念，这是最流行的 ETL 工具。

# Apache Spark 简介

Apache Spark 是一个用于数据处理的开源数据分析引擎。最流行的用例是 ETL。作为对 Spark 的介绍，我们将涵盖围绕 Spark 的关键概念和一些常见的 Spark 操作。具体来说，我们将从介绍**弹性分布式数据集** ( **RDDs** )和数据帧开始。然后，我们将讨论 ETL 任务需要了解的 Spark 基础知识:如何从数据存储中加载一组数据，应用各种转换，以及存储处理过的数据。Spark 应用程序可以使用多种编程语言实现:Scala、Java、Python 和 r。在本书中，我们将使用 Python，以便与其他实现保持一致。本节的代码片段可以在本书的 GitHub 知识库中找到:[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 5/spark](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/spark)。我们将在我们的示例中使用的数据集包括我们在 [*第 2 章*](B18522_02.xhtml#_idTextAnchor034) 、*深度学习* *项目的数据准备*中爬取的谷歌学术和 COVID 数据集，以及纽约时报提供的另一个 COVID 数据集([https://github.com/nytimes/covid-19-data](https://github.com/nytimes/covid-19-data))。我们将最后一个数据集称为 NY 乘以 COVID。

## 弹性分布式数据集和数据框架

Spark 的独特优势来自 rdd，不可变的分布式数据对象集合。通过利用 rdd，Spark 可以高效地处理利用并行性的数据。Spark 在 rdd 上运行的内置并行处理有助于数据处理，即使在其一个或多个处理器出现故障时也是如此。当一个 Spark 作业被触发时，输入数据的 RDD 表示被分割成多个分区，并分发到每个节点进行转换，从而最大化吞吐量。

像熊猫数据帧一样，Spark 也有数据帧的概念，它用命名列表示关系数据库中的表。数据帧也是 RDD，因此我们在下一节中描述的操作也同样适用。可以从结构化为表格的数据中创建数据帧，例如 CSV 数据、Hive 中的表格或现有的 rdd。数据帧附带了 RDD 不提供的模式。因此，RDD 用于非结构化和半结构化数据，而 DataFrame 用于结构化数据。

### rdd 和数据帧之间的转换

任何 Spark 操作的第一步都是创建一个`SparkSession`对象。具体来说，来自`pyspark.sql`的`SparkSession`模块用于创建一个`SparkSession`对象。模块中的`getOrCreate`函数用于创建会话对象，如下所示。一个`SparkSession`对象是 Spark 应用程序的入口点。它提供了一种在不同上下文下与 Spark 应用程序进行交互的方式，例如 Spark 上下文、Hive 上下文和 SQL 上下文:

```

from pyspark.sql import SparkSession

spark_session = SparkSession.builder\

        .appName("covid_analysis")\

        .getOrCreate()
```

将 RDD 转换成数据帧很简单。假设 RDD 没有任何方案，您可以创建一个没有任何方案的数据框架，如下所示:

```

# convert to df without schema

df_ri_freq = rdd_ri_freq.toDF() 
```

要将 RDD 转换成带有模式的数据帧，需要使用`StructType`类，它是`pyspark.sql.types`模块的一部分。一旦使用`StructType`方法创建了一个模式，Spark 会话对象的`createDataFrame`方法就可以用于将 RDD 转换成数据帧:

```

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# rdd for research interest frequency data

rdd_ri_freq = ... 

# convert to df with schema

schema = StructType(

          [StructField("ri", StringType(), False), 

           StructField("frequency", IntegerType(), False)])

df = spark.createDataFrame(rdd_ri_freq, schema)
```

既然我们已经学习了如何在 Python 中设置 Spark 环境，让我们学习如何加载数据集作为 RDD 或数据帧。

## 加载数据

Spark 可以加载存储在各种数据存储形式中的不同格式的数据。加载以 CSV 格式存储的数据是 Spark 的一个基本操作。这可以使用`spark_session.read.csv`功能轻松实现。它将位于本地或云中的 CSV 文件作为数据帧读取，例如在 S3 存储桶中。在下面的代码片段中，我们正在加载存储在 S3 的谷歌学术数据。`header`选项可用于指示 CSV 文件有标题:

```

# datasets location

google_scholar_dataset_path = "s3a://my-bucket/dataset/dataset_csv/dataset-google-scholar/output.csv"

# load google scholar dataset

df_gs = spark_session. \

        .read \

        .option("header", True) \

        .csv(google_scholar_dataset_path)
```

下图显示了`df_gs.show(n=3)`的结果。`show`函数打印第一个 *n* 行，以及列标题:

![Figure 5.2 – A sample DataFrame created by loading a CSV file
](img/B18522_05_02.jpg)

图 5.2–通过加载 CSV 文件创建的样本数据帧

同样，可以使用`SparkSession`模块的`read.json`函数读取数据存储器中的 JSON 文件:

```

# loada json file

json_file_path="s3a://my-bucket/json/cities.json"

df = spark_session.read.json(json_file_path)
```

在下一节中，我们将学习如何使用 Spark 操作处理加载的数据。

## 使用火花操作处理数据

Spark 提供了一组将 RDD 转换为不同结构的 RDD 的操作。实现 Spark 应用程序是在 RDD 上链接一组 Spark 操作以将数据转换成目标格式的过程。在本节中，我们将讨论最常用的——即`filter`、`map`、`flatMap`、`reduceByKey`、`take`、`groupBy`和`join`。

### 过滤器

在大多数情况下，通常首先应用过滤器来丢弃不必要的数据。将`filter`方法应用于数据帧可以帮助您从给定的数据帧中选择感兴趣的行。在下面的代码片段中，我们使用这个方法只保留`research_interest`不是`None`的行:

```

# research_interest cannot be None

df_gs_clean = df_gs.filter("research_interest != 'None'")
```

### 地图

像其他编程语言中的`map`函数一样，Spark 中的`map`操作将给定函数应用于每个数据条目。这里，我们使用`map`函数只保留`research_interest`列:

```

# we are only interested in research_interest column

rdd_ri = df_gs_clean.rdd.map(lambda x: (x["research_interest"]))
```

### 平面地图

`flatMap`函数在将给定函数应用于每个条目后展平 RDD，并返回新的 RDD。在本例中，`flatMap`操作使用`##`分隔符分割每个数据条目，然后创建一对`research_interest`和一个值为`1`的默认频率:

```

# raw research_interest data into pairs of research area and a frequency count

rdd_flattened_ri = rdd_ri.flatMap(lambda x: [(w.lower(), 1) for w in x.split('##')])
```

### reduceByKey

`reduceByKey`根据键对输入的 RDD 进行分组。这里，使用`reduceByKey`对频率求和，以了解每个`research_interest`的出现次数:

```

# The pairs are grouped based on research area and the frequencies are summed up

rdd_ri_freq = rdd_flattened_ri.reduceByKey(add)
```

### 拿

Spark 的基本操作之一是`take`。该函数用于从 RDD 中获取第一个 *n* 元素:

```

# we are interested in the first 5 entries

rdd_ri_freq_5 = rdd_ri_freq.take(5)
```

### 分组操作

分组的想法是将数据帧中相同的数据条目收集成组，并对组进行汇总(例如，平均或求和)。

例如，让我们使用 Moderna COVID 数据集，通过`groupby`操作获得每个辖区(州)分配的平均剂量数。这里，我们使用`sort`函数对各州的平均剂量数进行排序。`toDF`和`alias`功能可以帮助添加新数据帧的名称:

```

# calculate average number of 1st corona vaccine per jurisdiction (state)

df_avg_1 = df_covid.groupby("jurisdiction")\

  .agg(F.avg("_1st_dose_allocations")

  .alias("avg"))\

  .sort(F.col("avg").desc())\

  .toDF("state", "avg")
```

应用`groupby`时，可以在一个命令中应用多个聚合(`sum`和`avg`)。从聚合函数(如`F.avg`或`F.sum`创建的列可以使用`alias`重命名。在以下示例中，对 Moderna COVID 数据集执行聚合，以获得第一次和第二次剂量的平均数和总和:

```

# At jurisdiction (state) level, calculate at average weekly 1st & 2nd dose vaccine distribution. Similarly calculate sum for 1st and 2nd dose

df_avg = df_covid.groupby(F.lower("jurisdiction").alias("state"))\

  .agg(F.avg("_1st_dose_allocations").alias("avg_1"), \

       F.avg("_2nd_dose_allocations").alias("avg_2"), \

       F.sum("_1st_dose_allocations").alias("sum_1"), \

       F.sum("_2nd_dose_allocations").alias("sum_2")

       ) \

  .sort(F.col("avg_1").desc())
```

使用`groupby`功能在状态级执行计算。该数据集总共包含 63 个州，包括作为一个州的某些实体(联邦机构)。

### 加入

`join`功能帮助合并两个数据帧的行。

为了演示如何使用`join`,我们将连接 Moderna COVID 数据集和 NY Times COVID 数据集。在我们解释任何`join`操作之前，我们必须对 NY Times COVID 数据集应用聚合，就像我们之前处理 Moderna COVID 数据集一样。在下面的代码片段中，在州级别应用了`groupby`操作，以获得表示死亡总数和病例总数的聚合(`sum`)值:

```

# at jurisdiction (state) level, calculate total number of deaths and total number of cases

df_cases = df_covid2 \

          .groupby(F.lower("state").alias("state")) \

          .agg(F.sum("deaths").alias("sum_deaths"), \

               F.sum("cases").alias("sum_cases"))
```

*图 5.3* 显示了`df_cases.show(n=3)`操作的结果，可视化了被处理数据帧的顶部三行:

![Figure 5.3 – The top three rows of the aggregated results
](img/B18522_05_03.jpg)

图 5.3–汇总结果的前三行

我们现在准备演示两种类型的连接:等连接和左连接。

#### 相等联接(内部联接)

等价连接，也称为内部连接，是 Spark 中默认的`join`操作。一个内部连接用于连接公共列值上的两个数据帧。键不匹配的行将在最终的数据帧中被删除。在本例中，equi-join 将作为 Moderna COVID 数据集和 NY Times COVID 数据集之间的公共列应用于`state`列。

第一步是使用`alias`为数据帧创建别名。然后，我们在一个数据帧上调用`join`函数，同时传递定义列关系和连接类型的另一个数据帧:

```

# creating an alias for each DataFrame

df_moderna = df_avg.alias("df_moderna")

df_ny = df_cases.alias("df_ny")

df_inner = df_moderna.join(df_ny, F.col("df_moderna.state") == F.col("df_ny.state"), 'inner')
```

以下是`df_inner.show(n=3)`操作的输出:

![Figure 5.4 – The output of using the df_inner.show(n=3) operation
](img/B18522_05_04.jpg)

图 5.4–使用 df_inner.show(n=3)操作的输出

现在，让我们看看另一种类型的连接，左连接。

#### 左连接

一个左连接是另一个流行的用于数据分析的`join`操作。左连接返回一个数据帧中的所有行，而不管在另一个数据帧中找到的匹配。当`join`表达式不匹配时，它为缺少的条目分配`null`。

left join 语法类似于 equi-join。唯一的区别是，在指定连接类型时，需要传递关键字`left`，而不是`inner`。左连接获取第一个提到的数据帧(`df_m`)中提到的列(`df_m.state`)的所有值。然后，它尝试将条目与第二个提到的数据帧(`df_ny.state`)在提到的列上进行匹配。在这个例子中，如果一个特定的状态出现在两个数据帧上，`join`操作的输出将是该状态，以及来自两个数据帧的值。如果某个特定状态仅在第一个数据帧(`df_m`)中可用，而在第二个数据帧(`df_ny`)中不可用，那么它将添加仅具有第一个数据帧的值的状态，保持另一个条目为`null`:

```

# join results in all rows from the left table. Missing entries from the right table will result in "null"

df_left = df_moderna.join(df_ny, F.col("df_m.state") == F.col("df_ny.state"), 'left')
```

这里显示了`df_left.show(n=3`的输出:

![Figure 5.5 – The output of the df_inner.show(n=3) operation
](img/B18522_05_05.jpg)

图 5.5–df _ inner . show(n = 3)操作的输出

尽管 Spark 提供了大量涵盖不同情况的操作，但由于逻辑的复杂性，您可能会发现构建自定义操作更有用。

## 使用用户自定义函数处理数据

**用户自定义函数** ( **UDF** ) *是一个可重用的自定义函数，对 RDD* 执行转换。一个 UDF 函数可以在几个数据帧上重复使用。在本节中，我们将提供一个使用 UDF 处理谷歌学术数据集的完整代码示例。

首先，我们想介绍一下`pyspark.sql.function`模块，它允许您用`udf`方法定义一个 UDF，并提供各种列操作。`pyspark.sql.function`还包括用于聚合的函数，如`avg`或`sum`，分别用于计算平均值和总数:

```

import pyspark.sql.functions as F
```

在谷歌学术数据集中，`data_science`、`artificial_intelligence`和`machine_learning`都引用`research_interest`数据的同一个字段，并检查是否有任何数据可以归类为 AI。如果找到匹配，它会在新列中放入一个值`1`。否则它将分配`0`。使用`withColumn`方法将 UDF 的结果存储在名为`is_artificial_intelligence`的新列中。在下面的代码片段中，`@F.udf`注释通知 Spark 该函数是一个 UDF。来自`pyspark.sql.functions`的`col`方法通常用于传递一列作为 UDF 的参数。这里，`F.col("research_interest")`被传递给了 UDF `is_ai`方法，指示 UDF 应该操作哪一列:

```

# list of research_interests that are under same domain

lst_ai  = ["data_science", "artificial_intelligence",

           "machine_learning"]

@F.udf

def is_ai(research):

    """ return 1 if research in AI domain else 0"""

    try:

      # split the research interest with delimiter "##"  

      lst_research = [w.lower() for w in str(research).split("##")]

      for res in lst_research:

        # if present in AI domain

        if res in lst_ai:

          return 1

      # not present in AI domain

      return 0

    except:

      return -1

# create a new column "is_artificial_intelligence"

df_gs_new = df_gs.withColumn("is_artificial_intelligence",\ is_ai(F.col("research_interest")))
```

在处理完原始数据后，我们希望将它存储在数据存储器中，以便我们可以将它重新用于其他目的。

## 导出数据

在本节中，我们将学习如何将数据帧保存到 S3 桶中。在 RDD 的情况下，它必须被转换成数据帧以被适当地保存。

通常，数据分析师希望将聚合数据作为 CSV 文件写入以下操作。要将数据帧导出为 CSV 文件，必须使用`df.write.csv`功能。对于文本值，我们建议您使用`option("quoteAll", True)`，它会用引号将每个值括起来。

在下面的例子中，我们提供了一个 S3 路径来在 S3 桶中生成一个 CSV 文件。`coalesce(1)`用于编写单个 CSV 文件而不是多个 CSV 文件:

```

S3_output_path = "s3a:\\my-bucket\output\vaccine_state_avg.csv"

# writing a DataFrame as a CSVfile

sample_data_frame.\

        .coalesce(1) \

        .write \

        .mode("overwrite") \

        .option("header", True) \

        .option("quoteAll",True) \

        .csv(s3_output_path)
```

如果您想将数据帧保存为 JSON 文件，您可以使用`write.json`:

```

S3_output_path = "s3a:\\my-bucket\output\vaccine_state_avg.json"

# Writing a DataFrame as a json file

sample_data_frame \

        .write \

        .json(s3_output_path)
```

此时，您应该看到一个文件存储在 S3 存储桶中。

要记住的事情

a.RDD 是一个不可变的分布式集合，它被分割成多个分区，并在集群的不同节点上进行计算。

b.Spark 数据帧相当于关系数据库中带有命名列的表。

c.Spark 提供了一组操作，将 RDD 转换为具有不同结构的 RDD。实现 Spark 应用程序是在 RDD 上链接一组 Spark 操作以将数据转换成目标格式的过程。您可以使用 UDF 构建一个定制的 Spark 操作。

在这一节中，我们描述了 Apache Spark 的基础知识，它是 ETL 最常用的工具。从下一节开始，我们将讨论如何在云中为 ETL 设置一个 Spark 作业。首先，让我们看看如何在单个 EC2 实例上运行 ETL。

# 为 ETL 设置单节点 EC2 实例

EC2 实例可以拥有 CPU/GPU、内存、存储和网络容量的各种组合。您可以在官方文档中找到 EC2 的可配置选项:[https://aws.amazon.com/ec2/instance-types](https://aws.amazon.com/ec2/instance-types)。

创建 EC2 实例时，您可以选择一个 Docker 映像来运行，该映像已经为各种项目进行了预定义。这些被称为**亚马逊机器图像** ( **AMIs** )。例如，有一个为 DL 项目安装了 TF version 2 的图像和一个为一般 ML 项目安装了 Anacond a 的图像，如下面的截图所示。完整的 ami 列表请参考[https://docs . AWS . Amazon . com/AWS ec2/latest/user guide/AMIs . html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html):

![Figure 5.6 – Selecting an AMI for an EC2 instance
](img/B18522_05_06.jpg)

图 5.6–为 EC2 实例选择 AMI

AWS 提供**深度学习 ami**(**DLAMIs**)，为 DL 项目打造的ami；图像利用不同的 CPU 和 GPU 配置以及不同的计算架构([https://docs . AWS . Amazon . com/dlami/latest/dev guide/options . html](https://docs.aws.amazon.com/dlami/latest/devguide/options.html))。

正如 [*第一章*](B18522_01.xhtml#_idTextAnchor014) 、*深度学习驱动项目的有效规划*中提到的，许多数据科学家利用 EC2 实例来开发他们的算法，利用动态资源分配的灵活性。创建 EC2 实例和安装 Spark 的步骤如下:

1.  出于安全目的，创建一个**虚拟专用网** ( **VPN** )来限制对 EC2 实例的访问。
2.  用 EC2 密钥对创建一个`.pem`密钥。当用户试图从终端登录 EC2 实例时，使用一个`.pem`文件来执行身份验证。
3.  用必要的工具和包从 Docker 映像创建 EC2 实例。
4.  添加允许从本地终端访问新实例的入站规则。
5.  使用 SSH 访问 EC2 实例和在*步骤 2* 中创建的`.pem`文件。
6.  启动火花壳。

我们已经在[https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 5/ec2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/ec2)上为提供了每个步骤的详细描述和截图。

要记住的事情

a.EC2 实例可以具有 CPU/GPU、内存、存储和网络容量的各种组合

b.只需在 AWS web 控制台上点击几次，就可以从预定义的 Docker 映像(AMI)创建 EC2 实例

接下来，我们将学习如何建立一个集群，将一组 Spark workers 作为一个组来运行。

# 为 ETL 设置 EMR 集群

在DL 的情况下，单个 EC2 实例的计算能力可能不足以进行模型训练或数据处理。因此，通常将一组 EC2 实例放在一起以增加吞吐量。AWS 为此有专门的服务:**亚马逊弹性 MapReduce** ( **EMR** )。它是一个全托管集群平台，为 Apache Spark 和 Hadoop 等大数据框架提供分布式系统。通常，为 ETL 设置的 EMR 集群从 AWS 存储(亚马逊 S3)读取数据，处理数据，并将其写回 AWS 存储。Spark 作业通常用于处理与 S3 交互的 ETL 逻辑。EMR 提供了一个有趣的功能，名为 **Workspace** ，可以帮助开发人员组织笔记本，并与其他 EMR 用户共享以进行协作。

典型的 EMR 设置包含一个主节点和几个核心节点。在多节点集群的情况下，必须至少有一个核心节点。主节点管理运行分布式应用程序(例如 Spark 或 Hadoop)的集群。核心节点由主节点管理，运行数据处理任务，并将数据存储在数据存储器中(例如 S3 或 HDFS)。

任务节点由主节点管理，是可选的。它们通过在计算过程中引入另一种并行性来提高集群上运行的分布式应用程序的吞吐量。它们运行数据处理任务，但不在数据存储器中存储数据。

下面的屏幕截图显示了 EMR 集群创建页面。在整个表单中，我们需要提供集群的名称、启动模式、EMR 版本、在集群上运行的应用程序(例如，用于数据处理的 Apache Spark 和用于笔记本的 Jupyter)以及 EC2 实例的规范。用 DL 处理数据通常需要高计算能力的实例。在其他情况下，您可以构建一个具有更大内存限制的集群:

![Figure 5.7 – EMR cluster creation 
](img/B18522_05_07.jpg)

图 5.7–EMR 集群创建

具体步骤如下:

*   **步骤 1:软件和步骤**:在这里，你必须选择软件相关的配置——即 EMR 版本和应用程序(Spark、JupyterHub 等)。
*   **步骤 2:硬件**:在这里，您必须选择与硬件相关的配置，即实例类型、实例数量和 VPN 网络。
*   **第三步:一般集群设置**:选择操作日志的集群名称和 S3 桶路径。
*   `.pem`文件:
    *   只有当您想要登录到 EC2 主节点并像在单个 EC2 实例的情况下一样在 Spark shell 上工作时，才需要文件。

完成这些步骤后，您需要等待几分钟，直到集群的状态变为`running`。然后，您可以导航到 EMR 集群提供的端点来打开 Jupyter 笔记本。用户名为`jovyan`，密码为`jupyter`。

我们的 GitHub 存储库提供了这个过程的逐步说明，以及截图([https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 5/EMR](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/emr))。

要记住的事情

a.EMR 是一个完全托管的集群平台，运行大数据 ETL 框架，如 Apache Spark

b.您可以通过 AWS web 控制台创建一个包含各种 EC2 实例的 EMR 集群

EMR 的缺点在于它需要被明确地管理。一个组织通常有一组开发人员专门处理与 EMR 集群相关的问题。不幸的是，如果组织很小，这可能很难做到。在下一节中，我们将介绍 Glue，它不需要任何显式的集群管理。

# 为 ETL 创建粘合作业

AWS Glue([https://aws.amazon.com/glue](https://aws.amazon.com/glue/))支持无服务器方式的数据处理。Glue 的计算资源由 AWS 管理，因此不像专用集群(例如，EMR)那样需要更少的维护工作。除了最少的资源维护工作之外，Glue 还提供了额外的特性，比如内置的调度器和 Glue 数据目录，这将在后面讨论。

首先，让我们学习如何使用 Glue 设置数据处理作业。在开始定义数据处理逻辑之前，必须创建一个包含 S3 数据模式的粘合数据目录。一旦为输入数据定义了粘合数据目录，您就可以使用粘合 Python 编辑器来定义数据处理逻辑的细节(*图 5.8* )。该编辑器为您的应用程序提供了一个基本的设置，以降低设置胶合作业的难度:[https://docs.aws.amazon.com/glue/latest/dg/edit-script.html](https://docs.aws.amazon.com/glue/latest/dg/edit-script.html)。在这个模板代码之上，您将读入 Glue 数据目录作为输入，处理它，并存储处理后的输出。由于 Glue Data Catalog 很好地集成了 Spark，因此 Glue 作业中的操作通常使用 Spark 来实现:

![Figure 5.8 – AWS Glue job script editor
](img/B18522_05_08.jpg)

图 5.8–AWS 粘合作业脚本编辑器

在以下部分中，您将了解如何使用存储在 S3 存储桶中的谷歌学术数据集来设置粘合作业。完整的实现可以在[https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 5/glue](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/glue)找到。

## 创建胶水数据目录

首先，我们将创建一个胶水数据目录(参见*图 5.9* )。Glue 只能读取元数据存储在 Glue 数据目录中的一组数据。数据目录由数据库组成，数据库是表形式的元数据的集合。Glue 提供了一个称为**爬虫**的特性，它*为数据存储*中的数据文件创建元数据(例如，一个 S3 桶):

![Figure 5.9 – The first step of setting up a crawler
](img/B18522_05_09.jpg)

图 5.9–设置爬虫的第一步

前面的截图显示了创建爬虫的第一步。每个步骤的细节可以在[https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)找到。

## 设置粘合上下文

如果你看一下 AWS 为 Glue 提供的模板代码，你会发现一些关键包已经被导入了。`awsglue.utils`模块中的`getResolvedOptions`有助于利用运行时传递给胶合脚本的参数:

```

from awsglue.utils import getResolvedOptions

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
```

对于使用 Spark 的胶合作业，必须创建 Spark 上下文并将其传递给`GlueContext`。Spark 会话对象可以从 Glue 上下文中访问。可以使用`awsglue.job`模块通过传递一个粘合上下文对象来实例化粘合作业:

```

from pyspark.context import SparkContext

from awsglue.context import GlueContext

from awsglue.job import Job

# glue_job_google_scholar.py

# spark context

spark_context = SparkContext()

# glue context

glueContext = GlueContext(spark_context)

# spark

spark_session = glueContext.spark_session

# job

job = Job(glueContext)

# initialize job

job.init(args['JOB_NAME'], args)
```

接下来，我们将学习如何从粘合数据目录中读取数据。

## 阅读资料

在这个部分，您将学习在创建 Glue 表目录之后，如何在 Glue 上下文中读取位于 S3 桶中的数据。

*Glue 中的数据使用一种称为 DynamicFrame* 的特定数据结构从一个转换传递到另一个转换，DynamicFrame 是 Apache Spark 数据帧的扩展。DynamicFrame 具有自描述的特性，不需要任何模式。与 Spark 数据帧不同，DynamicFrame 的这个附加属性有助于容纳不符合固定模式的数据。所需的库可以从`awsglue.dynamicframe`导入。这个包使得将动态帧转换成 Spark 数据帧变得容易:

```

from awsglue.dynamicframe import DynamicFrame
```

在下面的例子中，我们在名为`google_scholar`的数据库中创建一个名为`google_authors`的粘合数据目录表。一旦数据库可用，`glueContext.create_dynamic_frame.from_catalog`可以用来读取`google_scholar`数据库中的`google_authors`表，并将其作为 Glue DynamicFrame 加载:

```

# glue context

google_authors = glueContext.create_dynamic_frame.from_catalog(

           database="google_scholar",

           table_name="google_authors")
```

使用`toDF`方法可将胶水动态帧转换成火花数据帧。将火花操作应用于数据时，需要进行此转换:

```

# convert the glue DynamicFrame to Spark DataFrame

google_authors_df = google_authors.toDF()
```

现在，让我们定义数据处理逻辑。

## 定义数据处理逻辑

可以在 Glue DynamicFrame 上执行的基本转换由`awsglue.transforms`模块提供。这些转换包括`join`、`filter`、`map`以及其他许多转换([https://docs . AWS . Amazon . com/glue/latest/DG/built-in-transforms . html](https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html))。您可以像在*Apache Spark 简介*一节中介绍的那样使用它们:

```

from awsglue.transforms import *
```

此外，如果胶水动态帧已经被转换成火花数据帧，那么在*使用火花操作*处理数据一节中描述的每个火花操作都可以应用于胶水中的数据。

## 写数据

在这个部分中，我们将学习如何将 Glue DynamicFrame 中的数据写入 S3 桶。

给定一个 Glue DynamicFrame，您可以使用 Glue 上下文的`write_dynamic_frame.from_options`将数据存储在给定的 S3 路径中。您需要在结束时调用作业的`commit`方法来执行单独的操作:

```

# path for output file

path_s3_write= "s3://google-scholar-csv/write/"

# write to s3 as a CSV file with separator |

glueContext.write_dynamic_frame.from_options(

    frame = dynamic_frame_write,

    connection_type = "s3",

    connection_options = {

            "path": path_s3_write

                         },

    format = "csv",

    format_options={

            "quoteChar": -1,

            "separator": "|"

                   })

job.commit()
```

对于 Spark 数据帧，必须先将其转换为 DynamicFrame，然后才能存储数据。`DynamicFrame.fromDF`函数接受一个 Spark DataFrame 对象、一个 Glue 上下文对象和新 DynamicFrame 的名称:

```

# create a DynamicFrame from a Spark DataFrame

dynamic_frame = DynamicFrame.fromDF(df_sort, glueContext, "dynamic_frame")
```

现在，您可以使用 Spark 操作和 Glue 转换来处理您的数据。

要记住的事情

a.AWS Glue 是为 ETL 操作设计的完全托管的服务

b.AWS Glue 是一个无服务器架构，这意味着底层服务器将由 AWS 维护

c.AWS Glue 提供了带有 Python 样板代码的内置编辑器。在这个编辑器中，您可以定义您的 ETL 逻辑，也可以利用 Spark

作为 ETL 的最后一个设置，我们将看看 SageMaker。

# 利用 SageMaker 进行 ETL

在本节中，我们将描述如何使用 SageMaker 设置 ETL 过程(下面的屏幕截图显示了 SageMaker 的 web 控制台)。SageMaker 的主要优势来自于这样一个事实，即它是一个完全托管的基础设施，用于构建、培训和部署 ML 模型。缺点是比 EMR 和胶水贵。

SageMaker Studio 是一个基于 web 的 SageMaker 开发环境。SageMaker 的理念是，它是数据分析管道的一体化平台。使用 SageMaker Studio 可以实现 ML 管道的每个阶段:数据处理、算法设计、调度作业、实验管理、开发和训练模型、创建推理端点、检测数据漂移和可视化模型性能。SageMaker Studio 笔记本也可以连接到 EMR 进行计算，但有一些限制；只能使用有限的 Docker 图片(如`Data Science`或`SparkMagic`)([https://docs . AWS . Amazon . com/sage maker/latest/DG/studio-notebooks-EMR-cluster . html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html)):

![Figure 5.10 – The SageMaker web console
](img/B18522_05_10.jpg)

图 5.10–sage maker web 控制台

SageMaker 提供了各种预定义的开发环境作为 Docker 镜像。流行的环境是那些已经安装了 PyTorch、TF 和 Anaconda 的 DL 项目。从基于 web 的开发环境中，可以很容易地将笔记本附加到这些图像中的任何一个，如下面的屏幕截图所示:

![Figure 5.11 – Updating the development environment dynamically for a SageMaker notebook
](img/B18522_05_11.jpg)

图 5.11-为 SageMaker 笔记本动态更新开发环境

创建一个 ETL 任务的过程可以分为四个步骤:

1.  在 SageMaker Studio 中创建一个用户。
2.  通过选择正确的 Docker 图像，在用户下创建一个笔记本。
3.  定义数据处理逻辑。
4.  安排作业。

*在 SageMaker web 控制台中，只需点击一下鼠标，即可完成步骤 1* 和步骤 2 。*第三步*可以使用 Spark 进行设置。要调度一个作业(*步骤 4* ，首先，您需要通过`pip`命令安装`run-notebook`命令行实用程序:

```
pip install https://github.com/aws-samples/sagemaker-run-notebook/releases/download/v0.20.0/sagemaker_run_notebook-0.20.0.tar.gz
```

在查看用于安排笔记本时间的`run-notebook`命令之前，我们将简要讨论`cron`命令，它定义了时间表的格式。如下图所示，六个数字用于表示时间戳。例如，`45 22 ** 6*`表示每周六晚上 10:45 的时间表。`*`(星号)通配符代表相应单位的每个值:

![Figure 5.12 – Cron schedule format
](img/B18522_05_12.jpg)

图 5.12–Cron 计划格式

`run-notebook`命令接收一个用`cron`表示的时间表和一个笔记本。在以下示例中，`notebook.ipynb`已被安排在 2021 年每天上午 8 点运行:

```
run-notebook schedule --at "cron(0 8 * * * 2021)" --name nightly notebook.ipynb
```

我们在我们的 GitHub 资源库中提供了一组每一步的截图:[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter _ 5/sage maker/sage maker _ studio . MD](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_5/sagemaker/sagemaker_studio.md)。

在接下来的部分中，我们将深入了解如何利用 SageMaker 笔记本来运行数据处理作业。

## 制作 SageMaker 笔记本

笔记本实例是运行 Jupyter 笔记本应用程序的 ML 计算实例。SageMaker 将创建这个实例，以及相关的资源。Jupyter 笔记本用于处理数据、训练模型、部署和验证模型。可以通过几个步骤创建一个笔记本实例。完整的描述可以在[https://docs . AWS . Amazon . com/sagemaker/latest/DG/how networks-create-ws . html](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)找到:

1.  转到 SageMaker web 控制台:[https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)。请注意，您需要使用 AWS 凭据登录。
2.  在**笔记本实例**下，选择 **创建笔记本实例**。
3.  在每个新笔记本的`pip install tensorflow`)上。各种这样的例子可以在 https://github . com/AWS-samples/Amazon-sage maker-notebook-instance-life cycle-config-samples/tree/master/scripts 找到:

![Figure 5.13 – Life cycle configuration script for a SageMaker notebook
](img/B18522_05_13.jpg)

图 5.13-sage maker 笔记本的生命周期配置脚本

虽然直接从 SageMaker 笔记本运行一组操作是一个选项，但 SageMaker 笔记本支持在笔记本外部运行明确定义的数据处理作业，以提高吞吐量和可重用性。让我们看看如何从笔记本上运行 Spark 作业。

## 通过 SageMaker 笔记本运行 Spark 作业

一旦笔记本准备就绪，您就可以使用`sagemaker.processing`模块配置一个 Spark 作业，并使用一组计算资源执行它。SageMaker 提供了`PySparkProcessor`类，该类为 Spark 作业提供了一个句柄([https://SageMaker . readthe docs . io/en/stable/Amazon _ SageMaker _ processing . html # data-processing-with-Spark](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark))。它的构造函数接受基本的设置细节，比如作业的名称和 Python 版本。它接受三个参数——`framework_version`、`py_version`和`container_version`——这些参数用于固定预先构建的火花容器以运行处理作业。自定义图像可通过`image_uri`参数注册并可用。`image_uri`将覆盖`framework_version`、`py_version`和`container_version`参数:

```

From sagemaker.processing import PySparkProcessor, ProcessingInput

# ecr image URI

ecr_image_uri = '664544806723.dkr.ecr.eu-central-1.amazonaws.com/linear-learner:latest'

# create PySparkProcessor instance with initial job setup

spark_processor = PySparkProcessor(

    base_job_name="my-sparkjob", # job name

    framework_version="2.4", # tensorflow version

    py_version="py37", # python version

    container_version="1", # container version

    role="myiamrole", # IAM role

    instance_count=2, # ec2 instance count

    instance_type="ml.c5.xlarge", # ec2 instance type

    max_runtime_in_seconds=1200, # maximum run time

    image_uri=ecr_image_uri # ECR image

)
```

在前面的代码中，使用了一个`PySparkProcessor`类来创建一个 Spark 实例。它接受`base_job_name`(作业名:`my-sparkjob`)、`framework_version`(tensor flow 框架版本:`2.0`)、`py_version`(Python 版本:`py37`)、`container_version`(容器版本:`1`)、`role`(SageMaker 的 IAM 角色:`myiamrole`)、`instance_count`(EC2 实例数:`2`)、`instance_type`(EC2 实例类型:`ml.c5.xlarge`)、`max_runtime_in_second`(超时前的最大运行时间(秒:)

接下来我们将讨论`PySparkProcessor`的`run`方法，它通过 Spark 启动提供的脚本:

```

# input s3 path

path_input = "s3://mybucket/input/"

# output s3 path

path_output = "s3://mybucket/output/"

# run method to execute job

spark_processor.run(

    submit_app="process.py", # processing python script

    arguments=['input', path_input, # input argument for script

               'output', path_output # input argument for script

              ])
```

在前面的代码中，`PySparkProcessor`的`run`方法执行给定的脚本，以及提供的参数。它接受`submit_app`(用 Python 编写的数据处理作业)和参数。在本例中，我们已经定义了输入数据的位置和输出数据的存储位置。

## 通过 SageMaker 笔记本运行自定义容器中的作业

在此部分，我们将讨论如何从自定义映像运行数据处理作业。为此，SageMaker 提供了作为`sagemaker.processing`模块一部分的`Processor`类。在这个例子中，我们将使用`ProcessingInput`和`ProcessingOutput`类分别创建输入和输出对象。这些对象将被传递给`Processor`实例的`run`方法。`run`方法执行数据处理作业:

```

# ecr image URI

ecr_image_uri = '664544806723.dkr.ecr.eu-central-1.amazonaws.com/linear-learner:latest'

# input data path

path_data = '/opt/ml/processing/input_data'

# output data path

path_data = '/opt/ml/processing/processed_data'

# s3 path for source

path_source = 's3://mybucket/input'

# s3 path for destination

path_dest = 's3://mybucket/output'

# create Processor instance

processor = Processor(image_uri=ecr_image_uri, # ECR image

               role='myiamrole', # IAM role

               instance_count=1, # instance count

               instance_type="ml.m5.xlarge" # instance type

           )

# calling "run" method of Processor instance

processor.run(inputs=[ProcessingInput(

                 source=path_source, # input source

                 destination=path_data # input destination)],

              outputs=[ProcessingOutput(

                 source=path_data, # output source

                 destination=path_dest # output destination)], ))
```

在前面的代码中，首先，我们创建一个`Processor`实例。它接受`image_uri`(ECR 映像的 URL 路径:`ecr_image_uri`)、`role`(访问 ECR 映像的 IAM 角色:`myiamrole`)、`instance_count`(EC2 实例计数:`1`)和`instance_type`(EC2 实例类型:`ml.m5.xlarge`)。`Processor`实例的`run`方法可以执行作业。它接受`inputs`(作为`ProcessingInput`对象传递的输入数据)和`outputs`(作为`ProcessingOutput`对象传递的输出数据)。虽然`Processor`提供了一组与`PySparkProcessor`类似的方法，但主要的区别在于`run`函数所接受的内容；`PySparkProcessor`接收一个运行 Spark 操作的 Python 脚本，而`Processor`接收一个支持各种数据处理作业的 Docker 映像。

愿意深究细节的，推荐阅读[https://docs . AWS . Amazon . com/sage maker/latest/DG/build-your-own-processing-container . html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html)。

要记住的事情

a.SageMaker 是一个完全托管的基础设施，用于构建、培训和部署 ML 模型。

b.SageMaker 提供了一组预定义的开发环境，用户可以根据自己的需要动态地改变这些环境。

c.SageMaker 笔记本支持通过`sagemaker.processing`模块在笔记本外部定义的数据处理作业。

了解了 AWS 中四种最流行的 ETL 工具之后，让我们一起比较一下这四个选项。

# 比较 AWS 中的 ETL 解决方案

到目前为止，我们已经查看了使用 AWS 建立 ETL 管道的四种不同方式。在本节中，我们将在一个表格中总结四种设置(*表 5.1* )。一些比较点包括对无服务器架构的支持、内置调度程序的可用性以及所支持的 EC2 实例类型的多样性。

| **支撑** | **单节点****EC2 实例** | **胶水** | **EMR** | **SageMaker** |
| 支持无服务器架构 | 不 | 是 | 不 | 不 |
| 为开发人员之间的协作提供内置工作空间 | 不 | 不 | 是 | 不 |
| 各种 EC2 实例类型 | 更大的 | 较少的 | 更大的 | 更大的 |
| 内置调度程序的可用性 | 不 | 是 | 不 | 是 |
| 内置作业监控用户界面的可用性 | 不 | 是 | 不 | 是 |
| 内置模型监控的可用性 | 不 | 不 | 不 | 是 |
| 支持从模型开发到部署的全面托管服务 | 不 | 不 | 不 | 是 |
| 用于分析已处理数据的内置可视化工具的可用性 | 不 | 不 | 不 | 是 |
| ETL 逻辑开发的预定义环境的可用性 | 是 | 不 | 是 | 是 |

表 5.1–各种数据处理设置的比较–单节点 EC2 实例、Glue、EMR 和 SageMaker

正确的设置取决于技术和非技术因素，包括数据源、数据量、MLOps 的可用性和成本。

要记住的事情

a.我们在本章中描述的四种 ETL 设置有明显的优势。

b.选择特定设置时，必须考虑各种因素:数据源、数据量、MLOps 的可用性和成本。

# 总结

DL 项目的困难之一来自于数据量。由于训练 DL 模型需要大量数据，因此数据处理步骤会占用大量资源。因此，在这一章中，我们学习了如何利用最流行的云服务 AWS 来高效地处理数 TB 和数 Pb 的数据。该系统包括调度程序、数据存储、数据库、可视化以及用于运行 ETL 逻辑的数据处理工具。

我们花了额外的时间来研究 ETL，因为它在数据处理中起着重要的作用。我们介绍了 Spark，它是最流行的 ETL 工具，并描述了使用 AWS 设置 ETL 作业的四种不同方式。这四个设置包括使用单节点 EC2 实例、EMR 集群、Glue 和 SageMaker。每种设置都有明显的优势，正确的设置可能会因情况而异。这是因为您需要同时考虑项目的技术和非技术方面。

类似于数据量如何成为处理数据的问题，它也在训练模型时引入了多个问题。在下一章，你将学习如何使用分布式系统有效地训练模型。