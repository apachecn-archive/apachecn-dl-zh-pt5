

# 八、简化深度学习模型部署

部署在生产环境中的**深度学习** ( **DL** )模型通常与刚从培训过程中出来的模型不同。它们通常会得到增强，以最高的性能处理传入的请求。然而，目标环境通常过于宽泛，因此有必要进行大量定制，以涵盖差异极大的部署设置。要克服这个困难，可以利用**开放神经网络交换** ( **ONNX** )，这是 ML 模型的标准文件格式。在这一章中，我们将介绍如何利用 ONNX 在 DL 框架之间转换 DL 模型，以及它如何将模型开发过程与部署分开。

在本章中，我们将讨论以下主要话题:

*   ONNX 简介
*   TensorFlow和 ONNX 之间的转换
*   PyTorch 和 ONNX 之间的转换

# 技术要求

可以从以下 GitHub 链接下载本章的补充资料:[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 8](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_8)。

# ONNX 简介

有各种各样的 DL 框架可以用来训练一个 DL 模型。然而，*DL 模型部署中的一个主要困难来自于这些框架之间缺乏互操作性*。例如，PyTorch 和**tensor flow**(**TF**)之间的转换会带来许多困难。

在许多情况下，利用底层硬件提供的加速，为部署环境进一步扩充 DL 模型，以提高准确性并减少推理延迟。不幸的是，这需要广泛的软件和硬件知识，因为每种类型的硬件为运行的应用程序提供不同的加速。常用于 DL 的硬件包括**中央处理器**(**CPU**)**图形处理器**(**GPU**)**联想处理器**(**APU**)**张量处理器**(**TPU**)**现场可编程门阵列** ( **FPGA** )

这个过程不是一次性的操作；一旦以任何方式更新了模型，这个过程可能需要重复。为了减少该领域的工程工作，一组工程师共同努力，提出了一个中介器，它标准化了模型组件:`.onnx`文件，该文件跟踪模型是如何设计的，以及网络中的每个操作是如何链接到其他组件的。`.onnx`档([https://github.com/lutzroeder/netron](https://github.com/lutzroeder/netron))。以下是可视化示例:

![Figure 8.1 – Netron visualization for an ONNX file
](img/B18522_08_01.jpg)

图 8.1–ONNX 文件的 Netron 可视化

如你所见，ONNX 是培训框架和部署环境之间的一层。虽然 ONNX 文件定义了一种交换格式，但是还存在 **ONNX 运行时** ( **ORT** )，它支持 ONNX 模型的硬件无关加速。换句话说，ONNX 生态系统允许您选择任何 DL 框架进行培训，并轻松实现针对部署的硬件优化:

![Figure 8.2 – The position of ONNX in a DL project
](img/B18522_08_02.jpg)

图 8.2–ONNX 在 DL 项目中的位置

总之，ONNX 帮助完成以下任务:

*   简化各种 DL 框架之间的模型转换
*   为 DL 模型提供硬件无关的优化

在下一节中，我们将进一步了解 ORT。

## 使用 ONNX 运行时运行推理

ORT旨在支持直接使用 ONNX 模型进行训练和推理，而无需将它们转换成特定的框架。然而，训练并不是 ORT 的主要用例，所以在这一节中，我们将重点关注后一个方面，即推理。

ORT 利用不同的硬件加速库，即所谓的**执行提供者** ( **EPs** )，来改善各种硬件架构的延迟和准确性。不管模型训练期间使用的 DL 框架和底层硬件如何，ORT 推理代码都将保持不变。

下面的代码片段是 ONNX 推理代码的示例。完整的细节可以在 https://onnxruntime.ai/docs/get-started/with-python.html[找到:](https://onnxruntime.ai/docs/get-started/with-python.html)

```

import onnxruntime as rt

providers = ['CPUExecutionProvider'] # select desired provider or use rt.get_available_providers()

model = rt.InferenceSession("model.onnx", providers=providers)

onnx_pred = model.run(output_names, {"input": x}) # x is your model's input
```

`InferenceSession`类接受一个文件名、一个序列化的 ONNX 模型或一个字节字符串形式的 ORT 模型。在前面的例子中，我们指定了 ONNX 文件的名称(`"model.onnx"`)。`providers`参数和按优先级排序的执行提供者列表(比如`CPUExecutionProvider`、`TvmExecutionProvider`、`CUDAExecutionProvider`等等)是可选的，但是很重要，因为它们定义了将要应用的硬件加速的类型。在最后一行中，`run`函数触发了模型预测。`run`函数有两个主要参数:`output_names`(模型输出的名称)和`input_feed`(包含您想要运行模型预测的输入名称和值的输入字典)。

要记住的事情

a.ONNX 为 ML 模型提供了标准化的跨平台表示。

b.ONNX 可以用来将一个 DL 框架中实现的 DL 模型转换成另一个 DL 框架，只需很少的努力。

c.ORT 为部署的模型提供硬件无关的加速。

在接下来的两节中，我们将看看使用 TF 和 PyTorch 创建 ONNX 模型的过程。

# TensorFlow和 ONNX 之间的转换

首先，我们来看看 TF 和 ONNX 之间的转换。我们将把这个过程分成两部分:将 TF 模型转换成 ONNX 模型，并将 ONNX 模型转换回 TF 模型。

## 将TensorFlow模型转换为 ONNX 模型

`tf2onnx`用于将 TF 车型转换为【https://github.com/onnx/tensorflow-onnx】ONNX 车型。这个库支持 TF 的两个版本(版本 1 和版本 2)。此外，还可以转换成特定于部署的 TF 格式，如 TensorFlow.js 和 TensorFlow Lite。

要将使用`saved_model`模块生成的 TF 模型转换成 ONNX 模型，可以使用`tf2onnx.convert`模块，如下所示:

```
python -m tf2onnx.convert --saved-model tensorflow_model_path --opset 9 --output model.onnx  
```

在前面的命令中，`tensorflow-model-path`指向一个保存在磁盘上的 TF 模型，`--output`定义生成的 ONNX 模型将保存在哪里，`--opset`将 ONNX 设置为`opset`，定义 ONNX 版本和操作符([https://github.com/onnx/onnx/releases](https://github.com/onnx/onnx/releases))。如果您的 TF 模型不是使用`tf.saved_model.save`功能保存的，您需要如下指定输入和输出格式:

```
# model in checkpoint format
python -m tf2onnx.convert --checkpoint tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0
# model in graphdef format
python -m tf2onnx.convert --graphdef tensorflow_model_graphdef-file --output model.onnx --inputs input0:0,input1:0 --outputs output0:0 
```

前面的命令描述了检查点([https://www . tensor flow . org/API _ docs/python/TF/train/check point](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint))和 graph def([https://www . tensor flow . org/API _ docs/python/TF/compat/v1/graph def](https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef))格式的模型转换。关键参数是`--checkpoint`和`--graphdef`，它们表示模型格式以及源模型的位置。

在 https://github.com/onnx/tensorflow-onnx 你可以找到`tf2onnx`提供的 Python API。

接下来，我们将看看如何将 ONNX 模型转换为 TF 模型。

## 将 ONNX 模型转换为TensorFlow模型

`tf2onnx`用于从 TF 转换成 ONNX，`onnx-tensorflow`([https://github.com/onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow))用于将 ONNX 模型转换成 TF 模型。与`tf2onnx`情况下的一样，是基于终端命令的。下面一行显示了一个简单的`onnx-tf`命令用例:

```
onnx-tf convert -i model.onnx -o tensorflow_model_file
```

在前面的命令中，`-i`参数用于指定源`.onnx`文件，而`-o`参数用于指定新 TF 模型的输出位置。`onnx-tf`命令的其他用例在[https://github . com/onnx/onnx-tensor flow/blob/main/doc/CLI . MD](https://github.com/onnx/onnx-tensorflow/blob/main/doc/CLI.md)中有很好的记录。

此外，您可以使用 Python API 执行相同的转换:

```

import onnx

from onnx_tf.backend import prepare

onnx_model = onnx.load("model.onnx") 

tf_rep = prepare(onnx_model)  

tensorflow-model-file-path = path/to/tensorflow-model

tf_rep.export_graph(tensorflow_model_file_path)
```

在前面的 Python 代码中，ONNX 模型是使用`onnx.load`函数加载的，然后使用从`onnx_tf.backend`导入的`prepare`进行转换调整。最后，使用`export_graph`函数将 TF 模型导出并保存到指定的位置(`tensorflow_model_file_path`)。

要记住的事情

a.从 TF 到 ONNX 和从 ONNX 到 TF 的转换分别通过`onnx-tensorflow`和`tf2onnx`执行。

b.`onnx-tensorflow`和`tf2onnx`都支持命令行接口，并提供 Python API。

接下来，我们将描述如何在 PyTorch 中执行 ONNX 和 ONNX 之间的转换。

# 【PyTorch 和 ONNX 之间的转换

在本节中，我们将解释如何将 PyTorch 模型转换为 ONNX 模型，以及如何将 ONNX 模型转换为 py torch 模型。上一节已经介绍了 TF 和 ONNX 之间的转换，到本节结束时，您应该能够在 TF 和 PyTorch 之间转换您的模型。

## 将 PyTorch 模型转换为 ONNX 模型

有趣的是，PyTorch 有内置支持将其模型导出为 ONNX 模型([https://py torch . org/tutorials/advanced/super _ resolution _ with _ ONNX runtime . html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html))。给定一个模型，您所需要的就是下面代码片段中所示的`torch.onnx.export`函数:

```

import torch

pytorch_model = ...

# Input to the model

dummy_input = torch.randn(..., requires_grad=True)

onnx_model_path = "model.onnx"

# Export the model

torch.onnx.export(

    pytorch_model,       # model being run

    dummy_input,         # model input (or a tuple for multiple inputs)

    onnx_model_path      # where to save the model (can be a file or file-like object) )
```

`torch.onnx.export`的第一个参数是您想要转换的 PyTorch 模型。作为第二个参数，您必须提供一个表示虚拟输入的张量。换句话说，这个张量必须是模型期望作为输入的大小。最后一个参数是 ONNX 模型的本地路径。

触发`torch.onnx.export`功能后，您应该会看到在您提供的路径(`onnx_model_path`)下生成了一个`.onnx`文件。

现在，让我们看看如何将 ONNX 模型加载为 PyTorch 模型。

## 将 ONNX 模型转换为 PyTorch 模型

不幸的是，PyTorch 没有内置支持加载 ONNX 模型。然而，对于这种转换，有一个流行的库叫做`onnx2pytorch`([https://github.com/ToriML/onnx2pytorch](https://github.com/ToriML/onnx2pytorch))。假设这个库是用一个`pip`命令安装的，下面的代码片段演示了这个转换:

```

import onnx

from onnx2pytorch import ConvertModel

onnx_model = onnx.load("model.onnx")

pytorch_model = ConvertModel(onnx_model)
```

我们从`onnx2pytorch`模块中需要的关键类是`ConverModel`。如前面的代码片段所示，我们将 ONNX 模型传递到该类中，以生成 PyTorch 模型。

要记住的事情

a.PyTorch 内置了将 PyTorch 模型导出为 ONNX 模型的支持。这个过程涉及到`torch.onnx.export`功能。

b.将 ONNX 模型导入 PyTorch 环境需要`onnx2pytorch`库。

在本节中，我们描述了 ONNX 和 PyTorch 之间的转换。既然我们已经知道如何在 ONNX 和 TF 之间转换一个模型，那么 TF 和 PyTorch 之间的转换也就水到渠成了。

# 总结

在这一章中，我们介绍了 ONNX，一种 ML 模型的通用表示。ONNX 的好处主要来自它的模型部署，因为它通过 ORT 在幕后为我们处理特定于环境的优化和转换。ONNX 的另一个优势来自于它的互操作性；它可以用于将一个框架生成的 DL 模型转换为其他框架。在本章中，我们专门讨论了 TensorFlow 和 PyTorch 的转换，因为它们是两个最标准的 DL 框架。

向高效的 DL 模型部署迈进一步，在下一章中，我们将学习如何使用**弹性 Kubernetes 服务** ( **EKS** )和 SageMaker 来建立一个模型推理端点。