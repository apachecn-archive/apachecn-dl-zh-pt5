

# 十一、移动设备上的深度学习

在本章中，我们将介绍如何分别使用**tensor flow Lite**(**TF Lite**)和 **PyTorch Mobile** 在移动设备上部署使用 **TensorFlow** ( **TF** )和 **PyTorch** 开发的**深度学习** ( **DL** )模型。首先，我们将讨论如何将 TF 模型转换为 TF Lite 模型。然后，我们将解释如何将 PyTorch 模型转换成 PyTorch Mobile 可以使用的 TorchScript 模型。最后，本章的最后两节将介绍如何将转换后的模型集成到 Android 和 iOS 应用程序(apps)中。

在本章中，我们将讨论以下主要话题:

*   为移动设备准备 DL 模型
*   使用 DL 模型创建 iOS 应用程序
*   使用 DL 模型创建 Android 应用程序

# 为移动设备准备 DL 模型

移动设备通过让人们能够轻松访问互联网，重塑了我们的日常生活方式；我们的许多日常任务严重依赖于移动设备。因此，如果我们能够在移动应用程序上部署 DL 模型，我们应该能够实现更高水平的便利性。常见的用例包括不同语言之间的翻译、对象检测和数字识别等。

以下截图提供了一些使用案例示例:

![Figure 11.1 – From left to right, the listed apps handle plant identification, 
object detection, and machine translation, exploiting the flexibility of DL
](img/B18522_11_011_Merged.jpg)

图 11.1–从左到右，列出的应用处理植物识别、物体检测和机器翻译，充分利用了 DL 的灵活性

移动设备有许多**操作系统**(**OS**)。然而，目前有两种操作系统主导着移动市场:iOS 和 Android。iOS 是苹果设备的操作系统，比如 iPhone 和 iPad。同样，Android 是三星和谷歌等公司生产的设备的标准操作系统。在这一章中，我们主要关注针对两种主流操作系统的部署。

不幸的是，TF 和 PyTorch 模型不能以其原始格式部署在移动设备上。我们需要将它们转换成可以在移动设备上运行推理逻辑的格式。在 TF 的情况下，我们需要一个 TF Lite 模型；我们将首先讨论如何使用`tensorflow`库将 TF 模型转换成 TF Lite 模型。另一方面，PyTorch 涉及 PyTorch 移动框架，它只能使用 TorchScript 模型。在 TF Lite 转换之后，我们将学习如何将 PyTorch 模型转换为 TorchScript 模型。此外，我们将解释如何针对目标移动环境优化 PyTorch 模型的某些层。

值得注意的是，一个 TF 模型或者 PyTorch 模型可以转换成**开放神经网络交换** ( **ONNX** )运行时并部署在移动([https://onnxruntime.ai/docs/tutorials/mobile](https://onnxruntime.ai/docs/tutorials/mobile))上。此外，SageMaker 为将 DL 模型加载到 edge 设备上提供了内置支持:sage maker Edge Manager([https://docs . AWS . Amazon . com/sage maker/latest/DG/Edge-getting-started-step 4 . html](https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step4.html))。

## 生成一个 TF Lite 模型

TF Lite 是一个用于在移动设备、微控制器和其他边缘设备([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite))上部署模型的库。一个经过训练的 TF 模型需要被转换成一个 TF Lite 模型才能在 edge 设备上运行。如下面的代码片段所示，`tensorflow`库具有将 TF 模型转换为 TF Lite 模型的内置支持(一个`.tflite`文件):

```

import tensorflow as tf

# path to the trained TF model

trained_model_dir = "s3://mybucket/tf_model"

# TFLiteConverter class is necessary for the conversion

converter = tf.lite.TFLiteConverter.from_saved_model(trained_model_dir)

tfl_model = converter.convert()

# save the converted model to TF Lite format 

with open('model_name.tflite', 'wb') as f:

  f.write(tfl_model)
```

在前面的 Python 代码中，`tf.lite.TFLiteConverter`类的`from_saved_model`函数加载一个经过训练的 TF 模型文件。这个类的`convert`方法将加载的 TF 模型转换成 TF Lite 模型。

正如第 10 章*提高推理效率*中所讨论的，TF Lite 对模型压缩技术有着多样的支持。TF Lite 提供的流行技术包括网络修剪和网络量化。

接下来，让我们看看如何将 PyTorch 模型转换成 PyTorch Mobile 的 TorchScript 模型。

## 生成 TorchScript 模型

在移动设备上运行 PyTorch 模型可以通过使用 py torch 移动框架[https://pytorch.org/mobile/home/](https://pytorch.org/mobile/home/)来实现。与 TF 的情况类似，为了使用 PyTorch Mobile([https://pytorch.org/docs/master/jit.html](https://pytorch.org/docs/master/jit.html))运行模型，必须将训练好的 py torch 模型转换成 TorchScript 模型。为 TorchScript 开发的`torch.jit`模块的主要优势是能够在 Python 环境之外运行 PyTorch 模块，比如 C++环境。这在将 DL 模型部署到移动设备时非常重要，因为它们不支持 Python，但支持 C++。`torch.jit.script`方法将给定 DL 模型的图形导出到一个可以在 C++环境中执行的低级表示中。有关跨语言支持的完整详细信息，请访问[https://py torch . org/docs/stable/JIT _ language _ reference . html # language-reference](https://pytorch.org/docs/stable/jit_language_reference.html#language-reference)。请注意，TorchScript 仍处于测试阶段。

为了从 PyTorch 模型获得 TorchScript 模型，您需要将训练好的模型传递给`torch.jit.script`函数，如下面的代码片段所示。通过使用`torch.utils.mobile_optimizer`模块的`optimize_for_mobile`方法([https://pytorch.org/docs/stable/mobile_optimizer.html](https://pytorch.org/docs/stable/mobile_optimizer.html))融合`Conv2D`和`BatchNorm`层或者移除不必要的`Dropout`层，TorchScript 模型可以进一步针对移动环境进行优化。请记住，`mobile_optimizer`方法也处于测试状态:

```

import torch

from torch.utils.mobile_optimizer import optimize_for_mobile

# load a trained PyTorch model

saved_model_file = "model.pt"

model = torch.load(saved_model_file)

# the model should be in evaluate mode for dropout and batch normalization layers

model.eval()

# convert the model into a TorchScript model and apply optimization for mobile environment

torchscript_model = torch.jit.script(model)

torchscript_model_optimized = optimize_for_mobile(torchscript_model)

# save the optimized TorchScript model into a .pt file 

torch.jit.save(torchscript_model_optimized, "mobile_optimized.pt")
```

在前面的例子中，我们首先在内存(`torch.load("model.pt")`)中加载训练好的模型。转换时，模型应处于`eval`模式。在下一行中，我们使用`torch.jit.script`函数将 PyTorch 模型转换成 TorchScript 模型(`torchscript_model`)。TorchScript 模型使用`optimize_for_mobile`方法针对移动环境进行了进一步优化；它生成一个优化的 TorchScript 模型(`torch_script_model_optimized`)。优化的 TorchScript 模型可以使用`torch.jit.save`方法保存为独立的`.pt`文件(`mobile_optimized.pt`)。

要记住的事情

a.在移动设备上运行 TF 模型涉及到 TF Lite 框架。需要将训练好的模型转换成 TF Lite 模型。来自`tensorflow.lite`库的`TFLiteCoverter`类用于转换。

b.在移动设备上运行 PyTorch 模型涉及 PyTorch 移动框架。鉴于 PyTorch Mobile 只支持 TorchScript 模型，需要使用 torch.jit 库将训练好的模型转换成 `TorchScript`模型。

接下来，我们将学习如何将 TF Lite 和 TorchScript 模型集成到一个 iOS 应用程序中。

# 使用 DL 模型创建 iOS 应用

在本节中，我们将介绍如何为 iOS 应用程序的 TF Lite 和 TorchScript 模型编写推理代码。虽然 Swift 和 Objective-C 是 iOS 的本地语言并且可以在单个项目中一起使用，但我们将主要关注 Swift 用例，因为它现在比 Objective-C 更受欢迎。

如果我们解释 iOS 应用程序开发的每一步，这一章会很长。因此，我们将基础知识归入苹果提供的官方教程:[https://developer.apple.com/tutorials/app-dev-training](https://developer.apple.com/tutorials/app-dev-training)。

## 在 iOS 上运行 TF Lite 模型推理

在本节中，我们将展示如何使用 TF Lite 的原生 iOS 库`TensorFlowLiteSwift`([https://github . com/tensor flow/tensor flow/tree/master/tensor flow/Lite/swift](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/swift))将 TF Lite 模型加载到 iOS 应用程序中。安装`TensorFlowLiteSwift`可以通过 CocoaPods 来实现，cocoa pods 是 iOS app 开发的标准包管理器([https://cocoapods.org](https://cocoapods.org))。要在 macOS 上下载 CocoaPods，可以在终端上运行`brew install cocoapods`命令。每个 iOS 应用程序开发都涉及一个 Podfile，其中列出了应用程序开发所依赖的库，必须将`TensorFlowLiteSwift`库添加到该文件中，如以下代码片段所示:

```

pod 'TensorFlowLiteSwift'
```

要在 Podfile 中安装所有的库，可以运行`pod install`命令。

以下步骤描述了如何为您的 iOS 应用程序加载 TF Lite 模型并运行推理逻辑。有关执行的完整详细信息，请访问[https://www . tensor flow . org/lite/guide/inference # load _ and _ run _ a _ model _ in _ swift](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift):

1.  可以使用`import`关键字:

    ```
    import TensorFlowLite
    ```

    加载已安装的库
2.  通过提供输入 TF Lite 模型的路径来初始化一个`Interpreter`类:

    ```
    let interpreter = try Interpreter(modelPath: modelPath) 
    ```

3.  为了将输入数据传递给模型，您需要使用`self.interpreter.copy`方法将输入数据复制到索引`0` :

    ```
    let inputData: Data inputData = ... try self.interpreter.copy(inputData, toInputAt: 0)
    ```

    处的输入`Tensor`对象中
4.  一旦输入的`Tensor`对象准备好了，就可以使用`self.interpreter.invoke`方法运行推理逻辑:

    ```
    try self.interpreter.invoke()
    ```

5.  生成的输出可以使用`self.interpreter.output`作为`Tensor`对象进行检索，该对象可以使用`UnsafeMutableBufferPointer`类进一步反序列化为数组:

    ```
    let outputTensor = try self.interpreter.output(at: 0) let outputSize = outputTensor.shape.dimensions.reduce(1, {x, y in x * y}) let outputData = UnsafeMutableBufferPointer<Float32>.allocate(capacity: outputSize) outputTensor.data.copyBytes(to: outputData)
    ```

在本节中，我们学习了如何在 iOS 应用程序中运行 TF Lite 模型推理。接下来，我们将介绍如何在一个 iOS 应用程序中运行 TorchScript 模型推理。

## 在 iOS 上运行 TorchScript 模型推理

在本节中，我们将学习如何使用 PyTorch Mobile 在 iOS 应用程序上部署 TorchScript 模型。我们将从一个 Swift 代码片段开始，使用`TorchModule`模块加载一个经过训练的 TorchScript 模型。PyTorch Mobile 需要的库叫做`LibTorch_Lite`。这个库也可以通过 CocoaPods 获得。您需要做的就是将下面一行添加到 Podfile 中:

```

pod 'LibTorch_Lite', '~>1.10.0'
```

如上一节所述，您可以运行`pod install`命令来安装库。

鉴于 TorchScript 模型是为 C++设计的，Swift 代码不能直接运行模型推理。为了弥补这个差距，有了`TorchModule`类，一个用于`torch::jit::mobile::Module`的 Objective-C 包装器。要在您的应用程序中使用该功能，需要在项目下创建一个名为`TorchBridge`的文件夹，其中包含`TorchModule.mm` (Objective-C 实现文件)、`TorchModule.h`(头文件)和一个带有`-Bridging-Header.h`后缀命名约定的桥接头文件(允许 Swift 加载 Objective-C 库)。完整的示例设置可以在[https://github . com/py torch/IOs-demo-app/tree/master/hello world/hello world/hello world/torch bridge](https://github.com/pytorch/ios-demo-app/tree/master/HelloWorld/HelloWorld/HelloWorld/TorchBridge)找到。

在以下步骤中，我们将展示如何加载 TorchScript 模型并触发模型预测:

1.  首先，您需要将`TorchModule`类导入到项目:

    ```
    #include "TorchModule.h"
    ```

2.  接下来，通过提供 TorchScript 模型文件的路径来实例化`TorchModule`:

    ```
    let modelPath = "model_dir/torchscript_model.pt" let module = TorchModule(modelPath: modelPath)
    ```

3.  `TorchModule`类的`predict`方法处理模型推理。需要向`predict`方法提供一个输入，输出将被返回。在罩下，`predict`方法会通过 Objective-C 包装器调用模型的`forward`函数。代码如下面的代码片段所示:

    ```
    let inputData: Data inputData = ... let outputs = module.predict(input: UnsafeMutableRawPointer(&inputData))
    ```

如果你对推理实际上如何在幕后工作感到好奇，我们建议你阅读[https://pytorch.org/mobile/ios/](https://pytorch.org/mobile/ios/)的*运行推理*部分。

要记住的事情

a.Swift 和 Objective-C 是开发 iOS 应用的标准语言。一个项目可以包含用两种语言编写的文件。

b.`TensorFlowSwift`库是 Swift 的 TF 库。`Interpreter`类支持 iOS 上的 TF Lite 模型推断。

c.`LibTorch_Lite`库通过`TorchModule`类支持 iOS 应用上的 TorchScript 模型推断。

接下来，我们将介绍如何在 Android 上为 TF Lite 和 TorchScript 模型运行推理。

# 使用 DL 模型创建 Android 应用程序

在本节中，我们将讨论 Android 如何支持 TF Lite 和 PyTorch Mobile。基于 Java 和 **Java 虚拟机** ( **JVM** )的语言(例如 Kotlin)是 Android 应用的首选语言。在本节中，我们将使用 Java。Android 应用程序开发的基础知识可以在 https://developer.android.com[找到。](https://developer.android.com)

我们首先关注使用`org.tensorflow:tensorflow-lite-support`库在 Android 上运行 TF Lite 模型推理。然后，我们讨论如何使用`org.pytorch:pytorch_android_lite`库运行 TorchScript 模型推断。

## 在 Android 上运行 TF Lite 模型推理

首先，让我们看看如何使用 Java 在 Android 上运行一个 TF Lite 模型。`org.tensorflow:tensorflow-lite-support`库用于在 Android 应用程序上部署 TF Lite 模型。该库支持 Java、C++(测试版)和 Swift(测试版)。支持环境的完整列表可以在 https://github.com/tensorflow/tflite-support[找到。](https://github.com/tensorflow/tflite-support)

Android 应用程序开发涉及 Gradle，一个管理依赖关系的构建自动化工具([https://gradle.org](https://gradle.org))。每个项目都有一个`.gradle`文件，它用基于 JVM 的语言(比如 Groovy 或 Kotlin)指定项目规范。在下面的代码片段中，我们在`dependencies`部分列出了项目所依赖的库:

```

dependencies {

     implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'

}
```

在前面的 Groovy Gradle 代码中，我们指定了`org.tensorflow:tensorflow-lite-support`库作为我们的依赖项之一。在[https://docs . Gradle . org/current/samples/sample _ building _ Java _ applications . html](https://docs.gradle.org/current/samples/sample_building_java_applications.html)中可以找到一个 grad le 示例文件。

在下面的步骤中，我们将看看如何加载一个 TF Lite 模型并运行推理逻辑。您可以在[https://www . tensor flow . org/lite/API _ docs/Java/org/tensor flow/lite/Interpreter](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter)中找到关于此过程的完整详细信息:

1.  首先是导入`org.tensorflow.lite`库，其中包含用于 TF Lite 模型推断的`Interpreter`类:

    ```
    import org.tensorflow.lite.Interpreter;
    ```

2.  然后，我们可以通过提供一个模型路径来实例化`Interpreter`类:

    ```
    let tensorflowlite_model_path = "tflitemodel.tflite"; Interpreter = new Interpreter(tensorflowlite_model_path);
    ```

3.  `Interpreter`类实例的`run`方法用于运行推理逻辑。它只接受类型`HashMap`的一个`input`实例，并且只提供`HashMap` :

    ```
    Map<> input = new HashMap<>(); Input = ... Map<> output = new HashMap<>(); interpreter.run(input, output);
    ```

    的一个`output`实例

在下一节中，我们将学习如何将 TorchScript 模型加载到 Android 应用程序中。

## 在 Android 上运行 TorchScript 模型推理

在这一节中，我们将解释如何在 Android 应用程序中运行 TorchScript 模型。要在 Android 应用中运行 TorchScript 模型推理，您需要一个由`org.pytorch:pytorch_android_lite`库提供的 Java 包装器。同样，您可以在`.gradle`文件中指定必要的库，如下面的代码片段所示:

```

dependencies {

    implementation 'org.pytorch:pytorch_android_lite:1.11'

}
```

在 Android 应用程序中运行 TorchScript 模型推理可以通过以下步骤实现。关键是使用来自`org.pytorch`库的`Module`类，该类调用一个 C++函数在后台进行推理([https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html](https://pytorch.org/javadoc/1.9.0/org/pytorch/Module.html)):

1.  首先，你需要导入`Module`类:

    ```
    import org.pytorch.Module;
    ```

2.  `Module`类提供了一个`load`函数，通过加载提供的模型文件

    ```
    let torchscript_model_path = "model_dir/torchscript_model.pt"; Module = Module.load(torchscript_model_path);
    ```

    来创建一个模块实例
3.  `Module`实例的`forward`方法用于运行推理逻辑并生成`org.pytorch.Tensor` :

    ```
    Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();
    ```

    类型的输出

虽然前面的步骤涵盖了`org.pytorch`模块的基本用法，但是您可以从它们的官方文档中找到其他细节:[https://pytorch.org/mobile/android](https://pytorch.org/mobile/android)。

要记住的事情

a.基于 Java 和 JVM 的语言(例如 Kotlin)是 Android 应用程序的本地语言。

b.`org.tensorflow:tensorflow-lite-support`库用于在 Android 上部署一个 TF Lite 模型。`Interpreter`类实例的`run`方法处理模型推理。

c.`org.pytorch:pytorch_android_lite`库是为在 Android 应用中运行 TorchScript 模型而设计的。来自`Module`类的`forward`方法处理推理逻辑。

这就完成了 DL 模型在 Android 上的部署。现在，您应该能够将任何 TF 和 PyTorch 模型集成到一个 Android 应用程序中。

# 总结

在本章中，我们介绍了如何将 TF 和 PyTorch 模型集成到 iOS 和 Android 应用程序中。本章一开始，我们描述了从 TF 模型到 TF Lite 模型以及从 PyTorch 模型到 TorchScript 模型的必要转换。接下来，我们提供了加载 TF Lite 和 TorchScript 模型以及在 iOS 和 Android 上使用加载的模型运行推理的完整示例。

在下一章，我们将学习如何关注已部署的模型。我们将看看为模型监控开发的一套工具，并描述如何有效地监控部署在**亚马逊弹性 Kubernetes 服务** ( **亚马逊 EKS** )和亚马逊 SageMaker 上的模型。