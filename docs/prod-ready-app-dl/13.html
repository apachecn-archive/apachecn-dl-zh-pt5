<html><head/><body>





	

		<title>B18522_10</title>

		

	

	

		<div><h1 id="_idParaDest-200" class="chapter-number"><a id="_idTextAnchor212"/> 10</h1>

			<h1 id="_idParaDest-201"><a id="_idTextAnchor213"/>提高推理效率</h1>

			<p>当一个<strong class="bold">深度学习</strong> ( <strong class="bold"> DL </strong>)模型部署在边缘设备上时，推理效率往往不尽如人意。这些问题主要来自训练网络的规模，因为它需要大量的计算。因此，当在边缘设备上部署DL模型时，许多工程师和科学家经常为了速度而牺牲准确性。此外，由于边缘设备通常具有有限的存储空间，他们专注于减小模型大小。</p>

			<p>在这一章中，我们将介绍在尽可能保持原有性能的同时改善推理延迟的技术。首先，我们将介绍<strong class="bold">网络量化</strong>，这是一种通过对模型参数使用较低精度的数据格式来减小网络规模的技术。接下来我们就来说说<strong class="bold">权重分享</strong>，也就是大家熟知的权重聚类。这是一个非常有趣的概念，其中一些模型权重值在整个网络中共享，减少了存储训练模型所需的磁盘空间。我们还将讨论<strong class="bold">网络修剪</strong>，它涉及到消除网络中不必要的连接。虽然这三种技术是最受欢迎的，但我们还将介绍另外两个有趣的主题:<strong class="bold">知识提炼</strong>和<strong class="bold">网络架构搜索</strong>。这两种技术通过在训练期间直接修改网络体系结构来实现模型尺寸减小和推理延迟改进。</p>

			<p>在本章中，我们将讨论以下主要话题:</p>

			<ul>

				<li>网络量化——减少用于模型参数的位数</li>

				<li>重量共享–减少不同重量值的数量</li>

				<li>网络修剪–消除网络中不必要的连接</li>

				<li>知识提炼——通过模仿预测获得一个更小的网络</li>

				<li>网络架构搜索–寻找最高效的网络架构</li>

			</ul>

			<h1 id="_idParaDest-202"><a id="_idTextAnchor214"/>技术要求</h1>

			<p>你可以从本书的GitHub资源库下载本章的补充材料，网址为<a href="https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_10">https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 10</a>。</p>

			<p>在我们深入研究个别技术之前，我们想介绍两个构建在<strong class="bold"> TensorFlow </strong> ( <strong class="bold"> TF </strong>)之上的库。第一个是<strong class="bold">tensor flow Lite</strong>(<strong class="bold">TF Lite</strong>)，处理移动、微控制器和其他边缘设备上的TF模型部署(<a href="https://www.tensorflow.org/lite">https://www.tensorflow.org/lite</a>)。我们将要描述的一些技术只适用于TF Lite。另一个库叫做TensorFlow模型优化工具包。该库旨在为TF模型(<a href="https://www.tensorflow.org/model_optimization">https://www.tensorflow.org/model_optimization</a>)提供各种优化技术。</p>

			<h1 id="_idParaDest-203"><a id="_idTextAnchor215"/>网络量化–减少用于模型参数的位数</h1>

			<p>如果我们详细查看DL <a id="_idIndexMarker995"/>模型训练，您会注意到模型学习处理有噪声的输入。换句话说，该模型试图为其训练的数据构建一个泛化，以便即使传入的数据中有一些噪声，它也可以生成合理的预测。此外，DL模型在训练后使用特定范围的数值进行推理。按照这种思路，网络量化旨在对这些值使用更简单的表示法。</p>

			<p>如图<em class="italic">图10.1 </em>所示，网络量化，也称为模型量化，是将模型与之交互的一系列数值重新映射到可以用更少比特表示的数系的过程——例如，用8比特而不是32比特来表示一个浮点数。这种修改在DL模型部署中带来了额外的优势，因为边缘设备通常缺少对基于32位浮点数的算法的稳定支持:</p>

			<div><div><img src="img/B18522_10_01.jpg" alt="Figure 10.1 – An illustration of the number system remapping from float 32 to int 8 in network quantization&#13;&#10;"/>

				</div>

			</div>

			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图10.1–网络量化中从浮点数32到整数8的数字系统重映射示意图</p>

			<p>不幸的是，网络<a id="_idIndexMarker996"/>量化不仅仅是将一个数字从高精度转换成低精度。这是因为DL模型推理涉及到的算法产生的数字比输入的精度更高。在这一章中，我们将看看网络量化中以不同方式克服挑战的各种选项。如果你有兴趣学习更多关于网络量化的知识，我们推荐<em class="italic">戈拉米等人的《高效神经网络推理量化方法综述</em>。</p>

			<p>网络量化技术可以分为两个领域。第一种是训练后量化，另一种是量化感知训练。前者被设计成量化已经被训练的模型，而后者通过训练具有较低精度的模型来最小化由于量化过程导致的精度下降。</p>

			<p>幸运的是，这两种技术在标准DL框架中都可用:TF和PyTorch。在下面的章节中，我们将看看如何在这些框架中执行网络量化。</p>

			<h2 id="_idParaDest-204"><a id="_idTextAnchor216"/>进行训练后量化</h2>

			<p>首先，我们将看看<a id="_idIndexMarker997"/>TF和PyTorch如何支持<a id="_idIndexMarker998"/>训练后量化。修改很简单，因为它只需要几行额外的代码。先说TF。</p>

			<h3>在TensorFlow中执行训练后量化</h3>

			<p>默认情况下，DL模型使用32位的<a id="_idIndexMarker999"/>浮点用于必要的计算和变量。在下面的例子中，我们将演示<a id="_idIndexMarker1000"/>动态范围量化，其中只有固定参数(如权重)被量化为使用16位而不是32位。请注意，您将需要安装TF Lite，以便在TF:</p>

			<pre class="source-code">

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.target_spec.supported_types = [tf.float16]

tflite_quant_model = converter.convert()</pre>

			<p>通过量子化，我们得到一个TF Lite模型。在前面的代码片段中，我们使用了<code>tf.lite.TFLiteConverter.from_saved_model</code>函数来加载一个经过训练的TF模型，并获得一个量化的TF Lite模型。在我们触发转换之前，我们需要配置一些东西。首先要设置量化模型权重的优化策略(<code>converter.optimizations = [tf.lite.Optimize.DEFAULT]</code>)。然后，我们需要指定我们想要来自量化的16位权重(<code>converter.target_spec.supported_types = [tf.float16]</code>)。当<code>convert</code>功能被触发时，实际的量化发生。在前面的代码中，如果我们没有为<code>supported_types</code>指定一个16位的浮点类型，我们将量化模型使用8位的整数。</p>

			<p>接下来，我们将引入全整数量化，其中模型推理的每个组件(输入、激活以及权重)都被量化到较低的精度。对于这种类型的量化，您需要提供一个代表性的数据集来估计激活的范围。让我们看看下面的例子:</p>

			<pre class="source-code">

import tensorflow as tf

# A set of data for estimating the range of numbers that the inference requires

representative_dataset = …

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.representative_dataset = representative_dataset

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.inference_input_type = tf.int8  # or tf.uint8

converter.inference_output_type = tf.int8  # or tf.uint8

tflite_quant_model = converter.convert()</pre>

			<p>前面的代码几乎是不言自明的。同样，我们使用<code>TFLiteConverter</code>类进行量化。首先，我们配置优化策略(<code>converter.optimizations = [tf.lite.Optimize.DEFAULT]</code>)并提供一个代表性的<a id="_idIndexMarker1002"/>数据集(<code>converter.representative_dataset = representative_dataset</code>)。接下来，我们将TF优化设置为在整数表示中执行。此外，我们需要通过配置<code>target_spec</code>、<code>inference_input_type</code>和<code>inference_output_type</code>来指定输入和输出数据类型。最后一行中的<code>convert</code>函数再次触发量化过程。</p>

			<p>TF中的两种训练后量化在<a href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training">https://www . tensor flow . org/model _ optimization/guide/quantization/post _ training</a>中有透彻的解释。</p>

			<p>接下来我们来看看PyTorch是如何实现训练后量化的。</p>

			<h3>在PyTorch中执行训练后量化</h3>

			<p>在PyTorch的情况下，有两种<a id="_idIndexMarker1003"/>不同的后训练<a id="_idIndexMarker1004"/>量化方法:<strong class="bold">动态量化</strong>和<strong class="bold">静态量化</strong>。它们因量化发生的时间而不同，并且具有不同的优点和缺点。在本节中，我们将提供每个算法的高级描述，以及代码示例。</p>

			<h4>动态量化——在运行时量化模型</h4>

			<p>首先，我们来看看动态<a id="_idIndexMarker1005"/>量化，PyTorch中最简单的量化形式。这种类型的算法提前对权重应用<a id="_idIndexMarker1006"/>量化，而对激活的量化在推断期间动态发生。因此，在模型执行主要通过加载权重来抑制而计算矩阵乘法不成问题的情况下，通常使用动态量化。这种类型的量化通常用于LSTM或变压器网络。</p>

			<p>给定一个训练好的模型，动态量化可以如下实现。完整的示例可从https://py torch . org/tutorials/recipes/recipes/dynamic _ quantization . html获得:</p>

			<pre class="source-code">

import torch

model = …

quantized_model = torch.quantization.quantize_dynamic(

    model,  # the original model

    qconfig_spec={torch.nn.Linear},  # a set of layers to quantize

    dtype=torch.qint8)  # data type which the quantized tensors will be</pre>

			<p>要应用动态量化，需要将训练好的模型传递给<code>torch.quantization.quantize_dynamic</code>函数。另外两个参数指的是将应用量化的一组模块(<code>qconfig_spec={torch.nn.Linear}</code>)和量化张量的目标数据类型(<code>dtype=torch.qint8</code>)。在这个例子中，我们将量化<code>Linear</code>层以使用8位整数。</p>

			<p>接下来，我们来看静态量化。</p>

			<h4>静态量化–使用代表性数据集确定最佳量化参数</h4>

			<p>另一种类型的量化称为<a id="_idIndexMarker1007"/>静态量化。像<a id="_idIndexMarker1008"/>TF的全整数量化一样，这种类型的量化通过使用代表性数据集估计模型与之交互的数字范围，最大限度地降低了模型性能的下降。</p>

			<p>不幸的是，静态量化比动态量化需要更多的编码。首先，您需要在网络之前和之后分别插入<code>torch.quantization.QuantStub</code>和<code>torch.quantization.DeQuantStub</code>运算，以进行必要的张量转换:</p>

			<pre class="source-code">

import torch

# A model with few layers

class OriginalModel(torch.nn.Module):

    def __init__(self):

        super(M, self).__init__()

        # QuantStub converts the incoming floating point tensors into a quantized tensor

        self.quant = torch.quantization.QuantStub()

        self.linear = torch.nn.Linear(10, 20)

        # DeQuantStub converts the given quantized tensor into a tensor in floating point

        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):

        # using QuantStub and DeQuantStub operations, we can indicate the region for quantization

        # point to quantized in the quantized model

        x = self.quant(x)

        x = self.linear(x)

        x = self.dequant(x)

        return x</pre>

			<p>在前面的网络中，我们有一个单一的<code>Linear</code>层，但也有两个额外的操作在<code>__init__</code>函数中初始化:<code>torch.quantization.QuantStub</code>和<code>torch.quantization.DeQuantStub</code>。前一个操作被应用于输入张量以指示<a id="_idIndexMarker1009"/>量化的开始。后一个操作作为<code>forward</code>功能中的最后一个操作，用于指示量化的结束。以下代码片段描述了静态量化的第一步——校准过程:</p>

			<pre class="source-code">

# model is instantiated and trained

model_fp32 = OriginalModel()

…

# Prepare the model for static quantization

model_fp32.eval()

model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')

model_fp32_prepared = torch.quantization.prepare(model_fp32)

# Determine the best quantization settings by calibrating the model on a representative dataset.

calibration_dataset = …

model_fp32_prepared.eval()

for data, label in calibration_dataset:

    model_fp32_prepared(data)</pre>

			<p>前面的代码片段从一个经过训练的模型<code>model_fp32</code>开始。为了将模型转换成校准过程的中间格式，您需要附加一个量化配置(<code>model_fp32.qconfig</code>)并将模型传递给<code>torch.quantization.prepare</code>方法。如果模型推理在服务器实例上运行，您必须将模型的<code>qconfig</code>属性设置为<code>torch.quantization.get_default_qconfig('fbgemm')</code>。如果目标环境是一个移动设备，您必须将<code>'qnnpack'</code>传递给<code>get_default_qconfig</code>函数。校准过程可以通过将代表性数据集传递给生成的模型<code>model_fp32_prepared</code>来实现。</p>

			<p>最后一步是将校准模型转换成量化模型:</p>

			<pre class="source-code">

model_int8 = torch.quantization.convert(model_fp32_prepared)</pre>

			<p>前面一行代码中的<code>torch.quantization.convert</code>操作量化校准模型(<code>model_fp32_prepared</code>)并生成模型的量化版本(<code>model_int8</code>)。</p>

			<p>关于静态量化的其他细节可以在<a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">https://py torch . org/tutorials/advanced/static _ quantization _ tutorial . html</a>找到。</p>

			<p>在下一节中，我们将描述如何在TF和PyTorch中执行量化感知训练。</p>

			<h2 id="_idParaDest-205"><a id="_idTextAnchor217"/>执行量化感知训练</h2>

			<p>训练后量化<a id="_idIndexMarker1011"/>可以显著减小模型<a id="_idIndexMarker1012"/>的大小。但是，这也可能会显著降低模型的准确性。因此，下面的问题出现了:我们能恢复一些损失的准确性吗？这个问题的答案可能是<strong class="bold">量化感知训练</strong> ( <strong class="bold"> QAT </strong>)。在这种情况下，模型在训练之前被量化，以便它可以使用较低精度的权重和激活直接学习泛化。</p>

			<p>首先，我们来看看如何在TF中实现这一点。</p>

			<h3>张量流中的量化感知训练</h3>

			<p>TF通过<a id="_idIndexMarker1014"/> TensorFlow模型优化<a id="_idIndexMarker1015"/>工具包提供QAT。下面的代码片段描述了如何在TF中设置QAT:</p>

			<pre class="source-code">

import tensorflow_model_optimization as tfmot

# A TF model

model = … 

q_aware_model = tfmot.quantization.keras.quantize_model(model)

q_aware_model.compile(

              optimizer=...,

              loss=...,

              metrics=['accuracy'])

q_aware_model.fit(...)</pre>

			<p>正如您所看到的，我们已经使用了<code>tfmot.quantization.keras.quantize_model</code>函数为QAT建立了一个模型。输出模型需要使用<code>compile</code>函数进行编译，并且可以使用<code>fit</code>函数进行训练，就像普通的TF模型一样。令人惊讶的是，这正是你所需要的。经过训练的模型将已经被量化，并且应该提供比从训练后量化生成的模型更高的精度。</p>

			<p>更多详情请参考原文档:<a href="https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide">https://www . tensor flow . org/model _ optimization/guide/quantization/training _ comprehensive _ guide</a>。</p>

			<p>接下来，我们将看看PyTorch案例。</p>

			<h3>PyTorch中的量化感知训练</h3>

			<p>PyTorch中的QAT经历类似的<a id="_idIndexMarker1016"/>过程。在整个训练<a id="_idIndexMarker1017"/>过程中，必要的计算是实现被箝位和舍入的数字，以模拟量化的效果。完整的细节可以在<a href="https://pytorch.org/docs/stable/quantization.html#quantization-aware-training-for-static-quantization">https://py torch . org/docs/stable/quantization . html # quantization-aware-training-for-static-quantization</a>找到。让我们看看如何为PyTorch模型设置QAT。</p>

			<p>QAT的设置几乎与我们在<em class="italic">静态量化——使用代表性数据集确定最佳量化参数</em>部分所经历的静态量化相同。对于静态量化和QAT，同样的修改对于模型是必要的；必须将<code>torch.quantization.QuantStub</code>和<code>torch.quantization.DeQuantStub</code>操作插入到模型定义中，以指示量化的区域。主要区别来自网络的中间<a id="_idIndexMarker1018"/>表示，因为QAT涉及<a id="_idIndexMarker1019"/>在整个训练过程中更新模型参数。以下代码片段更好地描述了这种差异:</p>

			<pre class="source-code">

model_fp32 = OriginalModel()

# model must be set to train mode for QAT

model_fp32.train()

model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')

model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)

# train the model

for data, label in train_dataset:

    pred = model_fp32_prepared(data)

    ...

# Generate quantized version of the trained model

model_fp32_prepared.eval()

model_int8 = torch.quantization.convert(model_fp32_prepared)</pre>

			<p>在前面的示例中，我们使用的是在<em class="italic">静态量化中定义的相同网络——使用代表性数据集</em>部分确定最佳量化参数:<code>OriginalModel</code>。对于QAT ( <code>model_fp32.train()</code>)，模型应处于<code>train</code>模式。这里，我们假设模型将被部署在一个服务器实例上:<code>torch.quantization.get_default_qat_qconfig('fbgemm')</code>。在QAT的情况下，模型的中间表示是通过将原始模型传递给<code>torch.quantization.prepare_qat</code>函数来创建的。你需要训练中间表示(<code>model_fp32_prepared</code>)而不是原始模型(<code>model_fp32</code>)。一旦<a id="_idIndexMarker1020"/>训练完成，您可以使用<code>torch.quantization.convert</code>函数生成量化模型。</p>

			<p>总的来说，我们研究了<a id="_idIndexMarker1021"/>TF和PyTorch如何提供QAT，以最大限度地降低量化带来的模型精度下降。</p>

			<p class="callout-heading">要记住的事情</p>

			<p class="callout">a.网络量化是一种简单的技术，通过以较低的精度表示它所处理的数字来减少推断延迟。</p>

			<p class="callout">b.有两种类型的网络量化:训练后量化，将量化应用于已训练的模型QAT，通过以较低精度训练模型来最小化精度下降。</p>

			<p class="callout">c.TF和PyTorch支持训练后量化和QAT，只需对训练代码进行最小的修改。</p>

			<p>在下一节中，我们将研究另一种改进推理延迟的方法:权重共享。</p>

			<h1 id="_idParaDest-206"><a id="_idTextAnchor218"/>重量共享–减少不同重量值的数量</h1>

			<p><strong class="bold">权重共享</strong>或<strong class="bold">权重聚类</strong>是另一种可以显著减小<a id="_idIndexMarker1022"/>模型大小的技术。这种技术背后的想法相当简单:让我们<a id="_idIndexMarker1023"/>将权重分组(或聚类),并使用质心值而不是单个权重值。在这种情况下，我们可以存储每个质心的值，而不是存储权重的每个值。因此，我们可以大大压缩模型大小，并可能加快推理过程。权重共享背后的关键思想在<em class="italic">图10.2 </em>中以图形方式呈现(改编自关于权重聚类API的TF官方博文:<a href="https://blog.tensorflow.org/2020/08/tensorflow-model-optimization-toolkit-weight-clustering-api.html">https://blog . tensor flow . org/2020/08/tensor flow-model-optimization-toolkit-weight-clustering-API . html</a>):</p>

			<div><div><img src="img/B18522_10_02.jpg" alt="Figure 10.2 – An illustration of weight sharing&#13;&#10;"/>

				</div>

			</div>

			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图10.2–重量分担示意图</p>

			<p>让我们先学习如何在TF中执行重量共享，然后再看如何在PyTorch中执行相同的操作。</p>

			<h2 id="_idParaDest-207"><a id="_idTextAnchor219"/>在TensorFlow中执行重量共享</h2>

			<p>TF通过TensorFlow模型优化工具包(<a href="https://www.tensorflow.org/model_optimization/guide/clustering/clustering_example">https://www . tensor flow . org/Model _ Optimization/guide/clustering/clustering _ example</a>)为<code>Sequential</code>和<code>Functional</code> TF模型提供权重<a id="_idIndexMarker1024"/>共享。</p>

			<p>首先，您需要定义集群<a id="_idIndexMarker1025"/>配置，如下面的代码片段所示:</p>

			<pre class="source-code">

import tensorflow_model_optimization as tfmot

# A trained model to compress

tf_model = ...

CentroidInitialization = tfmot.clustering.keras.CentroidInitialization

clustering_params = {

  'number_of_clusters': 10,

  'cluster_centroids_init': CentroidInitialization.LINEAR

}

clustered_model = tfmot.clustering.keras.cluster_weights(tf_model, **clustering_params)</pre>

			<p>如您所见，权重聚类涉及到了<code>tfmot.clustering.keras.cluster_weights</code>函数。我们需要提供经过训练的模型(<code>tf_model</code>)和集群配置(<code>clustering_params</code>)。集群配置定义了集群的数量以及如何初始化每个集群。在本例中，我们正在生成10个已使用线性质心初始化进行初始化的聚类(聚类质心将在<a id="_idIndexMarker1026"/>最小值和最大值之间均匀分布)。其他集群初始化选项可以在<a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/clustering/keras/CentroidInitialization">https://www . tensor flow . org/model _ optimization/API _ docs/python/TF mot/clustering/keras/centrid initialization</a>找到。</p>

			<p>生成具有<a id="_idIndexMarker1027"/>聚类权重的模型后，您可以使用<code>tfmot.clustering.keras.strip_clustering</code>函数删除所有在推理过程中不需要的变量:</p>

			<pre class="source-code">

final_model = tfmot.clustering.keras.strip_clustering(clustered_model) </pre>

			<p>接下来，我们将看看如何在PyTorch中执行重量共享。</p>

			<h2 id="_idParaDest-208"><a id="_idTextAnchor220"/>在PyTorch中执行重量共享</h2>

			<p>可惜PyTorch不支持重量<a id="_idIndexMarker1028"/>共享。相反，我们将提供一个可能实现的高级描述。在本例中，我们<a id="_idIndexMarker1029"/>将尝试实现<em class="italic">图10.2 </em>中描述的操作。首先，我们将向模型实现添加一个名为<code>cluster_weights</code>的自定义函数，您可以在训练后调用它来对权重进行聚类。然后，<code>forward</code>方法需要稍微修改，如下面的代码片段所示:</p>

			<pre class="source-code">

from torch.nn import Module

class SampleModel(Module):

# in the case of PyTorch Lighting, we inherit pytorch_lightning.LightningModule class

  def __init__(self):

    self.layer = …

    self.weights_cluster = … # cluster index for each weight

    self.weights_mapping = … # mapping from a cluster index to a centroid value

  def forward(self, input):

    if self.training: # in training mode

      output = self.layer(input)

    else: # in eval mode

      # update weights of the self.layer by reassigning each value based on self.weights_cluster and self.weights_mapping

    output = self.layer(input)

    return output

def cluster_weights(self):

  # cluster weights of the layer 

  # construct a mapping from a cluster index to a centroid value and store at self.weights_mapping

  # find cluster index for each weight value and store at self.weights_cluster

  # drop the original weights to reduce the model size

# First, we instantiate a model to train

model = SampleModel()

# train the model

…

# perform weight sharing

model.cluster_weights()

model.eval()</pre>

			<p>前面的代码<a id="_idIndexMarker1030"/>应该是不言自明的，因为它是带有注释的<a id="_idIndexMarker1031"/>伪代码，解释了关键操作。首先，模型被当作普通模型来训练。当<code>cluster_weights</code>函数被触发时，权重被聚类，权重共享的必要信息被存储在类内；每个权重的聚类索引是存储在<code>self.weights_cluster</code>中的<a id="_idIndexMarker1032"/>，每个聚类的<a id="_idIndexMarker1033"/>质心值存储在<code>self.weights_mapping</code>中。当模型处于<code>eval</code>模式时，<code>forward</code>操作使用由<code>self.weights_cluster</code>和<code>self.weights_mapping</code>构建的一组不同的权重。此外，您可以添加删除现有权重的功能，以在部署期间减小模型大小。我们在我们的存储库中提供了完整的实现:<a href="https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_10/weight_sharing_pytorch.ipynb">https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter _ 10/weight _ sharing _ py torch . ipynb</a>。</p>

			<p class="callout-heading">要记住的事情</p>

			<p class="callout">a.权重共享通过将不同的权重值分组并用质心值替换它们来减小模型大小。</p>

			<p class="callout">b.TF通过TensorFlow模型优化工具包提供权重共享，PyTorch不提供任何支持。</p>

			<p>接下来，让我们了解另一种流行的技术，叫做网络修剪。</p>

			<h1 id="_idParaDest-209"><a id="_idTextAnchor221"/>网络修剪——消除网络中不必要的连接</h1>

			<p><strong class="bold">网络修剪</strong>是一个优化<a id="_idIndexMarker1034"/>过程，消除不必要的连接。这种技术可以在训练之后应用，但是也可以在训练期间应用，其中可以进一步减少模型精度的降低。连接越少，需要的权重就越少。因此，我们可以减少模型大小以及推理延迟。在接下来的部分中，我们将介绍如何在TF和PyTorch中应用网络修剪。</p>

			<h2 id="_idParaDest-210"><a id="_idTextAnchor222"/>tensor flow中的网络修剪</h2>

			<p>与模型量化和权重<a id="_idIndexMarker1035"/>共享一样，TF的网络修剪可通过TensorFlow模型优化工具包<a id="_idIndexMarker1036"/>获得。因此，对于网络修剪，您需要做的第一件事是导入包含以下代码行的工具包:</p>

			<pre class="source-code">

import tensorflow_model_optimization as tfmot</pre>

			<p>要在训练期间应用网络修剪，您必须使用<code>tfmot.sparsity.keras.prune_low_magnitude</code>函数修改您的模型:</p>

			<pre class="source-code">

# data and configurations for training

x_train, y_train, x_text, y_test, x_valid, y_valid,  num_examples_train, num_examples_test, num_examples_valid  = …

batch_size = ...

end_step = np.ceil(num_examples_train / batch_size).astype(np.int32) * epochs

# pruning configuration

pruning_params = {

      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.3,

final_sparsity=0.5,

begin_step=0,

end_step=end_step)}

#  Prepare a model that will be pruned

model = ...

model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)</pre>

			<p>在前面的代码中，我们通过向<code>prune_low_magnitude</code>函数提供一个模型和一组参数<code>pruning_params</code>来配置网络修剪。正如您所看到的，我们已经应用了<code>PolynomialDecay</code>修剪，它以特定的稀疏度启动网络(<code>initial_sparsity</code>，并在整个训练过程中构建目标稀疏度的网络(<a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay">https://www . tensor flow . org/model _ optimization/API _ docs/python/TF mot/sparsity/keras/polynomial decay</a>)。如最后一行中的<a id="_idIndexMarker1037"/>所示，<code>prune_low_magnitude</code>函数返回另一个在训练期间执行网络修剪的模型。</p>

			<p>在我们查看我们需要对训练循环进行的<a id="_idIndexMarker1038"/>修改之前，我们想介绍另一个修剪配置<code>tfmot.sparsity.keras.ConstantSparsity</code>(<a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/ConstantSparsity">https://www . tensor flow . org/model _ optimization/API _ docs/python/TF mot/sparsity/keras/constants parsity</a>)。这种修剪配置在整个训练过程中应用恒定稀疏修剪。要应用这种类型的网络修剪，您可以简单地修改<code>pruning_params</code>,如下面的代码片段所示:</p>

			<pre class="source-code">

pruning_params = {

      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100) }</pre>

			<p>如下面的代码片段所示，训练循环需要对回调配置进行一次额外的修改；我们需要使用一个Keras回调函数，为每个优化器步骤应用修剪——即<code>tfmot.sparsity.keras.UpdatePruningStep</code>:</p>

			<pre class="source-code">

model_for_pruning.compile(…)

callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]

model_for_pruning.fit(x_train, y_train,

    batch_size=batch_size, epochs=epochs,     validation_data=(x_valid, y_vallid),

    callbacks=callbacks)</pre>

			<p>前面的代码编译了为网络修剪准备的模型，并执行了训练。请记住，键的改变来自为<code>fit</code>函数指定的<code>tfmot.sparsity.keras.UpdatePruningStep</code>回调。</p>

			<p>最后，您可以通过将模型传递给<code>tfmot.sparsity.keras.strip_pruning</code>函数来更新<a id="_idIndexMarker1039"/>训练的模型，以便只记住稀疏权重<a id="_idIndexMarker1040"/>。模型推理不需要的所有<code>tf.Variable</code>实例将被删除:</p>

			<pre class="source-code">

final_tf_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)</pre>

			<p>给出的例子可以直接应用于<code>Functional</code>和<code>Sequential</code> TF模型。要对模型的特定层或子集应用修剪，需要进行以下修改:</p>

			<pre class="source-code">

def apply_pruning_to_dense(layer):

    if isinstance(layer, tf.keras.layers.Dense):

        return tfmot.sparsity.keras.prune_low_magnitude(layer)

    return layer

model_for_pruning = tf.keras.models.clone_model(model, clone_function=apply_pruning_to_dense)</pre>

			<p>首先，我们定义了一个<code>apply_pruning_to_dense</code>包装函数，它将<code>prune_low_magnitude</code>函数应用于目标层。然后，我们需要做的就是将原始模型和<code>apply_pruning_to_dense</code>函数传递给<code>tf.keras.models.clone_model</code>函数，后者通过在给定模型上运行提供的函数来生成新模型。</p>

			<p>值得一提的是，<code>tfmot.sparsity.keras.PrunableLayer</code>抽象类是存在的，它是为定制网络剪枝而设计的。关于这个的更多细节可以在<a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PrunableLayer">https://www . tensor flow . org/model _ optimization/API _ docs/python/TF mot/sparsity/keras/PrunableLayer</a>和<a href="https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide#custom_training_loop">https://www . tensor flow . org/model _ optimization/guide/pruning/comprehensive _ guide # custom _ training _ loop</a>找到。</p>

			<p>接下来，我们将看看如何在<a id="_idIndexMarker1042"/> PyTorch中执行<a id="_idIndexMarker1041"/>修剪。</p>

			<h2 id="_idParaDest-211"><a id="_idTextAnchor223"/>py torch中的网络修剪</h2>

			<p>PyTorch通过<code>torch.nn.utils.prune</code>模块支持训练后的网络剪枝。给定一个训练好的网络，可以通过将模型传递给<code>global_unstructured</code>函数来实现剪枝。一旦模型被修剪，一个二进制<a id="_idIndexMarker1043"/>掩码被附加，它代表被修剪的参数集<a id="_idIndexMarker1044"/>。在<code>forward</code>操作之前，屏蔽应用于目标参数，消除不必要的计算。让我们来看一个例子:</p>

			<pre class="source-code">

# model is instantiated and trained

model = …

parameters_to_prune = (

    (model.conv, 'weight'),

    (model.fc, 'weight')

)

prune.global_unstructured(

    parameters_to_prune,

    pruning_method=prune.L1Unstructured, # L1-norm

    amount=0.2

)</pre>

			<p>如前面的代码片段所示，<code>global_unstructured</code>函数的第一个参数定义了修剪将应用到的网络组件(<code>parameters_to_prune</code>)。第二个参数定义了修剪算法(<code>pruning_method</code>)。最后一个参数<code>amount</code>表示要删除的参数的百分比。在本例中，我们根据L1规范修剪最低的20%的连接。如果你对其他算法感兴趣，你可以在https://pytorch.org/docs/stable/nn.html#utilities<a href="https://pytorch.org/docs/stable/nn.html#utilities">找到完整的列表。</a></p>

			<p>PyTorch还支持每层修剪，以及迭代修剪。你也可以定义一个自定义的修剪算法。上述功能的必要细节可以在https://py torch . org/tutorials/intermediate/pruning _ tutorial . html # pruning-tutorial找到。</p>

			<p class="callout-heading">要记住的事情</p>

			<p class="callout">a.网络修剪是一个优化过程，通过消除网络中不必要的连接来减小模型大小。</p>

			<p class="callout">b.TF和PyTorch都支持模型级和层级网络修剪。</p>

			<p>在本节中，我们描述了如何消除网络中不必要的连接，以改善推理延迟。在下一节中，我们将了解一种称为知识提炼的技术，它生成一个新的模型，而不是修改现有的模型。</p>

			<h1 id="_idParaDest-212"><a id="_idTextAnchor224"/>知识提炼——通过模仿预测获得一个更小的网络</h1>

			<p>2015年，Hinton等人在其名为<em class="italic">在神经网络</em>中提取知识的出版物中首次提出了知识提取的概念。在分类问题中，Softmax <a id="_idIndexMarker1047"/>激活通常被用作网络的最后操作，以概率来表示每个类别的置信度。因为具有最高概率的类被用于最终预测，所以其他类的概率被认为是不重要的。然而，作者认为它们仍然由代表模型如何解释输入的有意义的信息组成。例如，如果两个类不断报告多个样本的相似概率，则这两个类可能有许多共同的特征，这使得两者之间的区别变得困难。当网络很深时，这样的信息变得更有成效，因为它可以从它所看到的数据中提取更多的信息。基于这一思想，作者提出了一种将训练模型的知识转移到更小规模模型的技术:知识提取。</p>

			<p>知识<a id="_idIndexMarker1048"/>升华的过程通常被称为教师与学生分享知识；原始模型被称为教师模型，而较小的模型被称为学生模型。如下图所示，学生模型由两个不同的标签训练而成，这两个标签由单个输入构成。一个标签是基本事实标签，被称为硬标签。另一个标签称为软标签。软标签是教师模型的输出<a id="_idIndexMarker1049"/>概率。知识提炼的主要贡献来自软标签，它填充了硬标签中缺失的信息:</p>

			<div><div><img src="img/B18522_10_03.jpg" alt="Figure 10.3 – Overview of the knowledge distillation process&#13;&#10;"/>

				</div>

			</div>

			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图10.3-知识提炼过程概述</p>

			<p>从评估知识提炼的益处的许多实验中，已经证明使用较小的网络实现可比较的性能是可能的。令人惊讶的是，更简单的网络架构在某些情况下会导致规范化，并导致学生模型比教师模型表现得更好。</p>

			<p>自从这种技术首次出现以来，已经引入了许多变体。第一组变化来自如何定义知识:基于响应的知识(网络输出)、基于特征的知识(中间表示)和基于关系的知识(层或数据样本之间的关系)。另一组变体专注于如何实现知识转移:离线提炼(从预先训练的教师模型中训练学生模型)，在线提炼(在两个模型都被训练时共享知识)，以及自我提炼(在单个网络内共享知识)。我们相信，如果你愿意进一步探索这个领域，由苟等人撰写的一篇名为<em class="italic">知识提炼:一项调查</em>的论文可以是一个很好的起点。</p>

			<p>不幸的是，由于培训设置的复杂性，没有一个框架支持开箱即用的知识提炼。但是，如果模型网络很复杂，而输出结构很简单，那么这仍然是一个很好的选择。</p>

			<p class="callout-heading">要记住的事情</p>

			<p class="callout">a.知识提炼是一种将已训练模型的知识转移到更小规模模型的技术。</p>

			<p class="callout">b.在知识提炼中，原始模型被称为教师模型，而较小的模型被称为学生。学生模型由两个标签训练而成:基础事实标签和教师模型的输出。</p>

			<p>最后，我们介绍一种修改网络结构以减少模型参数数量的技术:网络结构搜索。</p>

			<h1 id="_idParaDest-213"><a id="_idTextAnchor225"/>网络架构搜索–寻找最高效的网络架构</h1>

			<p><strong class="bold">神经架构搜索(NAS) </strong>是为给定问题寻找各层最佳组织的过程。由于可能的网络架构的搜索空间<a id="_idIndexMarker1051"/>非常大，因此评估每个可能的网络架构是不可行的。因此，需要一种聪明的方法来识别有前途的网络体系结构并评估候选者。因此，NAS方法沿着三个不同的方面发展:</p>

			<ul>

				<li><strong class="bold">搜索空间</strong>:如何构造合理大小的搜索空间</li>

				<li><strong class="bold">搜索策略</strong>:如何高效探索搜索空间</li>

				<li><strong class="bold">性能评估策略</strong>:如何在不完全训练模型的情况下高效地评估性能</li>

			</ul>

			<p>尽管NAS是一个快速发展的研究领域，但有一些工具可用于TF和PyTorch模型:</p>

			<ul>

				<li>optuna(<a href="https://dzlab.github.io/dltips/en/tensorflow/hyperoptim-optuna/">https://dz lab . github . io/dl tips/en/tensor flow/hyperoptim-optuna</a>)</li>

				<li>Syne-Tune，可以与SageMaker(<a href="https://aws.amazon.com/blogs/machine-learning/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune/">https://AWS . Amazon . com/blogs/machine-learning/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-Tune</a>)一起使用</li>

				<li>katib(<a href="https://www.kubeflow.org/docs/components/katib/hyperparameter">https://www . kube flow . org/docs/components/katib/hyperparameter</a>)，</li>

				<li><strong class="bold">神经网络智能</strong>(<strong class="bold">NNI</strong>)(<a href="https://github.com/Microsoft/nni/blob/b6cf7cee0e72671672b7845ab24fcdc5aed9ed48/docs/en_US/GeneralNasInterfaces.md#example-enas-macro-search-space">https://github . com/Microsoft/nni/blob/b 6 cf 7 ce E0 e 72671672 b 7845 ab 24 fcdc 5 aed 9 ed 48/docs/en _ US/general nasinterfaces . MD # example-enas-macro-search-space</a>)</li>

				<li>SigOpt(<a href="https://sigopt.com/blog/simple-neural-architecture-search-nas-intel-sigopt/">https://SigOpt . com/blog/simple-neural-architecture-search-nas-Intel-SigOpt</a>)</li>

			</ul>

			<p>NAS实施的简单版本包括从组织的随机层中定义一个搜索空间。然后，我们只需选择性能最佳的型号。为了减少总的搜索时间，我们可以基于特定的评估度量应用早期停止，这将在评估度量不再变化时快速停止训练。这种<a id="_idIndexMarker1052"/>设置将NAS重新表述为一个超参数调优问题，其中模型架构已经成为一个参数。我们可以通过应用以下技术之一来进一步改进搜索算法:</p>

			<ul>

				<li>贝叶斯优化</li>

				<li><strong class="bold">强化学习</strong> ( <strong class="bold"> RL </strong>)</li>

				<li>梯度方法</li>

				<li>基于层次的方法</li>

			</ul>

			<p>如果您想进一步探索这一领域，我们建议您自行实施NAS。首先，您可以利用在<a href="B18522_07.xhtml#_idTextAnchor162"> <em class="italic">第7章</em> </a>、<em class="italic">揭示深度学习模型的秘密</em>中介绍的超参数调整技术。你可以从随机参数搜索或贝叶斯优化方法结合早期停止开始。然后，我们建议研究基于RL的实现。我们也推荐阅读一篇名为<em class="italic">神经结构搜索的全面调查:挑战和解决方案</em>的论文，作者是任等人。</p>

			<p class="callout-heading">要记住的事情</p>

			<p class="callout">a.NAS是为潜在问题寻找最佳网络体系结构的过程。</p>

			<p class="callout">b.NAS由三部分组成:搜索空间、搜索策略和性能评估策略。它包括评估不同架构的网络并找到最佳架构。</p>

			<p class="callout">c.有一些用于NAS的工具:Optuna、Syne-Tune、Katib、NNI和SigOpt。</p>

			<p>在这一部分中，我们介绍了NAS以及它如何生成更小规模的网络。</p>

			<h1 id="_idParaDest-214"><a id="_idTextAnchor226"/>总结</h1>

			<p>在这一章中，我们介绍了一组技术，您可以使用这些技术通过减少模型大小来改善推理延迟。我们介绍了三种最流行的技术，以及TF和PyTorch中的完整示例:网络量化、权重共享和网络修剪。我们还描述了通过直接修改网络架构来减少模型大小的技术:知识提取和NAS。</p>

			<p>在下一章中，我们将解释如何在移动设备上部署TF和PyTorch模型，在这一节中描述的技术可能是有用的。</p>

		</div>

		<div><div/>

		</div>

	



</body></html>