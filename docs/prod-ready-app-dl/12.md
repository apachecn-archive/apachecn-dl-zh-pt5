

# 九、扩展深度学习管道

**亚马逊 Web 服务** ( **AWS** )在**深度学习** ( **DL** )模型部署中开启了许多可能性。在这一章中，我们将介绍两个最流行的服务，它们被设计用于部署一个 DL 模型作为推理端点:**弹性 Kubernetes 服务** ( **EKS** )和 **SageMaker** 。

在前半部分，我们将描述基于 EKS 的方法。首先，我们将讨论如何为**tensor flow**(**TF**)和 PyTorch 模型创建推理端点，并使用 EKS 部署它们。我们还将引入**弹性推断** ( **EI** )加速器，可以在降低成本的同时提高吞吐量。EKS 集群拥有作为 web 服务器托管推理端点的 pod。作为基于 EKS 的部署的最后一个主题，我们将介绍如何针对动态传入流量水平扩展 pod。

在第二部分，我们将介绍基于 SageMaker 的部署。我们将讨论如何为 TF、PyTorch 和 ONNX 模型创建推断端点。此外，端点将使用**亚马逊 SageMaker Neo** 和 EI 加速器进行优化。然后，我们将为运行在 SageMaker 上的推理端点设置自动缩放。最后，我们将通过描述如何在单个 SageMaker 推理端点中托管多个模型来结束本章。

在本章中，我们将讨论以下主要话题:

*   使用弹性 Kubernetes 服务进行推理
*   使用 SageMaker 进行推理

# 技术要求

你可以从本书的 GitHub 资源库下载本章的补充材料，网址是[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 9](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9)。

# 使用弹性 Kubernetes 服务进行推理

通过简化复杂的集群管理流程([https://aws.amazon.com/eks](https://aws.amazon.com/eks))，EKS 旨在为应用部署提供 Kubernetes 集群。创建 EKS 集群的详细步骤可以在[https://docs . AWS . Amazon . com/eks/latest/user guide/create-cluster . html](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html)找到。通常，EKS 集群用于部署任何 web 服务应用程序，并根据需要进行扩展。EKS 上的推理端点只是一个处理模型推理请求的 web 服务应用程序。在本节中，您将学习如何在 EKS 上托管 DL 模型推理端点。

Kubernetes 集群有一个控制平面和一组节点。控制平面根据传入流量做出调度和扩展决策。通过调度，控制平面管理哪个节点在给定的时间点运行作业。通过扩展，控制平面会根据进入端点的流量增加或减少 pod 的大小。EKS 在幕后管理这些组件，以便您可以专注于高效和有效地托管您的服务。

本节首先描述如何设置 EKS 集群。然后，我们将描述如何在 EKS 集群上使用 TF 和 PyTorch 创建端点来处理模型推断请求。接下来，我们将讨论 EI 加速器，它提高了推理性能，同时降低了成本。最后，我们将介绍一种基于传入流量动态扩展服务的方法。

## 准备 EKS 簇

基于 EKS 的模型部署的第一步是创建一组适当的硬件资源。本节我们将使用 AWS 推荐的 GPU Docker 镜像([https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md))。这些标准图像已经在**弹性容器注册表** ( **ECR** )上注册并可用，该注册表为 Docker 图像([https://aws.amazon.com/ecr](https://aws.amazon.com/ecr))提供了一个安全、可扩展且可靠的注册表。接下来，我们应该将 NVIDIA 设备插件应用到容器中。这个插件使**机器学习** ( **ML** )操作能够利用底层硬件来实现更低的延迟。关于 NVIDIA 设备插件的更多细节，我们推荐阅读[https://github.com/awslabs/aws-virtual-gpu-device-plugin](https://github.com/awslabs/aws-virtual-gpu-device-plugin)。

在下面的代码片段中，我们将使用`kubectl`，`kubectl`，您需要提供一个 YAML 文件，其中包含关于集群、用户、名称空间和认证机制的信息([https://kubernetes . io/docs/concepts/configuration/organize-cluster-access-kube config](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig))。最流行的操作是`kubectl apply`，它在 EKS 集群中创建和修改资源:

```
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml
```

在前面的用例中，`kubectl apply`命令根据 YAML 文件中指定的规范将 NVIDIA 设备插件应用到 Kubernetes 集群。

## 配置 EKS

YAML 文件用于配置构成 Kubernetes 集群的机器和集群中运行的应用程序。YAML 文件中的配置可以根据它们的类型分为两部分:*部署*和*服务*。部署部分控制 pod 中运行的应用程序。在本节中，它将用于从 DL 模型中创建一个端点。在 EKS 环境中，在 Kubernetes 集群的一个或多个 pod 上运行的一组应用程序被称为服务。服务部分在集群上创建和配置服务。在整个服务部分，我们将为外部连接可以使用的服务创建一个唯一的 URL，并为传入流量配置负载平衡。

管理 EKS 集群时，名称空间非常有用，因为它们隔离了集群中的一组资源。要创建名称空间，只需使用`kubectl create namespace`终端命令，如下所示:

```
kubectl create namespace tf-inference
```

在前面的命令中，我们为推理端点和服务构建了`tf-inference`名称空间，我们将在下一节中创建这些服务。

## 使用 EKS 上的TensorFlow模型创建推理端点

在这一节中，我们将描述一个 EKS 配置文件(`tf.yaml`)，它被设计成使用一个 TF 模型来托管一个推理端点。端点由 *TensorFlow 服务*创建，这是一个为部署 TF 模型而设计的系统([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving))。由于我们的主要焦点是 EKS 配置，我们将简单地假设一个训练过的 TF 模型已经作为`.pb`文件在 S3 可用。

首先，让我们看看配置的`Deployment`部分，它处理端点创建:

```

kind: Deployment

  apiVersion: apps/v1

  metadata:

    name: tf-inference # name for the endpoint / deployment

    labels:

      app: demo

      role: master

  spec:

    replicas: 1 # number of pods in the cluster

    selector:

      matchLabels:

        app: demo

        role: master
```

正如我们所见，配置的`Deployment`部分从`kind: Deployment`开始。在配置的第一部分，我们提供了一些关于端点的元数据，并通过填写`spec`部分来定义系统设置。

端点最重要的配置在`template`中指定。我们将创建一个端点，可以使用**超文本传输协议** ( **HTTP** )请求以及**远程过程调用** ( **gRPC** )请求来访问。HTTP 是 web 客户端和服务器最基本的传输数据协议。gRPC 建立在 HTTP 之上，是一个以二进制格式发送请求和接收响应的开源协议:

```

    template:      

      metadata:

        labels:

          app: demo

          role: master

      spec:

        containers:

        - name: demo

          image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04 # ECR image for TensorFlow inference

          command:

          - /usr/bin/tensorflow_model_server # start inference endpoint

          args: # arguments for the inference serving

          - --port=9000

          - --rest_api_port=8500

          - --model_name=saved_model

          - --model_base_path=s3://mybucket/models

          ports:

          - name: http

            containerPort: 8500 # HTTP port

          - name: gRPC

            containerPort: 9000 # gRPC port
```

在`template`部分，我们指定了要使用的 ECR 映像(`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04`)、创建 TF 推断端点的命令(`command: /usr/bin/tensorflow_model_server`)、TF 服务的参数(`args`)以及容器的端口配置(`ports`)。

服务于参数的 TF 包含模型的名称(`--model_name=saved_model`)、模型在 S3 上的位置(`--model_base_path=s3://mybucket/models`)、HTTP 访问端口(`--rest_api_port=8500`)和 gRPC 访问端口(`--port=9000`)。`ports`下的两个`ContainerPort`配置用于将端点暴露给外部连接(`containerPort: 8500`和`containerPort: 9000`)。

接下来，让我们看看 YAML 文件的第二部分——即`Service`的配置:

```

kind: Service

  apiVersion: v1

  metadata:

    name: tf-inference # name for the service

    labels:

      app: demo

  spec:

    Ports:

    - name: http-tf-serving

      port: 8500 # HTTP port for the webserver inside the pods

      targetPort: 8500 # HTTP port for access outside the pods

    - name: grpc-tf-serving

      port: 9000 # gRPC port for the webserver inside the pods

      targetPort: 9000 # gRPC port for access outside the pods

    selector:

      app: demo

      role: master

    type: ClusterIP
```

配置的`Service`部分从`kind: Service`开始。在`name: http-tf-serving`部分下面，我们有`port: 8500`，它指的是 TF 服务 web 服务器正在监听的端口在 pods 内部的 HTTP 请求。`targetPort`指定pod 用来暴露相应端口的端口。在`name: grpc-tf-serving`部分，我们有另一组 gRPC 端口配置。

要将配置应用到底层集群，只需将这个 YAML 文件提供给`kubectl apply`命令。

接下来，我们将在 EKS 上为 PyTorch 模型创建一个端点。

## 在 EKS 上使用 PyTorch 模型创建推理端点

在本节中，您将学习如何在 EKS 上创建 PyTorch 模型推理端点。首先，要介绍的是 *TorchServe* ，一个 py torch([https://pytorch.org/serve](https://pytorch.org/serve))的开放源码模型服务框架。它旨在简化 PyTorch 模型的大规模部署过程。PyTorch 模型部署的 EKS 配置与我们在上一节中描述的部署 TF 模型的配置非常相似。

首先需要将一个 PyTorch 模型`.pth`文件转换成一个`.mar`文件，这是 torch serve([https://github . com/py torch/serve/blob/master/model-Archiver/readme . MD](https://github.com/pytorch/serve/blob/master/model-archiver/README.md))要求的格式。使用`torch-model-archiver`包可以实现转换。TorchServe 和`torch-model-archiver`可以通过`pip`下载安装，如下:

```
pip install torchserve torch-model-archiver
```

使用`torch-model-archiver`命令时，转换如以下代码所示:

```
torch-model-archiver --model-name archived_model --version 1.0 --serialized-file model.pth --handler run_inference
```

在前面的代码中，`torch-model-archiver`命令接受`model-name`(输出`.mar`文件的名称，即`archived_model`)、`version`(py torch 1.0 版)、`serialized-file`(输入 PyTorch `.pth`文件，即`model.pth`)和`handler`(定义 TorchServe 推理逻辑的文件的名称；即`run_inference`，表示名为`run_inference.py`的文件。该命令将生成一个`archived_model.mar`文件，该文件将通过 EKS 上传到端点托管的 S3 桶中。

在讨论 EKS 配置之前，我们想介绍的另一个命令是`mxnet-model-server`。该命令在 DLAMI 实例中可用，允许您托管一个 web 服务器，该服务器对传入的请求运行 PyTorch 推理:

```
mxnet-model-server --start --mms-config /home/model-server/config.properties --models archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar
```

在前面的例子中，带有`start`参数的`mxnet-model-server`命令为通过`models`参数提供的模型创建了一个端点。如您所见，`models`参数指向 S3 ( `archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar`)上模型的位置。模型的输入参数在`/home/model-server/config.properties`文件中指定，该文件通过`mms-config`参数传递给命令。

现在，我们将讨论如何填充 EKS 配置的`Deployment`部分。每个组件都可以保持与 TF 车型的版本相似。主要区别来自于`template`部分的，如下面的代码片段所示:

```

       containers:

       - name: pytorch-service

         image: "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04"

         args:

         - mxnet-model-server

         - --start

         - --mms-config /home/model-server/config.properties

         - --models archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar

         ports:

         - name: mms

           containerPort: 8080
```

在前面的代码中，我们使用了安装了 py torch(`image: "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04"`)的不同 Docker 映像。该配置接受`mxnet-model-server`命令来创建一个推理端点。我们将为这个端点使用的端口是`8080`。我们对`Service`部分所做的唯一更改可以在`Ports`部分找到；我们必须确保一个外部端口被分配并连接到端口`8080`，也就是端点所在的端口。同样，您可以使用`kubectl apply`命令来应用更改。

在下一节中，我们将描述如何与 EKS 集群托管的端点进行交互。

## 与 EKS 上的端点通信

既然我们已经有了一个正在运行的端点，我们将解释如何发送一个请求并获取一个推理结果。首先，我们需要使用`kubectl get services`来识别服务的 IP 地址，如下面的代码片段所示:

```
kubectl get services --all-namespaces -o wide
```

前面的命令将返回服务及其外部 IP 地址的列表:

```
NAME         TYPE      CLUSTER-IP   EXTERNAL-IP    PORT(S)  AGE 
tf-inference ClusterIP 10.3.xxx.xxx 104.198.xxx.xx 8500/TCP 54s
```

在这个例子中，我们将利用我们在*中创建的`tf-inference`服务，在 EKS* 部分使用 TensorFlow 模型创建推理端点。从`kubectl get services`的示例输出中，我们可以看到服务正在使用外部 IP 地址`104.198.xxx.xx`运行。要通过 HTTP 访问服务，您需要将 HTTP 的端口附加到 IP 地址:`http://104.198.xxx.xx:8500`。如果您有兴趣为 IP 地址创建一个显式 URL，请访问[https://AWS . Amazon . com/premium support/knowledge-center/eks-kubernetes-services-cluster](https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster)。

要向端点发送一个预测请求并接收一个推断结果，您需要发出一个 POST-typed HTTP 请求。如果您想从终端发送请求，您可以使用如下的`curl`命令:

```
curl -d demo_input.json -X POST http://104.198.xxx.xx:8500/v1/models/demo:predict
```

在前面的命令中，我们将 JSON 数据(`demo_input.json`)发送到端点(`http://104.198.xxx.xx:8500/v1/models/demo:predict`)。输入 JSON 文件`demo_input.json`由以下代码片段组成:

```

{

    "instances": [1.0, 2.0, 5.0]

}
```

我们将从端点接收的响应数据也包含 JSON 数据，如下所示:

```

{

    "predictions": [2.5, 3.0, 4.5]

}
```

关于输入输出 JSON 数据结构的详细解释可以在官方文档中找到:[https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest)。

如果你对使用 gRPC 而不是 HTTP 感兴趣，你可以在[https://AWS . Amazon . com/blogs/open source/the-versatility-of-gRPC-an-open-source-high-performance-RPC-framework](https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework)找到细节。

恭喜你！您已经成功地为您的模型创建了一个您的应用程序可以通过网络访问的端点。接下来，我们将介绍亚马逊 EI 加速器，它可以降低推理延迟和 EKS 成本。

## 使用亚马逊弹性推理提高 EKS 端点性能

在本节中，我们将描述如何使用 EI 加速器创建 EKS 集群，这是一种低成本的 GPU 加速。EI 加速器可以链接到 Amazon EC2 和 Sagemaker 实例或者`eia2.*`类型的实例。eia2 的完整描述。*实例可以在[https://AWS . Amazon . com/machine-learning/elastic-inference/pricing](https://aws.amazon.com/machine-learning/elastic-inference/pricing)找到。

为了充分利用 AWS 资源，您还需要使用 *AWS 神经元*([https://aws.amazon.com/machine-learning/neuron](https://aws.amazon.com/machine-learning/neuron))来编译您的模型。神经元模型的优势在于它们可以利用 Amazon EC2 Inf1 实例。这些类型的机器由 *AWS 推理*组成，这是 AWS 在云端为 ML 设计的定制芯片([https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia))。

AWS Neuron SDK 预装在 AWS DL 容器和**亚马逊机器镜像** ( **AMI** )中。在这一节中，我们将集中讨论 TF 模型。然而，PyTorch 模型编译也经历同样的过程。TF 的详细步骤可以在[https://docs . AWS . Amazon . com/dlami/latest/dev guide/tutorial-inferent ia-TF-neuron . html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html)找到，PyTorch 的步骤可以在[https://docs . AWS . Amazon . com/dlami/latest/dev guide/tutorial-inferent ia-py torch-neuron . html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html)找到。

使用 TF 的`tf.neuron.saved_model.compile`函数可以将 TF 模型编译成神经元模型:

```

import tensorflow as tf

tf.neuron.saved_model.compile(

    tf_model_dir, # input TF model dir 

    neuron_model_dir # output neuron compiled model dir

)
```

对于这个函数，我们只需要提供输入模型所在的位置(`tf_model_dir`)和我们想要存储输出神经元模型的位置(`neuron_model_dir`)。就像我们上传一个 TF 模型到一个 S3 桶来创建端点一样，我们也需要将神经元模型转移到一个 S3 桶。

同样，您需要对 EKS 配置进行的更改只需要在`Deployment`部件的`template`部分完成。以下代码片段描述了配置的更新部分:

```

       containers:

       - name: neuron-demo

         image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04

         command:

         - /usr/local/bin/entrypoint.sh

         args:

         - --port=8500

         - --rest_api_port=9000

         - --model_name=neuron_model

         - --model_base_path=s3://mybucket/neuron_model/

         ports:

         - name: http

           containerPort: 8500 # HTTP port

         - name: gRPC

           containerPort: 9000 # gRPC port
```

我们从前面的配置中注意到的第一件事是，它非常类似于我们在*一节中描述的在 EKS* 上使用TensorFlow模型创建推理端点。差异主要来自于`image`、`command`和`args`段。首先，我们需要使用一个带有 AWS Neuron 和 TensorFlow 服务应用程序的 DL 容器(`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04`)。接下来，模型工件文件的入口点脚本通过`command`键传递:`/usr/local/bin/entrypoint.sh`。入口点脚本用于使用`args`启动 web 服务器。为了从一个神经元模型创建一个端点，我们必须指定 S3 桶，目标神经元模型作为一个`model_base_path`参数(`--model_base_path=s3://mybucket/neuron_model/`)存储在该桶中。

要将更改应用到集群，只需将更新后的 YAML 文件传递给`kubectl apply`命令。

最后，我们将查看 EKS 的自动缩放功能，以提高端点的稳定性。

## 使用自动缩放动态调整 EKS 聚类的大小

EKS 集群可以根据流量自动调整集群的大小。水平 pod 自动伸缩的想法是随着传入请求数量的增加，通过增加 pod 的数量来扩大正在运行的应用程序的数量。类似地，当传入流量减少时，一些 pod 将被释放。

通过`kubectl apply`命令部署应用后，可使用`kubectl autoscale`命令设置自动缩放，如下所示:

```
kubectl autoscale deployment <application-name> --cpu-percent=60 --min=1 --max=10
```

如前面的示例所示，`kubectl autoscale`命令接受在 YAML 文件的`Deployment`部分中指定的应用程序的名称、`cpu-percent`(用于放大或缩小集群大小的截止 CPU 百分比)、`min`(要保留的最小 pod 数量)和`max`(要加速旋转的最大 pod 数量)。总而言之，示例命令将使用 1 到 10 个 pod 来运行服务，这取决于的流量，保持 CPU 使用率为 60%。

要记住的事情

a.EKS 旨在通过简化动态流量的复杂集群管理，为应用部署提供 Kubernetes 集群。

b.YAML 文件用于配置构成 Kubernetes 集群的机器和集群中运行的应用程序。配置的两个部分，`Deployment`和`Service`，分别控制在 pod 内运行的应用程序和为底层目标集群配置服务。

c.可以在 EKS 集群上使用 TF 和 PyTorch 模型创建和托管推理端点。

d.通过利用具有使用 AWS 神经元编译的模型的 EI 加速器，有可能改善推理延迟，同时节省 EKS 集群的操作成本。

b.EKS 集群可以配置为根据流量动态调整自身大小。

在本节中，我们讨论了 TF 和 PyTorch 模型的基于 EKS 的 DL 模型部署。我们描述了如何使用 AWS 神经元模型和 EI 加速器来提高服务性能。最后，我们介绍了自动伸缩，以便更有效地利用可用资源。在下一节中，我们将看看另一个托管推理端点的 AWS 服务:SageMaker。

# 使用 SageMaker 进行推理

在本节中，您将学习如何使用 SageMaker 而不是 EKS 集群来创建端点。首先，我们将描述独立于框架的创建推理端点的方法(T2 类)。然后，我们将看看如何使用`TensorFlowModel`和特定于 TF 的`Estimator`类来创建 TF 端点。下一节将关注使用`PyTorchModel`类和 PyTorch 特定的`Estimator`类为 PyTorch 模型创建端点。此外，我们将介绍如何从 ONNX 模型构建端点。此时，我们应该有一个针对传入请求的服务运行模型预测。之后，我们将描述如何使用 *AWS SageMaker Neo* 和 EI 加速器来提高服务质量。最后，我们将讨论自动伸缩，并描述如何在单个端点上托管多个模型。

如 [*第五章*](B18522_05.xhtml#_idTextAnchor106)*云中数据准备*中*利用 SageMaker 进行 ETL* 部分所述，SageMaker 提供了一个内置的笔记本环境，名为 SageMaker Studio。我们在本节中包含的代码片段将在本笔记本中执行。

## 使用模型类设置推理端点

一般来说，SageMaker 为端点创建提供了三个不同的类。最基础的是`Model`类，支持各种 DL 框架的模型。另一种选择是使用特定于框架的`Model`类。最后一个选项是使用`Estimator`类。在本节中，我们将查看第一个选项，即`Model`类。

在我们深入到端点创建过程之前，我们需要确保已经适当地准备好了必要的组件；必须为 SageMaker 配置正确的 IAM 角色，并且经过训练的模型应该在 S3 上可用。IAM 角色可以按如下方式在笔记本中准备:

```

from sagemaker import get_execution_role

from sagemaker import Session

# IAM role of the notebook

role = get_execution_role()

# A Session object for SageMaker

sess = Session()

# default bucket object

bucket = sess.default_bucket()
```

在前面的代码中，已经设置了 IAM 访问角色和默认存储桶。要加载 SageMaker 笔记本的当前 IAM 角色，可以使用`sagemaker.get_execution_role`功能。要创建 SageMaker 会话，您需要为`Session`类创建一个实例。`Session`实例的`default_bucket`方法将创建一个默认的 bucket，其名称为`sagemaker-{region}-{aws-account-id}`格式。

在将模型上传到 S3 铲斗之前，需要将模型压缩成一个`.tar`文件。以下代码片段描述了如何压缩模型并将压缩后的模型上传到笔记本内的目标 buck et:

```

import tarfile

model_archive = "model.tar.gz"

with tarfile.open(model_archive, mode="w:gz") as archive:

   archive.add("export", recursive=True) 

# model artifacts uploaded to S3 bucket

model_s3_path = sess.upload_data(path=model_archive, key_prefix="model")
```

在前面的代码片段中，压缩是使用`tarfile`库执行的。`Session`实例的`upload_data`方法是，用于将编译后的模型上传到与 SageMaker 会话链接的 S3 桶。

现在，我们准备创建一个`Model`类的实例。在这个特定的例子中，我们将假设模型已经用 TF 训练:

```

from sagemaker.tensorflow.serving import Model

# TF version

tf_framework_version = "2.8"

# Model instance for inference endpoint creation

sm_model = Model(

    model_data=model_s3_path, # S3 path for model

    framework_version=tf_framework_version, # TF version

    role=role) # IAM role of the notebook

predictor = sm_model.deploy(

    initial_instance_count=1, # number of instances used

    instance_type="ml.c5.xlarge")
```

如前面的代码所示，`Model`类的构造函数接受`model_data`(压缩模型文件所在的 S3 路径)、`framework_version`(TF 的一个版本)和`role`(笔记本的 IAM 角色)。`Model`实例的`deploy`方法处理实际的端点创建。它接受`initial_instance_count`(启动端点的实例数量)和`instance_type`(要使用的 EC2 实例类型)。

此外，您可以提供一个已定义的`image`和 drop `framework_version`。在这种情况下，将使用为`image`参数指定的 Docker 图像创建端点。它应该指向 ECR 上的一个图像。

接下来，我们将讨论如何使用创建的端点从笔记本中触发模型推理。`deploy`方法将返回一个`Predictor`实例。如下面的代码片段所示，您可以通过`Predictor`实例的`predict`函数来实现这一点。您需要传递给这个函数的只是一些表示输入的 JSON 数据:

```

input = {

    "instances": [1.0, 2.0, 5.0]

}

results = predictor.predict(input)
```

`predict`函数的输出`results`由 JSON 数据组成，在我们的例子中，如下所示:

```

{

    "predictions": [2.5, 3.0, 4.5]

}
```

`predict`函数支持 JSON、CSV、多维数组等不同格式的数据。如果需要使用 JSON 以外的类型，可以参考[https://sagemaker . readthe docs . io/en/stable/frameworks/tensor flow/using _ TF . html # tensor flow-serving-input-and-output](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output)。

触发模型推理的另一个选项是使用来自`boto3`库中的`SageMaker.Client`类。`SageMaker.Client`类是代表 Amazon SageMaker 服务的低级客户端。在下面的代码片段中，我们创建了一个`SageMaker.Client`的实例，并演示了如何使用`invoke_endpoint`方法访问端点:

```

import boto3

client = boto3.client("runtime.sagemaker")

# SageMaker Inference endpoint name

endpoint_name = "run_model_prediction"

# Payload for inference which consists of the input data

payload = "..."

# SageMaker endpoint called to get HTTP response (inference)

response = client.invoke_endpoint(

   EndpointName=endpoint_name,

   ContentType="text/csv", # content type

   Body=payload # input data to the endpoint)
```

如前面的代码片段所示，`invoke_endpoint`方法接受`EndpointName`(端点的名称；即`run_model_prediction`)、`ContentType`(输入数据的类型；即`"text/csv"`)，和`Body`(模型预测的输入数据；也就是`payload`)。

事实上，许多公司利用亚马逊 API 网关([https://aws.amazon.com/api-gateway](https://aws.amazon.com/api-gateway))和AWS Lambda([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda))以及 SageMaker 端点，在无服务器架构中与部署的模型进行通信。详细设置请参考[https://AWS . Amazon . com/blogs/machine-learning/call-an-Amazon-sage maker-model-endpoint-using-Amazon-API-gateway-and-AWS-lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda)。

接下来，我们将解释创建端点的特定于框架的方法。

## 设置TensorFlow推理端点

在本节中，我们将描述一个专门为 TF 设计的`Model`类——`TensorFlowModel`类。然后，我们将解释如何使用特定于 TF 的`Estimator`类来创建端点。本节代码片段的完整版本可以在[https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 9/sage maker](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker)找到。

### 使用 TensorFlowModel 类设置 TensorFlow 推理端点

`TensorFlowModel`级是为 TF 车型设计的`Model`级。如下面的代码片段所示，该类可以从`sagemaker.tensorflow`模块导入，其用法与`Model`类相同:

```

from sagemaker.tensorflow import TensorFlowModel

# Model instance

sm_model = TensorFlowModel(

   model_data=model_s3_path,

   framework_version=tf_framework_version,

   role=role) # IAM role of the notebook

# Predictor

predictor = sm_model.deploy(

   initial_instance_count=1,

   instance_type="ml.c5.xlarge")
```

`TensorFlowModel`类的构造函数接受与`Model`类的构造函数相同的参数:上传模型的 S3 路径(`model_s3_path`)、TF 框架版本(`Tf_framework_version`)和 SageMaker 的 IAM 角色(`role`)。此外，您可以通过提供`entry_point`来提供一个 Python 脚本，用于对模型推理的输入和输出进行预处理和后处理。在这种情况下，脚本需要命名为`inference.py`。更多详情请参考[https://sage maker . readthe docs . io/en/stable/frameworks/tensor flow/deploying _ tensor flow _ serving . html # proving-python-scripts-for-pre-post-processing](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing)。

作为`Model`的子类，`TensorFlowModel`也通过`deploy`方法提供了一个`Predictor`实例。它的用法与我们在上一节中描述的相同。

接下来，您将学习如何使用`Estimator`类部署您的模型，我们已经在 [*第 6 章*](B18522_06.xhtml#_idTextAnchor133) 、*高效模型训练*中为 SageMaker 上的模型训练介绍了该类。

### 使用估计器类设置TensorFlow推断端点

如第六章[](B18522_06.xhtml#_idTextAnchor133)*、*高效模型训练*的*使用 SageMaker* 训练 TensorFlow 模型一节所介绍的，SageMaker 提供了`Estimator`类，支持 SageMaker 上的模型训练。相同的类可用于创建和部署推理端点。在下面的代码片段中，我们利用为 TF、`sagemaker.tensorflow.estimator.TensorFlow`设计的`Estimator`类来训练 TF 模型，并使用训练好的模型部署端点:*

```

from sagemaker.tensorflow.estimator import TensorFlow

# create an estimator

estimator = TensorFlow(

    entry_point="tf-train.py",

    ...,

    instance_count=1,    

    instance_type="ml.c4.xlarge",

    framework_version="2.2",

    py_version="py37" )

# train the model

estimator.fit(inputs)

# deploy the model and returns predictor instance for inference

predictor = estimator.deploy(

    initial_instance_count=1, 

    instance_type="ml.c5.xlarge")
```

在前面的代码片段中，`sagemaker.tensorflow.estimator.TensorFlow`类接受以下参数:`entry_point`(处理训练的脚本；即`"tf-train.py"`)、`instance_count`(要使用的实例数；即`1`)、`instance_type`(实例的类型；也就是`"ml.c4.xlarge"`)，`framework_version`(一个 PyTorch 版本；也就是`"2.2"`)，和`py_version`(一个 Python 版本；也就是`"py37"`)。`Estimator`实例的`fit`方法执行模型训练。创建和部署端点的关键方法是`deploy`方法，它根据所提供的条件为它训练的模型创建和托管一个端点:即`instance_type` ( `"ml.c5.xlarge"`)的`initial_instance_count` ( `1`)实例。与`Model`类的情况一样，`Estimator`类的`deploy`方法返回一个`Predictor`实例。

在本节中，我们解释了如何在 SageMaker 上为 TF 模型创建一个端点。在下一节中，我们将看看 SageMaker 如何支持 PyTorch 模型。

## 设置 PyTorch 推断端点

本节旨在涵盖从 SageMaker 上的 PyTorch 模型创建和托管端点的不同方式。首先，我们将介绍一个为 PyTorch 模型设计的`Model`类:`PyTorchModel`类。然后，我们将描述 PyTorch 模型的一个`Estimator`类。本节代码片段的完整实现可以在[https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter _ 9/sage maker/py torch-inference . ipynb](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb)找到。

### 使用 PyTorchModel 类设置 PyTorch 推理端点

类似于`TensorFlowModel`类的，还有一个`Model`类是专门为 PyTorch 型号`PyTorchModel`设计的。它可以被如下实例化:

```

from sagemaker.pytorch import PyTorchModel

model = PyTorchModel(

    entry_point="inference.py",

    source_dir="s3://bucket/model",

    role=role, # IAM role for SageMaker

    model_data=pt_model_data, # model file

    framework_version="1.11.0", # PyTorch version

    py_version="py3", # python version

)
```

如前面的代码片段所示，构造函数接收`entry_point`，它为数据定义了定制的预处理和后处理逻辑、`source_dir`(入口点脚本的 S3 路径)、`role`(SageMaker 的 IAM 角色)、`model_data`(模型的 S3 路径)、`framework_version`(py torch 的版本)和`py_version`(Python 的版本)。

因为`PyTorchModel`类继承了`Model`类，所以它提供了`deploy`函数，该函数创建并部署一个端点，如*使用模型类*设置 PyTorch 推断端点一节所述。

接下来，我们将介绍一个为 PyTorch 模型设计的`Estimator`类。

### 使用 Estimator 类设置 PyTorch 推断端点

如果一个训练过的 PyTorch 模型没有可用，那么`sagemaker.pytorch.estimator.PyTorch`类可以用来训练和部署一个模型。训练可以通过`fit`方法实现，如 [*第六章*](B18522_06.xhtml#_idTextAnchor133) 、*高效模型训练*中*使用 SageMaker* 训练 PyTorch 模型部分所述。作为一个`Estimator`类，`sagemaker.pytorch.estimator.PyTorch`类提供了与`sagemaker.tensorflow.estimator.TensorFlow`相同的特性，我们在*使用估算器类*设置TensorFlow推断端点一节中已经介绍过。在下面的代码片段中，我们正在为 PyTorch 模型创建一个`Estimator`实例，训练模型，并创建一个端点:

```

from sagemaker.pytorch.estimator import PyTorch

# create an estimator

estimator = PyTorch(

    entry_point="pytorch-train.py",

    ...,

    instance_count=1,

    instance_type="ml.c4.xlarge",

    framework_version="1.11",

    py_version="py37")

# train the model

estimator.fit(inputs)

# deploy the model and returns predictor instance for inference

predictor = estimator.deploy(

   initial_instance_count=1, 

   instance_type="ml.c5.xlarge")
```

如前面的代码片段所示，`sagemaker.pytorch.estimator.PyTorch`的构造函数接受与为 TF 设计的`Estimator`类相同的一组参数:`entry_point`(处理训练的脚本；即`"pytorch-train.py"`)、`instance_count`(要使用的实例数；即`1`)、`instance_type`(EC2 实例的类型；也就是`"ml.c4.xlarge"`)、`framework_version`(py torch 版；也就是`"1.11.0"`)，和`py_version`(Python 版本；也就是`"py37"`)。模型训练(`fit`方法)和部署(`deploy`方法)的实现方式与*使用估算器类*部分设置TensorFlow推断端点中的示例相同。

在本节中，我们介绍了如何以两种不同的方式部署 PyTorch 模型:使用`PyTorchModel`类和使用`Estimator`类。接下来，我们将学习如何在 SageMaker 上为 ONNX 模型创建端点。

## 从 ONNX 模型设置推理端点

前面章节提到过， [*第八章*](B18522_08.xhtml#_idTextAnchor175) ，*简化深度学习模型部署*，DL 模型往往被转化为**开放神经网络交换** ( **ONNX** )模型进行部署。在这一节中，我们将描述如何在 SageMaker 上部署 ONNX 模型。

最标准的方法是使用基类`Model`。正如在*使用模型类*设置TensorFlow推断端点一节中提到的，`Model`类支持各种类型的 DL 模型。幸运的是，它还提供了对 ONNX 模型的内置支持:

```

from sagemaker.model import Model

# Load an ONNX model file for endpoint creation

sm_model= Model(    

    model_data=model_data, # path for an ONNX .tar.gz file

    entry_point="inference.py", # an inference script

    role=role,

    py_version="py3",

    framework="onnx",

    framework_version="1.4.1", # ONNX version

)

# deploy model

predictor = sm_model.deploy(

   initial_instance_count=1, # number of instances to use

   instance_type=ml.c5.xlarge) # instance type for deploy
```

在前面的例子中，我们在 S3 上有一个训练好的 ONNX 模型。`Model`实例创建中的密钥来自`framework="onnx"`。我们还需要向`framework_version`提供一个 ONNX 框架版本。在本例中，我们使用的是 ONNX 框架版本 1.4.0。其他一切几乎与前面的例子相同。同样，`deploy`函数是为创建和部署端点而设计的；将返回一个`Predictor`实例用于模型预测。

使用`TensorFlowModel`和`PyTorchModel`类从 ONNX 模型创建端点也很常见。下面的代码片段演示了这样的用例:

```

from sagemaker.tensorflow import TensorFlowModel

# Load ONNX model file as a TensorFlowModel

tf_model = TensorFlowModel(    

    model_data=model_data, # path to the ONNX .tar.gz file

    entry_point="tf_inference.py", 

    role=role,

    py_version="py3", # Python version

    framework_version="2.1.1", # TensorFlow version

)

from sagemaker.pytorch import PyTorchModel

# Load ONNX model file as a PyTorchModel

pytorch_model = PyTorchModel(

    model_data=model_data, # path to the ONNX .tar.gz file

    entry_point="pytorch_inference.py",

    role=role,

    py_version="py3", # Python version

    framework_version="1.11.0", # PyTorch version

)
```

前面的代码片段不言自明。这两个类都接受一个 ONNX 模型路径(`model_data`)、一个推理脚本(`entry_point`)、一个 IAM 角色(`role`)、一个 Python 版本(`py_version`，以及每个框架的版本(`framework_version`)。就像`Model`类如何部署端点一样，`deploy`方法将从每个模型中创建并托管一个端点。

虽然端点允许用户在任何时间点获得动态输入数据的模型预测，但在某些情况下，您需要对存储在 S3 存储桶中的整个输入数据进行推断，而不是一个接一个地输入它们。因此，我们将看看如何利用批处理转换来满足这一需求。

## 使用批量转换批量处理预测请求

我们可以使用 SageMaker 的批量转换功能([https://docs . AWS . Amazon . com/sage maker/latest/DG/Batch-Transform . html](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html))在一个队列中的大型数据集上运行推理。使用`sagemaker.transformer.Transformer`类，您可以在没有持久端点的情况下，对 S3 上的任何数据集批量执行模型预测。详细信息包含在下面的代码片段中:

```

from sagemaker import transformer

bucket_name = "my-bucket" # S3 bucket with data

# location of the input data

input_location = "s3://{}/{}".format(bucket_name, "input_data")

# location where the predictions will be stored

batch_output = "s3://{}/{}".format(bucket_name, "batch-results")

# initialize the transformer object

transformer = transformer.Transformer(

   base_transform_job_name="Batch-Transform", # job name

   model_name=model_name, # Name of the inference endpoint

   max_payload= 5, # maximum payload

   instance_count=1, # instance count to start with

   instance_type="ml.c4.xlarge", # ec2 instance type

   output_path=batch_output # S3 for batch inference output)

# triggers the prediction on the whole dataset

tf_transformer = transformer.transformer(

   input_location, # input S3 path for input data

   content_type="text/csv", # input content type as CSV

   split_type="Line" # split type for input as Line)
```

如前面的代码所示，`sagemaker.transformer.Transformer`类接受`base_transformer_job_name`(transformer 作业的作业名)、`model_name`(保存推理管道的模型名)、`max_payload`(允许的最大有效负载，以 MB 为单位)、`instance_count`(开始时 EC2 实例的数量)、`instance_type`(EC2 实例的类型)和`output_path`(存储输出的 S3 路径)。`transformer`方法将触发对指定数据集的模型预测。它接受以下参数:`input_location`(输入数据所在的 S3 路径)，`content_type`(输入数据的内容；即`"text/csv"`)，和`split_type`(这控制如何拆分输入数据；`"Line"`用于将每一行数据作为单独的输入输入到模型中)。现实中也有很多公司利用 SageMaker 处理作业([https://docs . AWS . Amazon . com/sage maker/latest/API reference/API _ processing job . html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html))进行批量推理，这个我们就不细说了。

到目前为止，我们已经了解了sage maker 如何支持托管一个推理端点来处理实时预测请求，并为 S3 上可用的静态数据集批量运行模型预测。在下一节中，我们将描述如何使用 **AWS SageMaker Neo** 来进一步改善部署模型的推理延迟。

## 使用 AWS SageMaker Neo 提高 SageMaker 端点性能

在本节中，我们将解释sage maker 如何通过利用底层硬件资源(EC2 实例或移动设备)来进一步提高应用程序的性能。想法是用**AWS sage maker Neo**([https://aws.amazon.com/sagemaker/neo](https://aws.amazon.com/sagemaker/neo))编译训练好的 DL 模型。编译后，生成的 Neo 模型可以更好地利用底层设备，从而减少推理延迟。AWS SageMaker Neo 支持不同框架(TF、PyTorch、MxNet 和 ONNX)的模型和各种类型的硬件(OS、芯片、架构和加速器)。支持资源的完整列表可在[https://docs . AWS . Amazon . com/sage maker/latest/DG/neo-supported-devices-edge-devices . html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html)找到。

Neo 模型生成可以使用`Model`类的`compile`方法实现。`compile`方法返回一个支持端点创建的`Estimator`实例。让我们看看下面的例子，了解的详细情况:

```

# sm_model created from Model

sm_model = Model(...)

# instance type of which the model will be optimized for

instance_family = "ml_c5"

# DL framework

framework = "tensorflow"

compilation_job_name = "tf-compile"

compiled_model_path = "s3:..."

# shape of an input data

data_shape = {"inputs":[1, data.shape[0], data.shape[1]]}

estimator = sm_model.compile(

   target_instance_family=instance_family,

   input_shape=data_shape,

   ob_name=compilation_job_name,

   role=role,

   framework=framework,

   framework_version=tf_framework_version,

   output_path=compiled_model_path)

# deploy the neo model on instances of the target type

predictor = estimator.deploy(

   initial_instance_count=1,

   instance_type=instance_family)
```

在前面的代码中，我们从一个名为`sm_model`的`Model`实例开始。我们触发`compile`方法将加载的模型编译成 Neo 模型。下表描述了这些参数:

*   `target_instance_family`:模型将被优化的 EC2 实例类型
*   `input_shape`:输入数据形状
*   `job_name`:编译作业的名称
*   `role`:编译后的模型输出的 IAM 角色
*   `framework`:一个 DL 框架，比如 TF 或者 PyTorch
*   `framework_version`:要使用的框架版本
*   `output_path`:编译后的模型将被存储的输出 S3 路径

`Estimator`实例由一个创建端点的`deploy`函数组成。输出是一个`Predictor`实例，您可以使用它来运行模型预测。在前面的例子中，我们优化了我们的模型，使其在`ml_c5`类型的实例上表现最佳。

接下来，我们将描述如何将 EI 加速器集成到运行在 SageMaker 上的端点中。

## 使用 Amazon 弹性推理提高 SageMaker 端点性能

在*使用 Amazon 弹性推理提高 EKS 端点性能*一节中，我们描述了 EI 加速器如何通过利用可用的 GPU 设备来降低推理端点的运营成本，同时改善推理延迟。在本节中，我们将介绍 SageMaker 的 EI 加速器集成。

必要的改变相当简单；你只需要在触发`Model`实例的`deploy`方法时提供`accelerator_type`:

```

# deploying a Tensorflow/PyTorch/other model files using EI

predictor = sm_model.deploy(

   initial_instance_count=1, # ec2 initial count

   instance_type="ml.m4.xlarge", # ec2 instance type

   accelerator_type="ml.eia2.medium" # accelerator type)
```

在前面的代码中，`deploy`方法为给定的`Model`实例创建一个端点。要将 EI 加速器附加到端点，您需要在默认参数(`initial_instance_count`和`instance_type`)之上指定您想要的加速器类型(`accelerator_type`)。关于对 SageMaker 端点使用 EI 的完整描述，请查看[https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html)。

在下一节中，我们将关注 SageMaker 的自动伸缩特性，它允许我们更好地处理传入流量的变化。

## 使用自动缩放动态调整 SageMaker 端点的大小

类似于 EKS 集群如何支持自动缩放以根据流量变化自动放大或缩小端点，SageMaker 也提供自动缩放功能。配置自动缩放包括配置缩放策略，该策略定义何时进行缩放，以及在缩放时创建和销毁多少资源。SageMaker 端点的伸缩策略可以从 SageMaker web 控制台进行配置。以下步骤描述了如何为从 SageMaker 记事本创建的推理端点配置自动缩放:

1.  访问 SageMaker web 控制台、[https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/)，点击左侧导航面板中**推理**下的**端点**。您可能需要提供凭据才能登录。
2.  接下来，您必须选择要配置的端点名称。在**端点运行时间**设置下，选择需要配置的型号变量。这个特性允许您在一个端点中部署一个模型的多个版本，每个版本一个容器。关于这个功能的详细信息可以在[https://docs . AWS . Amazon . com/sage maker/latest/API reference/API _ runtime _ invokeendpoint . html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html)找到。
3.  在**端点运行时**设置下，选择**配置自动缩放**。这将带您进入**配置变量自动缩放**页面:

![Figure 9.1 – The Configure variant automatic scaling page of the SageMaker web console
](img/B18522_09_01.jpg)

图 9.1–sage maker web 控制台的配置变量自动缩放页面

1.  在**最小实例数**字段中输入要维护的最小实例数。最小值为 1。该值定义了将一直保留的最小实例数。
2.  在**最大实例数**字段中输入要维护的扩展策略的最大实例数。该值定义了峰值流量时允许的最大实例数。
3.  填写**sagemakervariantinvocationspeinstance**字段。每个端点可以有多个模型(或模型版本),部署在跨一个或多个 EC2 实例托管的单个端点中。**sagemakervariantinvocationspeinstance**定义了每分钟每个型号变体允许的最大调用次数。该值用于负载平衡。有关计算该字段的正确数字的详细信息，请参见[https://docs . AWS . Amazon . com/sage maker/latest/DG/endpoint-scaling-load test . html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html)。
4.  填写放大冷却时间和缩小冷却时间。这些指示 SageMaker 在检查另一轮缩放之前将等待多长时间。
5.  选中复选框中的**禁用刻度。在流量增加的过程中，作为扩展过程的一部分，会启动更多实例。但是，如果流量在增加后立即下降，这些实例可以在扩大过程中迅速删除。要避免新创建的实例在创建后立即被释放，必须选中此复选框。**
6.  点击**保存**按钮应用配置。

一旦点击**保存**按钮，缩放将应用于所选的模型变型。SageMaker 将根据传入的流量增加和减少实例的数量。关于自动缩放的更多细节，请看一下[https://docs . AWS . Amazon . com/auto scaling/ec2/user guide/as-instance-termination . html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html)。

作为基于 SageMaker 的端点的最后一个主题，我们将描述如何通过单个端点部署多个模型。

## 在单个 SageMaker 推理端点上托管多个模型

SageMaker 支持通过**多模态端点** ( **MME** )在单个端点上部署多个模型。在设置 MME 之前，有几件事你必须记住。首先，如果你想保持低延迟，建议设置多个端点。第二，容器只能从同一个 DL 框架中部署模型。对于有兴趣托管不同框架模型的人，推荐阅读[https://docs . amazonaws . cn/en _ us/sage maker/latest/DG/multi-container-direct . html](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html)。当模型大小相似，并且预期具有相似的延迟时，MEE 效果最佳。

以下步骤描述了如何设置 MME:

1.  使用您的 AWS 凭证访问位于 https://console.aws.amazon.com/sagemaker[的 SageMaker web 控制台。](https://console.aws.amazon.com/sagemaker)
2.  在左侧导航面板的**推断**部分选择**型号**。然后，点击右上角的**创建模型**按钮。
3.  为**型号名称**字段输入一个值。这将用于在 SageMaker 的上下文中唯一地标识目标模型。
4.  使用**amazonseagemakerfullcaccess**IAM 策略选择一个 IAM 角色。
5.  在**容器定义**部分，选择**多模型**选项，并提供推理代码图像的位置和模型工件的位置(参见*图 9.2* ):

![Figure 9.2 – The Multi-modal endpoint configuration page of the SageMaker web console
](img/B18522_09_02.jpg)

图 9.2–sage maker web 控制台的多模式端点配置页面

前一个字段用于部署带有自定义 Docker 映像的模型([https://docs . AWS . Amazon . com/sagemaker/latest/DG/your-algorithms-inference-code . html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html))。在这个字段中，您应该提供图像在 Amazon ECR 中的注册路径。后一个字段指定了模型工件所在的 S3 路径。

1.  另外，填写**容器主机名**字段。这指定了将在其中创建推理代码映像的主机的详细信息。
2.  选择末端的**创建模型**按钮。

一旦 SageMaker 配置了 MME，我们就可以使用来自`boto3`库的`SageMaker.Client`来测试端点，如下面的代码片段所示:

```

import boto3

# Sagemaker runtime client instance

runtime_sagemaker_client = boto3.client("sagemaker-runtime")

# send a request to the endpoint targeting  specific model 

response = runtime_sagemaker_client.invoke_endpoint(

   EndpointName="<ENDPOINT_NAME>",

   ContentType="text/csv",

   TargetModel="<MODEL_FILENAME>.tar.gz",

   Body=body)
```

在前面的代码中，`SageMaker.Client`实例的`invoke_endpoint`函数向创建的端点发送一个请求。`invoke_endpoint`函数接受`EndpointName`(创建的端点的名称)、`ContentType`(请求体中的数据类型)、`TargetModel`(压缩后的模型文件，格式为`.tar.gz`；这用于指定请求将调用的目标模型)，以及`Body`(在`ContentType`中的输入数据)。从调用返回的`response`变量由预测结果组成。关于与端点通信的完整描述，请查看[https://docs . AWS . Amazon . com/sage maker/latest/DG/invoke-multi-model-endpoint . html](https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html)。

要记住的事情

a.SageMaker 通过其内置的`Model`类和`Estimator`类支持端点创建。这些类支持用各种 DL 框架训练的模型，包括 TF、PyTorch 和 ONNX。`Model`专门为 TF 和 PyTorch 框架设计的类也有:`TensorFlowModel`和`PyTorchModel`。

b.一旦使用 AWS SageMaker Neo 编译了模型，该模型就可以更好地利用底层硬件资源，展示更好的推理性能。

c.SageMaker 可以配置为使用 EI 加速器，减少推理端点的操作成本，同时改善推理延迟。

d.SageMaker 包括一个自动缩放功能，可以根据传入流量动态缩放端点。

e.SageMaker 支持通过 MME 在单个端点上部署多个模型。

在本节中，我们已经描述了 SageMaker 为部署 DL 模型作为推理端点而提供的各种特性。

# 总结

在这一章中，我们描述了两个最流行的 AWS 服务，它们是为部署 DL 模型作为推理端点而设计的:EKS 和 SageMaker。对于这两个选项，我们从最简单的设置开始:从 TF、PyTorch 或 ONNX 模型创建推理端点。然后，我们解释了如何使用 EI 加速器、AWS Neuron 和 AWS SageMaker Neo 来提高推理端点的性能。我们还讲述了如何设置自动伸缩来更有效地处理流量的变化。最后，我们讨论了 SageMaker 的 MME 特性，它用于在一个推理端点上托管多个模型。

在下一章中，我们将着眼于各种模型压缩技术:网络量化、权重共享、网络修剪、知识提炼和网络架构搜索。这些技术将进一步提高推理效率。*