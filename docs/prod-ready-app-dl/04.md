

# 3

# 开发强大的深度学习模型

在本章中，我们将描述如何设计和训练一个**深度学习**(**D1**)模型。在前一章描述的笔记本环境中，数据科学家调查各种网络设计和模型训练设置，以生成给定任务的工作模型。本章的主要话题包括 DL 背后的理论以及如何使用最流行的 DL 框架训练一个模型: **PyTorch** 和 **TensorFlow** ( **TF** )。在本章的最后，我们将分解 **StyleGAN** 实现，一个流行的用于图像生成的 DL 模型，来解释如何使用我们在本章中介绍的组件来构建一个复杂的模型。

在本章中，我们将讨论以下主要话题:

*   了解数字图书馆的基本理论
*   理解 DL 框架的组件
*   在 PyTorch 中实现和训练模型
*   在 TF 中实现和训练模型
*   分解一个复杂的、最先进的模型实现

# 技术要求

可以从以下 GitHub 链接下载本章的补充材料:[https://GitHub . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 3](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_3)。

本章中的示例可以在任何安装了必要包的 Python 环境中执行。可以使用上一章介绍的样例环境:[https://github . com/packt publishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter _ 2/docker files](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles)。

# 梳理 DL 的基础理论

正如 [*第一章*](B18522_01.xhtml#_idTextAnchor014)*深度学习驱动项目的有效规划*中所简要描述的，DL 是一种基于**人工神经网络** ( **ANNs** )的**机器学习** ( **ML** )技术。在这一部分，我们的目标是解释人工神经网络是如何工作的，而不会太深入数学。

## DL 是如何工作的？

人工神经网络基本上是一组相互连接的神经元。如*图 3.1* 所示，来自人工神经网络的神经元和来自我们大脑的神经元的行为方式相似。人工神经网络中的每个连接都包含一个可调的参数，称为**权重**。当从神经元 A 到神经元 B 有一个连接时，神经元 A 的输出乘以该连接的权重；加权值成为神经元 b 的输入。 **Bias** 是神经元内另一个可调的参数；一个神经元将所有的输入相加，然后加上偏差。最后一个操作是激活函数，它将计算值映射到不同的范围。新范围中的值是神经元的输出，它根据连接传递给其他神经元。

在整个研究过程中，人们发现神经元群根据它们的组织捕捉不同的模式。一些强大的组织被标准化为**层**，并且已经成为人工神经网络的主要构件，在神经元之间复杂的相互作用之上提供了一个抽象层。

![Figure 3.1 – A comparison of a biological neuron and a mathematical model of an ANN neuron
](img/B18522_03_01.jpg)

图 3.1–生物神经元和人工神经网络神经元数学模型的比较

如上图所述，DL 中的运算基于数值。因此，网络的输入数据必须转换为数值。例如，**红、绿、蓝** ( **RGB** )颜色代码是使用数值表示图像的标准方式。在文本数据的情况下，经常使用词嵌入。类似地，网络的输出将是一组数值。这些值的解释可能因任务和定义而异。

## DL 模型训练

总的来说，训练人工神经网络是一个寻找一组权重、偏差和激活函数的过程,使网络能够从数据中提取有意义的模式。现在，下一个问题如下:*我们如何找到正确的参数集？*许多研究人员试图用各种技术解决这个问题。在所有的试验中，发现的最有效的算法是一种叫做**梯度下降**的优化算法，这是一种寻找局部或全局最小值的迭代过程。

当训练一个 DL 模型时，我们需要定义一个函数，将预测和真实标签之间的差异量化为一个称为**损失**的数值。明确定义损失函数后，我们迭代生成中间预测，计算损失值，并朝着最小损失的方向更新模型参数。

考虑到优化的目标是找到最小的损失，模型参数需要根据**列车组**在坡度相反方向的样本进行更新(见*图 3.2* )。为了计算梯度，网络跟踪在预测过程中计算的中间值(**正向传播**)。然后，从最后一层开始，利用链规则(**反向传播**)计算每个参数的梯度。有趣的是，基于每次迭代中参数的更新方式，模型性能和训练时间会有很大的不同。不同的参数更新规则在优化器的概念中被捕获。DL 中的主要任务之一是选择能够产生具有最佳性能的模型的优化器类型。

![Figure 3.2 – With gradient descent, model parameters will be updated in the opposite direction of the gradient at every iteration
](img/B18522_03_02.jpg)

图 3.2–使用梯度下降，模型参数将在每次迭代中以梯度的相反方向更新

然而，这个过程有一个警告。如果模型被训练以实现特定训练集的最佳性能，则在看不见的数据上的性能可能会恶化。这就是所谓的**过拟合**；该模型专门针对它以前看到的数据进行训练，但无法对新数据做出正确的预测。另一方面，训练不足会导致**不适合**，这是一种模型无法捕捉训练集的基本模式的情况。为了防止这些问题，训练集的一部分被放在一边，用于在整个训练中评估被训练的模型:即**验证集**。总的来说，DL 的训练包括基于训练集更新模型参数的过程，但是选择在验证集上表现最好的模型。最后一种类型的数据集，即**测试集**，代表了模型一旦被部署将与之交互的内容。在模型训练时，测试集可能可用，也可能不可用。测试集的目的是了解经过训练的模型在生产中的表现。为了进一步理解整体训练逻辑，我们可以看看*图 3.3* :

![Figure 3.3 – The steps for training a DL model
](img/B18522_03_03.jpg)

图 3.3–训练 DL 模型的步骤

该图清楚地描述了迭代过程中的步骤以及每种类型的数据集在场景中扮演的角色。

要记住的事情

a.训练人工神经网络是一个寻找一组权重、偏差和激活函数的过程，使网络能够从数据中提取有意义的模式。

b.训练流中有三种类型的数据集。使用训练集更新模型参数，并选择在验证集上产生最佳性能的模型参数。测试集反映了经过训练的模型在部署时将与之交互的数据分布。

接下来，我们将看看设计用来帮助我们进行模型训练的 DL 框架。

# DL 框架的组件

由于不管底层任务如何，模型训练的配置都遵循相同的过程，所以许多工程师和研究人员已经将通用的构建模块整合到框架中。大多数框架通过保持数据加载逻辑和模型定义独立于训练逻辑来简化 DL 模型开发。

## 数据加载逻辑

**数据加载**逻辑包括从在内存中加载原始数据到准备每个样本用于训练和评估的一切。在许多情况下，训练集、验证集和测试集的数据存储在不同的位置，因此它们中的每一个都需要不同的加载和准备逻辑。标准框架将这些逻辑与其他构建模块分开，以便可以使用不同的数据集以动态方式训练模型，而对模型侧的更改最小。此外，框架已经标准化了定义这些逻辑的方式，以提高可重用性和可读性。

## 模型定义

另一个构建块，**模型定义**，指的是 ANN 架构本身和相应的前向和后向传播逻辑。尽管使用算术运算构建模型是一种选择，但标准的框架提供了通用的层定义，用户可以将它们放在一起构建一个复杂的模型。因此，用户负责实例化必要的网络组件，连接组件，并定义模型应该如何进行训练和推理。

在接下来的*在 PyTorch 中实现并训练一个模型*和*在 TF 中实现并训练一个模型*两节中，我们将分别介绍如何在 PyTorch 和 TF 中实例化流行层:密集(线性)、池化、规范化、下降、卷积和递归层。

## 模型训练逻辑

最后，我们需要组合两个组件，并定义训练逻辑的细节。这个包装器组件必须清楚地描述模型训练的基本部分，例如损失函数、学习率、优化器、时期、迭代和批量大小。

损失函数可以根据学习任务的类型分为两大类:**分类损失**和**回归损失**。这两个类别的主要区别来自于输出格式；分类任务的输出是分类的，而回归任务的输出是连续值。出于不同的损失，我们将主要讨论回归损失的**均方误差** ( **均方误差** ) **损失**和**平均绝对误差** ( **MAE** ) **损失**，以及**交叉熵** ( **CE** ) **损失**和**二元交叉熵** (

 ****学习率** ( **LR** )定义了梯度下降在局部最小值方向上的步长。选择LR 速率将有助于该过程更快地收敛，但是如果它太高或太低，则不能保证收敛(参见*图 3.4* ):

![Figure 3.4 – The impact of the LR within gradient descent
](img/B18522_03_04.jpg)

图 3.4–坡度下降中 LR 的影响

说到**优化器**，我们重点关注两个主要的优化器:**随机梯度下降** ( **SGD** )，一个具有固定 LR 的基本优化器，以及**自适应矩估计** ( **Adam** )，一个基于自适应 LR 的优化器，在大多数场景中工作得最好。如果你有兴趣了解不同的优化器和它们背后的数学原理，我们推荐你阅读 Choi 等人([https://arxiv.org/pdf/1910.05446.pdf](https://arxiv.org/pdf/1910.05446.pdf))的一篇调查论文。

单个**时期**表示训练集中的每个样本已经通过网络向前和向后传递，并且网络参数已经更新。在许多情况下，训练集中的样本数量太大，无法在一个队列中通过，因此被分成**小批**。**批量**是指单个小批量的样品数量。假设一组小批量构成了整个数据集，迭代次数指的是模型需要与每个样本交互的梯度更新事件的次数(更准确地说，是小批量的次数)。例如，如果小批量有 100 个样本，总共有 1000 个样本，则需要 10 次迭代来完成一个时期。选择正确的历元数不是一件容易的事情。它根据 LR 和批量等其他训练参数而变化。因此，它通常需要一个试错过程，记住欠拟合和过拟合。

要记住的事情

a.模型训练的组件可以分为数据加载逻辑、模型定义和模型训练逻辑。

b.数据加载逻辑包括从在存储器中加载原始数据到准备每个样本用于训练和评估的所有内容。

c.模型定义是指网络架构及其前向和后向传播逻辑的定义。

d.模型训练逻辑通过将数据加载逻辑和模型定义放在一起来处理实际的训练。

在各种可用的框架中，我们将讨论本书中最流行的两个: **TF** 和 **PyTorch** 。运行在 TF 上的 Keras 在今天已经很流行，而 PyTorch 以其异常的灵活性和简单性被大量用于研究。

# 在 PyTorch 中实现和训练模型

PyTorch 是 Torch 的 Python 库，Lua 的 ML 包。PyTorch 的主要特点包括**图形处理单元** - ( **GPU** -)加速矩阵计算和自动微分，用于构建和训练神经网络。PyTorch 随着代码的执行动态地创建计算图，由于它的灵活性和易用性，以及它在模型训练中的效率，它越来越受欢迎。

建立在 PyTorch 之上的**py torch Lightning**(**PL**)提供了另一个抽象层，隐藏了许多样板代码。新框架通过将 PyTorch 的研究相关组件与工程相关的组件分离，更加关注研究人员。PL 代码通常比 PyTorch 代码更具可扩展性，也更易于阅读。甚至尽管本书中的代码片段更强调 PL，PyTorch 和 PL 共享许多功能，所以大多数组件是可互换的。如果你愿意深入了解细节，我们推荐官方网站，[https://pytorch.org](https://pytorch.org)。

市场上还有 PyTorch 的其他扩展:

*   斯科奇([https://github.com/skorch-dev/skorch](https://github.com/skorch-dev/skorch))——一个 scikit-learn 兼容的神经网络库，它包装了 PyTorch
*   catalyst([https://github.com/catalyst-team/catalyst](https://github.com/catalyst-team/catalyst))–一个 PyTorch 框架，专门用于再现性、快速实验和代码库重用
*   fastai([https://github.com/fastai/fastai](https://github.com/fastai/fastai))——一个不仅为从业者标准化高级组件，也为研究人员交付低级组件的库
*   py torch Ignite([https://pytorch.org/ignite/](https://pytorch.org/ignite/))——一个旨在帮助培训和评估从业者的库

我们不会在本书中介绍这些库，但是如果您是这个领域的新手，您可能会发现它们很有帮助。

现在，让我们深入 PyTorch 和 PL。

## PyTorch 数据加载逻辑

为了可读性和模块化，PyTorch 和 PL 利用一个名为`Dataset`的类进行数据管理，另一个名为`DataLoader`的类用于迭代访问样本。

当`Dataset`类处理获取单个样本时，模型训练成批接受输入数据，并需要重新洗牌以减少模型过度拟合。`DataLoader`通过提供简单的 API 为用户抽象出这种复杂性。此外，它在幕后利用 Python 的多处理特性来加速数据检索。

`Dataset`的子类必须实现的两个核心功能是`__len__`和`__getitem__`。如下面的类大纲所述，`__len__`应该返回样本总数，而`__getitem__`应该返回给定索引的样本:

```

from torch.utils.data import Dataset

class SampleDataset(Dataset):

   def __len__(self):

      """return number of samples"""

   def __getitem__(self, index):

      """loads and returns a sample from the dataset at the given index"""
```

PL 的`LightningDataModule`封装了处理数据所需的所有步骤。关键组件包括下载和清理数据、预处理每个样本以及将每种类型的数据集包装在`DataLoader`内。下面的代码片段描述了如何创建一个`LightningDataModule`类。该类具有用于下载和预处理数据的`prepare_data`函数，以及用于实例化每种类型数据集的`DataLoader`、`train_dataloader`、`val_dataloader`和`test_dataloader`的三个函数:

```

from torch.utils.data import DataLoader

from pytorch_lightning.core.lightning import LightningDataModule

class SampleDataModule(LightningDataModule):

   def prepare_data(self):

       """download and preprocess the data; triggered only on single GPU"""

       ...

   def setup(self):

       """define necessary components for data loading on each GPU"""

       ...

   def train_dataloader(self):

       """define train data loader"""

       return data.DataLoader(

         self.train_dataset, 

           batch_size=self.batch_size, 

           shuffle=True)

   def val_dataloader(self):

       """define validation data loader"""

       return data.DataLoader(

          self.validation_dataset, 

          batch_size=self.batch_size, 

          shuffle=False)             

   def test_dataloader(self):

       """define test data loader"""

       return data.DataLoader(

          self.test_dataset, 

          batch_size=self.batch_size, 

          shuffle=False)
```

`LightningDataModule`的官方文档可以在[https://py torch-lightning . readthe docs . io/en/stable/extensions/data modules . html](https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html)找到。

## PyTorch 模型定义

PL 的主要优势来自`LightningModule`，它将复杂的 PyTorch 代码简化为六个部分:

*   计算(`__init__`)
*   列车回路(`training_step`)
*   验证循环(`validation_step`)
*   测试循环(`test_step`)
*   预测循环(`predict_step`)
*   优化器和 LR 调度器(`configure_optimizers`)

模型架构是计算部分的一部分。必要的层在`__init__`方法中实例化，计算逻辑在`forward`方法中定义。在下面的代码片段中，三个线性层在`__init__`方法中注册到`LightningModule`模块，它们之间的关系在`forward`方法中定义:

```

from pytorch_lightning import LightningModule

from torch import nn

class SampleModel(LightningModule):

   def __init__(self):

      """instantiate necessary layers"""

       self.individual_layer_1 = nn.Linear(..., ...)

       self.individual_layer_2 = nn.Linear(..., ...)

       self.individual_layer_3 = nn.Linear(..., ...)

   def forward(self, input):

       """define forward propagation logic"""

       output_1 = self.individual_layer_1(input)

       output_2 = self.individual_layer_2(output_1)

       final_output = self.individual_layer_3(output_2)

       return final_output
```

定义网络的另一种方式是使用`torch.nn.Sequential`，如下面的代码所示。使用这个模块，一组层可以被组合在一起，并且输出链接被自动实现:

```

class SampleModel(LightningModule):

   def __init__(self):

       """instantiate necessary layers"""

       self.multiple_layers = nn.Sequential(

       nn.Linear(    ,    ),

       nn.Linear(    ,    ),

       nn.Linear(    ,    ))

   def forward(self, input):

       """define forward propagation logic"""

       final_output = self.multiple_layers(input)

       return final_output
```

在前面的代码中，三个线性图层被组合在一起，并存储为一个实例变量`self.multiple_layers`。在`forward`方法中，我们简单地用输入张量触发`self.multiple_layers`，让张量逐个通过每一层。

以下部分旨在介绍流行的层实现。

### PyTorch DL 层

DL 框架的一个主要好处来自各种层定义:梯度计算逻辑已经是层定义的一部分，因此您可以专注于为您的任务找到最佳的模型架构。在这一部分，我们将了解项目中常用的层。如果您感兴趣的层没有包含在本节中，请参考官方文档([https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html))。

#### PyTorch 致密(线性)层

第一种类型的层是`torch.nn.Linear`。顾名思义，它对输入张量应用线性变换。该函数的两个主要参数是`in_features`和`out_features`，它们分别定义了输入和输出张量维度:

```

linear_layer = torch.nn.Linear(

              in_features,   # Size of each input sample

              out_features,  # Size of each output sample)

# N = batch size

# * = any number of additional dimensions

input_tensor = torch.rand(N, *, in_features)

output_tensor = linear_layer(input_tensor) # (N, *, out_features)
```

来自`torch.nn`模块的层实现已经定义了`forward`函数，因此您可以像使用函数一样使用层变量来触发向前传播。

#### PyTorch 池层

池层通常用于缩减张量采样。最流行的两种类型是最大池和平均池。这些层的关键参数是`kernel_size`和`stride`，它们定义了窗口的大小以及它如何为每个池操作移动。

最大池图层通过为每个窗口选择最大值对输入张量进行缩减采样:

```

# 2D max pooling

max_pool_layer = torch.nn.MaxPool2d(

   kernel_size,         # the size of the window to take a max over

   stride=None,         # the stride of the window. Default value is kernel_size

   padding=0,           # implicit zero padding to be added on both sides

   dilation=1,          # a parameter that controls the stride of elements in the window)

# N = batch size

# C = number of channels

# H = height of input planes in pixels

# W = width of input planes in pixels

input_tensor = torch.rand(N, C, H, W)

output_tensor = max_pool_layer(input_tensor) # (N, C, H_out, W_out) 
```

另一方面，平均池层通过计算每个窗口的平均值对输入张量进行下采样:

```

# 2D average pooling

avg_pool_layer = torch.nn.AvgPool2d(

   kernel_size,         # the size of the window to take a max over

   stride=None,         # the stride of the window. Default value is kernel_size

   padding=0,           # implicit zero padding to be added on both sides)

# N = batch size

# C = number of channels

# H = height of input planes in pixels

# W = width of input planes in pixels

input_tensor = torch.rand(N, C, H, W)

output_tensor = avg_pool_layer(input_tensor) # (N, C, H_out, W_out)
```

你可以在[https://pytorch.org/docs/stable/nn.html#pooling-layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)找到其他类型的汇集层。

#### PyTorch 标准化图层

通常用于数据处理，归一化的目的是在不扭曲分布的情况下，将数字数据缩放到一个共同的比例。在 DL 的情况下，归一化层用于训练具有更大数值稳定性的网络([https://py torch . org/docs/stable/nn . html # normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers))。

最受欢迎的规范化图层是批处理规范化图层，它以小批处理的形式缩放一组值。在下面的代码片段中，我们引入了`torch.nn.BatchNorm2d`，这是一个批量标准化层，它是为具有额外通道维度的小型批量 2D 张量设计的:

```

batch_norm_layer = torch.nn.BatchNorm2d(

   num_features,      # Number of channels in the input image

   eps=1e-05,         # A value added to the denominator for numerical stability

   momentum=0.1,      # The value used for the running_mean and running_var computation

   affine=True,       # a boolean value that when set to True, this module has learnable affine parameters)

# N = batch size

# C = number of channels

# H = height of input planes in pixels

# W = width of input planes in pixels

input_tensor = torch.rand(N, C, H, W)

output_tensor = batch_norm_layer(input_tensor) # same shape as input (N, C, H, W)
```

在各种参数中，最主要的是`num_features`，它表示通道的数量。该层的输入是 4D 张量，其中每个索引指示批量大小(`N`)、通道数量(`C`)、图像高度(`H`)和图像宽度(`W`)。

#### PyTorch 脱落层

dropout 层通过将一组值随机设置为零来帮助模型提取一般特征。此操作可防止模型过度适应训练集。话虽如此，PyTorch 的 dropout 层实现主要对单个参数`p`进行操作，该参数控制元素归零的概率:

```

drop_out_layer = torch.nn.Dropout2d(

   p=0.5,  # probability of an element to be zeroed )

# N = batch size

# C = number of channels

# H = height of input planes in pixels

# W = width of input planes in pixels

input_tensor = torch.rand(N, C, H, W)

output_tensor = drop_out_layer(input_tensor) # same shape as input (N, C, H, W)
```

在这个例子中，我们删除了 50%的元素(`p=0.5`)。类似于批量标准化层，`torch.nn.Dropout2d`的输入张量的大小为`N, C, H, W`。

#### PyTorch 卷积层

专门用于图像处理的卷积层设计用于使用滑动窗口技术对输入张量应用卷积运算。在图像处理的情况下，中间数据表示为大小为`N, C, H, W`的 4D 张量，`torch.nn.Conv2d`是标准选择:

```

conv_layer = torch.nn.Conv2d(

   in_channels,         # Number of channels in the input image

   out_channels,        # Number of channels produced by the convolution

   kernel_size,         # Size of the convolving kernel

   stride=1,            # Stride of the convolution

   padding=0,           # Padding added to all four sides of the input.

   dilation=1,          # Spacing between kernel elements)

# N = batch size

# C = number of channels

# H = height of input planes in pixels

# W = width of input planes in pixels

input_tensor = torch.rand(N, C_in, H, W)

output_tensor = conv_layer(input_tensor) # (N, C_out, H_out, W_out)
```

`torch.nn.Conv2d`类的第一个参数`in_channels`表示输入张量中的通道数量。第二个参数`out_channels`表示输出张量中通道的数量，等于滤波器的数量。其他参数`kernel_size`、`stride`和`padding`决定如何对该层执行卷积操作。

#### PyTorch 循环层

循环层是为顺序数据设计的。在各种类型的递归层中，我们将在本节中介绍`torch.nn.RNN`，它将多层 Elman **循环神经网络** ( **RNN** )应用于给定序列([https://online library . Wiley . com/doi/ABS/10.1207/s 15516709 cog 1402 _ 1](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1))。如果你想尝试不同的轮回层，可以参考官方文档:[https://pytorch.org/docs/stable/nn.html#recurrent-layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers):

```

# multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.

rnn = torch.nn.RNN(

   input_size,                     # The number of expected features in the input x

   hidden_size,                    # The number of features in the hidden state h

   num_layers = 1,                 # Number of recurrent layers

   nonlinearity="tanh",            # The non-linearity to use. Can be either 'tanh' or 'relu'

   bias=True,                      # If False, then the layer does not use bias weights

   batch_first=False,              # If True, then the input and output tensors are provided

                                                                                                 # as (batch, seq, feature) instead of (seq, batch, feature)

             dropout=0,                                                # If non-zero, introduces a Dropout layer on the outputs of each RNN layer

                                                                                                 # except the last layer, with dropout probability equal to dropout

   bidirectional=False,             # If True, becomes a bidirectional RNN)

# N = batch size

# L = sequence length

# D = 2 if bidirectionally, otherwise 1

# H_in = input_size

# H_out = hidden_size

rnn = nn.RNN(H_in, H_out, num_layers)

input_tensor = torch.randn(L, N, H_in)

# H_0 = tensor containing the initial hidden state for each element in the batch

h0 = torch.randn(D * num_layers, N, H_out)

# output_tensor (L, N, D * H_out)

# hn (D * num_layers, N, H_out) 

output_tensor, hn = rnn(input_tensor, h0)
```

`torch.nn.RNN`的三个关键参数是`input_size`、`hidden_size`和`num_layers`。它们分别指输入张量中的预期特征数、隐藏状态中的特征数以及要使用的递归层数。要触发前向传播，需要传递两个东西，一个输入张量和一个包含初始隐藏状态的张量。

## PyTorch 模特培训

在本节中，我们将描述 PL 的模型培训组件。如下面的代码块所示，`LightningModule`是您必须为该组件继承的基类。它的`configure_optimizers`函数用于定义用于训练的优化器。然后，在`training_step`功能中定义实际的训练逻辑:

```

class SampleModel(LightningModule):

   def configure_optimizers(self):

      """Define optimizer to use"""

      return torch.optim.Adam(self.parameters(), lr=0.02)

   def training_step(self, batch, batch_idx):

      """Define single training iteration"""

      x, y = batch

      y_hat = self(x)

      loss = F.cross_entropy(y_hat, y)

      return loss
```

验证、预测和测试循环具有相似的功能定义；将一批数据输入网络，以计算必要的预测和损失值。收集到的数据也可以使用 PL 的内置日志系统进行存储和显示。详情请参考官方文档([https://py torch-lightning . readthe docs . io/en/latest/common/lightning _ module . html](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)):

```

   def validation_step(self, batch, batch_idx):

      """Define single validation iteration"""

      loss, acc = self._shared_eval_step(batch, batch_idx)

      metrics = {"val_acc": acc, "val_loss": loss}

      self.log_dict(metrics)

      return metrics

   def test_step(self, batch, batch_idx):

      """Define single test iteration"""

      loss, acc = self._shared_eval_step(batch, batch_idx)

      metrics = {"test_acc": acc, "test_loss": loss}

      self.log_dict(metrics)

      return metrics

   def _shared_eval_step(self, batch, batch_idx):

      x, y = batch

      outputs = self(x)

      loss = self.criterion(outputs, targets)

      acc = accuracy(outputs.round(), targets.int())

      return loss, acc

   def predict_step(self, batch, batch_idx, dataloader_idx=0):

      """Compute prediction for the given batch of data"""

      x, y = batch

      y_hat = self(x)

      return y_hat
```

在罩下，`LightningModule`执行以下一组简化 PyTorch 代码:

```

model.train()

torch.set_grad_enabled(True)

outs = []

for batch_idx, batch in enumerate(train_dataloader):

   loss = training_step(batch, batch_idx)

   outs.append(loss.detach())

   # clear gradients

   optimizer.zero_grad()

   # backward

   loss.backward()

   # update parameters

   optimizer.step()

   if validate_at_some_point

      model.eval()

      for val_batch_idx, val_batch in enumerate(val_dataloader):

         val_out = model.validation_step(val_batch, val_batch_idx)

         model.train()
```

将`LightningDataModule`和`LightningModule`放在一起，对测试集的训练和推理可以简单地实现如下:

```

from pytorch_lightning import Trainer

data_module = SampleDataModule()

trainer = Trainer(max_epochs=num_epochs)

model = SampleModel()

trainer.fit(model, data_module)

result = trainer.test()
```

到目前为止，您应该已经了解了使用 PyTorch 建立模型训练需要实现什么。接下来的两个部分专门讨论损失函数和优化器，这是模型训练的两个主要组成部分。

### PyTorch 损失函数

首先，我们将查看 PL 中可用的不同损失函数。本节中的损失函数可在`torch.nn`模块中找到。

#### PyTorch MSE / L2 损失函数

MSE 损失函数可使用`torch.nn.MSELoss`创建。然而，这仅计算平方误差分量，并利用`reduction`参数提供变量。当`reduction`为`None`时，计算值原样返回。另一方面，当设置为`sum`时，输出将被累加。要获得准确的 MSE 损失，必须将减少量设置为`mean`，如以下代码片段所示:

```

loss = nn.MSELoss(reduction='mean')

input = torch.randn(3, 5, requires_grad=True)

target = torch.randn(3, 5)

output = loss(input, target)
```

接下来，我们来看看 MAE loss。

#### 皮托奇·梅/ L1 损失函数

MAE 损失函数可以使用`torch.nn.L1Loss`实例化。类似于 MSE 损失函数，该函数基于`reduction`参数计算不同的值:

```

Loss = nn.L1Loss(reduction='mean')

input = torch.randn(3, 5, requires_grad=True)

target = torch.randn(3, 5)

output = loss(input, target)
```

我们现在可以继续讨论 CE 损失，它用于多类分类任务。

#### PyTorch CE 损失函数

`torch.nn.CrossEntropyLoss`在为具有多个类别的分类问题训练模型时非常有用。如下面代码片段中的所示，这个类也有一个用于计算不同变量的`reduction`参数。您可以使用`weight`和`ignore_index`参数进一步改变损失的行为，这两个参数分别对每个类别进行加权并忽略特定的指数:

```

loss = nn.CrossEntropyLoss(reduction="mean")

input = torch.randn(3, 5, requires_grad=True)

target = torch.empty(3, dtype=torch.long).random_(5)

output = loss(input, target)
```

以类似的方式，我们可以定义 BCE 损失。

#### PyTorch BCE 损失函数

与 CE 损耗类似，PyTorch 将 BCE 损耗定义为具有相同参数集的`torch.nn.BCELoss`。然而，PyTorch 利用了`torch.nn.BCELoss`和 sigmoid 运算之间的密切关系，提供了`torch.nn.BCEWithLogitsLoss`，它通过在一个类中结合`softmax`运算和 BCE 损失计算来实现更高的数值稳定性。下面的代码片段显示了用法:

```

loss = torch.nn.BCEWithLogitsLoss(reduction="mean")

input = torch.randn(3, requires_grad=True)

target = torch.empty(3).random_(2)

output = loss(input, target)
```

最后，让我们来看看 PyTorch 中自定义损失的构造。

#### PyTorch 自定义损失函数

定义自定义损失函数非常简单。用 PyTorch 运算定义的任何函数都可以用作损失函数。

以下是使用`mean`操作符实现`torch.nn.MSELoss`的示例:

```

def custom_mse_loss(output, target):

   loss = torch.mean((output - target)**2)

   return loss

input = torch.randn(3, 5, requires_grad=True)

target = torch.randn(3, 5)

output = custom_mse_loss(input, target)
```

现在，我们将转向 PyTorch 中优化器的概述。

### PyTorch 优化器

如 *PyTorch 模型训练*部分所述，`LightningModule`的`configure_optimizers`函数指定训练的优化器。在 PyTorch 中，预定义的优化器可以在`torch.optim`模块中找到。优化器实例化需要模型参数，这些参数可以通过调用模型上的`parameters`函数来获得，如下面几节所示。

#### PyTorch SGD 优化器

下面的代码片段实例化了一个 LR 为`0.1`的 SGD 优化器，并演示了如何实现模型参数更新的单个步骤。

`torch.optim.SGD`内置动量和加速度支持，进一步提高训练性能。可以使用`momentum`和`nesterov`参数进行配置:

```

optimizer = torch.optim.SGD(model.parameters(), lr=0.1 momentum=0.9, nesterov=True)
```

#### PyTorch Adam 优化器

类似地，可以使用`torch.optim.Adam`实例化 Adam 优化器，如下面的代码行所示:

```

optimizer = torch.optim.Adam(model.parameters(), lr=0.1) 
```

如果你对 PyTorch 中的优化器是如何工作的感到好奇，我们推荐阅读官方文档:[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)。

要记住的事情

a.PyTorch 是一个流行的 DL 框架，提供 GPU 加速的矩阵计算和自动微分。PyTorch 因其灵活性、易用性以及在模型训练中的效率而越来越受欢迎。

b.为了可读性和模块化，PyTorch 使用了一个名为`Dataset`的类进行数据管理，另一个名为`DataLoader`的类用于迭代访问样本。

c.PL 的主要优势来自于`LightningModule`，它将复杂的 PyTorch 代码结构简化为六个部分:计算、训练循环、验证循环、测试循环、预测循环，以及优化器和 LR 调度器

d.PyTorch 和 PL 共享各种图层和损失函数的`torch.nn`模块。预定义的优化器可以在`torch.optim`模块中找到。

在下面的部分，我们将看看另一个 DL 框架，TF。用 TF 设置的训练与用 PyTorch 设置的训练非常相似。

# 在 TF 中实现和训练模型

PyTorch 面向研究项目，而 TF 更强调行业用例。虽然 PyTorch、Torch Serve 和 Torch Mobile 的部署特性仍处于试验阶段，但 TF、TF Serve 和 TF Lite 的部署特性是稳定的，正在积极使用中。TF 的第一个版本是由 Google Brain 团队在 2011 年推出的，他们一直在不断更新 TF，使其更加灵活、用户友好和高效。TF 和 PyTorch 的主要区别最初要大得多，因为 TF 的第一个版本使用静态图。然而，这种情况在版本 2 中有所改变，因为它引入了急切执行，模仿 PyTorch 中的动态图。TF 版本 2 经常和 **Keras** 一起使用，一个 ANN 的接口( [https://keras.io](https://keras.io/getting_started/) )。Keras 允许用户快速开发 DL 模型并运行实验。在接下来的部分中，我们将带您了解 TF 的关键组件。

## TF 数据加载逻辑

可以通过多种方式为 TF 型号加载数据。您应该了解的一个关键数据操作模块是`tf.data`，它帮助您构建高效的输入管道。`tf.data`提供了`tf.data.Dataset`和`tf.data.TFRecordDataset`类，用于加载不同数据格式的数据集。此外，还有`tensorflow_datasets` ( `tfds`)模块([【https://www.tensorflow.org/datasets/api_docs/python/tfds】](https://www.tensorflow.org/datasets/api_docs/python/tfds))和`tensorflow_addons`模块()在很多情况下进一步简化了数据加载过程。还值得一提的是 TF I/O 包([https://www.tensorflow.org/io/overview](https://www.tensorflow.org/io/overview))，它扩展了标准 TF 文件系统交互的能力。

不管您将使用哪个包，您都应该考虑创建一个`DataLoader`类。在本课程中，您将明确定义如何加载目标数据，以及如何在培训前对其进行预处理。以下代码片段是加载逻辑的示例实现:

```

import tensorflow_datasets as tfds

class DataLoader: 

   """ DataLoader class"""

   @staticmethod 

   def load_data(config): 

      return tfds.load(config.data_url)
```

在前面的例子中，我们使用`tfds`从外部 URL ( `config.data_url`)加载数据。更多关于`tfds.load`的信息可以在线查询:[https://www . tensor flow . org/datasets/API _ docs/python/tfds/load](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)。

数据有多种格式。因此，使用`tf.data`模块提供的功能将其预处理成 TF 模型可以使用的格式是很重要的。那么，让我们看看如何使用这个包来读取常见格式的数据:

*   首先，`tfrecord`中的数据，一种为存储二进制数据序列而设计的格式，可以如下读取:

    ```
    import tensorflow as tf  dataset = tf.data.TFRecordDataset(list_of_files)
    ```

*   我们可以使用`tf.data.Dataset.from_tensor_slices`函数从 NumPy 数组创建一个 dataset 对象，如下所示:

    ```
    dataset = tf.data.Dataset.from_tensor_slices(numpy_array)
    ```

*   使用相同的`tf.data.Dataset.from_tensor_slices`功能:

    ```
    dataset = tf.data.Dataset.from_tensor_slices((df_features.values, df_target.values))
    ```

    也可以将 Pandas 数据帧作为数据集加载
*   另一个选择是使用 Python 生成器。下面是一个简单的例子，它强调了如何使用生成器来输入成对的图像和标签:

    ```
    def data_generator(images, labels):    def fetch_examples():         i = 0         while True:            example = (images[i], labels[i])            i += 1            i %= len(labels)            yield example         return fetch_examples training_dataset = tf.data.Dataset.from_generator(    data_generator(images, labels),    output_types=(tf.float32, tf.int32),     output_shapes=(tf.TensorShape(features_shape), tf.TensorShape(labels_shape)))
    ```

如最后一段代码所示，`tf.data.Dataset`为我们提供了内置的数据加载功能，比如批处理、重复和混排。这些选项是不言自明的:批处理创建特定大小的小批，重复允许我们多次迭代数据集，而混排将每个时期的数据条目混合在一起。

在结束本节之前，我们想提一下用 Keras 实现的模型可以直接使用 NumPy 数组和 Pandas 数据帧。

## TF 模型定义

类似于 PyTorch 和 PL 处理模型定义的方式，TF 提供了多种定义网络架构的方式。首先，我们将看一下`Keras.Sequential`，它将一组层链接起来以构建一个网络。该类为您处理链接，因此您不需要显式定义层之间的链接:

```

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras import layers

input_shape = 50

model = keras.Sequential(

   [

      keras.Input(shape=input_shape),

      layers.Dense(128, activation="relu", name="layer1"),

      layers.Dense(64, activation="relu", name="layer2"),

      layers.Dense(1, activation="sigmoid", name="layer3"),

   ])
```

在前面的示例中，我们正在创建一个模型，该模型由一个输入层、两个密集层和一个生成单个神经元作为输出的输出层组成。这是一个简单的模型，可用于二元分类。

如果模型定义更复杂，并且不能以顺序方式构建，另一个选择是使用`keras.Model`类，如下面的代码片段所示:

```

num_classes = 5 

input_1 = layers.Input(50)

input_2 = layers.Input(10)

x_1 = layers.Dense(128, activation="relu", name="layer1x")(input_1)

x_1 = layers.Dense(64, activation="relu", name="layer1_2x")(x_1)

x_2 = layers.Dense(128, activation="relu", name="layer2x")(input_2)

x_2 = layers.Dense(64, activation="relu", name="layer2_1x")(x_2)

x = layers.concatenate([x_1, x_2], name="concatenate")

out = layers.Dense(num_classes, activation="softmax", name="output")(x)

model = keras.Model((input_1,input_2), out)
```

在本例中，我们有两个具有不同计算集合的输入。两条路径在最后一个连接层中合并，该层将连接的张量传输到具有五个神经元的最终密集层。鉴于最后一层使用`softmax`激活，该模型可用于多类分类。

如下所示，第三个选项是创建一个继承`keras.Model`的类。此选项为您提供了最大的灵活性，因为它允许您自定义模型和培训流程的每个部分:

```

class SimpleANN(keras.Model):

   def __init__(self):

      super().__init__()

      self.dense_1 = layers.Dense(128, activation="relu", name="layer1")

      self.dense_2 = layers.Dense(64, activation="relu", name="layer2")

      self.out = layers.Dense(1, activation="sigmoid", name="output")

   def call(self, inputs):

      x = self.dense_1(inputs)

      x = self.dense_3(x)

      return self.out(x)

model = SimpleANN()
```

`SimpleANN`，来自前面的代码，继承了`Keras.Model`。在`__init__`功能中，我们需要使用`tf.keras.layers`模块或基本 TF 操作来定义网络架构。正向传播逻辑是在`call`方法中定义的，就像 PyTorch 有`forward`方法一样。

当模型被定义为一个不同的类时，你可以将额外的功能链接到这个类。在下面的例子中，添加了`build_graph`方法来返回一个`keras.Model`实例，因此，例如，您可以使用`summary`函数将网络架构可视化为一个更简单的表示:

```

class SimpleANN(keras.Model):

   def __init__(self):

   ...

   def call(self, inputs):

   ...

   def build_graph(self, raw_shape):

      x = tf.keras.layers.Input(shape=raw_shape)

      return keras.Model(inputs=[x], outputs=self.call(x))
```

现在，我们来看看 TF 是如何通过 Keras 提供一套层实现的。

### TF DL 层

正如前面的部分提到的，`tf.keras.layers`模块提供了一组层实现，您可以使用它们来构建一个 TF 模型。在这一节中，我们将讨论我们在*在 PyTorch* 中实现和训练模型一节中描述的同一组层。可在[https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)找到本模块可用层的完整列表。

#### TF 密集(线性)层

第一个是`tf.keras.layers.Dense`，它执行线性变换:

```

tf.keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)
```

`units`参数定义了密集层中神经元的数量(输出的维度)。如果没有定义`activation`参数，层的输出将被原样返回。如以下代码所示，我们也可以在层定义之外应用`Activation`操作:

```

X = layers.Dense(128, name="layer2")(input)

x = tf.keras.layers.Activation('relu')(x)
```

在某些情况下，您需要构建一个自定义层。以下示例演示了如何通过继承`tensorflow.keras.layers.Layer`类使用基本 TF 操作创建密集层:

```

import tensorflow as tf

from tensorflow.keras.layers import Layer

class CustomDenseLayer(Layer):

   def __init__(self, units=32):

      super(SimpleDense, self).__init__()

      self.units = units

   def build(self, input_shape):

      w_init = tf.random_normal_initializer()

      self.w = tf.Variable(name="kernel", initial_value=w_init(shape=(input_shape[-1], self.units),

      dtype='float32'),trainable=True)

      b_init = tf.zeros_initializer()

      self.b = tf.Variable(name="bias",initial_value=b_init(shape=(self.units,), dtype='float32'),trainable=True)

   def call(self, inputs):

      return tf.matmul(inputs, self.w) + self.b
```

在`CustomDenseLayer`类的`__init__`函数中，我们定义了输出的维度(`units`)。然后，在`build`方法中实例化层的状态；我们创建并初始化层的权重和偏差。最后一个方法`call`，定义了计算本身。对于密集图层，它包括将输入乘以权重并添加偏差。

#### TF 池层

`tf.keras.layers`为一维时态数据、二维或三维空间数据提供不同种类的池图层:平均、最大、全局平均和全局最大池图层。在本节中，我们将向您展示二维最大池层和平均池层:

```

tf.keras.layers.MaxPool2D(

   pool_size=(2, 2), strides=None, padding='valid', data_format=None,

   kwargs)

tf.keras.layers.AveragePooling2D(

   pool_size=(2, 2), strides=None, padding='valid', data_format=None,

   kwargs)
```

这两层都接受`pool_size`，它定义了窗口的大小。`strides`参数用于定义窗口在整个汇集操作中的移动方式。

#### TF 标准化层

在下面的例子中，我们展示了一个用于批量标准化的图层，`tf.keras.layers.BatchNormalization`:

```

tf.keras.layers.BatchNormalization(

   axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,

   beta_initializer='zeros', gamma_initializer='ones',

   moving_mean_initializer='zeros',

   moving_variance_initializer='ones', beta_regularizer=None,

   gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, **kwargs)
```

该层的输出将具有接近`0`的平均值和接近`1`的标准偏差。关于每个参数的详细信息可以在[https://www . tensor flow . org/API _ docs/python/TF/keras/layers/batch normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)找到。

#### TF 辍学层

`Tf.keras.layers.Dropout`层应用 dropout，一种将选择的值随机设置为零的正则化方法:

```

tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)
```

在前面的层实例化中，`rate`参数，一个在`0`和`1`之间的浮点值，决定了将被丢弃的输入单元的分数。

#### TF 卷积层

`tf.keras.layers`提供了卷积层、`tf.keras.layers.Conv1D`、`tf.keras.layers.Conv2D`、 `tf.keras.layers.Conv3D`的各种实现，以及相应的转置卷积层(反卷积层)、`tf.keras.layers.Conv1DTranspose`、`tf.keras.layers.Conv2DTranspose`和 `tf.keras.layers.Conv3DTranspose`。

以下代码片段描述了二维卷积层的实例化:

```

tf.keras.layers.Conv2D(

   filters, kernel_size, strides=(1, 1), padding='valid',

   data_format=None, dilation_rate=(1, 1), groups=1,

   activation=None, use_bias=True,

   kernel_initializer='glorot_uniform',

   bias_initializer='zeros', kernel_regularizer=None,

   bias_regularizer=None, activity_regularizer=None,

   kernel_constraint=None, bias_constraint=None, **kwargs)
```

前一层定义中的主要参数是`filters`和`kernel_size`。`filters`参数定义输出的维度，而`kernel_size`参数定义二维卷积窗口的大小。其他参数请看[https://www . tensor flow . org/API _ docs/python/TF/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)。

#### TF 循环层

以下递归层列表在 Keras 中实现:`LSTM`层、`GRU`层、`SimpleRNN`层、`TimeDistributed`层、`Bidirectional`层、`ConvLSTM2D`层、`Base RNN`层。

在下面的代码片段中，我们演示了如何实例化`Bidirectional`和`LSTM`层:

```

model = Sequential()

model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))

model.add(Bidirectional(LSTM(10)))

model.add(Dense(5))

model.add(Activation('softmax'))
```

在前面的例子中，`LSTM`层被`Bidirectional`包装器修改，以向隐藏层的两个副本提供初始序列和反向序列。这两层的输出合并为最终输出。默认情况下，输出被连接，但是`merge_mode`参数允许我们选择不同的合并选项。输出空间的维度由第一个参数定义。要在每个时间步长访问每个输入的隐藏状态，可以启用`return_sequences`。更多详情请看[https://www . tensor flow . org/API _ docs/python/TF/keras/layers/LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)。

## TF 模型训练

对于 Keras 模型，用优化器和损失函数调用`compile`函数后，只需在模型上调用`fit`函数，就可以实现模型训练。`fit`函数使用所提供的数据集为给定数量的时期训练模型。

以下代码片段描述了`fit`函数的参数:

```

model.fit(

   x=None, y=None, batch_size=None, epochs=1,

   verbose='auto', callbacks=None, validation_split=0.0,

   validation_data=None, shuffle=True,

   class_weight=None, sample_weight=None, 

   initial_epoch=0, steps_per_epoch=None,

   validation_steps=None, validation_batch_size=None,

   validation_freq=1, max_queue_size=10, workers=1,

   use_multiprocessing=False)
```

`x`和`y`代表输入张量和标签。它们可以以各种格式提供:NumPy 数组、TF 张量、TF 数据集、生成器或`tf.keras.utils.experimental.DatasetCreator`。除了`fit`，Keras 模型还有一个`train_on_batch`函数，只对单批数据执行渐变更新。

虽然 TF 版本 1 要求为训练循环编译计算图，但 TF 版本 2 允许我们在没有任何编译的情况下定义训练逻辑，就像 PyTorch 一样。典型的训练循环如下所示:

```

Optimizer = tf.keras.optimizers.Adam()

loss_fn = tf.keras.losses.CategoricalCrossentropy()

train_acc_metric = tf.keras.metrics.CategoricalAccuracy()

for epoch in range(epochs):

   for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):

       with tf.GradientTape() as tape:

          logits = model(x_batch_train, training=True)

          loss_value = loss_fn(y_batch_train, logits)

       grads = tape.gradient(loss_value, model.trainable_weights)

       optimizer.apply_gradients(zip(grads, model.trainable_weights))

       train_acc_metric.update_state(y, logits)
```

在前面的代码片段中，外循环遍历所有的历元，内循环遍历所有的训练集。正向传播和损耗计算在`GradientTape`的范围内，记录每批自动微分的操作。在该范围之外，优化器使用计算的梯度来更新权重。在前面的示例中，TF 函数立即执行操作，而不是像在急切执行中那样将操作添加到计算图中。我们想提一下，如果您使用的是 TF 版本 1，那么您将需要使用`@tf.function` decorator，其中计算图的显式构造是必要的。

接下来，我们将看看 TF 中的损失函数。

### TF 损失函数

在 TF 中，需要在编译模型时指定损失函数。虽然你可以从头开始构建一个定制的损失函数，但是你可以通过`tf.keras.losses`模块([https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses))使用 Keras 提供的预定义损失函数。以下示例演示了如何使用 Keras 中的损失函数来编译模型:

```

model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True), ...)
```

此外，您可以将字符串别名传递给 loss 参数，如下面的代码片段所示:

```

model.compile(loss='sparse_categorical_crossentropy', ...)
```

在本节中，我们将解释如何在 TF 中实例化 *PyTorch 损失函数*一节中描述的损失函数。

#### TF 均方误差/ L2 损失函数

MSE / L2 损失函数可以定义如下([https://www . tensor flow . org/API _ docs/python/TF/keras/loss/meansquaderror](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)):

```

mse = tf.keras.losses.MeanSquaredError()
```

这是最常用的回归损失函数——它计算标签和预测之间的平方差的`mean`值。默认设置将计算 MSE。然而，与 PyTorch 实现类似，我们可以提供一个`reduction`参数来改变该行为。例如，如果您想应用`sum`运算而不是均值运算，您可以在损失函数中添加`reduction=tf.keras.losses.Reduction.SUM`。假设 PyTorch 中的`torch.nn.MSELoss`原样返回平方差，您可以通过将`reduction=tf.keras.losses.Reduction.NONE`传递给构造函数来获得 TF 中的相同损失。

接下来，我们将看看 MAE 损失。

#### TF 梅/ L1 损失函数

`tf.keras.losses.MeanAbsoluteError`是 Keras 中 MAE loss 的函数([https://www . tensor flow . org/API _ docs/python/TF/Keras/loss/meanasoluteerror](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError)):

```

mae = tf.keras.losses.MeanAbsoluteError()
```

顾名思义，这个损失计算真实值和预测值之间的绝对差值的平均值。它还有一个`reduction`参数，使用方式与`tf.keras.losses.MeanSquaredError`相同。

现在，我们来看一看损耗分类，即 CE 损耗。

#### TF CE 损失函数

CE loss 计算两个概率分布之间的差。Keras 提供了`tf.keras.losses.CategoricalCrossentropy`类，用于分类多个类([https://www . tensor flow . org/API _ docs/python/TF/keras/loss/CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy))。以下代码片段显示了实例化:

```

cce = tf.keras.losses.CategoricalCrossentropy()
```

在 Keras 的情况下，标签需要被格式化为一个热矢量。例如，当目标类是五个类中的第一个时，应该是`[1, 0, 0, 0, 0]`。

还存在为二进制分类设计的 CE 损失，BCE 损失。

#### TF BCE 损失函数

在二进制分类的情况下，标签为`0`或`1`。专门为二元分类设计的损失函数 BCE loss 可以定义如下([https://www . tensor flow . org/API _ docs/python/TF/keras/loss/BinaryFocalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy)):

```

loss = tf.keras.losses.BinaryFocalCrossentropy(from_logits=True)
```

这个损耗的关键参数是`from_logits`。当该标志设置为`False`时，我们必须提供概率，即`0`和`1`之间的连续值。当设置为`True`时，我们需要提供 logits，即`-infinity`和`+infinity`之间的值。

最后，让我们看看如何在 TF 中定义自定义损失。

#### TF 自定义损失函数

为了构建一个定制的损失函数，我们需要创建一个将预测和标签作为参数并执行所需计算的函数。虽然 TF 语法只需要这两个参数，但是我们也可以通过将函数包装到另一个返回损失的函数中来添加一些额外的参数。以下示例演示了如何将 Huber Loss 创建为自定义损失函数:

```

def custom_huber_loss(threshold=1.0):

   def huber_fn(y_true, y_pred):

       error = y_true - y_pred

       is_small_error = tf.abs(error) < threshold

       squared_loss = tf.square(error) / 2

       linear_loss = threshold * tf.abs(error) - threshold**2 / 2

       return tf.where(is_small_error, squared_loss, linear_loss)

   return huber_fn

model.compile(loss=custom_huber_loss (2.0), optimizer="adam"
```

另一个选择是创建一个继承`tf.keras.losses.Loss`类的类。我们需要在这种情况下实现`__init__`和`call`方法，如下所示:

```

class CustomLoss(tf.keras.losses.Loss):

   def __init__(self, threshold=1.0):

      super().__init__()

      self.threshold = threshold

   def call(self, y_true, y_pred):

      error = y_true - y_pred 

      is_small_error = tf.abs(error) < threshold

      squared_loss = tf.square(error) / 2 

      linear_loss = threshold*tf.abs(error) - threshold**2 / 2 

      return tf.where(is_small_error, squared_loss, linear_loss)

model.compile(optimizer="adam", loss=CustomLoss(),
```

为了使用这个 loss 类，您必须实例化它，并通过一个`loss`参数将其传递给`compile`函数，如本节开头所述。

### TF 优化器

在本节中，我们将描述如何在 TF 中为模型训练设置不同的优化器。类似于上一节中的损失函数，Keras 为 TF 到`tf.keras.optimizers`提供了一组优化器。在各种优化器中，我们将在下一节中研究两个主要的优化器，SGD 和 Adam 优化器。

#### TF SGD 优化器

SGD 优化器设计有固定的 LR，是可以用于许多模型的最典型的优化器。以下代码片段描述了如何在 TF 中实例化 SGD 优化器:

```

tf.keras.optimizers.SGD(

   learning_rate=0.01,

   momentum=0.0,

   nesterov=False,

   name='SGD',

   kwargs)
```

与 PyTorch 实现类似，`tf.keras.optimizers.SGD`也支持使用`momentum`和`nesterov`参数的增强 SGD 优化器。

#### TF Adam 优化器

如*模型训练逻辑*部分所述，Adam 优化器设计有自适应 LR。在 TF 中，可以将实例化如下:

```

tf.keras.optimizers.Adam(

   learning_rate=0.001, beta_1=0.9, beta_2=0.999,

   epsilon=1e-07, amsgrad=False, name='Adam', **kwargs)
```

对于这两个优化器，虽然`learning_rate`在定义初始 LR 中起着最重要的作用，但我们建议您也阅读官方文档，以熟悉其他参数:[https://www . tensor flow . org/API _ docs/python/TF/keras/optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)。

### TF 回调

在这一部分，我们将简要描述回调。这些是在训练的不同阶段用于执行特定动作的物品。使用最多的回调是`EarlyStopping`、`ModelCheckpoint`和`TensorBoard`，它们分别在满足特定条件时停止训练，在每个历元后保存模型，并可视化训练状态。

下面是一个`EarlyStopping`回调的例子，它监控验证损失，如果监控的损失停止减少，则停止训练:

```

tf.keras.callbacks.EarlyStopping(

   monitor='val_loss', min_delta=0.1, patience=2, 

   verbose=0, mode='min', baseline=None, 

   restore_best_weights=False)
```

`min_delta`参数定义了被视为改进的变化的监控量的最小变化，而`patience`参数定义了没有任何改进的时期数，在此之后训练将停止。

构建自定义回调可以通过继承`keras.callbacks.Callback`来实现。为特定事件定义逻辑可以通过覆盖其方法来实现，这些方法清楚地描述了它绑定到哪个事件:

*   `on_train_begin`
*   `on_train_end`
*   `on_epoch_begin`
*   `on_epoch_end`
*   `on_test_begin`
*   `on_test_end`
*   `on_predict_begin`
*   `on_predict_end`
*   `on_train_batch_begin`
*   `on_train_batch_end`
*   `on_predict_batch_begin`
*   `on_predict_batch_end`
*   `on_test_batch_begin`
*   或者`on_test_batch_end`

关于完整的细节，我们建议你看一下[https://www . tensor flow . org/API _ docs/python/TF/keras/callbacks/Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)。

要记住的事情

a.`tf.data`允许您构建高效的数据加载逻辑。`tfds`、`tensorflow addons`或 TF I/O 等包对于读取不同格式的数据很有用。

b.在 Keras 的支持下，TF 允许用户使用三种不同的方法构建模型:顺序的、功能的和子类化的。

c.为了简化使用 TF 的模型开发，`tf.keras.layers`模块提供了各种层实现，`tf.keras.losses`模块包括不同的损失函数，`tf.keras.optimizers`模块提供了一组标准优化器。

d.`Callbacks`可用于在训练的各个阶段执行特定的动作。常用的回调有`EarlyStopping`和`ModelCheckpoint`。

到目前为止，我们已经学习了如何使用最流行的 DL 框架 PyTorch 和 TF 建立一个 DL 模型训练。在下一节中，我们将看看我们在这一节中描述的组件在现实中是如何使用的。

# 分解复杂、先进的模型实现

即使你已经掌握了 TF 和 PyTorch 的基础知识，从头开始建立一个模型训练可能会让人不知所措。幸运的是，这两个框架有完整的文档和教程，很容易理解:

*   法国南部（French Southern Territories 的缩写）
    *   带卷积层的图像分类:[https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification)。
    *   带递归层的文本分类:[https://www . tensor flow . org/text/tutorials/text _ classification _ rnn](https://www.tensorflow.org/text/tutorials/text_classification_rnn)。
*   PyTorch
    *   卷积层的物体检测:[https://py torch . org/tutorials/intermediate/torch vision _ tutorial . html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)。
    *   带递归层的机器翻译:[https://py torch . org/tutorials/intermediate/seq 2 seq _ translation _ tutorial . html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)。

在这一节中，我们想看一个更加复杂的模型，StyleGAN。我们的主要目标是解释如何将前面章节中描述的组件放在一起用于一个复杂的 DL 项目。对于模型架构和性能的完整描述，我们推荐 NVIDIA 发布的出版物，可在[https://ieeexplore.ieee.org/document/8953766](https://ieeexplore.ieee.org/document/8953766)获得。

## StyleGAN

StyleGAN 作为一个**生成对抗网络** ( **GAN** )的变种，目的是从潜在代码(随机噪声向量)中生成新的图像。它的架构可以分为三个部分:一个映射网络、一个生成器和一个鉴别器。在高层次上，映射网络和生成器一起工作，从一组随机值生成图像。鉴别器在训练期间指导生成器生成逼真的图像方面起着关键作用。让我们仔细看看每个组件。

### 映射网络和生成器

虽然发生器被设计为在传统 GAN 中直接处理潜在代码，但潜在代码首先在 StyleGAN 中被馈送到映射网络，如图*图 3.5* 所示。映射网络的输出然后被馈送到生成器的每一步，改变生成的图像的风格和细节。生成器以较低的分辨率开始，以 4 x 4 或 8 x 8 的张量大小构建图像的轮廓。当生成器处理较大的张量时，图像的细节被填充。在最后几层，生成器与大小为 64 x 64 和 1024 x 1024 的张量交互以构建高分辨率要素:

![Figure 3.5 – A mapping network (left) and generator (right) of StyleGAN
](img/B18522_03_05.jpg)

图 3.5–StyleGAN 的映射网络(左)和生成器(右)

在上图中，网络接受一个潜在向量 **z** ，并生成 **w** 就是映射网络。右边的网络是发电机， **g** ，它接收一组噪声向量，还有 **w** 。与生成器相比，鉴别器相当简单。这些层在*图 3.6* 中描述:

![Figure 3.6 – A StyleGAN discriminator architecture for the FFHQ dataset at 1024 × 1024 resolution
](img/B18522_03_06.jpg)

图 3.6–1024×1024 分辨率的 FFHQ 数据集的 StyleGAN 鉴别器架构

如前面的图所示，鉴别器由多个卷积层块和下采样操作组成。它接收大小为 1024 x 1024 的图像，并生成一个介于`0`和`1`之间的数值，描述图像的逼真程度。

### 培训风格 g

训练 StyleGAN 需要大量的计算，因此需要多个 GPU 来实现合理的训练时间。估计值汇总在*图 3.7* 中:

![Figure 3.7 – The training time for StyleGAN with an FFHQ dataset on Tesla V100 GPUs 
](img/B18522_03_07.jpg)

图 3.7–style gan 在 Tesla V100 GPUs 上使用 FFHQ 数据集的训练时间

因此，如果你想玩 StyleGAN，我们建议遵循官方 GitHub 库的说明，那里提供了预先训练的模型:[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)。

## 在 PyTorch 中实现

遗憾的是，NVIDIA 还没有在 PyTorch 中分享 StyleGAN 的公开实现。相反，他们发布了 StyleGAN2，它共享了大部分相同的组件。因此，我们将在 PyTorch 示例中使用 StyleGAN2 实现:[https://github.com/NVlabs/stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch)。

所有的网络组件都在`training/network.py`下。三个组件的命名如前一节所述:`MappingNetwork`、`Generator`和`Discriminator`。

### PyTorch 中的映射网络

`MappingNetwork`的实现不言自明。以下代码片段包括映射网络的核心逻辑:

```

class MappingNetwork(torch.nn.Module):

   def __init__(self, ...):

       ...

       for idx in range(num_layers):

          in_features = features_list[idx]

          out_features = features_list[idx + 1]

          layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier= lr_multiplier) setattr(self, f'fc{idx}', layer)

   def forward(self, z, ...):

       # Embed, normalize, and concat inputs.

       x = normalize_2nd_moment(z.to(torch.float32))

       # Main layers

       for idx in range(self.num_layers):

          layer = getattr(self, f'fc{idx}')

          x = layer(x)

       return x
```

在这个网络定义中，`MappingNetwork`继承了`torch.nn.Module`。在`__init__`函数中，必要的`FullyConnectedLayer`实例被初始化。`forward`方法将潜在向量`z`输入到每一层。

### PyTorch 的发电机

下面的代码片段描述了生成器是如何实现的。由`MappingNetwork`和`SynthesisNetwork`组成，如图*图 3.5* 所示:

```

class Generator(torch.nn.Module):

   def __init__(self, …):

       self.z_dim = z_dim

       self.c_dim = c_dim

       self.w_dim = w_dim

       self.img_resolution = img_resolution

       self.img_channels = img_channels

       self.synthesis = SynthesisNetwork(

          w_dim=w_dim, 

          img_resolution=img_resolution,

          img_channels=img_channels,

          synthesis_kwargs)

       self.num_ws = self.synthesis.num_ws

       self.mapping = MappingNetwork(

          z_dim=z_dim, c_dim=c_dim, w_dim=w_dim,

          num_ws=self.num_ws, **mapping_kwargs)

   def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, **synthesis_kwargs):

       ws = self.mapping(z, c, 

       truncation_psi=truncation_psi, 

       truncation_cutoff=truncation_cutoff)

       img = self.synthesis(ws, **synthesis_kwargs)

       return img
```

发电机网络`Generator`也继承了`torch.nn.Module`。`SynthesisNetwork`和`MappingNetwork`在`__init__`函数中被实例化，并在`forward`函数中被顺序触发。下面的代码片段总结了`SynthesisNetwork`的实现:

```

class SynthesisNetwork(torch.nn.Module):

   def __init__(self, ...):

       for res in self.block_resolutions:

          block = SynthesisBlock(

             in_channels, out_channels, w_dim=w_dim,

             resolution=res, img_channels=img_channels,

             is_last=is_last, use_fp16=use_fp16,

             block_kwargs)

          setattr(self, f'b{res}', block)

       ...

   def forward(self, ws, **block_kwargs):

       ...

       x = img = None

       for res, cur_ws in zip(self.block_resolutions, block_ws):

          block = getattr(self, f'b{res}')

          x, img = block(x, img, cur_ws, **block_kwargs)

       return img
```

`SynthesisNetwork`有多个`SynthesisBlock`块。`SynthesisBlock`接收噪声矢量和`MappingNetwork`的输出，生成最终成为输出图像的张量。

### PyTorch 中的鉴别器

下面的代码片段总结了`Discriminator`的 PyTorch 实现。网络架构遵循*图 3.6* 所示的结构:

```

class Discriminator(torch.nn.Module):

   def __init__(self, ...):

       self.block_resolutions = [2 ** i for i in range(self.img_resolution_log2, 2, -1)]

       for res in self.block_resolutions:

          block = DiscriminatorBlock(

              in_channels, tmp_channels, out_channels,

              resolution=res,

              first_layer_idx = cur_layer_idx,

              use_fp16=use_fp16, **block_kwargs, 

              common_kwargs)

          setattr(self, f'b{res}', block)

   def forward(self, img, c, **block_kwargs):

       x = None

       for res in self.block_resolutions:

          block = getattr(self, f'b{res}')

          x, img = block(x, img, **block_kwargs)

       return x
```

与`SynthesisNetwork`类似，`Discriminator`利用`DiscriminatorBlock`类来动态创建一组不同大小的卷积层。它们是在`__init__`函数中定义的，张量在`forward`函数中被顺序馈送给每个模块。

### PyTorch 中的模型训练逻辑

训练逻辑在`training/train_loop.py`的`training_loop`功能中定义。原始实现包含了很多细节。在下面的代码片段中，我们将查看与我们在 *PyTorch 模型培训*部分所学内容一致的主要组件:

```

def training_loop(...):

   ...

training_set_iterator = iter(torch.utils.data.DataLoader(dataset=training_set, sampler=training_set_sampler, batch_size=batch_size//num_gpus, **data_loader_kwargs))

   loss = dnnlib.util.construct_class_by_name(device=device, **ddp_modules, **loss_kwargs) # subclass of training.loss.Loss

   while True:

      # Fetch training data.

      with torch.autograd.profiler.record_function('data_fetch'):

         phase_real_img, phase_real_c = next(training_set_iterator)

      # Execute training phases.

      for phase, phase_gen_z, phase_gen_c in zip(phases, all_gen_z, all_gen_c):

         # Accumulate gradients over multiple rounds.

      for round_idx, (real_img, real_c, gen_z, gen_c) in enumerate(zip(phase_real_img, phase_real_c, phase_gen_z, phase_gen_c)):

         loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, sync=sync, gain=gain)

      # Update weights.

      phase.module.requires_grad_(False)

      with torch.autograd.profiler.record_function(phase.name + '_opt'):

         phase.opt.step()
```

该功能接收各种训练组件的配置，并训练`Generator`和`Discriminator`。外环迭代训练样本，内环处理梯度计算和模型参数更新。训练设置由单独的脚本`main/train.py`配置。

这总结了 PyTorch 实现的结构。尽管由于大量的文件，存储库看起来很庞大，但是我们已经向您介绍了如何将实现分解成我们在*在 PyTorch* 部分中描述的组件。在下一节中，我们将看看 TF 中的实现。

## 在 TF 中实现

尽管的官方实现是在 TF([https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan))中，我们将在 Soon Yau Cheong 的*用 TensorFlow 实际生成图像:* *使用深度学习生成图像和视频的实用指南*中查看一个不同的实现。这个版本是基于 TF 版本 2 的，与我们在本书中描述的更加一致。实现可以在[https://github . com/packt publishing/Hands-On-Image-Generation-with-tensor flow-2.0/blob/master/chapter 07/ch7 _ faster _ style gan . ipynb](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter07/ch7_faster_stylegan.ipynb)找到。

与上一节描述的 PyTorch 实现类似，原始 TF 实现由映射网络的`G_mapping`、发生器的`G_style`和鉴别器的`D_basic`组成。

### TF 中的映射网络

让我们看看在[https://github . com/NV labs/style gan/blob/1e 0 D5 c 781384 ef 12 b 50 ef 20 a 62 fee 5d 78 b 38 e 88 f/training/networks _ style gan . py # L384](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L384)中定义的映射网络及其 TF 版本 2 实现如下所示:

```

def Mapping(num_stages, input_shape=512):

   z = Input(shape=(input_shape))

   w = PixelNorm()(z)

   for i in range(8):

      w = DenseBlock(512, lrmul=0.01)(w)

      w = LeakyReLU(0.2)(w)

      w = tf.tile(tf.expand_dims(w, 1), (1,num_stages,1))

   return Model(z, w, name='mapping') 
```

`MappingNetwork`的实现几乎是不言自明的。我们可以看到，映射网络从向量 w 开始，使用 PixelNorm 自定义层从潜在向量 z 构建。自定义层定义如下:

```

class PixelNorm(Layer):

   def __init__(self, epsilon=1e-8):

      super(PixelNorm, self).__init__()

      self.epsilon = epsilon                

   def call(self, input_tensor):

      return input_tensor / tf.math.sqrt(tf.reduce_mean(input_tensor**2, axis=-1, keepdims=True) + self.epsilon)
```

如 *TF dense (linear) layers* 部分所述，`PixelNorm`继承了`tensorflow.keras.layers.Layer`类，并在`call`函数中定义了计算。

剩余的`Mapping`的组件是一组带有`LeakyReLU`激活的密集层。

接下来，我们将看看发电机网络。

### TF 中的发电机

原码中的发电机`G_style`由两个网络组成:`G_mapping`和`G_synthesis`。参见以下:[https://github . com/NV labs/style gan/blob/1e 0 D5 c 781384 ef 12 b 50 ef 20 a 62 fee 5d 78 b 38 e 88 f/training/networks _ style gan . py # L299](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L299)。

存储库中的完整实现初看起来可能极其复杂。但是，你很快就会发现，`G_style`只是顺序调用了`G_mapping`和`G_synthesis`。

下面的代码片段总结了`SynthesisNetwork`的实现:[https://github . com/NV labs/style gan/blob/1e 0 D5 c 781384 ef 12 b 50 ef 20 a 62 fee 5d 78 b 38 e 88 f/training/networks _ style gan . py # L440](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L440)。

在 TF 版本 2 中，生成器的实现如下:

```

def GenBlock(filter_num, res, input_shape, is_base):

   input_tensor = Input(shape=input_shape, name=f'g_{res}')

   noise = Input(shape=(res, res, 1), name=f'noise_{res}')

   w = Input(shape=512)

   x = input_tensor

   if not is_base:

      x = UpSampling2D((2,2))(x)

      x = ConvBlock(filter_num, 3)(x)

   x = AddNoise()([x, noise])

   x = LeakyReLU(0.2)(x)

   x = InstanceNormalization()(x)

   x = AdaIN()([x, w])

   # Adding noise

   x = ConvBlock(filter_num, 3)(x)

   x = AddNoise()([x, noise])

   x = LeakyReLU(0.2)(x)

   x = InstanceNormalization()(x)                    

   x = AdaIN()([x, w])

   return Model([input_tensor, w, noise], x, name=f'genblock_{res}x{res}')
```

该网络遵循*图 3.5* 中描述的架构；`SynthesisNetwork`由一组`AdaIn`和`ConvBlock`自定义图层构成。

让我们继续讨论鉴别器网络。

### TF 中的鉴别器

`D_basic`函数实现了*图 3.6* 中描述的鉴别器。([https://github . com/NV labs/style gan/blob/1e 0 D5 c 781384 ef 12 b 50 ef 20 a 62 fee 5d 78 b 38 e 88 f/training/networks _ style gan . py # L562](https://github.com/NVlabs/stylegan/blob/1e0d5c781384ef12b50ef20a62fee5d78b38e88f/training/networks_stylegan.py#L562))。由于鉴别器由一组卷积层块组成，`D_basic`有一个专用函数`block`，它根据输入张量大小构建一个块。该函数的核心组件如下所示:

```

def block(x, res): # res = 2 … resolution_log2

   with tf.variable_scope('%dx%d' % (2**res, 2**res)):

       x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, gain=gain, use_wscale=use_wscale)))

       x = act(apply_bias(conv2d_downscale2d(blur(x), fmaps=nf(res-2), kernel=3, gain=gain, use_wscale=use_wscale, fused_scale=fused_scale)))

   return x
```

在前面的代码中，`block`函数通过组合卷积和下采样层来处理在鉴别器中创建每个块。`D_basic`的其余逻辑很简单，因为它只是通过将一个块的输出作为输入传递给下一个块来链接一组卷积层块。

#### TF 中的模型训练逻辑

TF 执行的训练逻辑可以在`train_step`函数中找到。理解实现细节并不困难，因为它们遵循了我们在 *TF 模型培训*部分的描述。

总的来说，我们已经了解了如何使用本章中描述的 TF 构件在 TF 版本 2 中实现 StyleGAN。

要记住的事情

a.无论实现的复杂性如何，任何 DL 模型训练实现都可以分为三个部分(数据加载逻辑、模型定义和模型训练逻辑)。

在这个阶段，您应该理解 StyleGAN 存储库在每个框架中是如何构造的。我们强烈建议您使用预先训练好的模型来生成有趣的图像。如果你掌握了 StyleGAN，那么跟随style gan 2([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958))、style gan 3([https://arxiv.org/abs/2106.12423](https://arxiv.org/abs/2106.12423))和 HyperStyle([https://arxiv.org/abs/2111.15666](https://arxiv.org/abs/2111.15666))的实现应该很容易。

# 总结

在这一章中，我们探讨了 DL 的灵活性从何而来。DL 使用数学神经元网络来学习一组数据中的隐藏模式。训练网络包括基于训练集更新模型参数并选择在验证集上表现最佳的模型的迭代过程，目标是在测试集上产生最佳性能。

意识到模型训练中的重复过程，许多工程师和研究人员已经将常见的构建块放入框架中。我们已经描述了两个最流行的框架:PyTorch 和 TF。这两个框架的结构相似，允许用户使用三个构建块来设置模型训练:数据加载逻辑、模型定义和模型训练逻辑。作为本章的最后一个主题，我们分解了最流行的 GAN 实现之一 StyleGAN，以了解构建模块在现实中是如何使用的。

由于 DL 需要大量的数据来进行成功的培训，因此有效的数据管理、模型实现和各种培训结果对于任何项目的成功都是至关重要的。在下一章中，我们将介绍用于 DL 实验监控的有用工具。**