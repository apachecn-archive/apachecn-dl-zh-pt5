

# 一、深度学习简介

深度学习是受人脑中神经网络启发的机器学习的子集。它已经存在十年了，但它现在如此流行的原因是由于计算的进步和大量数据的可用性。由于数据量巨大，深度学习算法的性能优于经典的机器学习。它已经在计算机视觉、**自然语言处理** ( **NLP** )、语音识别等多个跨学科的科学领域得到了广泛的应用。

在本章中，我们将了解以下主题:

*   深度学习的基本概念
*   生物和人工神经元
*   人工神经网络及其层次
*   激活功能
*   人工神经网络中的前向和后向传播
*   梯度检测算法
*   从头开始构建人工神经网络



# 什么是深度学习？

深度学习只是具有许多层的人工神经网络的现代名称。深度学习中的*深度*是什么呢？基本上是由于**人工神经网络** ( **ANN** )的结构。ANN 由一些 *n* 层组成，以执行任何计算。我们可以建立一个有几层的人工神经网络，每层负责学习数据中复杂的模式。由于计算技术的进步，我们甚至可以建立一个 100 或 1000 层深的网络。由于人工神经网络使用深层来执行学习，我们称之为深度学习，当人工神经网络使用深层来学习时，我们称之为深度网络。我们已经知道深度学习是机器学习的一个子集。深度学习和机器学习有什么不同？是什么让深度学习如此特别和受欢迎？

机器学习的成功在于正确的功能集。特征工程在机器学习中起着至关重要的作用。如果我们手工制作正确的特征集来预测某个结果，那么机器学习算法可以表现得很好，但找到和设计正确的特征集并不是一件容易的事情。

有了深度学习，我们就不用手工制作这样的功能了。由于深度人工神经网络采用几个层次，它自己学习复杂的内在特征和数据的多层次抽象表示。让我们用一个类比来探讨一下这个问题。

假设我们想要执行一个图像分类任务。比方说，我们正在学习识别一幅图像是否包含一只狗。通过机器学习，我们需要手工制作一些特征来帮助模型理解图像中是否包含一只狗。我们将这些手工制作的特征作为输入发送给机器学习算法，然后机器学习算法学习特征和标签(狗)之间的映射。但是从图像中提取特征是一项单调乏味的任务。通过深度学习，我们只需要向深度神经网络输入一堆图像，它就会通过学习正确的特征集来自动充当特征提取器。正如我们所了解的，ANN 使用多个层；在第一层中，它将学习表征狗的图像的基本特征，例如狗的身体结构，在随后的层中，它将学习复杂的特征。一旦它学习到正确的特征集，它将在图像中寻找这些特征的存在。如果这些特征存在，那么它表示给定的图像包含一只狗。因此，与机器学习不同，使用 DL，我们不必手动设计功能，相反，网络将自己学习任务所需的正确功能集。

由于深度学习的这一有趣方面，它主要用于难以提取特征的非结构化数据集，如语音识别、文本分类等。当我们拥有相当数量的庞大数据集时，深度学习算法擅长提取特征，并将提取的特征映射到它们的标签上。说了这么多，深度学习并不仅仅是把一堆数据点扔到一个深度网络里就能得到结果。也没那么简单。我们将有许多超参数作为调优旋钮来获得更好的结果，我们将在接下来的章节中探讨。

尽管深度学习的性能优于传统的机器学习模型，但不建议对较小的数据集使用 DL。当我们没有足够的数据点或者数据非常简单时，深度学习算法很容易过度适应训练数据集，并且无法在看不见的数据集上很好地推广。因此，只有当我们拥有大量数据点时，我们才应该应用深度学习。

深度学习的应用数不胜数，几乎无处不在。一些有趣的应用程序包括自动为图像生成标题、为无声电影添加声音、将黑白图像转换为彩色图像、生成文本等等。谷歌的语言翻译、网飞、亚马逊和 Spotify 的推荐引擎，以及自动驾驶汽车都是由深度学习驱动的一些应用。毫无疑问，深度学习是一项颠覆性的技术，并在过去几年中取得了巨大的技术进步。

在本书中，我们将从基本的深度学习算法中学习算法的状态，方法是从零开始构建深度学习的一些有趣应用，包括图像识别、生成歌词、预测比特币价格、生成逼真的人工图像、将照片转换为绘画等等。已经兴奋了？我们开始吧！



# 生物和人工神经元

在继续之前，首先，我们将探索什么是神经元以及神经元在我们大脑中实际上是如何工作的，然后我们将了解人工神经元。

一个**神经元**可以定义为人脑的基本计算单位。神经元是我们大脑和神经系统的基本单位。我们的大脑包含大约 1000 亿个神经元。每一个神经元都通过一种叫做**突触**的结构相互连接，这种结构负责接收外部环境的输入，感觉器官负责向我们的肌肉发送运动指令，并执行其他活动。

一个神经元也可以通过称为**树突**的分支状结构接收来自其他神经元的输入。这些输入被加强或削弱；也就是说，根据它们的重要性对它们进行加权，然后在称为**细胞体**的细胞体中对它们求和。从细胞体，这些相加的输入被处理并通过**轴突**被发送到其他神经元。

基本的单个生物神经元如下图所示:

![](img/c34c9841-5d50-4d12-ba6b-8a3437187c69.png)

现在，让我们看看人工神经元是如何工作的。假设我们有三个输入 [![](img/dbdfdc31-7b1f-4278-8390-b1e98e50047b.png)] 、 [![](img/99fdffc8-a9fe-4134-af1d-48fdef7ee8d7.png)] 和 [![](img/3799a7fd-21c3-4d2d-bbdb-252177cfa98a.png)] ，来预测输出 [![](img/c4d040ed-3f79-4281-8103-a5408f345d6e.png)] 。这些输入乘以权重 [![](img/50e7081f-b6b0-4249-949c-349b0f28ff13.png)] 、 [![](img/92e8cf78-f1fc-48f7-8a84-cd4ea925be6c.png)] 和 [![](img/12920197-2d25-4066-b8b0-e9bed69b3db6.png)] ，并汇总如下:

![](img/3d90597b-20ae-4167-9118-36496338db66.png)

但是为什么我们要用权重来乘以这些输入呢？因为在计算输出![](img/799d4419-1f3d-4337-97a3-a3f0a9f6a394.png)时，并非所有的输入都同等重要。假设与其他两个输入相比，![](img/ff460794-7422-40a7-b2a9-57947337bd5b.png)在计算输出时更重要。然后，我们赋予 [![](img/d174252e-29c4-44ba-96e6-5d7020ed6bd5.png)] 一个比其他两个权重更高的值。因此，在将权重与输入相乘时， [![](img/906e8174-801b-4eef-b5db-5a3142c3333a.png)] 将具有比其他两个输入更高的值。简而言之，权重用于加强输入。将输入乘以权重后，我们将它们相加，并添加一个名为 bias 的值，![](img/96616fe5-e29e-404a-aced-6dfe47191b40.png):

![](img/62b24d74-3072-4188-bcb0-402c2b2c8221.png)

如果你仔细看前面的等式，它可能看起来很熟悉？ [![](img/79f6816d-0766-498b-b832-1b334c9c0323.png)] 看起来不像线性回归的方程吗？不就是一条直线的方程吗？我们知道直线的方程式如下:

![](img/90183881-f6c9-4d97-addd-29b3d0efce9b.png)

这里 *m* 是权重(系数)， *x* 是输入， *b* 是偏差(截距)。

嗯，是的。那么，神经元和线性回归有什么区别呢？在神经元中，我们通过应用被称为**激活**或**传递函数**的函数![](img/2de94421-2982-487b-ba76-37fcbd4d91d6.png)，将非线性引入结果![](img/79f6816d-0766-498b-b832-1b334c9c0323.png)。因此，我们的输出变成:

![](img/bcc6610a-e632-404d-b3e0-69abb61ef764.png)

单个人工神经元如下图所示:

![](img/4cc8f0a1-8bf3-4fad-9844-d322c2056852.jpg)

因此，一个神经元接受输入， *x* ，乘以权重， *w，*，加上偏置， *b，*形成![](img/31ba413e-23d8-4ad1-8c7e-5e2b3f0f5d45.png)，然后我们对![](img/3aa2453f-17dc-49ca-ab0a-f844a21daf3d.png)应用激活函数，得到输出，![](img/c70627ba-a0d8-4c88-b471-4e09c1c94af5.png)。



# 人工神经网络及其层次

虽然神经元真的很酷，但我们不能只用一个神经元来执行复杂的任务。这就是为什么我们的大脑有数十亿个神经元，层层堆叠，形成一个网络。同样，人工神经元也是分层排列的。每一层都以这样的方式连接，信息从一层传递到另一层。

典型的人工神经网络由以下几层组成:

*   输入层
*   隐蔽层
*   输出层

每一层都有一组神经元，一层中的神经元与其他层中的所有神经元相互作用。然而，同一层中的神经元不会相互作用。这仅仅是因为相邻层的神经元之间有连接或边缘；但是，同一层的神经元没有任何联系。我们使用术语**节点**或**单元**来表示人工神经网络中的神经元。

典型的人工神经网络如下图所示:

![](img/ada614a6-bc63-4e6c-8faa-d4289e2bd97f.png)



# 输入层

**输入层**是我们向网络提供输入的地方。输入层中神经元的数量就是我们提供给网络的输入数量。每个输入都会对预测输出产生一些影响。然而，在输入层中不执行任何计算；它只是用来把外界的信息传递给网络。



# 隐蔽层

输入层和输出层之间的任何层都称为**隐藏层**。它处理从输入层接收的输入。隐藏层负责导出输入和输出之间的复杂关系。也就是说，隐藏层标识数据集中的模式。它主要负责学习数据表示和提取特征。

可以有任意数量的隐藏层；然而，我们必须根据我们的使用情况选择一些隐藏层。对于一个非常简单的问题，我们可以只使用一个隐藏层，但在执行图像识别等复杂任务时，我们会使用许多隐藏层，其中每一层都负责提取重要的特征。当我们有许多隐藏层时，该网络被称为**深度神经网络**。



# 输出层

处理输入后，隐藏层将其结果发送到输出层。顾名思义，输出层发出输出。输出层中神经元的数量取决于我们希望网络解决的问题类型。

如果是二进制分类，那么输出层的神经元数量就是告诉我们输入属于哪一类的数量。如果它是一个多类分类，比如说，有五个类，如果我们想得到每个类的概率作为输出，那么输出层中的神经元数量是五个，每个发射概率。如果是回归问题，那么我们在输出层有一个神经元。



# 探索激活功能

一个**激活函数**，也被称为**传递函数**，在神经网络中起着至关重要的作用。它用于在神经网络中引入非线性。正如我们之前所学的，我们将激活函数应用于输入，输入乘以权重并添加到偏差，即![](img/9cf85797-e709-49e7-a4c8-d7a84c9b78a2.png)，其中 *z =(输入*权重)+偏差*和![](img/d14376f3-d671-4abb-8956-7db1284efec4.png)是激活函数。如果我们不应用激活函数，那么神经元就简单地类似于线性回归。激活函数的目的是引入非线性变换来学习数据中复杂的潜在模式。

现在让我们来看看一些有趣的常用激活函数。



# sigmoid 函数

**s 形函数**是最常用的激活函数之一。它在 0 和 1 之间缩放值。sigmoid 函数可以定义如下:

![](img/85674218-0ed8-4c46-a215-d0ad662c3dac.png)

这是一条 S 形曲线，如下所示:

![](img/7f9d30e5-1892-4249-8ae0-c82e1254e33a.png)

它是可微的，这意味着我们可以找到曲线在任意两点的斜率。它是**单调的**，这意味着它要么完全不增加，要么完全不减少。sigmoid 函数也称为**逻辑**函数。正如我们所知，概率位于 0 和 1 之间，由于 sigmoid 函数压缩 0 和 1 之间的值，因此它用于预测输出的概率。

在 Python 中，sigmoid 函数可以定义如下:

```py
def sigmoid(x):

    return 1/ (1+np.exp(-x))
```



# 双曲正切函数

**双曲正切(tanh)** 函数输出-1 到+1 之间的值，表示如下:

![](img/2fd28ae8-16bc-4064-9f34-1f7ac917673f.png)

它也类似于 S 形曲线。与以 0.5 为中心的 sigmoid 函数不同，tanh 函数以 0 为中心，如下图所示:

![](img/c6f8b6b5-e378-469d-97f4-8598b7859e13.png)

类似于 sigmoid 函数，它也是一个可微且单调的函数。双曲正切函数实现如下:

```py
def tanh(x):
    numerator = 1-np.exp(-2*x)
    denominator = 1+np.exp(-2*x)

    return numerator/denominator
```



# 校正的线性单位函数

**整流线性单元** ( **ReLU** )功能是另一种最常用的激活功能。它输出一个从 0 到无穷大的值。它基本上是一个**分段**函数，可以表示如下:

![](img/4b1830c9-1f13-4bed-ba70-be53b28c4a5e.png)

即当 *x* 的值小于零时![](img/9b9b4d0d-62e5-4f60-9a70-b9b830397fd2.png)返回零，当 *x* 的值大于或等于零时![](img/d8c4dc45-d54f-4539-89f5-bcd67c4e4e6b.png)返回 *x* 。它也可以表示如下:

![](img/54c73360-a603-4ca4-b6de-9848f839e2ba.png)

ReLU 功能如下图所示:

![](img/9c3ebd42-c9ff-4ba7-9d14-df25c70a9125.png)

正如我们在上图中看到的，当我们将任何负输入提供给 ReLU 函数时，它会将其转换为零。所有负值为零的障碍是一个被称为**死亡的问题，如果一个神经元总是输出零，那么它就被称为死亡。ReLU 函数可以按如下方式实现:**

```py
def ReLU(x):
    if x<0:
        return 0
    else:
        return x
```



# 泄漏的 ReLU 函数

**Leaky ReLU** 是 ReLU 函数的变种，解决了将死的 ReLU 问题。它不是将每个负输入转换为零，而是对负值有一个小斜率，如下所示:

![](img/b7cbffba-0634-45c9-9227-8ce07581d3d6.png)

泄漏 ReLU 可以表示如下:

![](img/7698bd93-47b2-412a-a6f8-09f95f80e988.png)

![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png)的值通常设置为 0.01。泄漏 ReLU 功能实现如下:

```py
def leakyReLU(x,alpha=0.01):
    if x<0:
        return (alpha*x)
    else:
        return x
```

我们可以将它们作为参数发送给神经网络，并使网络学习最佳值![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png)，而不是将一些默认值设置为![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png)。这种激活功能可以称为**参数 ReLU** 功能。我们也可以将![](img/21bbeeeb-1cb8-497e-851b-064b57f8e1cc.png)的值设置为某个随机值，称为**Rando**mized ReLU 函数。



# 指数线性单位函数

**指数线性单元** ( **ELU** )和 Leaky ReLU 一样，对负值的斜率很小。但它不是直线，而是对数曲线，如下图所示:

![](img/5b5db978-888c-4ed6-9281-03376d1f6737.png)

它可以表示如下:

![](img/511a1bb4-db49-4bc2-a77e-a5567616c7e9.png)

`ELU`函数在 Python 中实现如下:

```py
def ELU(x,alpha=0.01):
    if x<0:
        return ((alpha*(np.exp(x)-1))
    else:
        return x
```



# Swish 函数

**Swish** 功能是谷歌最近推出的激活功能。与其他单调的激活函数不同，Swish 是一个非单调函数，这意味着它既不总是非递增也不总是非递减的。它提供了比 ReLU 更好的性能。它很简单，可以表达如下:

![](img/f188d07c-c687-470f-b22a-b652e77a8d0a.png)

这里，![](img/b347a90c-363c-4b47-b023-147ee0110f4e.png)是乙状结肠函数。Swish 函数如下图所示:

![](img/2352091f-84a6-4451-96f1-7a0a18ddce2f.png)

我们还可以重新参数化 Swish 函数，并将其表示如下:

![](img/661a40be-cffb-4345-9e47-ea0801a30812.png)

当![](img/165eb486-3642-4860-96ae-5d53b69eea23.png)的值为 0 时，那么我们得到恒等函数![](img/22835e77-9093-4d13-94c3-1abd677b0b28.png)。

它变成了一个线性函数，当![](img/165eb486-3642-4860-96ae-5d53b69eea23.png)的值趋于无穷大时，那么![](img/f199e52d-ee6f-4cdf-a12f-384e23beba5b.png)就变成了![](img/65683175-77c3-4cd5-b79d-e8770b79f073.png)，基本上就是 ReLU 函数乘以某个常数值。因此，![](img/165eb486-3642-4860-96ae-5d53b69eea23.png)的值充当了线性和非线性函数之间的良好插值。swish 函数可以如下所示实现:

```py
def swish(x,beta):
    return 2*x*sigmoid(beta*x)
```



# softmax 函数

**softmax 函数**基本上是 sigmoid 函数的推广。它通常应用于网络的最后一层，同时执行多类分类任务。它给出了每个类被输出的概率，因此，softmax 值的总和将始终等于 1。

它可以表示如下:

![](img/e3f8ae71-6850-4e09-985a-310b056c3c02.png)

如下图所示，softmax 函数将其输入转换为概率:

![](img/f7adea0b-6ffc-4df5-ae4e-422765495182.png)

`softmax`函数可以用 Python 实现，如下所示:

```py
def softmax(x):
    return np.exp(x) / np.exp(x).sum(axis=0)
```



# 人工神经网络中的前向传播

在这一节中，我们将看到人工神经网络如何学习神经元在层中的堆积位置。网络的层数等于隐藏层数加上输出层数。在计算网络的层数时，我们不考虑输入层。考虑一个两层神经网络，其中一个输入层为![](img/0ca38542-0bbd-4c1d-aab1-041856efa069.png)，一个隐藏层为![](img/23c5a8d4-df59-46d0-bad7-4fa3019edf96.png)，一个输出层为![](img/414c1121-e805-4f7f-a1b4-be8e0bc102a6.png)，如下图所示:

![](img/9324b53f-3c67-42a9-ac50-02d9561c0e8b.png)

假设我们有两个输入，![](img/e576bc39-3496-4769-83f1-0da92a9fd56b.png)和![](img/d98cc1ae-745a-4be3-b5c7-7d172de62d5b.png)，我们必须预测输出，![](img/00c11656-9e19-47ee-b758-117b6358ae91.png)。由于我们有两个输入，输入层的神经元数量将是两个。我们将隐藏层中的神经元数量设置为 4，将输出层中的神经元数量设置为 1。现在，输入将乘以权重，然后我们添加偏差，并将结果值传播到将应用激活函数的隐藏层。

在此之前，我们需要初始化权重矩阵。在现实世界中，我们不知道哪个输入比另一个更重要，因此我们可以对它们进行加权并计算输出。因此，我们将随机初始化权重和偏差值。隐藏层的输入之间的权重和偏差值分别由![](img/9439f056-b74c-40ae-b855-01f76b5f3de0.png)和![](img/97fdd9d3-3eeb-416c-855c-ba075b92e1f5.png)表示。权重矩阵的维数呢？权重矩阵的维数必须是当前层神经元的*数* x 下一层神经元的*数*。这是为什么呢？

因为这是一个基本的矩阵乘法规则。要将任意两个矩阵 *AB* 相乘，矩阵 *A* 的列数必须等于矩阵 *B* 的行数。所以，权重矩阵的维数![](img/1ae8dad5-6ffd-49e3-9c63-ad8f7a727b82.png)应该是*输入层的神经元数量* x *隐藏层的神经元数量*，即 2×4:

![](img/f9845f24-bfaa-4722-b430-631c893abb1c.png)

前面的等式表示，![](img/0a21b4e7-70c8-4efb-b08c-e55457170f0b.png)。现在，这被传递到隐藏层。在隐藏层中，我们对![](img/ccd2fedf-a381-487e-8e37-b3dedd059c0b.png)应用一个激活函数。让我们使用 sigmoid ![](img/4d1ee602-8255-4022-b0eb-d52c6dca7c54.png)激活功能。然后，我们可以写:

![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)

在应用激活函数之后，我们再次将结果![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png)乘以新的权重矩阵，并添加在隐藏层和输出层之间流动的新的偏置值。我们可以将这个权重矩阵和偏差分别表示为![](img/e460863e-35ab-41d3-855a-fdaa58c780c8.png)和![](img/910bb4a3-6b59-441d-a624-a2aec21c2190.png)。权重矩阵的维数![](img/c86587a7-4639-4ad9-9777-0ce185b1dbf4.png)将是隐藏层中神经元的*数量* x 输出层中神经元的数量。因为我们在隐藏层中有四个神经元，在输出层中有一个神经元，所以![](img/5268e1a5-2ffe-478b-b375-69e7bee62e29.png)矩阵维数将是 4×1。因此，我们将![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png)乘以权重矩阵![](img/9bbf9460-67f9-4cb8-8302-da3866a28c8c.png)，并添加偏差![](img/8030beb2-9a44-4823-99e8-438002a73a4d.png)，并将结果![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png)传递给下一层，即输出层:

![](img/d6c4af2f-65ac-4d56-a396-74add1443848.png)

现在，在输出层，我们对![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png)应用一个 sigmoid 函数，这将产生一个输出值:

![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)

从输入层到输出层的整个过程被称为**正向传播**。因此，为了预测输出值，输入从输入层传播到输出层。在此传播过程中，它们在每一层上乘以各自的权重，并在它们的顶部应用激活函数。完整的正向传播步骤如下所示:

![](img/4bd12cf0-2cc6-48ba-8f5a-d38dc68ded6b.png)

![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)

![](img/32e542ad-cb13-401d-92b4-40e1dae62ea7.png)

![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)

前面的正向传播步骤可以在 Python 中实现，如下所示:

```py
def forward_prop(X):
    z1 = np.dot(X,Wxh) + bh
    a1 = sigmoid(z1)
    z2 = np.dot(a1,Why) + by
    y_hat = sigmoid(z2)

    return y_hat
```

正向传播很酷吧？但是我们如何知道神经网络生成的输出是否正确呢？我们定义了一个新函数，称为 **co** **st 函数** ( ![](img/5bb1b6b9-cd5d-44b2-8a3a-896a26d1e80f.png))，也称为**损失函数** ( ![](img/fbc4098e-0480-492b-8742-2e055c6fbb8e.png))，它告诉我们我们的神经网络表现如何。有许多不同的成本函数。我们将使用均方误差作为成本函数，它可以定义为实际输出和预测输出之间的平方差的平均值:

![](img/c488ce70-38e6-4e74-b409-3334dfdf65ad.png)

这里，![](img/50e54b3e-22b0-4fe5-9a09-4ec202021219.png)是训练样本数，![](img/cf05ae33-0d55-44f2-b238-b3e2cef84419.png)是实际输出，![](img/2e0c94cb-54ad-4658-b9c7-f13cb993c739.png)是预测输出。

好了，我们知道了成本函数是用来评估我们的神经网络的。也就是说，它告诉我们我们的神经网络在预测输出方面有多好。但问题是我们的网络实际上是在哪里学习呢？在前向传播中，网络只是试图预测输出。但是它如何学会预测正确的输出呢？在下一节中，我们将对此进行研究。



# 安是如何学习的？

如果成本或损失非常高，那么这意味着我们的网络没有预测正确的输出。因此，我们的目标是最小化成本函数，以便我们的神经网络预测会更好。怎样才能最小化成本函数？也就是说，怎样才能把损失/成本降到最低？我们了解到神经网络使用前向传播进行预测。因此，如果我们可以改变正向传播中的一些值，我们就可以预测正确的输出，并将损耗降至最低。但是在正向传播中我们可以改变什么值呢？显然，我们不能改变输入和输出。我们现在只剩下权重和偏差值。记住，我们只是随机初始化权重矩阵。由于权重是随机的，它们不会是完美的。现在，我们将更新这些权重矩阵(![](img/3e007dfd-6705-4a5f-8d63-9b8e1663031f.png)和![](img/1c5358ea-4e8c-49b0-9e12-af1c8d04c16e.png))，以使我们的神经网络给出正确的输出。我们如何更新这些权重矩阵？这里出现了一种叫做**梯度下降**的新技术。

通过梯度下降，神经网络学习随机初始化的权重矩阵的最优值。有了权重的最优值，我们的网络可以预测正确的输出并最小化损失。

现在，我们将探索如何使用梯度下降来学习权重的最佳值。梯度下降是最常用的优化算法之一。它用于最小化成本函数，这允许我们最小化误差并获得尽可能低的误差值。但是梯度下降是如何找到最优权重的呢？先打个比方。

假设我们在一个山顶上，如下图所示，我们想到达山顶的最低点。可能有许多区域看起来像山的最低点，但我们必须到达最低点，实际上是所有最低点。

也就是说，当全球最低点存在时，我们不应该停留在一个点上，认为这是最低点:

![](img/99ca846c-c054-412d-8798-0485964e6051.png)

类似地，我们可以将成本函数表示如下。这是成本与重量的关系图。我们的目标是最小化成本函数。也就是说，我们必须达到成本最小的最低点。下图中的实心黑点显示了随机初始化的权重。如果我们把这一点向下移动，那么我们可以到达成本最小的点:

![](img/28e3cddf-581f-4795-ba7b-5621bdb76849.png)

但是怎么才能把这个点(初始重量)下移呢？怎样才能下降到最低点？渐变用于从一点移动到另一点。因此，我们可以通过计算成本函数相对于该点(初始权重)的梯度来移动该点(初始权重)，这是![](img/cdaa7865-fa7e-4521-b9ac-6e2029df1e56.png)。

梯度是导数，实际上是切线的斜率，如下图所示。因此，通过计算梯度，我们下降(向下移动)并到达成本最小的最低点。梯度下降是一种一阶优化算法，这意味着我们在执行更新时只考虑一阶导数:

![](img/cc6ee8c1-f612-4953-8e0d-41520c38fae7.png)

因此，通过梯度下降，我们将我们的权重移动到成本最小的位置。但是，我们如何更新权重呢？

由于正向传播，我们处于输出层。我们现在将**将网络从输出层反向传播**到输入层，并计算成本函数相对于输出层和输入层之间所有权重的梯度，以便我们可以最小化误差。计算梯度后，我们使用权重更新规则更新旧的权重:

![](img/c038f863-5459-4ada-a1a6-6406f93fee39.png)

这意味着*权重=权重-α *梯度*。

什么是![](img/8179ea37-437e-40e1-a563-98346e3be569.png)？它被称为**学习率**。如下图所示，如果学习率很小，那么我们向下迈一小步，我们的梯度下降可以很慢。

如果学习率很大，那么我们就迈出一大步，我们的梯度下降将会很快，但我们可能无法达到全局最小值，并陷入局部最小值。因此，学习率应该被最佳地选择:

![](img/975346d3-1711-4973-bacf-8bd77141ff5b.png)

将网络从输出层反向传播到输入层，并使用梯度下降来更新网络的权重以最小化损失的整个过程称为**反向传播**。现在我们对反向传播有了基本的了解，我们将通过一步一步地详细了解它来加强我们的理解。我们要看一些有趣的数学，所以戴上你的微积分帽子，按照步骤来。

因此，我们有两个权重，一个![](img/ca5dfcf7-41c2-4a8a-8883-9b30519930bd.png)，输入隐藏层权重，另一个![](img/9576ab5d-840e-4382-8130-bd525f556023.png)，隐藏输出层权重。我们需要找到这两个权重的最佳值，使我们的误差最小。因此，我们需要计算成本函数![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)相对于这些权重的导数。由于我们是反向传播，也就是从输出层到输入层，我们的第一个权重将是![](img/487d3133-d88e-46eb-abdf-cc4c577a5fe4.png)。所以，现在我们需要计算![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)对![](img/caea536c-bf53-4095-83b8-56981f591219.png)的导数。我们如何计算导数？首先，让我们回忆一下我们的成本函数，![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png):

![](img/89657612-2da7-480d-9adc-4ba544bec713.png)

我们不能直接从前面的方程计算导数，因为没有

![](img/5f7cfa6f-9a88-4dfe-834b-f8a0e432d721.png)术语。所以，我们不直接计算导数，而是计算偏导数。让我们回忆一下正向传播方程:

![](img/cfff4ced-f916-459e-91ec-436bb2571df5.png)

![](img/35a873f3-75f8-4c90-9429-dfba3073c530.png)

首先，我们将计算关于 [![](img/34155884-cff2-46fe-8289-ec304f441a2e.png)] 的偏导数，然后从 [![](img/28d2e95e-c6cc-4fa3-bbf5-2570bd4df04d.png)] 我们将计算关于 [![](img/1096ac8b-40a8-400d-904a-0dce7783570f.png)] 的偏导数。从 [![](img/f2071a3b-05b8-4315-8f6b-0cb42f3bfb29.png)] ，我们可以直接计算出我们的导数 [![](img/0895be9d-fcbd-4237-9b14-3db88a268320.png)] 。基本上是链式法则。因此， [![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)] 相对于![](img/fc832c43-00ac-4825-8cc2-945ab173d55e.png)的导数变成如下:

![](img/2e4c510e-c439-4877-8a81-5dffd3972c41.png)

现在，我们将计算上述等式中的每一项:

![](img/74146337-3cbe-4c45-95b8-7043c21022bd.png)

![](img/2fb19f9c-87bc-4e67-ab23-54d2e2553a41.png)

这里，![](img/10c7ef2e-5667-46f5-a0f7-c200f50165ed.png)是我们的 sigmoid 激活函数的导数。我们知道，sigmoid 函数是 [![](img/92e46632-0b6b-4fa7-80f2-80f925d761ad.png)] ，那么 sigmoid 函数的导数就会是 [![](img/032ca043-1110-4ca5-9b24-7f617d042364.png)] :

![](img/51925f25-3fc2-4e4d-90d5-f1e6ecb8b5a4.png)

因此，代入等式 *(1)* 中的所有前述项，我们可以写出:

![](img/23f8268d-7224-44e9-9a8c-ac130056191d.png)

现在我们需要计算 [![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)] 相对于下一个权重 [![](img/913e2324-d25d-4230-b0ca-dea93a027c15.png)] 的导数。

同样，我们不能直接从![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)计算![](img/092f60bc-0ed1-4a0a-887a-960a2768b4e2.png)的导数，因为我们在![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)中没有任何![](img/83bd97fb-7a2b-4b55-bb8e-cc8f00c7c295.png)项。所以，我们需要使用链式法则。让我们再次回忆一下正向传播的步骤:

![](img/1301fd01-bc90-4a01-aeb3-53298fbf474c.png)

![](img/6cbad05b-da62-4fb0-a8da-1612262b3ba9.png)

![](img/49fbf478-b44b-4544-a88d-fa38af76c837.png)

![](img/dd27058f-3ed2-42c9-85f0-708b0e63134c.png)

现在，根据链式法则，![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)对![](img/0b948307-d86f-476f-b942-96b67cc16136.png)的导数为:

![](img/aa8ae1ab-3d15-43a4-bbd8-96a0b90c9357.png)

我们已经看到如何计算前面等式中的第一项；现在，我们将了解如何计算其余各项:

![](img/238243e0-d20f-441c-b5ba-2d78d2c0f265.png)

![](img/f6fb1473-f05d-4beb-b75d-f9a06d398cb1.png)

![](img/40c69666-cd5e-49e2-995d-c7a662f86659.png)

因此，代入等式 *(3)* 中的所有前述项，我们可以写出:

![](img/552fd026-1f25-4a95-a947-90f8eb1a0301.png)

在我们计算了两个权重![](img/3d784cf7-f536-456e-8515-2eb49e30e4b8.png)和![](img/7a0fa5e1-2c18-4cb8-97dc-1f4c265608e5.png)的梯度后，我们将根据权重更新规则更新我们的初始权重:

![](img/7d4f9ae6-6194-43ad-b261-a74a597d85b6.png)

![](img/ca8087d7-8cdd-47d7-ac17-260216c22fb7.png)

就是这样！这就是我们如何更新网络的权重并最小化损失。如果你还不了解梯度下降，不用担心！在[第 3 章](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml)、*梯度下降及其变体*中，我们将深入基础，更详细地学习梯度下降及其几种变体。现在，让我们看看如何用 Python 实现反向传播算法。

在两个方程 *(2)* 和 *(4)* 中，我们都有![](img/e8bea459-13b0-41fa-84e4-506d4731e466.png)这一项，所以不用反复计算它们，我们就称它们为`delta2`:

```py
delta2 = np.multiply(-(y-yHat),sigmoidPrime(z2))
```

现在，我们计算相对于![](img/cbc989b7-173e-4463-967d-fff236c07ffc.png)的梯度。参考等式 *(2)* :

```py
dJ_dWhy = np.dot(a1.T,delta2)
```

我们计算相对于![](img/0ca26aa5-e066-454a-b0ac-a0042b545572.png)的梯度。参考方程式 *(4)* :

```py
delta1 = np.dot(delta2,Why.T)*sigmoidPrime(z1)

dJ_dWxh = np.dot(X.T,delta1)
```

我们将根据我们的权重更新规则等式 *(5)* 和 *(6)* 更新权重，如下所示:

```py
Wxh = Wxh - alpha * dJ_dWhy
Why = Why - alpha * dJ_dWxh 
```

反向传播的完整代码如下所示:

```py
def backword_prop(y_hat, z1, a1, z2):
    delta2 = np.multiply(-(y-y_hat),sigmoid_derivative(z2))
    dJ_dWhy = np.dot(a1.T, delta2)

    delta1 = np.dot(delta2,Why.T)*sigmoid_derivative(z1)
    dJ_dWxh = np.dot(X.T, delta1) 

    Wxh = Wxh - alpha * dJ_dWhy
    Why = Why - alpha * dJ_dWxh

    return Wxh,Why
```

在继续之前，让我们熟悉一下神经网络中的一些常用术语:

*   **正向传递**:正向传递意味着从输入层到输出层的正向传播。
*   **反向传递**:反向传递意味着从输出层到输入层的反向传播。
*   **Epoch**:Epoch 指定了神经网络看到我们整个训练数据的次数。因此，我们可以说，对于所有训练样本，一个历元等于一个正向传递和一个反向传递。
*   **批量**:批量指定我们在一次正向传递和一次反向传递中使用的训练样本的数量。
*   **迭代次数**:迭代次数是指通过的次数，其中*一次通过=一次正向通过+一次反向通过*。

假设我们有 12，000 个训练样本，并且我们的批量大小是 6，000。我们需要两次迭代来完成一个纪元。也就是说，在第一次迭代中，我们传递前 6000 个样本，并执行前向传递和后向传递；在第二次迭代中，我们传递接下来的 6000 个样本，并执行前向传递和后向传递。经过两次迭代后，我们的神经网络将看到全部 12，000 个训练样本，这使其成为一个纪元。



# 使用梯度检查调试梯度下降

我们刚刚学习了梯度下降是如何工作的，以及如何为一个简单的两层网络从头开始编写梯度下降算法。但是对复杂的神经网络实现梯度下降并不是一项简单的任务。除了实现之外，为复杂的神经网络架构调试梯度下降也是一项乏味的任务。令人惊讶的是，即使有一些错误的梯度下降实现，网络也会学到一些东西。然而，显然，与梯度下降的无缺陷实现相比，它的性能不会很好。

如果模型没有给我们任何错误，甚至在梯度下降算法的错误实现中也学到了一些东西，我们如何评估并确保我们的实现是正确的？这就是为什么我们使用梯度检查算法。这将有助于我们通过数值检查导数来验证梯度下降的实现。

梯度检查主要用于调试梯度下降算法，并验证我们的实现是否正确。

好吧。那么，梯度检查是如何工作的呢？在梯度检查中，我们主要比较数值梯度和解析梯度。等等！什么是数值梯度和解析梯度？

**分析梯度**意味着我们通过反向传播计算的梯度。**数值梯度**是梯度的数值近似值。让我们用一个例子来探讨这个问题。假设我们有一个简单的平方函数，![](img/2b8960b3-4692-488f-a07a-ef6e3fc3e048.png)。

上述函数的解析梯度使用幂法则计算，如下所示:

![](img/421425f3-93c8-4fce-b526-90e13997f24c.png)

现在，让我们看看如何在数值上近似梯度。我们使用梯度的定义来计算梯度，而不是使用幂法则来计算梯度。我们知道函数的梯度或斜率基本上给出了函数的陡度。

因此，函数的梯度或斜率定义如下:

![](img/0d706589-2d5e-4491-8114-39a488a68528.png)

函数的梯度可以如下给出:

![](img/e8bc90bc-d88e-4c15-b986-c292d0c6c2e2.png)

我们使用前面的方程，用数值方法近似梯度。这意味着我们正在手动计算函数的斜率，而不是使用下图所示的幂法则:

![](img/91c3dccd-2d04-46ff-9f7e-7c088c80f5ab.png)

通过幂法则 *(7)* 计算梯度，并在数值上近似梯度 *(8)* 本质上给我们相同的值。我们来看看【T6(7)】和 *(8)* 如何在 Python 中给出相同的值。

定义平方函数:

```py
def f(x):
    return x**2
```

定义ε和输入值:

```py
epsilon = 1e-2
x=3
```

计算分析梯度:

```py
analytical_gradient = 2*x

print analytical_gradient

6
```

计算数值梯度:

```py
numerical_gradient = (f(x+epsilon) - f(x-epsilon)) / (2*epsilon)

print numerical_gradient

6.000000000012662
```

您可能已经注意到，计算平方函数的数值梯度和解析梯度实际上给了我们相同的值，即当 *x =3* 时的`6`。

当反向传播网络时，我们计算分析梯度以最小化成本函数。现在，我们需要确保我们计算的解析梯度是正确的。因此，让我们验证我们近似成本函数的数值梯度。

![](img/b89b62f6-a88e-4d9d-9293-b4c8e2cb0f69.png)相对于![](img/3de54252-f200-48b3-8ade-0c6e5fc5e2b3.png)的梯度可以数值近似如下:

![](img/176aded0-ffbb-43c7-b899-b15888bcd4a3.png)

它表示如下:

![](img/2d6e039e-148f-4c8b-a60c-03c190eef054.png)

我们检查解析梯度和近似数值梯度是否相同；如果不是，那么在我们的解析梯度计算中有一个错误。我们不想检查数值梯度和解析梯度是否完全相同；因为我们只是在近似数值梯度，所以我们把解析梯度和数值梯度之间的差异作为一个误差来检查。如果差值小于或等于一个很小的数，比如说， *1e-7* ，那么我们的实现就很好。如果差值大于 *1e-7* ，那么我们的实现是错误的。

我们计算相对误差，而不是将误差直接计算为数值梯度和解析梯度之差。它可以定义为差值与梯度绝对值之比的比值:

![](img/b4c41974-fd9c-45ea-9fa8-49b7eca8c7a9.png)

当相对误差的值小于或等于一个小的阈值时，比如说， *1e-7* ，那么我们的实现是好的。如果相对误差大于 *1e-7* ，那么我们的实现就是错误的。现在让我们看一下如何一步一步地用 Python 实现梯度检查算法。

首先，我们计算权重。参考方程式 *(9):*

```py
weights_plus = weights + epsilon 
weights_minus = weights - epsilon 
```

计算`J_plus`和`J_minus`。参考方程式 *(9):*

```py
J_plus = forward_prop(x, weights_plus) 
J_minus = forward_prop(x, weights_minus) 
```

现在，我们可以计算 *(9)* 中给出的数值梯度，如下所示:

```py
numerical_grad = (J_plus - J_minus) / (2 * epsilon) 
```

分析梯度可以通过反向传播获得:

```py
analytical_grad = backword_prop(x, weights)
```

根据等式 *(10)* 计算相对误差如下:

```py
numerator = np.linalg.norm(analytical_grad - numerical_grad) 
denominator = np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad) 
relative_error = numerator / denominator 
```

如果相对误差小于一个小的阈值，比如说`1e-7`，那么我们的梯度下降实现就是正确的；否则，它就是错误的:

```py
if relative_error < 1e-7:
       print ("The gradient is correct!")
else:
       print ("The gradient is wrong!")
```

因此，在梯度检查的帮助下，我们确保我们的梯度下降算法是无缺陷的。



# 把所有的放在一起

把我们到目前为止学到的所有概念放在一起，我们将看到如何从头开始构建一个神经网络。我们将理解神经网络如何学习执行异或门操作。只有当 XOR 门只有一个输入为 1 时，它才返回 1，否则它返回 0，如下表所示:

![](img/27147219-0adc-49ba-a3d4-40df55825d41.png)



# 从头开始构建神经网络

为了执行 XOR 门运算，我们构建了一个简单的两层神经网络，如下图所示。如您所见，我们有一个包含两个节点的输入层:一个包含五个节点的隐藏层和一个包含一个节点的输出层:

![](img/584c586c-8500-4150-907b-f32a004254ca.png)

我们将逐步理解神经网络如何学习 XOR 逻辑:

1.  首先，导入库:

```py
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

2.  按照前面 XOR 表中所示准备数据:

```py
X = np.array([ [0, 1], [1, 0], [1, 1],[0, 0] ])
y = p.array([ [1], [1], [0], [0]])
```

3.  定义每层中的节点数:

```py
num_input = 2
num_hidden = 5
num_output = 1
```

4.  随机初始化权重和偏差。首先，我们将输入初始化为隐藏层权重:

```py
Wxh = np.random.randn(num_input,num_hidden)
bh = np.zeros((1,num_hidden))
```

5.  现在，我们初始化隐藏到输出层的权重:

```py
Why = np.random.randn (num_hidden,num_output)
by = np.zeros((1,num_output))
```

6.  定义 sigmoid 激活函数:

```py
def sigmoid(z):
    return 1 / (1+np.exp(-z))
```

7.  定义 sigmoid 函数的导数:

```py
def sigmoid_derivative(z):
     return np.exp(-z)/((1+np.exp(-z))**2)
```

8.  定义正向传播:

```py
def forward_prop(X,Wxh,Why):
    z1 = np.dot(X,Wxh) + bh
    a1 = sigmoid(z1)
    z2 = np.dot(a1,Why) + by
    y_hat = sigmoid(z2)

    return z1,a1,z2,y_hat
```

9.  定义反向传播:

```py
def backword_prop(y_hat, z1, a1, z2):
    delta2 = np.multiply(-(y-y_hat),sigmoid_derivative(z2))
    dJ_dWhy = np.dot(a1.T, delta2)
    delta1 = np.dot(delta2,Why.T)*sigmoid_derivative(z1)
    dJ_dWxh = np.dot(X.T, delta1) 

    return dJ_dWxh, dJ_dWhy
```

10.  定义成本函数:

```py
def cost_function(y, y_hat):
    J = 0.5*sum((y-y_hat)**2)

    return J
```

11.  设置学习率和训练迭代次数:

```py
alpha = 0.01
num_iterations = 5000
```

12.  现在，让我们用下面的代码开始训练网络:

```py
cost =[]

for i in range(num_iterations):
    z1,a1,z2,y_hat = forward_prop(X,Wxh,Why)    
    dJ_dWxh, dJ_dWhy = backword_prop(y_hat, z1, a1, z2)

    #update weights
    Wxh = Wxh -alpha * dJ_dWxh
    Why = Why -alpha * dJ_dWhy

    #compute cost
    c = cost_function(y, y_hat)

    cost.append(c)
```

13.  绘制成本函数:

```py
plt.grid()
plt.plot(range(num_iteratins),cost)

plt.title('Cost Function')
plt.xlabel('Training Iterations')
plt.ylabel('Cost')
```

正如您在下图中所观察到的，损失随着训练迭代而减少:

![](img/8491d0d9-3387-4789-944b-8315befbc019.png)

因此，在这一章中，我们对人工神经网络和它们如何学习有了一个总体的了解。



# 摘要

我们从理解什么是深度学习以及它与机器学习的不同开始这一章。后来，我们学习了生物和人工神经元是如何工作的，然后我们探索了人工神经网络中的输入、隐藏和输出层，以及几种类型的激活函数。

接下来，我们学习了什么是前向传播，以及 ANN 如何使用前向传播来预测输出。在这之后，我们学习了人工神经网络如何使用反向传播进行学习和优化。我们学习了一种叫做梯度下降的优化算法，它可以帮助神经网络最小化损失并做出正确的预测。我们还学习了梯度检查，这是一种用于评估梯度下降的技术。在本章的最后，我们从头开始实现了一个神经网络来执行 XOR 门操作。

在下一章中，我们将了解一个最强大和最常用的深度学习库，名为 **TensorFlow** 。



# 问题

让我们通过回答以下问题来评估我们新获得的知识:

1.  深度学习和机器学习有什么不同？
2.  深度学习中的 *deep* 这个词是什么意思？
3.  我们为什么要使用激活功能？
4.  解释将死的 ReLU 问题。
5.  定义正向传播。
6.  什么是反向传播？
7.  解释梯度检查。



# 进一步阅读

您也可以查看这些资源以了解更多信息:

*   从这个惊人的视频中了解更多关于梯度下降:[https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)
*   了解如何从头开始实现一个神经网络来识别手写数字:[https://github.com/sar-gupta/neural-network-from-scratch](https://github.com/sar-gupta/neural-network-from-scratch)