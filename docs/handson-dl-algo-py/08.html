<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Demystifying Convolutional Networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">揭秘卷积网络</h1>

                

            

            

                

<p><strong>卷积神经网络</strong>(<strong>CNN</strong>)是最常用的深度学习算法之一。它们广泛用于与图像相关的任务，如图像识别、对象检测、图像分割等。CNN的应用是无止境的，从自动驾驶汽车的视觉供电到脸书图片中朋友的自动标记。尽管CNN广泛用于图像数据集，但它们也可以应用于文本数据集。</p>

<p>在这一章中，我们将详细了解CNN，掌握CNN的窍门以及它们是如何工作的。首先，我们将直观地了解CNN，然后我们将深入探究其背后的数学原理。接下来，我们将逐步了解如何在TensorFlow中实现CNN。接下来，我们将探索不同类型的CNN架构，如LeNet、AlexNet、VGGNet和GoogleNet。在本章的最后，我们将研究CNN的缺点以及如何使用胶囊网络来解决这些问题。此外，我们将学习如何使用TensorFlow构建胶囊网络。</p>

<p>在本章中，我们将探讨以下主题:</p>

<ul>

<li>CNN是什么？</li>

<li>CNN背后的数学</li>

<li>在TensorFlow中实现CNN</li>

<li>不同的CNN架构</li>

<li>胶囊网络</li>

<li>在TensorFlow中构建胶囊网络</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>What are CNNs?</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">CNN是什么？</h1>

                

            

            

                

<p>CNN，也被称为<strong> ConvNet </strong>，是计算机视觉任务中使用最广泛的深度学习算法之一。假设我们正在执行一个图像识别任务。考虑下面的图像。我们想让我们的CNN认识到它包含了一匹马:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/5c930822-cdbf-43ac-a5f2-2c8418f3039f.jpg" style="width:17.75em;height:18.33em;"/></p>

<p>我们如何做到这一点？当我们将图像输入计算机时，它基本上将其转换成像素值的矩阵。像素值的范围从0到255，这个矩阵的尺寸将是[ <em xmlns:epub="http://www.idpf.org/2007/ops">图像</em>T9】宽度 x <em xmlns:epub="http://www.idpf.org/2007/ops">图像高度</em> x <em xmlns:epub="http://www.idpf.org/2007/ops">通道数</em>。灰度图像有一个通道，彩色图像有三个通道<strong xmlns:epub="http://www.idpf.org/2007/ops">红、绿、蓝</strong> ( <strong xmlns:epub="http://www.idpf.org/2007/ops"> RGB </strong>)。</p>

<p>假设我们有一个宽度为11、高度为11的彩色输入图像，即11×11，那么我们的矩阵维数将是<em>【11×11×3】</em>。在<em>【11 x 11 x 3】</em>中可以看到，11 x 11代表图像的宽度和高度，3代表通道号，因为我们有一个彩色图像。因此，我们将有一个三维矩阵。</p>

<p>但是很难将3D矩阵可视化，因此，为了便于理解，让我们考虑将灰度图像作为输入。由于灰度图像只有一个通道，我们将得到一个2D矩阵。</p>

<p>如下图所示，输入灰度图像将被转换为范围从0到255的像素值矩阵，像素值代表该点的像素强度:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-995 image-border" src="img/6d93e2fb-b07f-4165-a395-80c9e0dbc857.png" style="width:30.17em;height:10.25em;"/></p>

<p>输入矩阵中给出的值只是我们理解的任意值。</p>

<p>好了，现在我们有了一个像素值的输入矩阵。接下来会发生什么？美国有线电视新闻网是如何理解图像包含一匹马的？CNN由以下三个重要层组成:</p>

<ul>

<li>卷积层</li>

<li>汇集层</li>

<li>完全连接的层</li>

</ul>

<p>在这三层的帮助下，CNN识别出图像包含一匹马。现在，我们将详细探讨每一层。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Convolutional layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">卷积层</h1>

                

            

            

                

<p>卷积层是CNN的第一层，也是核心层。它是CNN的组成部分之一，用于从图像中提取重要特征。</p>

<p>我们有一匹马的图像。你认为有哪些特征可以帮助我们理解这是一匹马的图像？我们可以说身体结构、脸、腿、尾巴等等。但是CNN是如何理解这些特征的呢？这就是我们使用卷积运算从图像中提取马的所有重要特征的地方。所以，卷积运算有助于我们理解图像是怎么回事。</p>

<p>好吧，这个卷积运算到底是什么？它是如何执行的？它是如何提取重要特征的？我们来详细看看这个。</p>

<p>众所周知，每个输入图像都由像素值矩阵表示。除了输入矩阵，我们还有另一个矩阵叫做<strong>滤波器矩阵</strong>。过滤器矩阵也被称为<strong>内核</strong>，或简称为<strong>过滤器</strong>，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-996 image-border" src="img/bdda8d40-64ac-4f3e-ad41-129f0ecb9c6c.png" style="width:18.75em;height:9.58em;"/></p>

<p>我们采用滤波器矩阵，将其在输入矩阵上滑动一个像素，执行逐元素乘法，对结果求和，并产生一个数字。这很令人困惑，不是吗？让我们借助下图更好地理解这一点:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-998 image-border" src="img/b9169595-1bfc-4898-8095-46e20d9e0efb.png" style="width:24.33em;height:9.00em;"/></p>

<p>正如您在前面的图表中看到的，我们将滤波器矩阵放在输入矩阵之上，执行元素乘法，将结果相加，并生成一个数字。这一点如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/7bbdbdf4-a60e-4ff7-844a-e92b578c1518.png" style="width:18.08em;height:1.25em;"/></p>

<p>现在，我们将滤波器在输入矩阵上滑动一个像素，并执行相同的步骤，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-999 image-border" src="img/a2a9f9f2-de3c-4bc6-a051-18eded4470d2.png" style="width:23.00em;height:9.25em;"/></p>

<p>这一点如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f13d7db8-4d70-456e-b553-174debcd9340.png" style="width:18.42em;height:1.17em;"/></p>

<p>同样，我们将滤波器矩阵滑动一个像素，并执行相同的操作，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1001 image-border" src="img/e6ecbe08-83fe-47b9-811f-9f65df0d65a6.png" style="width:22.75em;height:8.33em;"/></p>

<p class="mce-root">这一点如下所示:</p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/98f7173b-8d73-4abf-a4bd-1b016a30b82d.png" style="width:16.83em;height:1.17em;"/></p>

<p>现在，我们再次将滤波器矩阵在输入矩阵上滑动一个像素，并执行相同的操作，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1966 image-border" src="img/a4725aa5-08ef-4f96-9ded-b761cfc4e5a0.png" style="width:23.58em;height:9.58em;"/></p>

<p>那就是:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f72ca299-aeaf-442d-97e8-102631737f49.png" style="width:18.58em;height:1.17em;"/></p>

<p>好吧。我们在这里做什么？我们基本上是将滤波器矩阵在整个输入矩阵上滑动一个像素，执行逐元素乘法并对其结果求和，这创建了一个新矩阵，称为<strong>特征图</strong>或<strong>激活图</strong>。这被称为<strong>卷积运算</strong>。</p>

<p>正如我们所了解的，卷积运算用于提取特征，新的矩阵，即特征映射，表示提取的特征。如果我们绘制特征图，那么我们可以看到通过卷积运算提取的特征。</p>

<p>下图显示了实际图像(输入图像)和卷积图像(特征图)。我们可以看到，我们的滤波器已经从实际图像中检测到边缘作为特征:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1004 image-border" src="img/4e7080aa-e990-41b5-8d2a-35f73e337d15.png" style="width:28.17em;height:10.33em;"/></p>

<p>各种滤波器用于从图像中提取不同的特征。例如，如果我们使用一个锐化过滤器，<img class="fm-editor-equation" src="img/c26a4782-aa88-4916-a1bf-341300987e17.png" style="width:7.00em;height:3.75em;"/>，那么它将锐化我们的图像，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/507a9d26-2aa8-4443-a4ea-6ba2adc37947.png" style="width:23.92em;height:24.75em;"/></p>

<p>因此，我们已经知道，有了过滤器，我们可以使用卷积运算从图像中提取重要的特征。因此，我们可以使用多个过滤器从图像中提取不同的特征，而不是使用一个过滤器，并产生多个特征图。因此，特征图的深度将是过滤器的数量。如果我们使用七个过滤器从图像中提取不同的特征，那么我们的特征图的深度将是七:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1007 image-border" src="img/75d06dec-64f7-4888-ad46-b0366fcb90df.png" style="width:14.00em;height:19.50em;"/></p>

<p>好的，我们已经知道不同的过滤器从图像中提取不同的特征。但问题是，我们如何为滤波器矩阵设置正确的值，以便从图像中提取重要的特征？不要担心！我们只需随机初始化滤波矩阵，然后通过反向传播学习滤波矩阵的最优值，从而从图像中提取重要特征。然而，我们只需要指定过滤器的大小和我们想要使用的过滤器的数量。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Strides</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">大步</h1>

                

            

            

                

<p>我们刚刚学习了卷积运算是如何工作的。我们将带有滤波器矩阵的输入矩阵滑动一个像素，并执行卷积运算。但是我们不能只在输入矩阵上滑动一个像素。我们也可以在输入矩阵上滑动任意数量的像素。</p>

<p>我们通过滤波器矩阵在输入矩阵上滑动的像素数量被称为<strong>步幅</strong>。</p>

<p>如果我们将步幅设置为2，那么我们将在带有滤波器矩阵的输入矩阵上滑动两个像素。下图显示了步长为2的卷积运算:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1967 image-border" src="img/11bc6120-28bc-4d19-9ad4-5372e5bda4d6.png" style="width:34.58em;height:12.92em;"/></p>

<p>但是我们如何选择步幅呢？我们刚刚了解到，步幅是我们移动滤波器矩阵时所经过的像素数。因此，当步幅被设置为小数值时，我们可以对图像进行比步幅被设置为大数值时更详细的编码。然而，具有高值的步幅比具有低值的步幅需要更少的计算时间。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Padding</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">填料</h1>

                

            

            

                

<p>通过卷积运算，我们使用滤波器矩阵在输入矩阵上滑动。但是在某些情况下，滤波器并不完全适合输入矩阵。我们这样说是什么意思？例如，假设我们正在执行步长为2的卷积运算。存在这样一种情况，当我们将滤波器矩阵移动两个像素时，它到达边界，并且滤波器矩阵不适合输入矩阵。也就是说，我们的滤波器矩阵的某些部分在输入矩阵之外，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1009 image-border" src="img/e7a24559-4882-4df0-b801-02065e8b3fd7.png" style="width:18.92em;height:13.92em;"/></p>

<p>在这种情况下，我们执行填充。我们可以简单地用零填充输入矩阵，这样滤波器就可以适应输入矩阵，如下图所示。在输入矩阵上填充零被称为<strong>相同填充</strong>或<strong>零填充</strong>:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1010 image-border" src="img/718bf23e-c425-4f9c-a2e6-6d9e37d070b7.png" style="width:20.17em;height:15.08em;"/></p>

<p>除了用零填充它们，我们还可以简单地丢弃滤波器不适合的输入矩阵区域。这被称为<strong>有效填充</strong>:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1679 image-border" src="img/f3349d9e-becb-4495-8655-71014560c081.png" style="width:20.75em;height:15.42em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Pooling layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">池层</h1>

                

            

            

                

<p>好吧。现在，我们完成了卷积运算。作为卷积运算的结果，我们得到了一些特征图。但是特征地图的维数太大。为了减少特征图的维度，我们执行一个池操作。这减少了特征图的维度，并且仅保留必要的细节，从而可以减少计算量。</p>

<p>比如从图像中识别一匹马，我们需要提取并只保留马的特征；我们可以简单地丢弃不想要的特征，比如图像的背景等等。汇集操作也称为<strong>下采样</strong>或<strong>子采样</strong>操作，它使CNN平移不变。因此，池层通过仅保留重要的要素来减少空间维度。</p>

<p>汇集操作不会改变特征地图的深度；只会影响高度和宽度。</p>

<p>有不同类型的池化操作，包括最大池化、平均池化和总和池化。</p>

<p>在最大池中，我们滑过输入矩阵上的过滤器，只需从过滤器窗口中获取最大值，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1012 image-border" src="img/faf168e5-c7f9-438b-9dd3-70a27d03a408.png" style="width:33.42em;height:13.50em;"/></p>

<p>顾名思义，在平均池中，我们取筛选器窗口内输入矩阵的平均值，而在总和池中，我们对筛选器窗口内输入矩阵的所有值求和，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1013 image-border" src="img/a660315a-b637-4f49-8ed6-e118993a1386.png" style="width:32.58em;height:19.67em;"/></p>

<p>最大池是最常用的池操作之一。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fully connected layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">完全连接的层</h1>

                

            

            

                

<p>到目前为止，我们已经了解了卷积层和池层的工作原理。CNN可以有多个卷积层和池层。但是，这些图层将仅从输入图像中提取要素并生成要素地图；也就是说，它们只是特征提取器。</p>

<p>给定任何图像，卷积层从图像中提取特征并产生特征图。现在，我们需要对这些提取的特征进行分类。所以，我们需要一种算法，可以对这些提取的特征进行分类，并告诉我们提取的特征是马的特征，还是其他的特征。为了进行这种分类，我们使用前馈神经网络。我们展平特征图并将其转换成向量，并将其作为输入馈送给前馈网络。前馈网络把这个扁平化的特征图作为输入，应用一个激活函数，比如sigmoid，返回输出，说明图像是否包含马；这称为全连接层，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1021 image-border" src="img/affa9ab7-63a4-45f0-865d-a1bbc6c75062.png" style="width:44.75em;height:16.75em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The architecture of CNNs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">CNN的体系结构</h1>

                

            

            

                

<p>CNN的架构如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1968 image-border" src="img/b4e34a47-e0c6-45d8-ad8c-0e03396efb57.png" style="width:44.67em;height:17.25em;"/></p>

<p>您会注意到，首先我们将输入图像输入到卷积层，在这里我们应用卷积运算从图像中提取重要特征并创建特征图。然后，我们将特征映射传递到池层，在池层中特征映射的维度将会减少。如上图所示，我们可以有多个卷积层和池层，我们还应该注意，池层不一定要在每个卷积层之后出现；可以有许多卷积层，后面跟着一个汇集层。</p>

<p>因此，在卷积层和池层之后，我们将得到的特征图展平，并将其馈送到一个完全连接的层，这基本上是一个前馈神经网络，它根据特征图对给定的输入图像进行分类。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The math behind CNNs</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">CNN背后的数学</h1>

                

            

            

                

<p>到目前为止，我们已经直观地了解了CNN是如何工作的。但是CNN到底是如何学习的呢？它如何使用反向传播找到滤波器的最佳值？为了回答这个问题，我们将从数学上探索CNN是如何工作的。不同于第五章第一章第一章，RNN第三章的T2改进，CNN背后的数学很简单也很有趣。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Forward propagation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">正向传播</h1>

                

            

            

                

<p>让我们从正向传播开始。我们已经看到了前向传播是如何工作的，以及CNN如何对给定的输入图像进行分类。让我们用数学的方法来描述它。让我们考虑一个输入矩阵，<em> X </em>，以及滤波器，<em> W </em>，其值如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1292 image-border" src="img/77c1def1-3668-4604-ac80-273dfb885ff1.png" style="width:23.17em;height:11.92em;"/></p>

<p>首先，让我们熟悉一下符号。每当我们写<img class="fm-editor-equation" src="img/d683705e-7017-43b2-8fd2-31377943d59c.png" style="width:1.50em;height:1.08em;"/>时，它意味着输入矩阵的<img class="fm-editor-equation" src="img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png" style="width:0.92em;height:0.83em;"/>行和<img class="fm-editor-equation" src="img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png" style="width:0.83em;height:0.92em;"/>列中的元素。这同样适用于滤波器和输出矩阵；也就是说，<img class="fm-editor-equation" src="img/d34f998a-542d-4bd0-b85e-3b6ede8d1626.png" style="width:1.67em;height:1.08em;"/>和<img class="fm-editor-equation" src="img/401d921f-f24e-41ac-991c-3bf09f3573d2.png" style="width:1.50em;height:1.17em;"/>分别代表滤波器和输出矩阵中的<img class="fm-editor-equation" src="img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png" style="width:0.92em;height:0.83em;"/>行和<img class="fm-editor-equation" src="img/ad1b30d0-b7c2-4138-a9dc-902fa6b5d9fc.png" style="width:0.83em;height:0.92em;"/>列值。在上图中，<img class="fm-editor-equation" src="img/4432ea26-b8cd-4fff-bcaa-80460506ea8d.png" style="width:1.42em;height:0.67em;"/> = <img class="fm-editor-equation" src="img/e0c53dec-a173-4d9b-b7fe-332690e0ceae.png" style="width:1.08em;height:0.75em;"/>，即<img class="fm-editor-equation" src="img/0fa58a79-c956-40d3-8832-cc2c6d53d6e9.png" style="width:1.17em;height:0.75em;"/>是输入矩阵第一行第一列的元素。</p>

<p>如下图所示，我们采用滤波器，将其滑过输入矩阵，执行卷积运算，并生成输出矩阵(特征映射)，正如我们在上一节中所学:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1293 image-border" src="img/2d7956a5-7923-4c51-a5c1-7c0e5fcb7e6c.png" style="width:26.75em;height:14.67em;"/></p>

<p>因此，输出矩阵(特征图)中的所有值计算如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/0f7cb78b-c6e7-41a0-95de-d3cb4b3af679.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f9c00bd1-1088-409e-aa64-a1b4c452bedd.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/d40ab13d-bd4d-45fd-b80d-766865b9a3eb.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ed7a8206-de1d-4958-be08-9056f1feee1d.png" style="width:20.17em;height:1.25em;"/></p>

<p>好，我们知道卷积运算是如何执行的，输出是如何计算的。我们可以用一个简单的等式来表示吗？假设我们有一个输入图像，<em> X，</em>，宽度为<em> W </em>，高度为<em> H </em>，大小为<em> P </em> x <em> Q </em>的滤波器，那么卷积运算可以表示如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/20f207a6-3090-4335-8e1f-c124e2aceeb1.png" style="width:34.25em;height:2.58em;"/></p>

<p>该等式基本上表示如何使用卷积运算计算输出<img class="fm-editor-equation" src="img/d17ac902-2af3-4c84-b127-3ba512459135.png" style="width:1.33em;height:1.00em;"/>(即输出矩阵的<img class="fm-editor-equation" src="img/abcdeb9c-4778-4e48-937e-8bec5ef517fa.png" style="width:1.33em;height:1.25em;"/>行和<img class="fm-editor-equation" src="img/fbd568a7-6a06-437d-a179-d4c530797237.png" style="width:1.17em;height:1.25em;"/>列中的元素)。</p>

<p>一旦执行了卷积运算，我们将结果<img class="fm-editor-equation" src="img/d17ac902-2af3-4c84-b127-3ba512459135.png" style="width:1.50em;height:1.17em;"/>馈送给前馈网络<img class="fm-editor-equation" src="img/a9e5a9e6-0ec1-4ac5-a813-442c18dcabd9.png" style="width:0.50em;height:1.00em;"/>，并预测输出<img class="fm-editor-equation" src="img/9913c405-9178-4da0-930c-5afa513240ca.png" style="width:0.92em;height:1.08em;"/>:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b833c3e0-2ef9-4d5d-b1bf-4d7dc11a450d.png" style="width:6.25em;height:1.58em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Backward propagation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">反向传播</h1>

                

            

            

                

<p>一旦我们预测了产量，我们就可以计算损失。我们使用均方误差作为损失函数，即实际输出<img class="fm-editor-equation" src="img/7b4f6183-c77d-4521-bb70-849f7bf94f51.png" style="width:1.17em;height:1.08em;"/>和预测输出<img class="fm-editor-equation" src="img/21977035-1547-4a83-a0f4-a35d0e227cac.png" style="width:1.08em;height:1.25em;"/>之间的平方差的平均值，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8515dd01-439a-4cec-a247-73861ef4b010.png" style="width:12.25em;height:3.83em;"/></p>

<p>现在，我们将看看如何使用反向传播来最小化损失<img class="fm-editor-equation" src="img/6c8b8da5-079e-4817-8a65-336e26f89086.png" style="width:0.67em;height:0.83em;"/>。为了将损耗降至最低，我们需要找到滤波器<em> W </em>的最佳值。我们的滤波器矩阵由四个值组成，<em> w1 </em>、<em> w2 </em>、<em> w3 </em>和<em> w4 </em>。为了找到最佳滤波器矩阵，我们需要计算损失函数相对于所有这四个值的梯度。我们如何做到这一点？</p>

<p>首先，让我们回忆一下输出矩阵的方程，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/73f8263a-b92c-4929-ac10-7c461980d67b.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b88649d8-8201-40e2-8672-2c53962903f2.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/9cf41a2f-3084-4bfd-87af-56233d26714d.png" style="width:20.17em;height:1.25em;"/></p>

<p>不要被即将到来的方程式吓倒；它们实际上非常简单。</p>

<p>首先，让我们计算相对于<img class="fm-editor-equation" src="img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png" style="width:1.25em;height:0.75em;"/> <em>的梯度。</em>可以看到，<img class="fm-editor-equation" src="img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png" style="width:1.25em;height:0.75em;"/>出现在所有的输出方程中；我们计算损失对<img class="fm-editor-equation" src="img/e36bae42-8a8f-4613-95ce-3bf0d3cdd6a6.png" style="width:1.25em;height:0.75em;"/>的偏导数如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/63f9fe80-68b3-4d10-8565-1aed0af0fb6a.png" style="width:25.08em;height:2.58em;"/></p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png" style="width:20.33em;height:2.58em;"/></p>

<p>类似地，我们计算损失相对于<img class="fm-editor-equation" src="img/b7ea87fb-dbe7-4ca2-8006-eb5d0227fca7.png" style="width:1.42em;height:0.83em;"/>重量的偏导数如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/7c16a2cb-18bf-4d51-b67b-49b6eca0ae53.png" style="width:25.08em;height:2.58em;"/></p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png" style="width:20.33em;height:2.58em;"/></p>

<p>相对于<img class="fm-editor-equation" src="img/cc642919-e0bc-40be-ad22-3f8bd770015c.png" style="width:1.42em;height:0.92em;"/>重量的损失梯度计算如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6a740913-b834-4054-908e-7febd562836d.png" style="width:25.08em;height:2.58em;"/></p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a7e01557-af09-4335-8d95-3abafc555a57.png" style="width:20.33em;height:2.58em;"/></p>

<p>相对于<img class="fm-editor-equation" src="img/761a3045-a3d4-42c8-9f9b-38183d0bec79.png" style="width:1.25em;height:0.75em;"/>重量的损耗梯度如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/986e1d83-6913-4c43-8bfc-670cd50e47b4.png" style="width:25.08em;height:2.58em;"/></p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/51967733-80f6-45a7-b857-71f6bf2a5ada.png" style="width:20.33em;height:2.58em;"/></p>

<p>因此，简而言之，我们关于所有重量的损耗梯度的最终方程如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/198fe63d-ac36-464e-bd4a-61067cc2ecc3.png" style="width:20.33em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b8bb6276-af5c-42d0-81e3-82f4ff5151ac.png" style="width:20.33em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a7e01557-af09-4335-8d95-3abafc555a57.png" style="width:20.33em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/51967733-80f6-45a7-b857-71f6bf2a5ada.png" style="width:20.33em;height:2.58em;"/></p>

<p>事实证明，计算损耗相对于滤波器矩阵的导数非常简单，这只是另一种卷积运算。如果我们仔细观察前面的等式，我们会发现它们看起来像是输入矩阵与作为滤波矩阵的输出损耗梯度之间的卷积运算结果，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1295 image-border" src="img/00d7add4-0916-49a8-9897-77e206151643.png" style="width:57.42em;height:19.50em;"/></p>

<p>例如，让我们看看如何通过输入矩阵和作为过滤矩阵的输出的损失梯度之间的卷积运算来计算相对于权重<img class="fm-editor-equation" src="img/ebe3ea15-3942-46c9-b837-2369257cad35.png" style="width:1.25em;height:0.83em;"/>的损失梯度，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1299 image-border" src="img/3d8b588c-5392-4ca1-9d78-7cb614c10765.png" style="width:36.33em;height:18.42em;"/></p>

<p>因此，我们可以这样写:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a7e01557-af09-4335-8d95-3abafc555a57.png" style="width:20.33em;height:2.58em;"/></p>

<p>因此，我们理解，计算相对于滤波器的损耗梯度(即权重)只是输入矩阵和相对于作为滤波器矩阵的输出的损耗梯度之间的卷积运算。</p>

<p>除了计算相对于滤波器的损耗梯度，我们还需要计算相对于输入的损耗梯度。但是我们为什么要这样做呢？因为它用于计算前一层中存在的滤波器的梯度。</p>

<p>我们的输入矩阵由从<img class="fm-editor-equation" src="img/1a62ae19-7c7c-462f-8f2d-6a34dadb3021.png" style="width:1.00em;height:0.67em;"/>到<img class="fm-editor-equation" src="img/89e4142d-a6fe-4b74-8fc9-5a030c915ac0.png" style="width:1.00em;height:0.75em;"/>的九个值组成，因此我们需要计算所有这九个值的损耗梯度。让我们回忆一下输出矩阵是如何计算的:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/73f8263a-b92c-4929-ac10-7c461980d67b.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a80b9012-eaee-4aaf-8488-5ee26d4ad270.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b88649d8-8201-40e2-8672-2c53962903f2.png" style="width:20.17em;height:1.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/9cf41a2f-3084-4bfd-87af-56233d26714d.png" style="width:20.17em;height:1.25em;"/></p>

<p>如您所见，<img class="fm-editor-equation" src="img/fd93e013-64ad-487b-894b-19c26344a3cc.png" style="width:1.25em;height:0.83em;"/>仅出现在<img class="fm-editor-equation" src="img/56e5404b-91dd-49f5-946b-2a447d893e41.png" style="width:1.17em;height:0.92em;"/>中，因此我们可以单独计算相对于<img class="fm-editor-equation" src="img/cf1784c6-b3a1-4629-828d-e88c17764212.png" style="width:1.17em;height:0.92em;"/>的损耗梯度，因为其他项为零:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/bdc411e1-040b-45a8-8aad-21ba41f61d86.png" style="width:7.75em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png" style="width:6.92em;height:2.58em;"/></p>

<p>现在，让我们计算相对于<img class="fm-editor-equation" src="img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png" style="width:1.17em;height:0.83em;"/> <em>的梯度；</em>由于<img class="fm-editor-equation" src="img/146e3b03-bc7f-477d-a6a8-dcad3ca57693.png" style="width:1.17em;height:0.83em;"/>只出现在<img class="fm-editor-equation" src="img/b3a81d52-41a7-4d87-8012-881d3559401c.png" style="width:1.08em;height:0.83em;"/>和<img class="fm-editor-equation" src="img/4c0f720d-ad3f-43a7-b52d-76f3d5083e02.png" style="width:1.00em;height:0.75em;"/>中，我们单独计算相对于<img class="fm-editor-equation" src="img/70fd3562-41b7-4080-b3d0-ac23f22d37ed.png" style="width:1.00em;height:0.75em;"/>和<img class="fm-editor-equation" src="img/9f4c6517-1607-42a0-8d30-ac53f753119c.png" style="width:1.00em;height:0.75em;"/>的梯度:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/44bc61b2-09bf-4225-9468-0ac63294909a.png" style="width:13.25em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/efae36f7-eb83-4461-8a1d-ed9ac9468b27.png" style="width:11.58em;height:2.58em;"/></p>

<p>以非常相似的方式，我们计算所有输入的损耗梯度，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/3721b26c-ea6f-4830-ab9b-be534e05ba6a.png" style="width:6.92em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/31638cdf-1ee2-4670-9a53-2e0edf004b7b.png" style="width:11.58em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/7c44f895-a2e7-4d9c-9549-cddb214fb2d5.png" style="width:20.00em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/9f822bc7-39ea-4bdb-a4b4-b3373a8c0965.png" style="width:11.58em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/87fb3292-de91-4957-acf4-f18a5f8bac3f.png" style="width:6.92em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/58625695-d3df-4add-8a30-79bf551a8ed4.png" style="width:11.17em;height:2.58em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5060bb60-9610-4752-a75b-7bf372964d4f.png" style="width:6.92em;height:2.58em;"/></p>

<p>正如我们使用卷积运算来表示损失相对于权重的梯度一样，我们是否也可以这样做？事实证明答案是肯定的。实际上，我们可以使用作为输入矩阵的滤波矩阵和作为滤波矩阵的输出矩阵的损耗梯度之间的卷积运算来表示前述方程，即，相对于输入的损耗梯度。但诀窍在于，我们不是直接使用滤波器矩阵，而是将它们旋转180度，并且不是执行卷积，而是执行全卷积。我们这样做是为了利用卷积运算推导出前面的方程。</p>

<p>下图显示了内核旋转180度后的样子:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1301 image-border" src="img/e433f90c-42dc-4142-8f5e-317acf3308da.png" style="width:31.75em;height:9.25em;"/></p>

<p>好吧，那么什么是全卷积？与卷积运算的方式相同，在全卷积中，我们使用一个滤波器，并在输入矩阵上滑动它，但我们滑动滤波器的方式与我们之前看到的卷积运算不同。下图显示了全卷积运算的工作原理。我们可以看到，阴影矩阵表示滤波器矩阵，无阴影矩阵表示输入矩阵；我们可以看到滤波器如何逐步滑过输入矩阵，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1969 image-border" src="img/51512ba8-b3ab-40d5-9792-229050fabd1c.png" style="width:30.17em;height:20.58em;"/></p>

<p>因此，我们可以说，相对于输入矩阵的损耗梯度可以使用旋转180度作为输入矩阵的滤波器和相对于输出作为滤波器矩阵的损耗梯度之间的全卷积运算来计算:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1978 image-border" src="img/6e9eb3a4-7b6b-403c-9ed9-368662189258.png" style="width:77.42em;height:22.25em;"/></p>

<p>例如，如下图所示，我们将注意到输入的损耗梯度<img class="fm-editor-equation" src="img/0425842a-097d-4394-8a1b-4f81719f427f.png" style="width:1.00em;height:0.58em;"/>是如何通过旋转180度的滤波器矩阵与作为滤波器矩阵的输出矩阵的损耗梯度之间的全卷积运算来计算的:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1311 image-border" src="img/39d37a5e-1442-43a1-bad9-c70594773ad1.png" style="width:38.67em;height:17.58em;"/></p>

<p>这一点如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/36fcb0ad-9bd0-4afb-854a-a448d8b6fdd0.png" style="width:6.92em;height:2.58em;"/></p>

<p>因此，我们理解，计算相对于输入的损耗梯度只是完整的卷积运算。因此，我们可以说CNN中的反向传播只是另一种卷积运算。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Implementing a CNN in TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在TensorFlow中实现CNN</h1>

                

            

            

                

<p>现在我们将学习如何使用TensorFlow构建CNN。我们将使用MNIST手写数字数据集，了解CNN如何识别手写数字，我们还将可视化卷积层如何从图像中提取重要特征。</p>

<p>首先，让我们加载所需的库:</p>

<pre class="mce-root">import warnings<br/>warnings.filterwarnings('ignore')<br/><br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.examples.tutorials.mnist import input_data<br/>tf.logging.set_verbosity(tf.logging.ERROR)<br/><br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</pre>

<p class="mce-root">加载MNIST数据集:</p>

<pre>mnist = input_data.read_data_sets('data/mnist', one_hot=True)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining helper functions</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义助手功能</h1>

                

            

            

                

<p class="mce-root">现在，我们定义初始化权重和偏差的函数，以及执行卷积和汇集操作的函数。</p>

<p>通过从截断的正态分布中提取来初始化权重。记住，权重实际上是我们在执行卷积运算时使用的滤波器矩阵:</p>

<pre>def initialize_weights(shape):<br/>    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))</pre>

<p>用一个恒定值初始化偏置，比如说，<kbd>0.1</kbd>:</p>

<pre>def initialize_bias(shape):<br/>    return tf.Variable(tf.constant(0.1, shape=shape))</pre>

<p>我们用<kbd>tf.nn.conv2d()</kbd>定义了一个叫卷积的函数，它实际上执行的是卷积运算；即输入矩阵(<kbd>x</kbd>)与滤波器(<kbd>W</kbd>)的逐元素乘法，步长为<kbd>1</kbd>，填充相同。我们设定<kbd>strides = [1,1,1,1]</kbd>。步幅的第一个和最后一个值被设置为<kbd>1</kbd>，这意味着我们不想在训练样本和不同通道之间移动。<kbd>strides</kbd>的第二个和第三个值也被设置为<kbd>1</kbd>，这意味着我们在高度和宽度方向上按<kbd>1</kbd>像素移动过滤器:</p>

<pre>def convolution(x, W):<br/>    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')</pre>

<p>我们定义了一个名为<kbd>max_pooling</kbd>的函数，使用<kbd>tf.nn.max_pool()</kbd>来执行池操作。我们使用<kbd>2</kbd>的<kbd>stride</kbd>执行最大池化，相同的<kbd>padding</kbd>和<kbd>ksize</kbd>暗示我们的池化窗口形状:</p>

<pre>def max_pooling(x):<br/>    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')</pre>

<p class="mce-root">定义输入和输出的占位符。</p>

<p>输入图像的<kbd>placeholder</kbd>定义如下:</p>

<pre class="mce-root">X_ = tf.placeholder(tf.float32, [None, 784])</pre>

<p>整形后的输入图像的<kbd>placeholder</kbd>定义如下:</p>

<pre class="mce-root">X = tf.reshape(X_, [-1, 28, 28, 1])</pre>

<p>输出标签的<kbd>placeholder</kbd>定义如下:</p>

<pre class="mce-root">y = tf.placeholder(tf.float32, [None, 10])</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining the convolutional network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义卷积网络</h1>

                

            

            

                

<div><div><div><p>我们的网络架构由两个卷积层组成。每个卷积层后面是一个池层，我们使用一个全连接层，后面是一个输出层；也就是<kbd>conv1-&gt;pooling-&gt;conv2-&gt;pooling2-&gt;fully connected layer-&gt; output layer</kbd>。</p>

</div>

</div>

</div>

<div><div><div><p>首先，我们定义第一个卷积层和池层。</p>

<p>权重实际上是卷积层中的滤波器。因此，权重矩阵将被初始化为<kbd>[ filter_shape[0], filter_shape[1], number_of_input_channel, filter_size ]</kbd>。</p>

<p>我们使用一个<kbd>5 x 5</kbd>过滤器。由于我们使用灰度图像，输入通道的数量将是<kbd>1</kbd>，我们将过滤器的大小设置为<kbd>32</kbd>。因此，第一卷积层的权重矩阵将是<kbd>[5,5,1,32]</kbd>:</p>

</div>

</div>

</div>

<pre class="mce-root">W1 = initialize_weights([5,5,1,32])</pre>

<p>偏置的形状就是滤波器的大小，也就是<kbd>32</kbd>:</p>

<pre class="mce-root">b1 = initialize_bias([32])</pre>

<p>使用ReLU激活执行第一个卷积运算，然后使用最大池:</p>

<pre class="mce-root">conv1 = tf.nn.relu(convolution(X, W1) + b1)<br/>pool1 = max_pooling(conv1)</pre>

<p>接下来，我们定义第二个卷积层和池层。</p>

<p class="mce-root">由于第二卷积层从具有32通道输出的第一卷积层获取输入，因此第二卷积层的输入通道数变为32，我们使用滤波器大小为<kbd>64</kbd>的5×5滤波器。因此，第二卷积层的权重矩阵变成<kbd>[5,5,32,64]</kbd>:</p>

<pre class="mce-root">W2 = initialize_weights([5,5,32,64])</pre>

<p>偏置的形状就是滤波器的大小，也就是<kbd>64</kbd>:</p>

<pre class="mce-root">b2 = initialize_bias([64])</pre>

<p>对ReLU激活执行第二次卷积运算，然后是最大池化:</p>

<pre class="mce-root">conv2 = tf.nn.relu(convolution(pool1, W2) + b2)<br/>pool2 = max_pooling(conv2)</pre>

<div><div><p class="output">在两个卷积和池层之后，我们需要在将输出馈送到完全连接的层之前将其展平。因此，我们将第二个池层的结果扁平化，并将其提供给完全连接的层。</p>

</div>

</div>

<div><div><div><p>展平第二个池层的结果:</p>

</div>

</div>

</div>

<pre class="mce-root">flattened = tf.reshape(pool2, [-1, 7*7*64])</pre>

<p>现在我们定义全连接层的权重和偏差。我们知道我们把权重矩阵的形状设为<kbd>[number of neurons in the current layer, number of neurons layer in the next layer]</kbd>。这是因为输入图像的形状在展平后变成了<kbd>7x7x64</kbd>，我们在隐藏层中使用了<kbd>1024</kbd>神经元。重物的形状变为<kbd>[7x7x64, 1024]</kbd>:</p>

<pre class="mce-root">W_fc = initialize_weights([7*7*64, 1024])<br/>b_fc = initialize_bias([1024])</pre>

<p>这是一个带有ReLU激活的完全连接的层:</p>

<pre class="mce-root">fc_output = tf.nn.relu(tf.matmul(flattened, W_fc) + b_fc)</pre>

<p>定义输出层。我们在当前层中有<kbd>1024</kbd>个神经元，并且由于我们需要预测10个类别，我们在下一层中有10个神经元，因此权重矩阵的形状变为<kbd>[1024 x 10]</kbd>:</p>

<pre class="mce-root">W_out = initialize_weights([1024, 10])<br/>b_out = initialize_bias([10])</pre>

<div><div><div><div><div><div><p>使用softmax激活计算输出:</p>

</div>

</div>

</div>

</div>

</div>

</div>

<pre class="mce-root">YHat = tf.nn.softmax(tf.matmul(fc_output, W_out) + b_out)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">计算损失</h1>

                

            

            

                

<p>使用交叉熵计算损失。我们知道交叉熵损失如下所示:</p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/69145fd8-d596-4fad-b1ca-7f4ac8fca30c.png" style="width:15.08em;height:2.67em;"/></p>

<p class="mce-root">这里，<img class="fm-editor-equation" src="img/b16d392d-d81a-4471-9a35-a41d748c2dee.png" style="width:0.50em;height:0.83em;"/>是实际标签，<img class="fm-editor-equation" src="img/9455b313-8e3b-465f-8f71-c4f1d9e7286d.png" style="width:0.50em;height:1.00em;"/>是预测标签。因此，交叉熵损失实现如下:</p>

<pre class="mce-root">cross_entropy = -tf.reduce_sum(y*tf.log(YHat))</pre>

<p class="mce-root">使用Adam优化器将损失降至最低:</p>

<pre class="mce-root">optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</pre>

<p class="mce-root">计算准确度:</p>

<pre class="mce-root">predicted_digit = tf.argmax(y_hat, 1)<br/>actual_digit = tf.argmax(y, 1)<br/><br/>correct_pred = tf.equal(predicted_digit,actual_digit)<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Starting the training</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">开始训练</h1>

                

            

            

                

<p>启动一个张量流<kbd>Session</kbd>并初始化所有变量:</p>

<pre class="mce-root">sess = tf.Session()<br/>sess.run(tf.global_variables_initializer())</pre>

<p>为<kbd>1000</kbd>时代训练模型。打印每个<kbd>100</kbd>时期的结果:</p>

<pre>for epoch in range(1000):<br/>    <br/>    #select some batch of data points according to the batch size (100)<br/>    X_batch, y_batch = mnist.train.next_batch(batch_size=100)<br/>    <br/><br/>    #train the network<br/>    loss, acc, _ = sess.run([cross_entropy, accuracy, optimizer], feed_dict={X_: X_batch, y: y_batch})<br/>    <br/><br/>    #print the loss on every 100th epoch<br/>    if epoch%100 == 0:<br/>        print('Epoch: {}, Loss:{} Accuracy: {}'.format(epoch,loss,acc))</pre>

<p>您会注意到，随着时间的推移，损耗会降低，精度会提高:</p>

<pre>Epoch: 0, Loss:631.2734375 Accuracy: 0.129999995232

Epoch: 100, Loss:28.9199733734 Accuracy: 0.930000007153

Epoch: 200, Loss:18.2174377441 Accuracy: 0.920000016689

Epoch: 300, Loss:21.740688324 Accuracy: 0.930000007153</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Visualizing extracted features</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">可视化提取的特征</h1>

                

            

            

                

<p>现在我们已经训练了我们的CNN模型，我们可以看到我们的CNN提取了什么特征来识别图像。正如我们所知，每个卷积层都从图像中提取重要特征。我们将看到我们的第一个卷积层提取了什么特征来识别手写数字。</p>

<p>首先，让我们从训练集中选择一个图像，比如数字1:</p>

<pre class="mce-root">plt.imshow(mnist.train.images[7].reshape([28, 28]))</pre>

<p>输入图像如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/908dbc36-2c73-460d-aa03-07bb29122736.png" style="width:22.00em;height:21.67em;"/></p>

<p class="mce-root">将此图像送入第一个卷积层，即<kbd>conv1</kbd>，得到特征图:</p>

<pre class="mce-root">image = mnist.train.images[7].reshape([-1, 784])<br/>feature_map = sess.run([conv1], feed_dict={X_: image})[0]</pre>

<p>绘制特征图:</p>

<pre>for i in range(32):<br/>    feature = feature_map[:,:,:,i].reshape([28, 28])<br/>    plt.subplot(4,8, i + 1)<br/>    plt.imshow(feature)<br/>    plt.axis('off')<br/>plt.show()<br/><br/></pre>

<p class="mce-root">正如您在下图中看到的，第一个卷积层已经学会从给定图像中提取边缘:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/793fdc93-7882-47e6-90de-acbbca82a500.png" style="width:38.42em;height:24.83em;"/></p>

<p>因此，这就是CNN如何使用多个卷积层从图像中提取重要特征，并将这些提取的特征馈送到全连接层以对图像进行分类。现在我们已经了解了CNN是如何工作的，在下一节中，我们将了解几个有趣的CNN架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>CNN architectures</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">CNN架构</h1>

                

            

            

                

<p>在这一部分，我们将探索不同类型的有趣的CNN架构。当我们说不同类型的CNN架构时，我们基本上是指卷积层和池层如何相互堆叠。此外，我们还将了解使用了多少卷积层、池层和全连接层，以及过滤器的数量和尺寸等。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>LeNet architecture</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">LeNet架构</h1>

                

            

            

                

<p>LeNet架构是CNN的经典架构之一。如下图所示，该架构非常简单，仅由七层组成。在这七层中，有三个卷积层、两个汇集层、一个全连接层和一个输出层。它使用步长为1的5 x 5卷积，并使用平均池。什么是5×5卷积？这意味着我们正在用一个5×5的滤波器进行卷积运算。</p>

<p>如下图所示，LeNet由三个卷积层(<kbd>C1</kbd>、<kbd>C3</kbd>、<kbd>C5</kbd>)、两个池层(<kbd>S2</kbd>、<kbd>S4</kbd>)、一个全连接层(<kbd>F6</kbd>)和一个输出层(<kbd>OUTPUT</kbd>)组成，每个卷积层之后是一个池层:</p>

<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/ffae1810-3122-412d-bfa0-554f2fabfa12.png"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding AlexNet</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">了解AlexNet</h1>

                

            

            

                

<p>AlexNet是一个经典而强大的深度学习架构。它通过将错误率从26%大幅降低到15.3%，赢得了2012年ILSVRC。ILSVRC代表ImageNet大规模视觉识别竞赛，这是专注于计算机视觉任务(如图像分类、定位、对象检测等)的最大竞赛之一。ImageNet是一个巨大的数据集，包含超过1500万张带标签的高分辨率图像，超过22，000个类别。每年，研究人员都竞相使用创新建筑赢得比赛。</p>

<p>AlexNet由先驱科学家设计，包括Alex Krizhevsky、Geoffrey Hinton和Ilya Sutskever。它由五个卷积层和三个全连接层组成，如下图所示。它使用ReLU激活函数而不是tanh函数，ReLU在每一层后应用。它使用dropout来处理过拟合，并且在第一个和第二个完全连接的层之前执行dropout。它使用数据增强技术，如图像翻译，并在两个GTX 580 GPU上使用批量随机梯度下降进行5至6天的训练:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1970 image-border" src="img/a1414b54-38c8-4512-8ba1-53f2acbe9dd0.png" style="width:57.92em;height:22.17em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Architecture of VGGNet</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">VGGNet架构</h1>

                

            

            

                

<p>VGGNet是最常用的CNN架构之一。它是由牛津大学的视觉几何小组 ( <strong xmlns:epub="http://www.idpf.org/2007/ops"> VGG </strong>)发明的。当它成为ILSVRC 2014的亚军时，它开始变得非常受欢迎。</p>

<p>它基本上是一个深度卷积网络，广泛用于对象检测任务。牛津团队向公众提供了网络的权重和结构，因此我们可以直接使用这些权重来执行几项计算机视觉任务。它还被广泛用作图像的良好基线特征提取器。</p>

<p>VGG网络的架构非常简单。它由卷积层和池层组成。它在整个网络中使用3 x 3卷积和2 x 2池。它被称为VGG- <em> n </em>，其中<em> n </em>对应于多个层，不包括pooling和softmax层。下图显示了VGG-16网络的架构:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1971 image-border" src="img/fc6cc749-7c91-44dd-ab86-0682f7e7a216.png" style="width:42.83em;height:36.17em;"/></p>

<p>从下图可以看出，AlexNet的架构特点是金字塔形状，初始层宽，后期层窄。您会注意到它由多个卷积层和一个池层组成。由于池层减少了空间维度，随着我们深入网络，它会缩小网络:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1373 image-border" src="img/e778b341-6235-4202-adcc-6716effb5248.png" style="width:44.25em;height:24.75em;"/></p>

<p class="ui_qtext_para u-ltr u-text-align--start">VGGNet的一个缺点是计算量很大，它有超过1.6亿个参数。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>GoogleNet</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">谷歌网</h1>

                

            

            

                

<p><strong> GoogleNet </strong>，也被称为<strong> inception net </strong>，是2014年ILSVRC的获胜者。它由各种版本组成，每个版本都是前一个版本的改进版本。我们将逐一探讨每个版本。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Inception v1</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">盗梦空间v1</h1>

                

            

            

                

<p>Inception v1是网络的第一个版本。图像中的对象以不同的大小出现在不同的位置。比如看第一个图像；正如你所看到的，当近距离观看时，鹦鹉占据了图像的整个部分，但是在第二个图像中，当从远处观看鹦鹉时，它占据了图像的较小区域:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1972 image-border" src="img/00b99050-2195-47da-99e9-8ae8ec30e92f.png" style="width:55.58em;height:18.92em;"/></p>

<p>因此，我们可以说物体(在给定的图像中，是一只鹦鹉)可以出现在图像的任何区域。它可能小或大。它可能会占据图像的整个区域，或者只是很小的一部分。我们的网络必须准确识别目标。但是这里的问题是什么呢？还记得我们如何学习使用过滤器从图像中提取特征吗？现在，由于我们感兴趣的对象在每幅图像中的大小和位置各不相同，因此选择正确的滤波器大小非常困难。</p>

<p>当对象尺寸较大时，我们可以使用大尺寸的过滤器，但是当我们必须检测图像的小角落中的对象时，大尺寸的过滤器不合适。因为我们使用固定的感受野，即固定的滤波器大小，所以很难识别图像中位置变化很大的对象。我们可以使用深度网络，但它们更容易过度拟合。</p>

<p>为了克服这一点，初始网络在相同的输入上使用不同大小的多个滤波器，而不是使用相同大小的单个滤波器。一个初始网络由九个相互堆叠的初始块组成。下图显示了一个单独的先启块。您会注意到，我们使用三种不同大小的滤波器(即1 x 1、3 x 3和5 x 5)对给定图像执行卷积运算。一旦所有这些不同的滤波器执行了卷积运算，我们就将结果连接起来，并将其提供给下一个先启块:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1379 image-border" src="img/f335394b-72c1-411c-9169-fccbf7e0b4ec.png" style="width:43.83em;height:24.00em;"/></p>

<p>当我们连接来自多个过滤器的输出时，连接结果的深度将增加。虽然我们使用的填充只匹配输入和输出的形状，但我们仍然会有不同的深度。因为一个先启块的结果是对另一个的反馈，所以深度不断增加。因此，为了避免深度增加，我们只需在3 x 3和5 x 5卷积之前添加1 x 1卷积，如下图所示。我们还执行最大汇集操作，并且在最大汇集操作之后添加1 x 1卷积:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1419 image-border" src="img/6307804b-fdfa-4d8b-adc2-66b96100293d.png" style="width:79.08em;height:61.75em;"/></p>

<p>每个先启块提取一些特性，并将它们提供给下一个先启块。假设我们正试图识别一张鹦鹉的图片。前几层中的先启块检测基本特性，后面的先启块检测高级特性。正如我们所见，在卷积网络中，初始块将只提取特征，而不执行任何分类。因此，我们将由inception块提取的特征提供给分类器，该分类器将预测图像是否包含鹦鹉。</p>

<p>由于初始网络很深，有九个初始块，它容易受到消失梯度问题的影响。为了避免这一点，我们在初始块之间引入了分类器。由于每个先启块都学习了图像的有意义的特征，所以我们也尝试执行分类并计算中间层的损失。如下图所示，我们有九个先启块。我们获取第三个先启块的结果<img class="fm-editor-equation" src="img/b06294bd-c05c-4396-8c5a-93105abd48e4.png" style="width:0.83em;height:1.00em;"/>，并将其提供给一个中间分类器，还将第六个先启块的结果<img class="fm-editor-equation" src="img/dcd2ad2a-d433-4347-8e03-acde05424092.png" style="width:0.92em;height:1.00em;"/>提供给另一个中间分类器。在最终的初始块的末尾还有另一个分类器。该分类器基本上由平均池、1 x 1卷积和具有softmax激活的线性层组成:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1391 image-border" src="img/af8b6bd7-d29c-463b-985a-16cd92565a1c.png" style="font-size: 1em;text-align: center;width:97.00em;height:49.17em;"/></p>

<p>中间的量词实际上叫做辅助量词。因此，初始网络的最终损耗是辅助分类器损耗和最终分类器损耗(实际损耗)的加权和，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/cb05a5d5-deb7-4763-a5c4-9da8477c8b9b.png" style="width:30.17em;height:1.25em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Inception v2 and v3</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Inception v2和v3</h1>

                

            

            

                

<p>《盗梦空间》第二版和第三版由克里斯蒂安·塞格迪(Christian Szegedy)在<em>延伸阅读</em>一节中介绍的<em>与回旋</em>一起深入。作者建议使用因子分解卷积，也就是说，我们可以将一个滤波器大小较大的卷积层分解为一个滤波器大小较小的卷积层堆栈。因此，在初始模块中，一个具有5 x 5滤波器的卷积层可以分解为两个具有3 x 3滤波器的卷积层，如下图所示。使用因子分解卷积可以提高性能和速度:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1420 image-border" src="img/b4491e9b-d121-451b-8aa6-297fab8f2381.png" style="width:89.83em;height:61.75em;"/></p>

<p class="mce-root">作者还建议将滤波器大小为<em> n </em> x <em> n </em>的卷积层分解成一堆滤波器大小为<em> 1 </em> x <em> n </em>和<em> n </em> x <em> 1 </em>的卷积层。例如，在上图中，我们有<em> 3 </em> x <em> 3 </em>卷积，现在它被分解为<em> 1 </em> x <em> 3 </em>卷积，后面是<em> 3 </em> x <em> 1 </em>卷积，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1421 image-border" src="img/8fa9174f-ded7-4078-ba94-9df45bdfa063.png" style="width:76.42em;height:79.25em;"/></p>

<p class="mce-root">正如你在上图中注意到的，我们基本上是在以更深的方式扩展我们的网络，这将导致我们丢失信息。因此，我们没有让它变得更深，而是让我们的网络变得更宽，如下所示:</p>

<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1422 image-border" src="img/679c32af-bc3b-47e1-95d7-72293c3627cb.png" style="width:95.25em;height:64.17em;"/></p>

<p class="mce-root CDPAlignLeft CDPAlign">在inception net v3中，我们使用带有RMSProp优化器的因子分解的7 x 7卷积。此外，我们在辅助分类器中应用批量归一化。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Capsule networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">胶囊网络</h1>

                

            

            

                

<p>Geoffrey Hinton提出了胶囊网络来克服卷积网络的局限性。</p>

<p>辛顿声明如下:</p>

<p>“卷积神经网络中使用的池操作是一个巨大的错误，它如此有效的事实是一场灾难。”</p>

<p>但是，合并操作有什么问题呢？还记得我们使用池操作来减少维度和删除不需要的信息吗？汇集操作使得我们的CNN表示对于输入中的小平移不变。</p>

<p class="mce-root">CNN的这种平移不变性并不总是有益的，并且容易被错误分类。例如，假设我们需要识别一个图像是否有人脸；CNN将寻找这个图像是否有眼睛、鼻子、嘴巴和耳朵。它不关心他们在哪个位置。如果它找到了所有这样的特征，那么它就把它归类为一张脸。</p>

<p>考虑两个图像，如下图所示。第一个图像是实际的脸，在第二个图像中，眼睛放在左边，一个在另一个上面，耳朵和嘴放在右边。但是CNN仍然会将这两张图片归类为人脸，因为这两张图片都具有人脸的所有特征，即耳朵、眼睛、嘴巴和鼻子。CNN认为这两张图片都是由一张脸组成的。它不学习每个特征之间的空间关系；眼睛应该放在顶部，然后是鼻子，等等。它只检查组成面部的特征是否存在。</p>

<p>当我们有一个深度网络时，这个问题会变得更糟，因为在深度网络中，特征将变得抽象，并且它的大小也会由于几个汇集操作而缩小:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1973 image-border" src="img/0ed60e6a-2da0-4a5e-8179-3d1234e032d8.png" style="width:34.67em;height:20.92em;"/></p>

<p class="mce-root">为了克服这一点，Hinton引入了一种新的网络，称为胶囊网络，由胶囊而不是神经元组成。像CNN一样，胶囊网络检查某些特征的存在以对图像进行分类，但是除了检测特征之外，它还将检查它们之间的空间关系。也就是说，它学习特征的层次结构。以我们识别人脸为例，胶囊网络会学习眼睛应该在顶部，鼻子应该在中间，然后是嘴等等。如果图像不遵循这种关系，则胶囊网络不会将其归类为面部:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1974 image-border" src="img/934ce657-a389-4b18-9af9-a9655338c006.png" style="width:34.08em;height:21.00em;"/></p>

<p>胶囊网络由几个连接在一起的胶囊组成。但是等等。什么是胶囊？</p>

<p>胶囊是一组学习检测图像中特定特征的神经元；说，眼睛。与返回标量的神经元不同，胶囊返回矢量。向量的长度告诉我们特定的特征是否存在于给定的位置，并且向量的元素表示特征的属性，例如位置、角度等等。</p>

<p class="mce-root">假设我们有一个向量<img class="fm-editor-equation" src="img/de2c431c-b1a4-4adf-8d7a-d5e7207e9b65.png" style="width:0.67em;height:0.83em;"/>，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/61f11bd7-ee13-4d67-bb43-a5fbef5eba3b.png" style="width:6.00em;height:1.33em;"/></p>

<p>向量的长度可以计算如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/d9683d9c-30b1-4c92-a117-2437ce209a93.png" style="width:15.75em;height:2.17em;"/></p>

<p>我们已经知道，向量的长度代表特征存在的概率。但是前面的长度不代表概率，因为它超过1。所以，我们用一个叫做挤压函数的函数将这个值转换成一个概率。挤压功能有一个优点。除了计算概率，它还保持向量的方向:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/482bbb73-5d3e-4ba8-8ae7-0d0df58c8b45.png" style="width:15.25em;height:1.25em;"/></p>

<p>就像CNN一样，早期层中的胶囊检测包括眼睛、鼻子等在内的基本特征，而较高层中的胶囊检测高级特征，例如整个面部。因此，较高层中的胶囊从较低层中的胶囊获取输入。为了让更高层的胶囊检测到面部，它们不仅检查鼻子和眼睛等特征的存在，还检查它们的空间关系。</p>

<p>现在我们对什么是胶囊有了一个基本的了解，我们将更详细地研究它，看看胶囊网络到底是如何工作的。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding Capsule networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">了解胶囊网络</h1>

                

            

            

                

<p class="mce-root">假设我们有两层，<img class="fm-editor-equation" src="img/c682874b-001b-47df-89c0-2a1e2c9c965d.png" style="width:0.33em;height:0.92em;"/>和<img class="fm-editor-equation" src="img/d999d9de-a068-411b-a73a-e5bc97fe347d.png" style="width:2.08em;height:0.92em;"/>。<img class="fm-editor-equation" src="img/bcbdc041-43ec-438a-b9d0-75c0100912e1.png" style="width:0.33em;height:0.92em;"/>是下层，有<img class="fm-editor-equation" src="img/4d5b5876-e2fe-4c08-bb59-32eec8809635.png" style="width:0.42em;height:1.08em;"/>轿厢，<img class="fm-editor-equation" src="img/b30eca11-e52d-408f-beba-503f68cc450e.png" style="width:2.08em;height:0.92em;"/>是上层，有<img class="fm-editor-equation" src="img/1ba6e654-dbcc-434f-818d-c1a76de69601.png" style="width:0.42em;height:1.00em;"/>轿厢。来自较低层的胶囊将它们的输出发送到较高层的胶囊。<img class="fm-editor-equation" src="img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png" style="width:1.00em;height:0.83em;"/>将是来自下层的胶囊的激活<img class="fm-editor-equation" src="img/523529a6-1330-4a87-bbfb-8e1131f32111.png" style="width:0.33em;height:0.92em;"/>。<img class="fm-editor-equation" src="img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png" style="width:0.92em;height:0.92em;"/>将是来自更高层的胶囊的激活，<img class="fm-editor-equation" src="img/b30eca11-e52d-408f-beba-503f68cc450e.png" style="width:2.08em;height:0.92em;"/>。</p>

<p class="mce-root">下图显示了一个胶囊<img class="fm-editor-equation" src="img/f4b54408-3d5c-4620-9a6b-ed0a95fcdac0.png" style="width:0.42em;height:1.00em;"/>，正如您所看到的，它将前面胶囊<img class="fm-editor-equation" src="img/e7ce2599-7060-40dd-9cdb-b9fec0ff1330.png" style="width:1.00em;height:0.83em;"/>的输出作为输入，并计算其输出<img class="fm-editor-equation" src="img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png" style="width:1.00em;height:1.00em;"/>:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1975 image-border" src="img/8411ec40-4aa9-4263-ab6a-216ebba8edac.png" style="width:44.75em;height:29.08em;"/></p>

<p class="mce-root">我们将继续学习如何计算<img class="fm-editor-equation" src="img/681ca106-0a2f-4d3f-85e9-aa73b05ae397.png" style="width:1.08em;height:1.08em;"/>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing prediction vectors</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">计算预测向量</h1>

                

            

            

                

<p>在上图中，<img class="fm-editor-equation" src="img/7221635f-36de-465d-a298-4d78bdbe3005.png" style="width:1.33em;height:0.92em;"/>、<img class="fm-editor-equation" src="img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png" style="width:1.25em;height:0.92em;"/>和<img class="fm-editor-equation" src="img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png" style="width:1.17em;height:0.83em;"/>表示来自前一个胶囊的输出向量。首先，我们将这些向量乘以权重矩阵，并计算预测向量:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e6c26383-1d9f-4096-b233-6f0c6e13e213.png" style="width:7.00em;height:1.67em;"/></p>

<p class="mce-root">好，那么我们到底在做什么，什么是预测向量？让我们考虑一个简单的例子。假设胶囊<img class="fm-editor-equation" src="img/1ab28ef0-32dd-489e-983c-256897634e7a.png" style="width:0.42em;height:1.00em;"/>试图预测一幅图像是否有脸。我们已经知道，早期层中的胶囊检测基本特征，并将它们的结果发送到较高层中的胶囊。因此，早期层中的胶囊<img class="fm-editor-equation" src="img/7221635f-36de-465d-a298-4d78bdbe3005.png" style="width:1.50em;height:1.00em;"/>、<img class="fm-editor-equation" src="img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png" style="width:1.50em;height:1.00em;"/>和<img class="fm-editor-equation" src="img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png" style="width:1.25em;height:1.00em;"/>检测基本的低特征，例如眼睛、鼻子和嘴，并将它们的结果发送到高级层中的胶囊，即检测面部的胶囊<img class="fm-editor-equation" src="img/1ab28ef0-32dd-489e-983c-256897634e7a.png" style="width:0.42em;height:1.00em;"/>。</p>

<p>因此，胶囊<img class="fm-editor-equation" src="img/1ab28ef0-32dd-489e-983c-256897634e7a.png" style="width:0.42em;height:1.00em;"/>将先前的胶囊<img class="fm-editor-equation" src="img/7221635f-36de-465d-a298-4d78bdbe3005.png" style="width:1.33em;height:0.92em;"/>、<img class="fm-editor-equation" src="img/61e2c8c9-c2cd-4c55-9237-f4a4484f92fe.png" style="width:1.33em;height:0.92em;"/>和<img class="fm-editor-equation" src="img/568350e9-af6f-4dcb-8cfb-e8fa57847758.png" style="width:1.17em;height:0.92em;"/>作为输入，并将它们乘以权重矩阵<img class="fm-editor-equation" src="img/daf9bc01-bd76-48aa-866c-f4efcc275d91.png" style="width:1.08em;height:0.83em;"/>。</p>

<p>权重矩阵<img class="fm-editor-equation" src="img/39d372ef-547c-40ad-abd2-2b55badabb04.png" style="width:1.08em;height:0.83em;"/>表示低级特征和高级特征之间的空间和其他关系。例如，重量<img class="fm-editor-equation" src="img/e5a11cc9-8a2c-4438-8599-a1e1fe25466e.png" style="width:1.50em;height:1.00em;"/>告诉我们眼睛应该在顶部。<img class="fm-editor-equation" src="img/2394add2-cdb3-40e9-8558-e722eb4df2f6.png" style="width:1.58em;height:1.08em;"/>告诉我们鼻子应该在中间。<img class="fm-editor-equation" src="img/8ea16cdd-9ec9-43e3-a1de-ea08e508d657.png" style="width:1.58em;height:1.08em;"/>告诉我们，一张嘴应该在最下面。注意，权重矩阵不仅捕捉位置(即空间关系)，还捕捉其他关系。</p>

<p>因此，通过将输入乘以权重，我们可以预测人脸的位置:</p>

<ul>

<li><img class="fm-editor-equation" src="img/d994f6ef-9071-44bd-8f97-0b6eb62bb99e.png" style="width:5.58em;height:1.25em;"/>暗示基于眼睛的面部的预测位置</li>

<li><img class="fm-editor-equation" src="img/a51b641b-3884-482b-b29c-9ded8b6f29aa.png" style="width:6.33em;height:1.42em;"/>暗示基于鼻子的面部的预测位置</li>

<li><img class="fm-editor-equation" src="img/c53a8493-8c9d-4610-aaf4-c0334f70087d.png" style="width:6.33em;height:1.42em;"/>暗示基于嘴的面部的预测位置</li>

</ul>

<p>当所有预测的人脸位置都相同，即彼此一致时，那么我们可以说该图像包含人脸。我们使用反向传播来学习这些权重。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Coupling coefficients</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">耦合系数</h1>

                

            

            

                

<p>接下来，我们将预测向量<img class="fm-editor-equation" src="img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png" style="width:1.50em;height:1.33em;"/>乘以耦合系数<img class="fm-editor-equation" src="img/2d45e983-c3ec-464d-940b-75c71790fcec.png" style="width:1.25em;height:1.00em;"/>。任何两个胶囊之间都存在耦合系数。我们知道来自较低层的胶囊将它们的输出发送到较高层的胶囊。耦合系数有助于较低层中的胶囊理解它必须将其输出发送到较高层中的哪个胶囊。</p>

<p>例如，让我们考虑同一个例子，我们试图预测一个图像是否由一张脸组成。<img class="fm-editor-equation" src="img/2d45e983-c3ec-464d-940b-75c71790fcec.png" style="width:1.50em;height:1.17em;"/>代表<img class="fm-editor-equation" src="img/49647a36-9898-4c0a-99b4-23cf0fe2d5b4.png" style="width:0.50em;height:1.25em;"/>和<img class="fm-editor-equation" src="img/4cb9f8f3-5451-46ee-a62f-bbc3cef70423.png" style="width:0.42em;height:1.00em;"/>之间的协议。</p>

<p><img class="fm-editor-equation" src="img/c658d700-33da-4b71-a264-2eee36515b37.png" style="width:1.42em;height:1.00em;"/>代表眼睛和脸的一致。既然我们知道眼睛在脸上，那么<img class="fm-editor-equation" src="img/c658d700-33da-4b71-a264-2eee36515b37.png" style="width:1.42em;height:1.00em;"/>值就会增加。我们知道预测向量<img class="fm-editor-equation" src="img/8aab866e-89b4-49b7-ae83-568a49f9f77d.png" style="width:1.83em;height:1.50em;"/>暗示了基于眼睛的面部的预测位置。用<img class="fm-editor-equation" src="img/1ae779e1-3dad-46f3-a74f-2bfed0ea20c2.png" style="width:1.50em;height:1.25em;"/>乘以<img class="fm-editor-equation" src="img/44a776bc-7835-4a04-b8e6-5a163beee271.png" style="width:1.50em;height:1.08em;"/>意味着我们在增加眼睛的重要性，因为<img class="fm-editor-equation" src="img/44a776bc-7835-4a04-b8e6-5a163beee271.png" style="width:1.50em;height:1.08em;"/>的值很高。</p>

<p><img class="fm-editor-equation" src="img/1757d487-16aa-422d-a8c8-421f92b96a62.png" style="width:1.42em;height:1.00em;"/>代表鼻子和脸的一致。既然我们知道鼻子在脸上，那么<img class="fm-editor-equation" src="img/a0aa2114-5372-4e01-bb5b-3670a27b9225.png" style="width:1.50em;height:1.08em;"/>值就会增加。我们知道预测向量<img class="fm-editor-equation" src="img/50b80e08-9d68-43f4-93e1-9e863f302dd9.png" style="width:1.58em;height:1.33em;"/>暗示了基于鼻子的面部预测位置。将<img class="fm-editor-equation" src="img/3d9ab4a0-f640-410f-9a28-7f56fb1d30e3.png" style="width:1.58em;height:1.33em;"/>乘以<img class="fm-editor-equation" src="img/07ddd2de-d13f-47c7-a594-9c546cb1ec61.png" style="width:1.42em;height:1.00em;"/>意味着我们在增加鼻子的重要性，因为<img class="fm-editor-equation" src="img/6e19a055-6cb3-4a00-bf84-c50931a89ce5.png" style="width:1.42em;height:1.00em;"/>的值很高。</p>

<p>让我们考虑另一个低级特征，比如说<img class="fm-editor-equation" src="img/1f7021c5-124b-4510-9045-ac9e08df3874.png" style="width:1.25em;height:0.83em;"/>，它检测手指。现在，<img class="fm-editor-equation" src="img/29e60844-2b1a-4f13-b806-8d5dca8abd49.png" style="width:1.50em;height:1.08em;"/>代表手指和脸的吻合度，会低。将<img class="fm-editor-equation" src="img/b761a33c-c89b-487c-bfc4-478297e98a63.png" style="width:1.67em;height:1.33em;"/>乘以<img class="fm-editor-equation" src="img/a44a4354-8d73-4870-8662-e46fc9b96e65.png" style="width:1.42em;height:1.00em;"/>意味着我们正在降低手指的重要性，因为<img class="fm-editor-equation" src="img/a44a4354-8d73-4870-8662-e46fc9b96e65.png" style="width:1.42em;height:1.00em;"/>的值很低。</p>

<p>但是这些耦合系数是怎么学来的呢？与权重不同，耦合系数是在前向传播本身中学习的，它们是使用一种称为动态路由的算法学习的，我们将在下一节讨论这种算法。</p>

<p>将<img class="fm-editor-equation" src="img/a6c7f3ae-f920-4319-8990-ff1d0de42cc6.png" style="width:1.42em;height:1.25em;"/>乘以<img class="fm-editor-equation" src="img/2d45e983-c3ec-464d-940b-75c71790fcec.png" style="width:1.50em;height:1.17em;"/>后，我们将它们相加，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1d918e1b-d57e-42d2-98f8-a3daca4670cb.png" style="width:15.67em;height:1.58em;"/></p>

<p>因此，我们可以将等式写成如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/00a46584-ac9d-48a5-bac9-eba1cf110992.png" style="width:7.08em;height:2.67em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Squashing function</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">挤压功能</h1>

                

            

            

                

<p>我们开始说胶囊<img class="fm-editor-equation" src="img/8117a7a5-ad1c-46b9-b9e4-3831995ce70e.png" style="width:0.42em;height:1.00em;"/>试图检测图像中的人脸。所以，我们需要将<img class="fm-editor-equation" src="img/4475e081-5628-4b62-8ba3-d38f9e364a3f.png" style="width:1.17em;height:1.83em;"/>转换成概率，从而得到图像中人脸存在的概率。</p>

<p>除了计算概率，我们还需要保持向量的方向，所以我们使用一个激活函数，称为挤压函数。其给出如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/d3abea8b-efbf-4e38-9f43-a39c0b72adc2.png" style="width:10.50em;height:3.58em;"/></p>

<p>现在，<img class="fm-editor-equation" src="img/47acd07c-4abb-4824-9b7b-6a21fe18405a.png" style="width:0.92em;height:1.08em;"/>(也称为活动向量)给出了给定图像中人脸存在的概率。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Dynamic routing algorithm</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">动态路由算法</h1>

                

            

            

                

<p>现在，我们将了解动态路由算法如何计算耦合系数。让我们引入一个新变量叫做<img class="fm-editor-equation" src="img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png" style="width:1.25em;height:1.33em;"/>，它只是一个临时变量，与耦合系数<img class="fm-editor-equation" src="img/0e8aeb81-9ded-4ea1-b1f9-6f431b657fcf.png" style="width:1.17em;height:0.92em;"/>相同。首先，我们将<img class="fm-editor-equation" src="img/0c803f36-d666-496c-b10b-b0df922e17bd.png" style="width:1.08em;height:1.17em;"/>初始化为0。这意味着较低层<img class="fm-editor-equation" src="img/0670a3bb-bb11-482d-b63f-a276da98c15d.png" style="width:0.33em;height:0.92em;"/>中的胶囊<img class="fm-editor-equation" src="img/1102139e-972c-446f-ba3b-4af96e8fde18.png" style="width:0.33em;height:1.00em;"/>和较高层<img class="fm-editor-equation" src="img/57f70dd5-15ca-4d9e-b01d-b55f406c0d64.png" style="width:2.08em;height:0.92em;"/>中的胶囊<img class="fm-editor-equation" src="img/81a66aa8-aad8-4120-980d-d884d3d37c00.png" style="width:0.50em;height:1.17em;"/>之间的耦合系数被设置为0。</p>

<p>设<img class="fm-editor-equation" src="img/1e3a3e33-adda-4961-a733-f61ec467327a.png" style="width:0.67em;height:1.08em;"/>为<img class="fm-editor-equation" src="img/0c803f36-d666-496c-b10b-b0df922e17bd.png" style="width:1.00em;height:1.08em;"/>的向量表示。给定预测向量<img class="fm-editor-equation" src="img/b84324a1-f6d6-49de-b61b-30b6343d6174.png" style="width:1.67em;height:1.50em;"/>，对于一些<em> n </em>次迭代，我们做如下:</p>

<ol>

<li>对于层<img class="fm-editor-equation" src="img/fcc44e5a-e3ef-4c53-ba87-ebc12c074820.png" style="width:0.42em;height:1.08em;"/>中的所有胶囊<img class="fm-editor-equation" src="img/77b2dc9f-3926-4f32-89c0-d3425563f7a5.png" style="width:0.50em;height:1.25em;"/>，计算如下:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/7fde1dd7-8523-4f2d-817a-55f7310d647b.png" style="width:9.17em;height:2.08em;"/></p>

<ol start="2">

<li>对于层<img class="fm-editor-equation" src="img/79a8569c-6df0-427f-a606-32fd6508d47a.png" style="width:2.42em;height:1.08em;"/>中的所有胶囊<img class="fm-editor-equation" src="img/9c4ef49d-c423-43d2-9d13-49e8d0e42eb5.png" style="width:0.50em;height:1.17em;"/>，计算如下:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e90ff949-6b08-4f0c-8bd4-ea5c0b6f5d3a.png" style="width:8.58em;height:3.25em;"/></p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/26849c78-0b84-44c4-834d-7f41e2c656c3.png" style="width:11.25em;height:3.83em;"/></p>

<ol start="3">

<li>对于<img class="fm-editor-equation" src="img/c857672e-e8b6-4bc5-bec4-e03185379626.png" style="width:0.42em;height:1.08em;"/>中的所有胶囊<img class="fm-editor-equation" src="img/e529ebfd-14de-40ca-9270-29301931285b.png" style="width:0.50em;height:1.25em;"/>和<img class="fm-editor-equation" src="img/bd605ba5-adde-471d-bead-2749d794b71d.png" style="width:0.50em;height:1.17em;"/>中的所有胶囊，计算<img class="fm-editor-equation" src="img/0c803f36-d666-496c-b10b-b0df922e17bd.png" style="width:1.00em;height:1.08em;"/>如下:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/302e217a-c547-46d2-be4a-d4ae7f42f078.png" style="width:9.92em;height:1.67em;"/></p>

<p>必须仔细注意前面的等式。这是我们更新耦合系数的地方。点积<img class="fm-editor-equation" src="img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png" style="width:3.50em;height:1.50em;"/>意味着较低层中的胶囊的预测向量<img class="fm-editor-equation" src="img/4e68442b-e295-4ea4-adba-21217e91221a.png" style="width:1.58em;height:1.42em;"/>和较高层中的胶囊的输出向量<img class="fm-editor-equation" src="img/19cc1d0c-f95d-4cd8-88fc-65a1d397c85f.png" style="width:1.00em;height:1.33em;"/>之间的点积。如果点积高，<img class="fm-editor-equation" src="img/1c8106b9-359c-4b86-bf94-ebb7dc4ff3fb.png" style="width:1.08em;height:1.17em;"/>将增加各自的耦合系数<img class="fm-editor-equation" src="img/69f2393a-d5aa-4a94-9b21-8f386ffbb9c0.png" style="width:1.17em;height:0.92em;"/>，这使得<img class="fm-editor-equation" src="img/4e9acdab-0658-4c9b-b1b9-3b1457c6bb91.png" style="width:2.92em;height:1.25em;"/>更强。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Architecture of the Capsule network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">胶囊网络的体系结构</h1>

                

            

            

                

<p>假设我们的网络试图预测手写数字。我们知道，早期层中的胶囊检测基本特征，而后期层中的胶囊检测数字。所以，让我们把早期层中的胶囊称为<strong>初级胶囊</strong>，把后期层中的胶囊称为<strong>数字胶囊</strong>。</p>

<p>胶囊网络的架构如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2037 image-border" src="img/d14d16fb-2c2e-4f8e-b9a6-09a0026f2522.png" style="width:95.00em;height:24.50em;"/></p>

<p>在上图中，我们可以观察到以下情况:</p>

<ol>

<li>首先，我们将输入图像送入标准卷积层，结果称为卷积输入。</li>

<li>然后，我们将卷积输入馈送到主胶囊层，并获得主胶囊。</li>

<li>接下来，我们使用动态路由算法以主胶囊作为输入来计算数字胶囊。</li>

<li>数字胶囊由10行组成，每行代表预测数字的概率。也就是说，第1行表示输入数字为0的概率，第2行表示数字为1的概率，依此类推。</li>

<li>由于输入图像是前一图像中的数字3，所以表示数字3的概率的行4在数字胶囊中将是高的。</li>

</ol>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>The loss function</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">损失函数</h1>

                

            

            

                

<p>现在我们将探讨胶囊网络的损失函数。损失函数是称为边际损失和重建损失的两个损失函数的加权和。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Margin loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">利润损失</h1>

                

            

            

                

<p>我们了解到胶囊返回一个向量，向量的长度代表特征存在的概率。假设我们的网络正在试图识别图像中的手写数字。为了检测给定图像中的多个数字，我们对每个数字胶囊<img class="fm-editor-equation" src="img/b9dd5c83-2949-44d7-986b-60a7c8c48982.png" style="font-size: 1em;width:0.58em;height:0.92em;"/>使用余量损失<img class="fm-editor-equation" src="img/41912fb7-6561-4262-bdef-7252e7f5806d.png" style="font-size: 1em;width:1.00em;height:0.92em;"/>，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f038fbd8-a9ec-4841-b4d8-ddd2430b95aa.png" style="width:31.42em;height:1.50em;"/></p>

<p>在这里，情况如下:</p>

<ul>

<li><img class="fm-editor-equation" src="img/642d706d-8618-4d97-be58-ec1c5a724004.png" style="width:3.08em;height:1.08em;"/>，如果类别<img class="fm-editor-equation" src="img/7f1c19ea-9906-42eb-9175-c4524ee9f281.png" style="width:0.67em;height:1.08em;"/>的数字存在</li>

<li><img class="fm-editor-equation" src="img/ba3ab593-1bde-4137-9de1-937379784247.png" style="width:1.25em;height:0.83em;"/>是余量，<img class="fm-editor-equation" src="img/5739739c-5563-4d46-acfc-b602f1715e16.png" style="width:1.58em;height:1.08em;"/>设置为0.9，<img class="fm-editor-equation" src="img/b83b1abf-cf5d-47c4-9ec1-3841703f993c.png" style="width:1.58em;height:1.08em;"/>设置为0.1</li>

<li><img class="fm-editor-equation" src="img/9993a5d8-1324-44e2-94d6-b443be02872e.png" style="width:0.58em;height:0.83em;"/>防止初始学习缩小所有数字胶囊的向量长度，通常设置为0.5</li>

</ul>

<p>总利润损失是所有类别损失的总和，<img class="fm-editor-equation" src="img/2df2e71e-36eb-4fe7-b622-3b5d36e3e4cd.png" style="width:0.58em;height:0.92em;"/>，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1707881e-bf20-42cd-b127-55e5e71f7c7c.png" style="width:10.00em;height:2.42em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reconstruction loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">重建损失</h1>

                

            

            

                

<p>为了确保网络已经学习了胶囊中的重要特征，我们使用重建损失。这意味着我们使用一个称为解码器网络的三层网络，它试图从数字胶囊中重建原始图像:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1977 image-border" src="img/e714ade3-5b08-4d16-8bdd-5cc6438a31c2.png" style="width:41.17em;height:18.17em;"/></p>

<p>重建损失由重建图像和原始图像之间的平方差给出，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/23dd475d-dd9e-4e17-9310-7b800aef04ff.png" style="width:30.67em;height:1.42em;"/></p>

<p>最终损失如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/106c782e-4b1e-49f0-ad58-d9e3ecddd21b.png" style="width:23.42em;height:1.17em;"/></p>

<p>这里，α是一个正则项，因为我们不希望重建损失比边际损失具有更高的优先级。因此，alpha乘以重建损失以降低其重要性，通常设置为0.0005。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building Capsule networks in TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在TensorFlow中构建胶囊网络</h1>

                

            

            

                

<p>现在我们将学习如何在TensorFlow中实现胶囊网络。我们将使用我们最喜欢的MNIST数据集来学习胶囊网络如何识别手写图像。</p>

<p>导入所需的库:</p>

<pre>import warnings<br/>warnings.filterwarnings('ignore')<br/><br/>import numpy as np<br/>import tensorflow as tf<br/><br/>from tensorflow.examples.tutorials.mnist import input_data<br/>tf.logging.set_verbosity(tf.logging.ERROR)</pre>

<p>加载MNIST数据集:</p>

<pre>mnist = input_data.read_data_sets("data/mnist",one_hot=True)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining the squash function</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义挤压功能</h1>

                

            

            

                

<p>我们了解到squash函数将向量的长度转换为概率，其给出如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ee00267a-bcf4-4a2d-92ee-bba5e1c162c5.png" style="width:12.67em;height:4.33em;"/></p>

<p><kbd>squash</kbd>功能可定义如下:</p>

<pre>def squash(sj):<br/>    <br/>    sj_norm = tf.reduce_sum(tf.square(sj), -2, keep_dims=True)<br/>    scalar_factor = sj_norm / (1 + sj_norm) / tf.sqrt(sj_norm + epsilon)<br/><br/>    vj = scalar_factor * sj <br/><br/>    return vj</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining a dynamic routing algorithm</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义动态路由算法</h1>

                

            

            

                

<p>现在我们来看看动态路由算法是如何实现的。我们使用在动态路由算法中学到的相同符号的变量名，这样我们就可以很容易地遵循这些步骤。我们将一步一步地看函数中的每一行。你也可以在GitHub的<a href="http://bit.ly/2HQqDEZ">http://bit.ly/2HQqDEZ</a>查看完整的代码。</p>

<p>首先，定义名为<kbd>dynamic_routing</kbd>的函数，该函数将前面的胶囊、<kbd>ui</kbd>、耦合系数、<kbd>bij</kbd>和路由迭代次数、<kbd>num_routing</kbd>作为输入，如下所示:</p>

<pre>def dynamic_routing(ui, bij, num_routing=10):</pre>

<p>通过从随机正态分布中抽取来初始化<kbd>wij</kbd>权重，并用常数值初始化<kbd>biases</kbd>:</p>

<pre>    wij = tf.get_variable('Weight', shape=(1, 1152, 160, 8, 1), dtype=tf.float32,<br/><br/>                        initializer=tf.random_normal_initializer(0.01))<br/><br/>    biases = tf.get_variable('bias', shape=(1, 1, 10, 16, 1))</pre>

<p>定义初级胶囊<kbd>ui</kbd> ( <kbd>tf.tile</kbd>复制张量<em> n次</em>):</p>

<pre>    ui = tf.tile(ui, [1, 1, 160, 1, 1])</pre>

<p>计算预测向量<img class="fm-editor-equation" src="img/b4ca5bae-01ba-43f0-ba90-7543d6de315c.png" style="width:6.67em;height:1.58em;"/>，如下所示:</p>

<pre>    u_hat = tf.reduce_sum(wij * ui, axis=3, keep_dims=True)</pre>

<p>重塑预测向量:</p>

<pre>    u_hat = tf.reshape(u_hat, shape=[-1, 1152, 10, 16, 1])</pre>

<p>停止预测向量中的梯度计算:</p>

<pre>    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')</pre>

<p>执行多次路由迭代的动态路由，如下所示:</p>

<pre>    for r in range(num_routing):<br/><br/>        with tf.variable_scope('iter_' + str(r)):<br/>            <br/>            #step 1<br/>            cij = tf.nn.softmax(bij, dim=2)<br/>            <br/>            #step 2<br/>            if r == num_routing - 1:<br/><br/>                sj = tf.multiply(cij, u_hat)<br/><br/>                sj = tf.reduce_sum(sj, axis=1, keep_dims=True) + biases<br/><br/>                vj = squash(sj)<br/><br/>            elif r &lt; num_routing - 1: <br/><br/>                sj = tf.multiply(cij, u_hat_stopped)<br/><br/>                sj = tf.reduce_sum(sj, axis=1, keep_dims=True) + biases<br/><br/>                vj = squash(sj)<br/><br/>                vj_tiled = tf.tile(vj, [1, 1152, 1, 1, 1])<br/><br/>                coupling_coeff = tf.reduce_sum(u_hat_stopped * vj_tiled, axis=3, keep_dims=True)<br/><br/>                #step 3<br/>                bij += coupling_coeff<br/>   return vj</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing primary and digit capsules</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">计算主胶囊和数字胶囊</h1>

                

            

            

                

<p>现在我们将计算提取基本特征的主胶囊和识别数字的数字胶囊。</p>

<p>启动张量流<kbd>Graph</kbd>:</p>

<pre>graph = tf.Graph()<br/>with graph.as_default() as g:</pre>

<p>定义输入和输出的占位符:</p>

<pre>    x = tf.placeholder(tf.float32, [batch_size, 784])<br/>    y = tf.placeholder(tf.float32, [batch_size,10])<br/>    x_image = tf.reshape(x, [-1,28,28,1])</pre>

<p class="mce-root">执行卷积运算并获得卷积输入:</p>

<pre>    with tf.name_scope('convolutional_input'):<br/>        input_data = tf.contrib.layers.conv2d(inputs=x_image, num_outputs=256, kernel_size=9, padding='valid')</pre>

<p>计算提取基本特征(如边缘)的主胶囊。首先，使用卷积运算计算胶囊，如下所示:</p>

<pre> capsules = []<br/><br/> for i in range(8):<br/><br/> with tf.name_scope('capsules_' + str(i)):<br/> <br/> #convolution operation <br/> output = tf.contrib.layers.conv2d(inputs=input_data, num_outputs=32,kernel_size=9, stride=2, padding='valid')<br/><br/> #reshape the output<br/> output = tf.reshape(output, [batch_size, -1, 1, 1])<br/> <br/> #store the output which is capsule in the capsules list<br/> capsules.append(output)</pre>

<p>连接所有胶囊并形成主胶囊，挤压主胶囊，并获得如下概率:</p>

<pre> primary_capsule = tf.concat(capsules, axis=2)</pre>

<p>将<kbd>squash</kbd>函数应用于主胶囊，获得概率:</p>

<pre> primary_capsule = squash(primary_capsule)</pre>

<p>使用动态路由算法计算数字胶囊，如下所示:</p>

<pre>    with tf.name_scope('dynamic_routing'):<br/>        <br/>        #reshape the primary capsule<br/>        outputs = tf.reshape(primary_capsule, shape=(batch_size, -1, 1, primary_capsule.shape[-2].value, 1))<br/><br/>        #initialize bij with 0s<br/>        bij = tf.constant(np.zeros([1, primary_capsule.shape[1].value, 10, 1, 1], dtype=np.float32))<br/><br/><br/>        <br/>        #compute the digit capsules using dynamic routing algorithm which takes <br/>        #the reshaped primary capsules and bij as inputs and returns the activity vector <br/>        digit_capsules = dynamic_routing(outputs, bij)<br/>   <br/>   <br/> digit_capsules = tf.squeeze(digit_capsules, axis=1)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Masking the digit capsule</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">掩蔽数字胶囊</h1>

                

            

            

                

<p class="mce-root">为什么我们需要屏蔽数字胶囊？我们了解到，为了确保网络已经学习了重要的特征，我们使用了一个称为解码器网络的三层网络，它试图从数字胶囊中重建原始图像。如果解码器能够成功地从数字胶囊中重建图像，那么这意味着网络已经学习了图像的重要特征；否则，网络没有学习到图像的正确特征。</p>

<p class="mce-root">数字胶囊包含所有数字的活动向量。但是解码器只想重建给定的输入数字(输入图像)。因此，我们屏蔽掉所有数字的活动向量，除了正确的数字。然后，我们使用这个掩蔽的数字胶囊来重建给定的输入图像:</p>

<pre>with graph.as_default() as g:<br/>    with tf.variable_scope('Masking'):<br/>        <br/>        #select the activity vector of given input image using the actual label y and mask out others<br/>        masked_v = tf.multiply(tf.squeeze(digit_capsules), tf.reshape(y, (-1, 10, 1)))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining the decoder</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义解码器</h1>

                

            

            

                

<p class="mce-root">定义用于重建图像的解码器网络。它由以下三个完全连接的网络组成:</p>

<pre>with tf.name_scope('Decoder'):<br/><br/>    #masked digit capsule<br/>    v_j = tf.reshape(masked_v, shape=(batch_size, -1))<br/><br/>    #first fully connected layer <br/>    fc1 = tf.contrib.layers.fully_connected(v_j, num_outputs=512)<br/><br/>    #second fully connected layer<br/>    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)<br/><br/>    #reconstructed image<br/>    reconstructed_image = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing the accuracy of the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">计算模型的精确度</h1>

                

            

            

                

<p>现在我们计算模型的精确度:</p>

<pre>with graph.as_default() as g:<br/>    with tf.variable_scope('accuracy'):</pre>

<p>计算数字胶囊中每个活动向量的长度:</p>

<pre>        v_length = tf.sqrt(tf.reduce_sum(tf.square(digit_capsules), axis=2, keep_dims=True) + epsilon)</pre>

<p>将<kbd>softmax</kbd>应用于长度并获得概率:</p>

<pre>        softmax_v = tf.nn.softmax(v_length, dim=1)</pre>

<p>选择概率最高的索引；这将给我们预测的数字:</p>

<pre>        argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1)) <br/>        predicted_digit = tf.reshape(argmax_idx, shape=(batch_size, ))</pre>

<p>计算<kbd>accuracy</kbd>:</p>

<pre>        actual_digit = tf.to_int32(tf.argmax(y, axis=1))<br/><br/>        correct_pred = tf.equal(predicted_digit,actual_digit)<br/>        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Calculating loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">计算损失</h1>

                

            

            

                

<p class="mce-root">众所周知，我们计算两种类型的损失——边际损失和重建损失。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Margin loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">利润损失</h1>

                

            

            

                

<p>我们知道利润损失如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/c7a02188-ebba-41ac-a44e-4172aabfef28.png" style="width:29.42em;height:1.42em;"/></p>

<p>计算左边的最大值和右边的最大值:</p>

<pre>max_left = tf.square(tf.maximum(0.,0.9 - v_length))<br/>max_right = tf.square(tf.maximum(0., v_length - 0.1))</pre>

<p>将<img class="fm-editor-equation" src="img/305c1497-9b9f-42e9-9869-d19d8e80c646.png" style="width:1.17em;height:1.08em;"/>设置为<img class="fm-editor-equation" src="img/6c67bda9-c871-4a51-9eb9-c9503a77223c.png" style="width:0.58em;height:1.00em;"/>:</p>

<pre class="mce-root">T_k = y<br/><br/>lambda_ = 0.5<br/>L_k = T_k * max_left + lambda_ * (1 - T_k) * max_right</pre>

<p>总利润损失计算如下:</p>

<pre class="mce-root">margin_loss = tf.reduce_mean(tf.reduce_sum(L_k, axis=1))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reconstruction loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">重建损失</h1>

                

            

            

                

<p>使用以下代码整形并获得原始图像:</p>

<pre>original_image = tf.reshape(x, shape=(batch_size, -1))</pre>

<p>计算重建图像和原始图像之间的平方差的平均值:</p>

<pre>squared = tf.square(reconstructed_image - original_image)<br/><br/></pre>

<p>计算重建损失:</p>

<pre>reconstruction_loss = tf.reduce_mean(squared) </pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Total loss</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">全损</h1>

                

            

            

                

<p>定义总损失，即边际损失和重建损失的加权和:</p>

<pre>alpha = 0.0005<br/>total_loss = margin_loss + alpha * reconstruction_loss</pre>

<p>使用Adam优化器优化损失:</p>

<pre>optimizer = tf.train.AdamOptimizer(0.0001)<br/>train_op = optimizer.minimize(total_loss)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training the Capsule network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练胶囊网络</h1>

                

            

            

                

<p>设置时期数和步数:</p>

<pre>num_epochs = 100<br/>num_steps = int(len(mnist.train.images)/batch_size)</pre>

<div><div><div><div><div><p>现在启动张量流<kbd>Session</kbd>并进行训练:</p>

</div>

</div>

</div>

</div>

</div>

<pre>with tf.Session(graph=graph) as sess:<br/><br/>    init_op = tf.global_variables_initializer()<br/>    sess.run(init_op)<br/><br/><br/>    for epoch in range(num_epochs):<br/>        for iteration in range(num_steps):<br/>            batch_data, batch_labels = mnist.train.next_batch(batch_size)<br/>            feed_dict = {x : batch_data, y : batch_labels}<br/>     <br/>            _, loss, acc = sess.run([train_op, total_loss, accuracy], feed_dict=feed_dict)<br/><br/>            if iteration%10 == 0:<br/>                print('Epoch: {}, iteration:{}, Loss:{} Accuracy: {}'.format(epoch,iteration,loss,acc))</pre>

<p>您可以看到在各种迭代中损失是如何减少的:</p>

<pre>Epoch: 0, iteration:0, Loss:0.55281829834 Accuracy: 0.0399999991059

Epoch: 0, iteration:10, Loss:0.541650533676 Accuracy: 0.20000000298

Epoch: 0, iteration:20, Loss:0.233602654934 Accuracy: 0.40000007153</pre>

<p>因此，我们已经逐步了解了胶囊网络的工作原理，以及如何在TensorFlow中构建胶囊网络。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>我们从理解CNN开始这一章。我们学习了CNN的不同层次，比如卷积和池化；其中来自图像的重要特征将被提取并被馈送到完全收集的层；以及所提取的特征将被分类的位置。我们还通过对手写数字进行分类，使用TensorFlow可视化了从卷积层提取的特征。</p>

<p>后来我们了解了CNN的几种架构，包括LeNet，AlexNet，VGGNet，GoogleNet。在本章的最后，我们研究了胶囊网络，它克服了卷积网络的缺点。我们了解到胶囊网络使用动态路由算法对图像进行分类。</p>

<p>在下一章，我们将研究用于学习文本表示的各种算法。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Questions</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">问题</h1>

                

            

            

                

<p>让我们尝试回答以下问题，以评估我们对CNN的了解:</p>

<ol>

<li>CNN有哪些不同的层次？</li>

<li>定义步幅。</li>

<li>为什么需要填充？</li>

<li>定义池化。有哪些不同类型的池操作？</li>

<li>解释VGGNet的架构。</li>

<li>《盗梦空间》网络中的因子分解卷积是什么？</li>

<li>胶囊网络和CNN有什么不同？</li>

<li>定义挤压函数。</li>

</ol>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Further reading</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">进一步阅读</h1>

                

            

            

                

<p>有关更多信息，请参考以下内容:</p>

<ul>

<li><em>用于大规模图像识别的深度卷积网络</em>由https://arxiv.org/pdf/1409.1556.pdf<a href="https://arxiv.org/pdf/1409.1556.pdf">的卡伦·西蒙扬和安德鲁·齐泽曼开发</a></li>

<li>克里斯蒂安·塞格迪等人撰写的关于《盗梦空间网》<em>的论文</em>，可在<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">https://www . cv-foundation . org/open access/content _ cvpr _ 2015/papers/Szegedy _ Going _ Deeper _ With _ 2015 _ CVPR _ paper . pdf</a>查阅</li>

<li><em>Sara Sabour、Nicholas Frosst和Geoffrey E. Hinton编写的胶囊间动态路由</em>，可在<a href="https://arxiv.org/pdf/1710.09829.pdf">https://arxiv.org/pdf/1710.09829.pdf</a>获得</li>

</ul>





            



            

        

    </body>



</html></body></html>