

# 学习文本表示

神经网络只需要数字输入。因此，当我们有文本数据时，我们将它们转换成数字或向量表示，并将其提供给网络。有多种方法可以将输入文本转换成数字形式。比较流行的一些方法有**词频-逆文档频** ( **tf-idf** )、**包词(BOW)** 等等。然而，这些方法不能捕捉单词的语义。这意味着这些方法不会理解单词的意思。

在这一章中，我们将学习一种叫做 **word2vec** 的算法，它将文本输入转换成一个有意义的向量。他们学习给定输入文本中每个单词的语义向量表示。我们将从了解 word2vec 模型和两种不同类型的 word2vec 模型开始，这两种模型称为**连续词袋** ( **CBOW** )和跳格模型。接下来，我们将学习如何使用 gensim 库构建 word2vec 模型，以及如何在 tensorboard 中可视化高维词嵌入。

接下来，我们将学习用于学习文档表示的 **doc2vec** 模型。我们会了解 doc2vec 中两种不同的方法叫做**段向量-** **分布式内存** **模型** ( **PV-DM** )和**段向量-** **分布式包字** ( **PV-DBOW** )。我们还将看到如何使用 doc2vec 执行文档分类。在本章的最后，我们将学习用于学习句子表达的跳过思维算法和快速思维算法。

在本章中，我们将了解以下主题:

*   word2vec 模型
*   使用 gensim 构建 word2vec 模型
*   TensorBoard 中词嵌入的可视化
*   Doc2vec 模型
*   使用 doc2vec 查找相似的文档
*   跳过思想
*   思维敏捷



# 了解 word2vec 模型

Word2vec 是最流行和广泛使用的生成词嵌入的模型之一。但是什么是词嵌入呢？词嵌入是单词在向量空间中的向量表示。word2vec 模型生成的嵌入捕获单词的语法和语义含义。对单词进行有意义的矢量表示有助于神经网络更好地理解单词。

例如，让我们来看下面这篇课文:阿奇过去住在纽约，后来他搬到了圣克拉拉。他喜欢苹果和草莓。

Word2vec 模型为前面文本中的每个单词生成向量表示。如果我们在嵌入空间中投影和可视化向量，我们可以看到所有相似的单词是如何紧密地绘制在一起的。如下图所示，单词*苹果*和*草莓*绘制在一起，单词*纽约州*和*圣克拉拉*绘制在一起。它们被绘制在一起是因为 word2vec 模型了解到*苹果*和*草莓*是相似的实体，即水果，而*纽约*和*圣克拉拉*是相似的实体，即*城市*，因此它们的向量(嵌入)彼此相似，这就是它们之间的距离较小的原因:

![](img/30025401-9037-42e2-a7c7-09ebeeb6caa0.png)

因此，使用 word2vec 模型，我们可以学习单词的有意义的向量表示，这有助于神经网络理解单词是关于什么的。对一个单词有一个好的表示在各种任务中都是有用的。由于我们的网络可以理解单词的上下文和句法含义，这将扩展到各种用例，如文本摘要、情感分析、文本生成等。

好吧。但是 word2vec 模型是如何学习词嵌入的呢？有两种学习词嵌入的 word2vec 模型:

1.  CBOW 模型
2.  跳格模型

我们将深入细节，了解这些模型是如何学习单词的矢量表示的。



# 了解 CBOW 模型

假设我们有一个神经网络，它有一个输入层、一个隐藏层和一个输出层。网络的目标是根据一个词周围的词来预测这个词。我们试图预测的单词被称为**目标单词**，围绕目标单词的单词被称为**上下文单词**。

我们用多少上下文单词来预测目标单词？我们使用大小为![](img/565e5e2c-349b-4dec-8198-86027ebd3b89.png)的窗口来选择上下文单词。如果窗口大小是 2，那么我们使用目标单词之前的两个单词和之后的两个单词作为上下文单词。

我们来考虑一下句子，*太阳从东方升起*以*升起*这个词为目标词。如果我们将窗口大小设置为 2，那么我们将前面的两个词*、*和*、*、中的*和目标词*上升到*之后的两个词*、*作为上下文词，如下图所示:*

![](img/d67de55c-90c2-4457-a746-c9427430741e.png)

因此，网络的输入是上下文单词，输出是目标单词。我们如何将这些输入馈入网络？神经网络只接受数字输入，所以我们不能将原始的上下文单词直接作为网络的输入。因此，我们使用 one-hot 编码技术将给定句子中的所有单词转换成数字形式，如下图所示:

![](img/50b341fe-78f6-4142-8858-caec284b7d07.png)

下图显示了 CBOW 模型的体系结构。如您所见，我们将上下文单词、、、孙、in、T22 和 T24 作为输入输入到网络中，它预测目标单词将上升为输出:

![](img/38a8ac22-8330-4477-9692-b04fe3326bf6.png)

在初始迭代中，网络不能正确预测目标单词。但是经过一系列的迭代，它学会了使用梯度下降来预测正确的目标单词。使用梯度下降，我们更新网络的权重，并找到可以预测正确目标词的最佳权重。

由于我们有一个输入层、一个隐藏层和一个输出层，如上图所示，我们将有两个权重:

*   输入层到隐藏层的权重，![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
*   隐藏层到输出层的权重，![](img/dfbb80c7-0625-43b1-9d59-c3f84e0fa17e.png)

在训练过程中，网络将试图找到这两组权重的最佳值，以便它可以预测正确的目标单词。

原来，输入到隐藏层![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)之间的最佳权重形成了单词的向量表示。它们基本上构成了单词的语义。因此，在训练之后，我们简单地移除输出层，取输入层和隐藏层之间的权重，并将它们分配给相应的单词。

训练之后，如果我们看一下![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)矩阵，它代表了每个单词的嵌入。所以，*孙*的嵌入为[0.0，0.3，0.3，0.6，0.1 ]:

![](img/2ae49ea5-39d7-46be-bbfd-b06845fb7491.png)

因此，CBOW 模型学习用给定的上下文单词来预测目标单词。它学习使用梯度下降来预测正确的目标单词。在训练过程中，它通过梯度下降来更新网络的权重，并找到可以预测正确目标词的最佳权重。输入层和隐藏层之间的最佳权重形成了单词的矢量表示。因此，在训练之后，我们简单地取输入层和隐藏层之间的权重，并将它们作为向量分配给相应的单词。

现在我们已经对 CBOW 模型有了直观的理解，我们将进入细节并从数学上学习如何精确地计算词嵌入。

我们了解到，输入层和隐藏层之间的权重基本上构成了单词的矢量表示。但是 CBOW 模型到底是如何预测目标词的呢？它如何使用反向传播来学习最佳权重？让我们在下一节看看这个。



# 只有一个上下文单词的 CBOW

我们了解到，在 CBOW 模型中，我们试图预测给定上下文单词的目标单词，因此它将一些![](img/1be066cd-9ae6-4c55-8ea4-bc2e5f280d8e.png)数量的上下文单词作为输入，并返回一个目标单词作为输出。在只有一个上下文词的 CBOW 模型中，我们将只有一个上下文词，即![](img/f767c138-9bb5-4bc2-b598-8c19caab3d5b.png)。因此，网络只接受一个上下文单词作为输入，并返回一个目标单词作为输出。

在继续之前，首先让我们熟悉一下符号。我们语料库中所有的独特词汇被称为**词汇**。考虑到我们在*理解 CBOW 模型*部分看到的例子，我们在句子中有五个独特的词——*the*、*孙*、 *rises* 、中的*和 *east* 。这五个词是我们的词汇。*

设![](img/28e68adf-0cf3-41d7-b313-250b1a777f93.png)表示词汇量的大小(即字数)![](img/5395fd7f-8249-4750-afec-9c901d8a7f3c.png)表示隐含层神经元的数量。我们了解到我们有一个输入层、一个隐藏层和一个输出层:

*   输入层由![](img/10bb5dbd-fdf0-4699-8297-d12490cf3480.png)表示。当我们说![](img/70f5dd90-b264-4e2a-9132-1df2e29c5974.png)时，代表的是词汇表中的![](img/24d01c97-ea8f-4a0c-8a4c-a1c95e5543fe.png)输入词。
*   隐藏层用![](img/d4db72ad-4b63-4e36-95bd-4bed53452038.png)表示。当我们说![](img/55fb0590-0038-4c33-a828-f1e509469be6.png)时，它代表隐藏层中的![](img/0c4beba7-b212-4130-b23c-5891a33120e2.png)神经元。
*   输出层用![](img/bf19185d-47ce-48e8-a5b9-8abe0d83f9be.png)表示。当我们说![](img/1707ff27-1948-48c5-9986-3760050ca4d2.png)时，它代表词汇表中的![](img/c7991abb-c811-4e4f-b9d0-d150baeb0300.png)输出单词。

输入到隐层权重![](img/16c17c70-d122-4dd9-8efc-180583d17de6.png)的维度是![](img/93436413-de5f-4eed-be42-4804aa6e9c51.png)(也就是我们的词汇量的*大小 x 隐层神经元的数量*)隐藏到输出层权重的维度![](img/053566fc-39df-4ac1-be14-0504c8e951f0.png)是![](img/0938958c-85c1-4482-8438-b0c890b67e05.png)(也就是隐层神经元的*数量 x 词汇量的*)。矩阵元素的表示如下:

*   ![](img/88598b9c-17d3-48e2-bf20-f4d173924600.png)表示输入层节点![](img/4312bbef-3b4c-4994-8b2d-383de32644dc.png)和隐藏层节点![](img/cd0d6d00-0870-44d0-8273-d6d21b5bc57a.png)之间矩阵中的一个元素
*   ![](img/625fd42e-d7ca-4a5c-9878-aec1f414b10b.png)表示隐藏层的节点![](img/95948634-5121-4bb3-ac04-87b3ce8a417b.png)和输出层的节点![](img/efe5da3d-88d4-4aef-9e6e-cd8787aa0008.png)之间的矩阵中的元素

下图将有助于我们清楚地理解符号:

![](img/8deba7bd-3467-46a7-bcff-38e287a52b68.png)



# 正向传播

为了预测给定上下文单词的目标单词，我们需要执行前向传播。

首先，我们将输入![](img/b59575fa-9574-4b08-847f-c60923c09f48.png)乘以隐藏层权重的输入![](img/42869efe-315b-4bb8-acd5-cfba8e46acb6.png):

![](img/b08663e6-7e08-4cc9-a0c6-77ce38474012.png)

我们知道每个输入字都是一位热编码的，所以当我们将![](img/ddf8720e-6c3e-4ade-9245-d78a0e6714bd.png)乘以![](img/fd23003b-f971-47e3-8bfb-4cab40fff647.png)时，我们基本上获得了![](img/6bf8629c-5577-4e5e-a744-09a8328af6c0.png)到![](img/d27e266c-0b00-4cf2-94aa-31dd74ffd685.png)的![](img/b1d3087b-3f76-4ff0-ad4a-1b1b76a3e242.png)行。所以，我们可以直接写为:

![](img/772add17-ac4f-4bbe-a9fb-ca263fa739eb.png)

![](img/0f435fb3-2560-4c5d-9863-7b4647a5a058.png)基本上暗示了输入单词的矢量表示。让我们用![](img/4831b672-87ba-497d-8b52-5e80d9098041.png)来表示输入单词![](img/1e51dde8-5816-498b-8e09-bd5038617a9d.png)的向量表示。因此，前面的等式可以写成如下形式:

![](img/14c7bb66-a44c-40c8-b990-51b6b8bc7fc4.png)

现在我们在隐藏层![](img/4acc897c-0c04-4a77-8527-6e382bdb80f8.png)，我们有另一组权重，隐藏输出层权重![](img/fe6b3a36-a507-41df-9cc2-a6aeffa627d4.png)。我们知道我们的词汇表中有![](img/64f5782d-47fe-4555-96c5-d5b489ad2acf.png)个单词，我们需要计算词汇表中每个单词成为目标单词的概率。

让![](img/b2ebffb6-d301-46a8-bfa5-4fcf004cfc00.png)表示我们的词汇表中的![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)单词作为目标单词的分数。分数![](img/86f338d1-6f8e-4058-9df1-30d71bbeb2c4.png)通过将隐藏层的值![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)和隐藏到输出层的权重![](img/bea30eb3-8cc3-46aa-9be7-ca17659e4f14.png)相乘来计算。由于我们正在计算单词![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png)的分数，我们将隐藏层![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)乘以矩阵的![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)列![](img/e0101955-901b-4d4a-9027-a00033476af1.png):

![](img/2454457a-0a5e-4451-90ad-2b1909c822ab.png)

权重矩阵![](img/e0101955-901b-4d4a-9027-a00033476af1.png)的![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)列基本上表示单词![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png)的向量表示。让我们用![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png)来表示![](img/9ef9f1db-e40c-410e-ab64-dc8166e5406b.png)单词的矢量表示。因此，前面的等式可以写成如下形式:

![](img/f5984dac-97c6-4384-ab34-eed4b8989cb5.png)

将方程 *(1)* 代入方程 *(2)* ，我们可以写出如下:

![](img/8454782c-a236-49f6-a8be-29b282f72385.png)

你能推断出前面的等式想要表达什么吗？我们基本上是在计算输入上下文单词表示![](img/b1fc0aac-ccdb-40d1-a876-afe56832946b.png)和我们的词汇表![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png)中的![](img/7c4ef403-fb01-45cb-ad55-474e50a656be.png)单词表示之间的点积。

计算任意两个向量之间的点积有助于我们理解它们有多相似。因此，计算![](img/54377cf3-2f02-4baa-a8a3-b7c4c818d703.png)和![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png)之间的点积可以告诉我们词汇表中的![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)单词与输入的上下文单词有多相似。因此，当词汇表中的![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)单词![](img/e1af9677-c6f2-466f-bb02-6dcbce530271.png)的得分高时，则意味着单词![](img/b7ea99e1-97ed-411e-8593-1eff0dea4578.png)与给定的输入单词相似，并且它是目标单词。类似地，当词汇表![](img/a00856ab-ad98-4502-8f0a-49340d60e79d.png)中的![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)单词的分数低时，则意味着单词![](img/7d1306a6-05a2-4227-addf-8ed05aec706d.png)与给定的输入单词不相似，并且它不是目标单词。

因此，![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)基本上给出了作为目标单词的单词![](img/facc6cb1-db9d-4ecf-a60e-28a4fd152571.png)的分数。但是我们没有将![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)作为原始分数，而是将它们转换成概率。我们知道 softmax 函数压缩 0 到 1 之间的值，所以我们可以使用 softmax 函数将![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)转换成概率。

我们可以将输出编写如下:

![](img/55d7d9f4-6b63-48e6-8891-d6c3ac30236e.png)

这里，![](img/c571759f-1422-406d-9713-80ca2e80320e.png)告诉我们在给定输入上下文单词的情况下，单词![](img/bb81e391-269b-4e9d-b719-c6a562784121.png)相对于目标单词的概率。我们计算词汇表中所有单词的概率，并选择概率最高的单词作为目标单词。

好的，我们的目标函数是什么？也就是说，我们如何计算损失？

我们的目标是找到正确的目标词。设![](img/ee7a65a2-78a3-48fb-8473-375153d20017.png)表示正确目标词的概率。所以，我们需要最大化这个概率:

![](img/3e04e42c-0178-48ac-a12a-611cf58a1ba6.png)

我们不是最大化原始概率，而是最大化对数概率:

![](img/62e56fe1-4a68-4389-aa0a-597fbf1e35ae.png)

但是为什么我们要最大化对数概率而不是原始概率呢？因为机器在表示分数的浮点方面有局限性，当我们乘以许多概率时，它会导致一个无限小的值。因此，为了避免这种情况，我们使用对数概率，这将确保数值的稳定性。

现在我们有了一个最大化目标，我们需要将其转换为最小化目标，这样我们就可以应用我们最喜欢的梯度下降算法来最小化目标函数。怎样才能把我们的最大化目标变成最小化目标？我们可以通过简单地加上负号来实现。所以我们的目标函数变成如下:

![](img/802c0bdc-a33b-4eab-9a82-36152c243dd9.png)

损失函数可由下式给出:

![](img/ccf589ba-6418-4f6b-94bd-610879c30b62.png)

将方程 *(3)* 代入方程 *(4)* ，我们得到如下结果:

![](img/f75fdf2f-9b33-4340-979b-a69612d2568d.png)

根据对数商法则， *log(a/b) = log(a) - log(b)* ，我们可以将前面的等式改写如下:

![](img/24907d6f-88c6-4b8d-a094-0f962c167621.png)

![](img/602521e1-4cca-47e3-b589-217278f67647.png)

我们知道 *log* 和 *exp* 相互抵消，所以我们可以在第一项中取消 *log* 和 *exp* ，我们最终的损失函数变成如下:

![](img/15049d3f-2a48-497c-bbf4-e9c1d6b877d0.png)



# 反向传播

我们使用梯度下降算法最小化损失函数。因此，我们反向传播网络，计算损失函数相对于权重的梯度，并更新权重。我们有两组权重，输入到隐藏层权重![](img/a97130e6-b8ec-4bf8-b44c-cfb431e50af0.png)和隐藏到输出层权重![](img/25402288-f99a-46b3-9409-5fc1ad0130a9.png)。我们计算关于这两个权重的损失梯度，并根据权重更新规则更新它们:

![](img/1b6c6b97-6b29-4494-aaee-2d46ad552afa.png)

![](img/6fda6f1d-81a1-4361-9f7c-55da60e9de05.png)

为了更好地理解反向传播，让我们回忆一下正向传播中涉及的步骤:

![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png)

![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)

![](img/5a7d022d-3be1-404c-a4dc-9bb4c72ad987.png)

首先，我们计算相对于隐藏到输出层![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)的损失梯度。我们不能直接从![](img/6ccca083-758a-4ec5-ae2b-e48ea51a46f7.png)计算![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)相对于![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)的损失梯度，因为损失函数![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)中没有![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)项，所以我们应用链式法则如下:

![](img/8cb9185c-2894-40bd-96fc-9c8b4ad92600.png)

请参考向前传播的方程式来理解导数是如何计算的。

第一项的导数如下:

![](img/64182944-a208-4c28-957a-246af8a54e18.png)

这里，![](img/29f4b902-e2cd-4949-88e6-40e4e4755cb2.png)是误差项，是实际字和预测字之差。

现在，我们将计算第二项的导数。

既然我们知道![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):

**![](img/65ed56d8-8fd2-421f-a888-85926a972ec6.png)**

因此，相对于![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) 的**损耗梯度![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)为:**

![](img/d556d3b2-2bbb-4fba-bfe7-79c8115d3b2b.png)

现在，我们计算相对于隐藏层权重![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png)的输入的梯度。我们不能直接从![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png)计算导数，因为![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png)中没有![](img/f3a8d544-9c42-4f28-93e3-a3be04ec4cea.png)项，所以我们应用链式法则如下:

![](img/0e103418-91ce-41e4-84bb-481386f8170f.png)

为了计算上式中第一项的导数，我们再次应用链式法则，因为我们不能直接从![](img/bc927050-b111-4bb2-a438-bd78614fcd8d.png)计算![](img/3814d69c-7064-4f02-82b2-75db5faba6cd.png)相对于![](img/9785cb31-f498-41ff-919a-40eb62a0853f.png)的导数:

![](img/df7ae6d8-1595-46b2-86c4-ae9315cb4965.png)

从方程 *(5)* ，我们可以写出:

![](img/fc911a96-65f1-4cba-8891-3906670cf422.png)

因为我们知道![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):

![](img/f1b8ba1a-1ea5-4d87-a495-220e5c316ccd.png)

除了求和，我们可以写:

![](img/a700b684-1d54-49b2-9058-1bafb30d9f98.png)

![](img/be3580f6-a59d-49c0-8ce0-becdb699e0b8.png)表示词汇表中所有单词的输出向量之和，用它们的预测误差加权。

现在让我们计算第二项的导数。

从我们认识开始，![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png):

![](img/ad2b53dd-8193-4b02-9366-77fb060d676b.png)

因此，损耗梯度![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)相对于![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png) 的**给出为:**

![](img/f33f4252-bd4e-4c03-bca5-6b760b09221f.png)

因此，我们的权重更新等式变成如下:

![](img/8ec5dabb-8b62-406f-8345-41e3407e8e15.png)

![](img/8a80e3b8-52a7-438c-9a8d-d277a7cefc07.png)

我们使用前面的等式更新网络的权重，并在训练期间获得最佳权重。隐藏层权重的最佳输入![](img/2bd9fff6-c950-4bba-afa7-2fcb03f4c13e.png)，成为我们词汇表中单词的向量表示。

`Single_context_CBOW`的 Python 代码如下:

```
 def Single_context_CBOW(x, label, W1, W2, loss):

    #forward propagation
    h = np.dot(W1.T, x)
    u = np.dot(W2.T, h)
    y_pred = softmax(u)

    #error
    e = -label + y_pred

    #backward propagation
    dW2 = np.outer(h, e)
    dW1 = np.outer(x, np.dot(W2.T, e))

    #update weights
    W1 = W1 - lr * dW1
    W2 = W2 - lr * dW2

    #loss function
    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))

    return W1, W2, loss
```



# 具有多个上下文单词的 CBOW

现在我们已经了解了 CBOW 模型如何使用单个单词作为上下文，我们将了解当您使用多个单词作为上下文单词时它将如何工作。以多个输入单词作为上下文的 CBOW 的架构如下图所示:

![](img/d8067832-9cf4-4d07-951b-6ad2f0720d48.png)

作为上下文的多个单词和作为上下文的单个单词之间没有太大区别。不同之处在于，对于作为输入的多个上下文单词，我们取所有输入上下文单词的平均值。也就是说，作为第一步，我们向前传播网络，并通过将输入![](img/239da2e3-155c-48ba-bb32-71d50b821224.png)与权重![](img/52646ca3-7984-4386-b57e-0dd3c053db60.png)相乘来计算![](img/ba4894e5-f69c-48e4-ad6b-7deb10ff2e41.png)的值，正如我们在使用单个上下文单词的 *CBOW 一节中看到的:*

![](img/ff073da4-ac16-4bd6-9137-26007776bf56.png)

但是，在这里，由于我们有多个上下文单词，我们将有多个输入(即![](img/7f74c8cb-6fd0-4033-b097-374c7277c5e3.png))，其中![](img/b17c0fb1-83ff-4a04-8e14-3ff7cc39688f.png)是上下文单词的数量，我们简单地取它们的平均值并乘以权重矩阵，如下所示:

![](img/21dee3f2-48b0-48ad-a685-3cf220a5cea8.png)

![](img/79ec7be5-7da6-485f-9a99-5d69c0ebd372.png)

类似于我们在 *CBOW 和单个上下文单词*章节中所学的，![](img/37ec8731-2be1-4f09-8eef-07cf1731d4f2.png)代表输入上下文单词![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png)的向量表示。![](img/591d1ae7-16d0-468c-84b9-db6ce8a5a1cd.png)代表输入单词![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png)的矢量表示，以此类推。

我们用![](img/09ff6572-5c5b-41a9-8116-494b59b704e8.png)表示输入上下文单词![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png)，用![](img/bfe95f24-ad89-44b6-bd71-d5d4145a9541.png)表示输入上下文单词![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png)，以此类推。因此，我们可以将前面的等式改写为:

![](img/84b27cb4-4d63-4717-a014-15aa30a2cff9.png)

这里，![](img/ee8ad712-1053-455a-a687-887767c82a94.png)代表上下文字数。

计算![](img/a19f3259-021d-4713-be65-ffb8cfeb8f8a.png)的值与我们在上一节看到的相同:

![](img/cae8ddb8-09da-423d-b76c-7dcaf67e9e15.png)

这里，![](img/f2c71772-b8c8-4e08-85a5-81cd43f8e129.png)表示词汇表中![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png)单词的向量表示。

将方程 *(6)* 代入方程*【7】*，我们写出如下:

![](img/26f9683b-8929-4579-be93-0e9b4e3d1b99.png)

前面的等式给出了词汇表中的![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png)单词和给定输入上下文单词的平均表示之间的相似性。

损失函数与我们在单个单词上下文中看到的相同，它被给出为:

![](img/b9a1c6a1-2d8e-49ad-a12a-4e6ac59c4e14.png)

现在，反向传播有一个小的不同。我们知道，在反向传播中，我们计算梯度并根据权重更新规则更新我们的权重。回想一下，在上一节中，我们是这样更新权重的:

![](img/b12d13c2-602e-474c-b900-3691b46e703c.png)

![](img/35ac212c-b0b5-4a0e-98e4-320b0e9e2e90.png)

因为在这里，我们有多个上下文单词作为输入，所以在计算![](img/ca797f1c-a327-4135-b840-9a76c4a8e94d.png)时，我们取上下文单词的平均值:

![](img/15493e8e-84c3-43f6-84e0-e8d06230a8a4.png)

计算![](img/c64c8923-7b7f-428f-960e-96b2caa8f052.png)与我们在上一节看到的一样:

![](img/7b604533-b018-41f3-ab17-968e367fc5fa.png)

因此，简而言之，在多单词上下文中，我们只需取多个上下文输入单词的平均值，并像在 CBOW 的单个单词上下文中一样构建模型。



# 理解跳格模型

现在，让我们看看 word2vec 模型的另一个有趣的类型，称为 skip-gram。Skip-gram 正好是 CBOW 模型的反向。也就是说，在跳格模型中，我们试图在给定目标单词作为输入的情况下预测上下文单词。如下图所示，我们可以注意到，当*上升*时我们有了目标单词，我们需要预测上下文单词 *the、*中的 sun 和 *the* :

![](img/870ba64d-fdef-49ca-890c-1accb0b47366.png)

类似于 CBOW 模型，我们使用窗口大小来确定我们需要预测多少上下文单词。跳跃图模型的体系结构如下图所示。

我们可以看到，它将单个目标单词作为输入，并尝试预测多个上下文单词:

![](img/af0c565a-b398-4b1b-bb7a-0a2e5fb818e9.png)

在跳格模型中，我们试图根据目标词来预测上下文词。所以，它将一个目标单词作为输入，返回![](img/91936d04-a92a-47b2-808c-3e1389753372.png)上下文单词作为输出，如上图所示。因此，在训练 skip-gram 模型来预测上下文单词之后，我们对隐藏层![](img/5f024499-4591-477e-9667-e24e23d802fd.png)的输入之间的权重变成了单词的向量表示，就像我们在 CBOW 模型中看到的那样。

现在我们对跳格模型有了一个基本的了解，让我们深入细节，了解它们是如何工作的。



# 跳格向前传播

首先，我们将了解向前传播在跳格模型中是如何工作的。让我们使用 CBOW 模型中使用的相同符号。跳跃图模型的体系结构如下图所示。如您所见，我们只输入一个目标单词![](img/60664943-7c5b-41c9-b69c-38d707ad0e14.png)，它返回![](img/7073a847-9879-4511-a235-aab29edb2b44.png)上下文单词作为输出![](img/1abce1f4-baae-4202-9e58-c3b08e0c7698.png):

![](img/552225d9-108c-46a4-9b72-dc8895415c23.png)

类似于我们在 CBOW 中看到的，在*正向传播*部分，首先我们将输入![](img/e1fd5da0-c909-4186-b9dd-53dee6356719.png)乘以隐藏层权重的输入![](img/8e685519-eb49-4b24-9f91-b9ddacc3eae8.png):

![](img/abe3c86e-c276-456d-ae31-f1760fa3416e.png)

我们可以直接将前面的等式改写为:

![](img/7439c251-2328-4c30-8ef3-efa342622a35.png)

这里，![](img/a79e9b8a-f7f9-497b-b60a-ff4f80021260.png)意味着输入单词![](img/3c25d777-af88-49fa-a1ba-4de48ad5b12f.png)的矢量表示。

接下来，我们计算![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png)，这意味着我们的词汇表中的单词![](img/fdb95f62-9fad-4e54-bdec-1a085f9cc023.png)和输入的目标单词之间的相似性得分。类似于我们在 CBOW 模型中看到的，![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png)可以给出为:

![](img/60fdc55d-bbab-4355-8cc6-b1d6abfbf7dc.png)

我们可以直接将上面的等式改写为:

![](img/c57122ec-3d10-48a7-bd65-abb5f5aa4519.png)

这里，![](img/1ccd2510-7ece-4313-b6dd-dbd119cd2dc2.png)暗示了单词![](img/040097ca-cb43-4dab-94ef-290e17b125d0.png)的向量表示。

但是，与 CBOW 模型不同，我们只预测了一个目标单词，这里我们预测了![](img/1a0af03f-a854-4368-9990-43aeec26fd66.png)个上下文单词。因此，我们可以将上面的等式改写为:

![](img/f5131630-008b-437d-b8e4-b808d21baebc.png)

因此，![](img/e6ff7f32-90bd-453e-8df9-76411762aa7b.png)意味着词汇表中的![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png)单词的分数是上下文单词![](img/e61144a0-5581-48d9-a1b0-8fd115c5f4d1.png)。那就是:

*   ![](img/0cea83ee-9d5b-47ef-b2ef-bf952c60401d.png)意味着单词![](img/b28be818-fb99-49f2-9932-dd39f8c7a8b8.png)的分数是第一个上下文单词
*   ![](img/8edb41b3-3b2f-4c0a-9ec3-1def443cfd5c.png)暗示单词![](img/23dade3d-bf24-4335-976a-08795e347b89.png)的分数是第二上下文单词
*   ![](img/2dbc1d19-b16b-43a5-923b-9a8695fbbdda.png)暗示单词![](img/2049eb6f-1418-41db-a43f-0dc3ea3cb896.png)的分数是第三个上下文单词

由于我们想将分数转换成概率，我们应用 softmax 函数并计算![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png):

![](img/830ca969-b68f-4f5c-942d-4c7d2ab8eba4.png)

这里，![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png)意味着词汇表中的![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png)单词成为上下文单词![](img/8df6187b-ea84-4630-b2bb-99f3513ec3a1.png)的概率。

现在，让我们看看如何计算损失函数。让![](img/85f31ebd-9b63-4b6a-a2bd-074745b99561.png)表示正确上下文单词的概率。所以，我们需要最大化这个概率:

![](img/c6abc546-b4ee-4b41-abea-1d9e0e3ae7ef.png)

不是最大化原始概率，而是最大化对数概率:

![](img/749e5fbc-9416-46bb-8e62-aa1836828d68.png)

类似于我们在 CBOW 模型中看到的，我们通过添加负号将其转化为最小化目标函数:

![](img/97c83a18-fdcb-408d-87f0-dcd3a45889d0.png)

将等式 *(8)* 代入前面的等式，我们可以写出如下:

![](img/f437214e-7a5f-4717-82ae-a94798058816.png)

因为我们有![](img/2e76d1e7-1b07-4196-8292-82464decc20f.png)上下文单词，我们取概率的乘积和为:

![](img/cbe9479d-e864-4e77-a76b-a0161b23dadc.png)

因此，根据对数规则，我们可以改写上述方程，我们的最终损失函数变成:

![](img/a98abd9e-b5bc-4c3f-80ca-ec8b73504bc4.png)

看看 CBOW 和 skip-gram 模型的损失函数。你会注意到 CBOW 损失函数和 skip-gram 损失函数之间的唯一区别是添加了上下文单词![](img/1ac9a6b6-fc2e-4a8d-b43d-0f5fd7016ceb.png)。



# 反向传播

我们使用梯度下降算法最小化损失函数。因此，我们反向传播网络，计算损失函数相对于权重的梯度，并根据权重更新规则更新权重。

首先，我们计算相对于隐藏到输出层的损失梯度![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png)。我们不能直接从![](img/bc366eee-0a8a-4feb-8986-4d7ee848fd5b.png)计算损失相对于![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png)的导数，因为其中没有![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png)项，所以我们应用如下所示的链式法则。这与我们在 CBOW 模型中看到的基本相同，只是这里我们总结了所有上下文单词:

![](img/152f51e3-e4c6-455e-a9a5-5db71a134958.png)

首先，让我们计算第一项:

![](img/217ea4cc-4208-4abc-858a-ec9639c90c62.png)

我们知道![](img/78f049e9-aa2c-4b6e-be4e-e6d530469692.png)是误差项，是实际单词和预测单词的差。为了符号简单，我们可以将所有上下文单词的总和写成:

![](img/806b7879-6d72-42c7-9ae4-90bf16dcd0a9.png)

所以，我们可以说:

![](img/4512aed6-0806-43f4-bd04-1bd927e25737.png)

现在，让我们计算第二项。既然我们知道![](img/04ffe61d-f457-440f-b2ee-065d0de515f4.png)，我们可以写:

**![](img/146d13a2-d5ea-4031-b930-5da86abe06cf.png)**

因此，相对于![](img/864f4d04-3109-43ff-ba54-2309fe6318d5.png) 的**损耗梯度![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)给出如下:**

![](img/3f716372-88a2-4c88-91e8-3d423577e498.png)

现在，我们计算相对于隐藏层权重![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png)输入的损失梯度。这很简单，与我们在 CBOW 模型中看到的完全相同:

![](img/26ac31ee-0c19-4154-8bc7-2dab5be38e1a.png)

因此，相对于![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png) 的**损耗梯度![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)为:**

![](img/73a7b000-52d0-44d6-ae90-2ba997439cb3.png)

在计算梯度之后，我们将我们的权重 *W* 和*W’*更新为:

![](img/77a93b00-0f6a-485b-9213-2f053d348185.png)

![](img/a1a480ec-be9c-4f1f-a546-9ae95f156091.png)

因此，在训练网络时，我们使用前面的等式更新网络的权重，并获得最佳权重。输入到隐藏层之间的最佳权重，![](img/8006cddb-a1a5-414d-bf56-cb77755ba238.png)成为我们词汇表中单词的向量表示。



# 各种培训策略

现在，我们将看看不同的训练策略，它们可以优化和提高我们的 word2vec 模型的效率。



# 分级 softmax

在 CBOW 和 skip-gram 模型中，我们使用 softmax 函数来计算单词出现的概率。但是使用 softmax 函数计算概率在计算上是昂贵的。比方说，我们正在建立一个 CBOW 模型；我们将词汇表中的![](img/ba9c9d7a-046a-46c2-ad93-3c770306ac88.png)单词作为目标单词的概率计算如下:

![](img/9679ecaa-45cf-4bce-8b85-21ed6c323ba1.png)

如果你看看前面的等式，我们基本上是用词汇表中所有单词![](img/31854ccf-fede-424d-818f-305ac15843bd.png)的指数来驱动![](img/3551b1d9-5ce6-4433-92ef-5a593c9e6068.png)的指数。我们的复杂度是![](img/87cf4f3a-baf5-491e-b80b-e45540ec7feb.png)，其中![](img/d7d624f5-bf6a-4a35-ae3e-059a99240350.png)是词汇量。当我们用包含数百万个单词的词汇表来训练 word2vec 模型时，它的计算开销肯定会很大。因此，为了解决这个问题，我们不使用 softmax 函数，而是使用分层的 softmax 函数。

分层的 softmax 函数使用霍夫曼二叉查找树，并将复杂度显著降低到![](img/638cfce0-635f-44e9-be0a-1e0d5d99f46e.png)。如下图所示，在分层 softmax 中，我们将输出图层替换为二叉查找树:

![](img/ff41b350-161f-4d98-86c1-52519381e659.png)

树中的每个叶节点代表词汇表中的一个单词，所有中间节点代表其子节点的相对概率。

给定一个上下文单词，我们如何计算目标单词的概率？我们简单地通过决定左转还是右转来遍历树。如下图所示，在给定某个上下文单词![](img/839fbeee-e568-440f-be47-d080e80da01e.png)的情况下，单词*飞到*成为目标单词的概率被计算为沿着路径的概率的乘积:

![](img/6af5744d-76c7-4328-9ecd-fd0fe9b5395f.png)

![](img/5b801e9f-5594-41b0-b3ea-791da2013858.png)

目标词的概率如下所示:

![](img/dc53fb4a-6996-483f-be03-86885b53298e.png)

但是我们如何计算这些概率呢？每个节点![](img/f85382f5-7066-4de6-bfa9-a1248991e99a.png)都有一个关联的嵌入(比如说，![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png))。为了计算节点的概率，我们将节点的嵌入值![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)与隐藏层值![](img/80974a3f-5565-4985-81de-08a900f927c7.png)相乘，并应用 sigmoid 函数。例如，给定上下文单词![](img/4d992bd5-1e88-477b-a9fd-31bc407d6b3c.png)，节点![](img/cc7a19fa-e393-4c69-9b3d-6b9a485cd2ef.png)取得权利的概率计算如下:

![](img/022eb4f4-f05f-488b-8ad5-092cac240c54.png)

一旦我们计算了右转的概率，我们就可以通过简单地从 1 中减去右转的概率来计算左转的概率。

![](img/d3390104-af49-424d-9cab-9a2f84176099.png)

如果我们将所有叶节点的概率相加，那么它等于 1，意味着我们的树已经被规范化，为了找到一个单词的概率，我们只需要评估![](img/ed24f8a7-44e8-4e19-b7a5-97c4566cdd0c.png)个节点。



# 负采样

假设我们正在构建一个 CBOW 模型，我们有一个句子*鸟儿在天空飞翔。*设语境词为*鸟*，*为*，中的*，*中的*和目标词为*飞翔。**

每当网络预测到不正确的目标单词时，我们都需要更新网络的权重。所以，除了单词*飞*，如果一个不同的单词被预测为目标单词，那么我们更新网络。

但这只是一小部分词汇。考虑一下我们的词汇表中有数百万个单词的情况。在这种情况下，我们需要执行多次权重更新，直到网络预测到正确的目标词。这既费时又不是一个有效的方法。因此，我们没有这样做，而是将正确的目标单词标记为正面类别，并从词汇表中抽取一些单词，将其标记为负面类别。

我们在这里所做的基本上是将我们的多项式类问题转换为二元分类问题(即，模型不是试图预测目标词，而是对给定词是否是目标词进行分类)。

该单词被选为负样本的概率被给出为:

![](img/eceda56e-380e-4bb8-8910-b96d059c4269.png)



# 二次抽样常用词

在我们的语料库中，会有某些词出现的频率非常高，比如*中的*、*就是*等等，也有某些词出现的频率很低。为了在这两者之间保持平衡，我们使用了二次采样技术。所以，我们以概率![](img/d0c8b060-74f7-4551-b98b-be70f5cf0e9f.png)去除出现频率超过某个阈值的词，可以表示为:

![](img/58b3bab2-32ac-4361-98c7-003ca670032e.png)

这里，![](img/e0a0cb07-0547-4e52-a1ab-f0364c8ee42b.png)是阈值，![](img/3720374d-c131-4e69-9d11-5bf517a05b92.png)是单词![](img/f66887f6-2312-434a-8866-adfc1965a158.png)的频率。



# 使用 gensim 构建 word2vec 模型

现在我们已经了解了 word2vec 模型的工作原理，让我们看看如何使用`gensim`库构建 word2vec 模型。Gensim 是广泛用于构建向量空间模型的流行科学软件包之一。可以通过`pip`安装。所以，我们可以在终端中输入下面的命令来安装`gensim`库:

```
pip install -U gensim
```

现在我们已经安装了 gensim，我们将看到如何使用它来构建 word2vec 模型。你可以从 GitHub 的[http://bit.ly/2Xjndj4](http://bit.ly/2Xjndj4)下载本节使用的数据集以及完整的代码和一步一步的解释。

首先，我们将导入必要的库:

加载数据集

```
import warnings
warnings.filterwarnings(action='ignore')

#data processing
import pandas as pd
import re
from nltk.corpus import stopwords
stopWords = stopwords.words('english')

#modelling
from gensim.models import Word2Vec
from gensim.models import Phrases
from gensim.models.phrases import Phraser
```



# 加载数据集:

让我们看看我们从数据中得到了什么:

```
data = pd.read_csv('data/text.csv',header=None)
```

上述代码生成以下输出:

```
data.head()
```

![](img/120fcfd9-82fd-49ed-b5ec-ebcd744496be.png)

预处理和准备数据集



# 定义预处理数据集的函数:

通过运行以下代码，我们可以看到预处理文本的样子:

```
def pre_process(text):

    # convert to lowercase
    text = str(text).lower()

    # remove all special characters and keep only alpha numeric characters and spaces
    text = re.sub(r'[^A-Za-z0-9\s.]',r'',text)

    #remove new lines
    text = re.sub(r'\n',r' ',text)

    # remove stop words
    text = " ".join([word for word in text.split() if word not in stopWords])

    return text
```

我们得到的输出为:

```
pre_process(data[0][50])
```

预处理整个数据集:

```
'agree fancy. everything needed. breakfast pool hot tub nice shuttle airport later checkout time. noise issue tough sleep through. awhile forget noisy door nearby noisy guests. complained management later email credit compd us amount requested would return.'
```

genism 库需要列表形式的输入:

```
data[0] = data[0].map(lambda x: pre_process(x))
```

`*text = [ [word1, word2, word3], [word1, word2, word3] ]*`

我们知道数据中的每一行都包含一组句子。所以，我们用`'.'`把它们分开，并把它们转换成一个列表:

上述代码生成以下输出:

```
data[0][1].split('.')[:5]
```

The preceding code generates the following output:

```
['stayed crown plaza april april ',

 ' staff friendly attentive',

 ' elevators tiny ',

 ' food restaurant delicious priced little high side',

 ' course washington dc']
```

因此，如图所示，现在，我们有一个列表中的数据。但是我们需要将它们转换成列表的列表。所以，现在我们再用一个空格`' '`把它分开。也就是说，首先，我们用`'.'`分割数据，然后我们用`' '`分割它们，这样我们就可以在一个列表列表中得到我们的数据:

您可以看到，我们以列表的形式输入内容:

```
corpus = []
for line in data[0][1].split('.'):
    words = [x for x in line.split()]
    corpus.append(words)
```

将数据集中的整个文本转换为列表列表:

```
corpus[:2]

[['stayed', 'crown', 'plaza', 'april', 'april'], ['staff', 'friendly', 'attentive']]
```

如图所示，我们成功地将数据集中的整个文本转换为一个列表列表:

```
data = data[0].map(lambda x: x.split('.'))

corpus = []
for i in (range(len(data))):
    for line in data[i]:
        words = [x for x in line.split()]
        corpus.append(words)

print corpus[:2]
```

现在，我们的问题是，我们的语料库只包含一元词，当我们输入一个二元词时，它不会给出结果，例如， *san francisco* 。

```
[['room', 'kind', 'clean', 'strong', 'smell', 'dogs'],

 ['generally', 'average', 'ok', 'overnight', 'stay', 'youre', 'fussy']]
```

所以我们使用 gensim 的`Phrases`函数，它收集所有一起出现的单词，并在它们之间添加一个下划线。所以，现在*旧金山*变成了*旧金山*。

So we use gensim's `Phrases` functions, which collects all the words that occur together and adds an underscore between them. So, now *san francisco* becomes *san_francisco*.

我们将`min_count`参数设置为`25`，这意味着我们忽略所有出现在`min_count`后面的单词和二元模型:

如你所见，现在，在我们的语料库中，一个下划线被添加到了二元模型中:

```
phrases = Phrases(sentences=corpus,min_count=25,threshold=50)
bigram = Phraser(phrases)

for index,sentence in enumerate(corpus):
    corpus[index] = bigram[sentence]
```

我们从语料库中再检查一个值，看看如何为二元模型添加下划线:

```
corpus[111]

[u'connected', u'rivercenter', u'mall', u'downtown', u'san_antonio']
```

构建模型

```
corpus[9]

[u'course', u'washington_dc']
```



# 现在让我们建立我们的模型。让我们定义我们的模型需要的一些重要的超参数:

`size`参数表示向量的大小，也就是我们向量的维数，来表示一个单词。大小可以根据我们的数据大小来选择。如果我们的数据非常小，那么我们可以将大小设置为一个小值，但是如果我们有一个非常大的数据集，那么我们可以将大小设置为`300`。在我们的例子中，我们将大小设置为`100`。

*   `window_size`参数表示目标单词与其相邻单词之间应该考虑的距离。超过目标单词的窗口大小的单词将不被考虑用于学习。通常，小窗口尺寸是优选的。
*   `min_count`参数代表单词的最小频率。如果特定单词的出现次数少于一个`min_count`，那么我们可以简单地忽略这个单词。
*   `workers`参数指定了我们训练模型所需的工作线程的数量。
*   设置`sg=1`意味着我们使用 skip-gram 模型进行训练，但是如果它被设置为`sg=0`，则意味着我们使用 CBOW 模型进行训练。
*   Setting `sg=1` implies that we use the skip-gram model for training, but if it is set to `sg=0`, then it implies that we use CBOW model for training.

使用以下代码定义所有超参数:

让我们使用 gensim 的`Word2Vec`函数来训练模型:

```
size = 100
window_size = 2
epochs = 100
min_count = 2
workers = 4
sg = 1
```

一旦我们成功地训练了模型，我们就拯救它们。保存和加载模型非常简单；我们可以简单地分别使用`save`和`load`函数:

```
model = Word2Vec(corpus, sg=1,window=window_size,size=size, min_count=min_count,workers=workers,iter=epochs)
```

我们还可以通过使用以下代码`load`已经保存的`Word2Vec`模型:

```
model.save('model/word2vec.model')
```

评估嵌入

```
model = Word2Vec.load('model/word2vec.model')
```



# 现在让我们评估一下我们的模型学到了什么，以及我们的模型对文本语义的理解程度。`genism`库提供了`most_similar`函数，它给出了与给定单词相关的前几个相似单词。

正如您在下面的代码中看到的，给定`san_diego`作为输入，我们得到所有其他最相似的相关城市名称:

As you can see in the following code, given `san_diego` as an input, we are getting all the other related city names that are most similar:

```
model.most_similar('san_diego')

[(u'san_antonio', 0.8147615790367126),

 (u'indianapolis', 0.7657858729362488),

 (u'austin', 0.7620342969894409),

 (u'memphis', 0.7541092038154602),

 (u'phoenix', 0.7481759786605835),

 (u'seattle', 0.7471771240234375),

 (u'dallas', 0.7407466769218445),

 (u'san_francisco', 0.7373261451721191),

 (u'la', 0.7354192137718201),

 (u'boston', 0.7213659286499023)]
```

我们还可以对向量进行算术运算，以检查向量的精确度，如下所示:

我们还可以在给定的单词集中找到不匹配的单词；例如，在下面名为`text`的列表中，除了单词`holiday`，其他都是城市名。因为 Word2Vec 已经理解了这种差异，所以它返回与列表中其他单词不匹配的单词`holiday`,如下所示:

```
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)

[(u'queen', 0.7255150675773621)]
```

TensorBoard 中词嵌入的可视化

```
text = ['los_angeles','indianapolis', 'holiday', 'san_antonio','new_york']

model.doesnt_match(text)

'holiday'
```



# 在上一节中，我们学习了如何使用 gensim 构建 word2vec 模型来生成词嵌入。现在，我们将看到如何使用 TensorBoard 可视化这些嵌入。可视化词嵌入有助于我们理解投影空间，也有助于我们容易地验证嵌入。TensorBoard 为我们提供了一个名为**嵌入投影仪**的内置可视化工具，用于交互式可视化和分析高维数据，如我们的词嵌入。我们将学习如何使用 TensorBoard 的投影仪一步一步地可视化词嵌入。

导入所需的库:

Import the required libraries:

```
import warnings

warnings.filterwarnings(action='ignore')

import tensorflow as tf 
from tensorflow.contrib.tensorboard.plugins import projector tf.logging.set_verbosity(tf.logging.ERROR)

import numpy as np
import gensim 
import os
```

加载保存的模型:

加载模型后，我们将把模型中的字数保存到`max_size`变量中:

```
file_name = "model/word2vec.model"
model = gensim.models.keyedvectors.KeyedVectors.load(file_name)
```

我们知道单词向量的维数会是![](img/660327a5-a215-420a-a69c-d7f9dc635989.png)。因此，我们初始化一个名为`w2v`的矩阵，将形状作为我们的`max_size`，这是词汇表的大小，也是模型的第一层大小，这是隐藏层中神经元的数量:

```
max_size = len(model.wv.vocab)-1
```

现在，我们创建一个名为`metadata.tsv`的新文件，其中保存了模型中的所有单词，并将每个单词的嵌入存储在`w2v`矩阵中:

```
w2v = np.zeros((max_size,model.layer1_size))
```

接下来，我们初始化 TensorFlow 会话:

```
if not os.path.exists('projections'):
    os.makedirs('projections')

with open("projections/metadata.tsv", 'w+') as file_metadata:

    for i, word in enumerate(model.wv.index2word[:max_size]):

        #store the embeddings of the word
        w2v[i] = model.wv[word]

        #write the word to a file 
        file_metadata.write(word + '\n')
```

初始化名为`embedding`的 TensorFlow 变量，它保存单词 embeddings:

```
sess = tf.InteractiveSession()
```

初始化所有变量:

```
with tf.device("/cpu:0"):

    embedding = tf.Variable(w2v, trainable=False, name='embedding')
```

为`saver`类创建一个对象，它实际上用于保存和恢复检查点的变量:

```
tf.global_variables_initializer().run()
```

Create an object to the `saver` class, which is actually used for saving and restoring variables to and from our checkpoints:

```
saver = tf.train.Saver()
```

使用`FileWriter`，我们可以将摘要和事件保存到我们的事件文件中:

现在，我们初始化投影仪并添加`embeddings`:

```
writer = tf.summary.FileWriter('projections', sess.graph)
```

接下来，我们将我们的`tensor_name`指定为`embedding`，将`metadata_path`指定为`metadata.tsv`文件，这里我们有这样的话:

```
config = projector.ProjectorConfig()

embed = config.embeddings.add()
```

最后，保存模型:

```
embed.tensor_name = 'embedding'
embed.metadata_path = 'metadata.tsv'
```

现在，打开终端，输入以下命令打开`tensorboard`:

```
projector.visualize_embeddings(writer, config)

saver.save(sess, 'projections/model.ckpt', global_step=max_size)
```

打开 TensorBoard 后，转到投影仪选项卡。我们可以看到输出，如下图所示。正如您所注意到的，当我们键入单词`delighted`时，我们可以看到所有相关的单词，例如`pleasant`、`surprise`以及更多类似的单词，与此相邻:

```
tensorboard --logdir=projections --port=8000
```

![](img/48201f67-190b-45b6-bcbb-f5141142d804.png)

Doc2vec



# 到目前为止，我们已经看到了如何为单词生成嵌入。但是我们如何为文档生成嵌入呢？一种简单的方法是为文档中的每个单词计算一个单词向量，并取其平均值。Mikilow 和 Le 引入了一种新的方法来生成文档的嵌入，而不是仅仅取词嵌入的平均值。他们引入了两种新方法，称为 PV-DM 和 PV-DBOW。这两种方法都只是添加了一个新的向量，称为**段落 id** 。让我们看看这两种方法到底是如何工作的。

段落向量——分布式内存模型



# PV-DM 类似于 CBOW 模型，其中我们试图预测给定上下文单词的目标单词。在 PV-DM 中，除了单词向量，我们还引入了一个向量，叫做段落向量。顾名思义，段落向量学习整个段落的向量表示，它捕捉段落的主题。

如下图所示，每个段落映射到一个唯一的向量，每个单词也映射到一个唯一的向量。因此，为了预测目标单词，我们通过连接或平均来组合单词向量和段落向量:

![](img/46304231-6efb-4064-a6b3-309982ffa2a8.png)

但是说了这么多，段落向量对预测目标单词有什么用呢？拥有段落向量到底有什么用？我们知道，我们试图根据上下文来预测目标单词。上下文单词具有固定的长度，并且它们在一个滑动窗口内从一个段落中被采样。

除了上下文单词，我们还利用段落向量来预测目标单词。因为段落向量包含关于段落主题的信息，所以它们包含上下文单词不包含的意思。也就是说，上下文单词仅包含关于特定单词的信息，而段落向量包含关于整个段落的信息。因此，我们可以将段落向量视为一个新单词，它与上下文单词一起用于预测目标单词。

段落向量对于从同一段落中采样的所有上下文单词是相同的，并且不跨段落共享。假设我们有三个段落， *p1* ， *p2* ，和 *p3* 。如果上下文是从段落 *p1* 中采样的，则 *p1* 向量用于预测目标单词。如果从段落 *p2、*中采样上下文，则使用 *p2* 向量。因此，段落向量不会跨段落共享。然而，单词向量在所有段落中是共享的。也就是说，单词 *sun* 的向量在所有段落中都是相同的。我们称我们的模型为段落向量的分布式记忆模型，因为我们的段落向量充当了保存当前上下文单词中缺失的信息的存储器。

因此，使用随机梯度下降来学习段落向量和单词向量。在每次迭代中，我们从随机段落中抽取上下文单词，尝试预测目标单词，计算误差，并更新参数。在训练之后，段落向量捕获段落(文档)的嵌入。

段落向量——分布式单词包模型



# PV-DBOW 类似于 skip-gram 模型，在该模型中，我们尝试基于目标单词来预测上下文单词:

![](img/8e0ff5d7-de13-4d77-ba4c-0f997acb9883.png)

![](img/8e0ff5d7-de13-4d77-ba4c-0f997acb9883.png)

与以前的方法不同，这里我们不试图预测接下来的单词。相反，我们使用段落向量对文档中的单词进行分类。但是它们是如何工作的呢？我们训练模型来理解这个单词是否属于一个段落。我们对一些单词集进行采样，然后将其输入分类器，分类器会告诉我们这些单词是否属于某个特定的段落，这样我们就可以学习段落向量。

使用 doc2vec 查找相似的文档



# 现在，我们将看到如何使用 doc2vec 执行文档分类。在本节中，我们将使用 20 个`news_dataset`。它由超过 20 个不同新闻类别的 20，000 个文档组成。我们将只使用四个类别:`Electronics`、`Politics`、`Science`和`Sports`。因此，我们在这四个类别下各有 1，000 个文档。我们用前缀`category_`重命名文档。例如，所有科学文档都被重命名为`Science_1`、`Science_2`等等。重命名后，我们将所有文档合并到一个文件夹中。合并后的数据和完整的代码可以在 http://bit.ly/2KgBWYv[的 GitHub 上的 Jupyter 笔记本上找到。](http://bit.ly/2KgBWYv)

现在，我们训练我们的 doc2vec 模型来分类并找到这些文档之间的相似之处。

首先，我们导入所有必需的库:

First, we import all the necessary libraries:

```
import warnings
warnings.filterwarnings('ignore')

import os
import gensim
from gensim.models.doc2vec import TaggedDocument

from nltk import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stopWords = set(stopwords.words('english'))
```

现在，我们加载所有的文档，并将文件名保存在`docLabels`列表中，将文档内容保存在一个名为`data`的列表中:

您可以在`docLabels`列表中看到我们所有文档的名称:

```
docLabels = []
docLabels = [f for f in os.listdir('data/news_dataset') if f.endswith('.txt')]

data = []
for doc in docLabels:
      data.append(open('data/news_dataset/'+doc).read()) 
```

定义一个名为`DocIterator`的类，它充当遍历所有文档的迭代器:

```
docLabels[:5]

['Electronics_827.txt',

 'Electronics_848.txt',

 'Science829.txt',

 'Politics_38.txt',

 'Politics_688.txt']
```

为`DocIterator`类创建一个名为`it`的对象:

```
class DocIterator(object):
    def __init__(self, doc_list, labels_list):
        self.labels_list = labels_list
        self.doc_list = doc_list

    def __iter__(self):
        for idx, doc in enumerate(self.doc_list):
            yield TaggedDocument(words=doc.split(), tags=                        [self.labels_list[idx]])
```

现在，让我们建立模型。让我们首先定义模型的一些重要超参数:

```
it = DocIterator(data, docLabels)
```

`size`参数代表我们的嵌入大小。

*   `alpha`参数代表我们的学习率。
*   `min_alpha`参数意味着我们的学习率`alpha`将在训练期间衰减到`min_alpha`。
*   设置`dm=1`意味着我们使用分布式内存(PV-DM)模型，如果我们设置`dm=0`，则意味着我们使用分布式单词包(PV-DBOW)模型进行训练。
*   `min_count`参数表示单词的最小频率。如果特定单词的出现次数少于一个`min_count`，那么我们可以简单地忽略这个单词。
*   这些超参数定义如下:

现在让我们使用`gensim.models.Doc2ec()`类来定义模型:

```
size = 100
alpha = 0.025
min_alpha = 0.025
dm = 1
min_count = 1
```

训练模型:

```
model = gensim.models.Doc2Vec(size=size, min_count=min_count, alpha=alpha, min_alpha=min_alpha, dm=dm)
model.build_vocab(it)
```

训练后，我们可以使用`save`功能保存模型:

```
for epoch in range(100):
    model.train(it,total_examples=120)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
```

我们可以加载保存的模型，使用`load`功能:

```
model.save('model/doc2vec.model')
```

现在，让我们来评估我们模型的性能。下面的代码显示，当我们将`Sports_1.txt`文档作为输入时，它将输入所有具有相应分数的相关文档:

```
d2v_model = gensim.models.doc2vec.Doc2Vec.load('model/doc2vec.model')
```

理解跳过思想算法

```
d2v_model.docvecs.most_similar('Sports_1.txt')

[('Sports_957.txt', 0.719024658203125),

 ('Sports_694.txt', 0.6904895305633545),

 ('Sports_836.txt', 0.6636477708816528),

 ('Sports_869.txt', 0.657712459564209),

 ('Sports_123.txt', 0.6526877880096436),

 ('Sports_4.txt', 0.6499642729759216),

 ('Sports_749.txt', 0.6472041606903076),

 ('Sports_369.txt', 0.6408025026321411),

 ('Sports_167.txt', 0.6392412781715393),

 ('Sports_104.txt', 0.6284008026123047)]
```



# Skip-thoughts 是一种流行的无监督学习算法，用于学习句子嵌入。我们可以把跳跃式思维看作是跳跃式语法模型的类比。我们了解到，在 skip-gram 模型中，我们试图预测给定目标词的上下文词，而在 skip-thoughts 模型中，我们试图预测给定目标句的上下文句。换句话说，我们可以说 skip-gram 用于学习单词级向量，skip-thoughts 用于学习句子级向量。

跳过思想的算法非常简单。它由编码器-解码器架构组成。编码器的作用是将句子映射到向量，解码器的作用是生成周围的句子，即给定输入句子的前一个和下一个句子。如下图所示，跳过思想向量由一个编码器和两个解码器组成，称为上一个解码器和下一个解码器:

![](img/a3a66dd3-7fd4-4146-96b1-bfa996767b07.png)

接下来讨论编码器和解码器的工作方式:

**编码器**:编码器按顺序取出句子中的单词，并生成嵌入。假设我们有一个句子列表。![](img/9ff4d7cc-8397-4f89-97c7-3c5d7abdb882.png)。![](img/6fc4dd51-a29f-4f7b-aab1-7186033226df.png)表示句子中的![](img/bef8d295-0f98-49f4-a410-1ddb325c8dcb.png)词![](img/519ba00b-ad85-4229-86f4-5f445cbcb6d0.png)和![](img/007bc484-5995-4611-95b4-ce2cdfef0737.png)表示其词嵌入。因此，编码器的隐藏状态被解释为句子表示。

*   **解码器**:有两个解码器，叫做上一个解码器和下一个解码器。顾名思义，上一个解码器用来生成上一句，下一个解码器用来生成下一句。假设我们有一个句子![](img/827fbe9b-c496-4d53-8496-661a034c727c.png)，它的嵌入是![](img/1eace04e-e015-42b2-8cfc-642b9c74520c.png)。两个解码器都将嵌入内容![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)作为输入，前一个解码器试图生成前一个句子![](img/b1ce0037-7f17-4bb4-b2d1-26262cfd1ba7.png)，下一个解码器试图生成下一个句子![](img/e7786bd8-d584-4f33-ae2c-413d6cf2ecb6.png)。
*   因此，我们通过最小化前一个和下一个解码器的重构误差来训练我们的模型。因为当解码器重建/生成正确的上一个和下一个句子时，这意味着我们有一个嵌入了![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)的有意义的句子。我们将重构误差发送给编码器，以便编码器可以优化嵌入并将更好的表示发送给解码器。一旦我们训练了我们的模型，我们使用我们的编码器为一个新的句子生成嵌入。

句子嵌入的快速思考



# quick-thinks 是另一种学习句子嵌入的有趣算法。在 skip-thoughts 中，我们看到了如何使用编码器-解码器架构来学习句子嵌入。在快速思考中，我们试图了解给定的句子是否与候选句子相关。因此，我们不使用解码器，而是使用分类器来学习给定的输入句子是否与候选句子相关。

假设![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)是输入句子，![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png)是包含与给定输入句子![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)相关的有效上下文句子和无效上下文句子的候选句子集合。设 **![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)** 为![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png)中的任意候选句子。

我们使用两个编码函数，![](img/5ef11cca-cb04-4487-8662-f8e0b37bffcf.png)和![](img/ccb8587f-cc34-49bb-97fd-a0b718b60350.png)。这两个函数![](img/2d8df112-c7bf-4a59-aa08-d8d871e8451b.png)和![](img/7b85920f-f7e4-434c-bd5e-e42bb0cb8022.png)的作用是学习嵌入，即分别学习给定句子![](img/155d14d5-772b-4d94-8817-d0ab1c63d1d6.png)和候选句子![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)的向量表示。

一旦这两个函数生成了嵌入，我们就使用一个分类器![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png)，它返回每个候选句子与给定输入句子相关的概率。

如下图所示，第二个候选句子![](img/414b1656-46a2-4012-925a-ba38a5b3680d.png)的概率很高，因为它与给定的输入句子![](img/4b7f2269-0f75-43d1-ae97-1ff5953cd9cd.png)相关:

![](img/e1111caf-a7dd-49d0-8727-6d5bf29d2906.png)

因此，![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)是正确句子，即![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)与给定输入句子![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)相关的概率计算如下:

![](img/2c9a1c15-b8bc-46cf-892d-d508edf27ee3.png)

这里，![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png)是个量词。

我们的分类器的目标是识别与给定输入句子![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)相关的有效上下文句子。因此，我们的成本函数是最大化为给定的输入句子![](img/280b50e3-3e78-4898-98b4-408efed0901d.png)找到正确上下文句子的概率。如果它对句子进行了正确的分类，那么这意味着我们的编码器学习了句子的更好的表达。

摘要



# 我们从理解词嵌入开始这一章，我们看了两种不同类型的 Word2Vec 模型，称为 CBOW，其中我们试图预测给定上下文单词的目标单词，以及 skip-gram，其中我们试图预测给定目标单词的上下文单词。

We started off the chapter by understanding word embeddings and we looked at two different types of Word2Vec model, called CBOW, where we try to predict the target word given the context word, and skip-gram, where we try to predict the context word given the target word.

然后，我们学习了 Word2Vec 中的各种训练策略。我们查看了分层 softmax，其中我们用霍夫曼二叉树替换了网络的输出层，并将复杂度降低到了![](img/9508f84c-0cd7-4394-a7f5-cd3f9879ccf9.png)。我们还学习了负抽样和二次抽样常用词方法。然后，我们了解了如何使用 gensim 库构建 Word2Vec 模型，以及如何投影高维词嵌入以在 TensorBoard 中可视化它们。接下来，我们研究了 doc2vec 模型如何与两种类型的 doc2vec 模型(PV-DM 和 PV-DBOW)一起工作。接下来，我们学习了跳过思维模型，通过预测给定句子的前一个和下一个句子来学习句子的嵌入，我们还在本章结尾探索了快速思维模型。

在下一章，我们将学习生成模型以及如何使用生成模型生成图像。

问题



# 让我们通过回答以下问题来评估我们新获得的知识:

skip-gram 和 CBOW 模型之间有什么区别？

1.  CBOW 模型的损失函数是什么？
2.  负抽样的必要性是什么？
3.  定义 PV-DM。
4.  编码器和解码器在跳跃思维向量中的作用是什么？
5.  什么是快速思维矢量？
6.  进一步阅读



# 浏览以下链接，深入了解文本的学习表征:

*单词和短语的分布式表示及其组合性*，托马斯·米科洛夫等人著[https://papers . nips . cc/paper/5021-单词和短语的分布式表示及其组合性. pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

*   *句子和文件的分布式表示*，作者郭勒和托马斯·米科洛夫，[https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)
*   *跳过思维向量*，瑞安·基罗斯等人著[https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)
*   *学习句子表述的有效框架*，作者拉詹努根·洛格斯瓦兰和洪拉克·李，[https://arxiv.org/pdf/1803.02893.pdf](https://arxiv.org/pdf/1803.02893.pdf)
*   *An Efficient Framework for Learning Sentence Representations*, by Lajanugen Logeswaran and Honglak Lee, [https://arxiv.org/pdf/1803.02893.pdf](https://arxiv.org/pdf/1803.02893.pdf)