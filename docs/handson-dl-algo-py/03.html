<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Getting to Know TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">了解张量流</h1>

                

            

            

                

<p>在本章中，我们将了解TensorFlow，这是最常用的深度学习库之一。在本书中，我们将使用TensorFlow从头开始构建深度学习模型。因此，在本章中，我们将掌握TensorFlow及其功能。我们还将了解TensorBoard，这是一个由TensorFlow提供的可视化工具，用于可视化模型。接下来，我们将学习如何构建我们的第一个神经网络，使用TensorFlow来执行手写数字分类。接下来，我们将了解TensorFlow 2.0，这是TensorFlow的最新版本。我们将了解TensorFlow 2.0与之前的版本有何不同，以及它如何使用Keras作为其高级API。</p>

<p>在本章中，我们将讨论以下主题:</p>

<ul>

<li>张量流</li>

<li>计算图形和会话</li>

<li>变量、常量和占位符</li>

<li>张量板</li>

<li>TensorFlow中的手写数字分类</li>

<li>张量流中的数学运算</li>

<li>TensorFlow 2.0和Keras</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>What is TensorFlow?</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">什么是张量流？</h1>

                

            

            

                

<p>TensorFlow是Google的开源软件库，广泛用于数值计算。它是构建深度学习模型最常用的库之一。它具有高度的可伸缩性，可以在多种平台上运行，比如Windows、Linux、macOS和Android。它最初是由谷歌大脑团队的研究人员和工程师开发的。</p>

<p> </p>

<p>TensorFlow支持在任何设备上执行，包括CPU、GPU和TPU(张量处理单元)以及移动和嵌入式平台。由于其灵活的架构和易于部署，它已经成为许多研究人员和科学家构建深度学习模型的热门选择。</p>

<p>在TensorFlow中，每个计算都由数据流图表示，也称为<strong>计算图</strong>，其中一个节点表示运算，如加法或乘法，一条边表示张量。数据流图也可以在许多不同的平台上共享和执行。TensorFlow提供了一个可视化工具，称为TensorBoard，用于可视化数据流图。</p>

<p>一个<strong>张量</strong>只是一个多维数组。因此，当我们说张量流时，它实际上是计算图中多维数组(张量)的流。</p>

<p>您只需在终端中键入以下命令，即可通过<kbd>pip</kbd>轻松安装TensorFlow。我们将安装TensorFlow 1.13.1:</p>

<pre><strong>pip install tensorflow==1.13.1</strong></pre>

<p>我们可以通过运行以下简单的<kbd>Hello TensorFlow!</kbd>程序来检查TensorFlow的安装是否成功:</p>

<pre>import tensorflow as tf<br/><br/>hello = tf.constant("Hello TensorFlow!")<br/>sess = tf.Session()<br/>print(sess.run(hello))</pre>

<p>前面的程序应该打印<kbd>Hello TensorFlow!</kbd>。如果您得到任何错误，那么您可能没有正确安装TensorFlow。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Understanding computational graphs and sessions</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">理解计算图形和会话</h1>

                

            

            

                

<p>正如我们所了解的，TensorFlow中的每个计算都由一个计算图来表示。它们由若干个节点和边组成，其中节点是数学运算，如加法和乘法，边是张量。计算图在优化资源和促进分布式计算方面非常有效。</p>

<p>计算图由排列在节点图中的几个张量流运算组成。</p>

<p>让我们考虑一个基本的加法运算:</p>

<pre>import tensorflow as tf<br/><br/>x = 2<br/>y = 3<br/>z = tf.add(x, y, name='Add')</pre>

<p>上述代码的计算图如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1910 image-border" src="img/583ee1fd-e4fe-4000-90df-72303f077c33.png" style="width:8.92em;height:5.17em;"/></p>

<p>当我们致力于构建一个真正复杂的神经网络时，计算图有助于我们理解网络架构。例如，让我们考虑一个简单的层，<img class="fm-editor-equation" src="img/dc6ada10-965d-445e-8de0-c874c351c6a6.png" style="width:6.67em;height:0.92em;"/>。其计算图将表示如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1912 image-border" src="img/cfe1fcb5-392c-4f41-a8a7-d6d8c616eb93.png" style="width:15.42em;height:15.75em;"/></p>

<p>在计算图中有两种类型的依赖，称为直接和间接依赖。假设我们有<kbd>b</kbd>节点，它的输入依赖于<kbd>a</kbd>节点的输出；这种类型的依赖称为<strong>直接依赖</strong>，如下面的代码所示:</p>

<pre>a = tf.multiply(8,5)<br/>b = tf.multiply(a,1)</pre>

<p>当<kbd>b</kbd>节点的输入不依赖于<kbd>a</kbd>节点时，称为<strong>间接依赖</strong>，如下面的代码所示:</p>

<pre>a = tf.multiply(8,5)<br/>b = tf.multiply(4,3)</pre>

<p>因此，如果我们能够理解这些依赖性，我们就可以在可用的资源中分配独立的计算，并减少计算时间。每当我们导入TensorFlow时，会自动创建一个默认图形，并且我们创建的所有节点都与该默认图形相关联。我们也可以创建我们自己的图，而不是使用默认的图，这在一个文件中构建多个不相互依赖的模型时非常有用。可以使用<kbd>tf.Graph()</kbd>创建一个张量流图，如下所示:</p>

<pre class="graf graf--pre graf-after--p">graph = tf.Graph()<br/><br/>with graph.as_default():<br/>     z = tf.add(x, y, name='Add')</pre>

<p>如果我们想要清除默认图形(也就是说，如果我们想要清除先前定义的变量和操作)，那么我们可以使用<kbd>tf.reset_default_graph()</kbd>来完成。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Sessions</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">会议</h1>

                

            

            

                

<p>我们将创建一个计算图，它的节点上有操作，边上有张量，为了执行这个图，我们使用了一个TensorFlow会话。</p>

<p>可以使用<kbd>tf.Session()</kbd>创建TensorFlow会话，如下面的代码所示，它将分配内存来存储变量的当前值:</p>

<pre>sess = tf.Session()</pre>

<p>创建会话后，我们可以使用<kbd>sess.run()</kbd>方法执行我们的图表。</p>

<p>TensorFlow中的每一个计算都用一个计算图来表示，所以我们需要对每一件事都运行一个计算图。也就是说，为了在TensorFlow上计算任何东西，我们需要创建一个TensorFlow会话。</p>

<p>让我们执行以下代码将两个数字相乘:</p>

<pre>a = tf.multiply(3,3)<br/>print(a)</pre>

<p class="mce-root"/>

<p>上面的代码将打印一个TensorFlow对象<kbd>Tensor("Mul:0", shape=(), dtype=int32)</kbd>，而不是打印<kbd>9</kbd>。</p>

<p>正如我们之前讨论的，每当我们导入TensorFlow时，将自动创建一个默认的计算图，并且所有节点都将附加到该图。因此，当我们打印<kbd>a</kbd>时，它只返回TensorFlow对象，因为<kbd>a</kbd>的值还没有被计算，因为计算图还没有被执行。</p>

<p>为了执行图表，我们需要初始化并运行TensorFlow会话，如下所示:</p>

<pre>a = tf.multiply(3,3)<br/>with tf.Session as sess:<br/>    print(sess.run(a))</pre>

<p>前面的代码将打印<kbd>9</kbd>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Variables, constants, and placeholders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">变量、常量和占位符</h1>

                

            

            

                

<p>变量、常数和占位符是张量流的基本元素。然而，这三者之间总是混淆不清。让我们一个一个地看每一个元素，并了解它们之间的区别。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Variables</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">变量</h1>

                

            

            

                

<p>变量是用来存储值的容器。变量被用作计算图中其他几个运算的输入。可以使用<kbd>tf.Variable()</kbd>函数创建一个变量，如下面的代码所示:</p>

<pre>x = tf.Variable(13)</pre>

<p>让我们使用<kbd>tf.Variable()</kbd>创建一个名为<kbd>W</kbd>的变量，如下所示:</p>

<pre class="code">W = tf.Variable(tf.random_normal([500, 111], stddev=0.35), name="weights")</pre>

<p>正如您在前面的代码中看到的，我们通过从标准偏差为<kbd>0.35</kbd>的正态分布中随机抽取值来创建变量<kbd>W</kbd>。</p>

<p><kbd>tf.Variable()</kbd>中那个叫<kbd>name</kbd>的参数是什么？</p>

<p class="mce-root">它用于设置计算图形中变量的名称。因此，在前面的代码中，Python将变量保存为<kbd>W</kbd>，但是在TensorFlow图中，它将被保存为<kbd>weights</kbd>。</p>

<p class="mce-root">我们也可以使用<kbd>initialized_value()</kbd>用另一个变量的值初始化一个新变量。例如，如果我们想创建一个名为<kbd>weights_2</kbd>的新变量，使用先前定义的<kbd>weights</kbd>变量中的一个值，可以如下完成:</p>

<p>然而，在定义了一个变量之后，我们需要初始化计算图中的所有变量。这可以使用<kbd>tf.global_variables_initializer()</kbd>来完成。</p>

<p>一旦我们创建了一个会话，首先，我们运行初始化操作，这将初始化所有已定义的变量，然后我们才能运行其他操作，如下面的代码所示:</p>

<pre class="code">W2 = tf.Variable(weights.initialized_value(), name="weights_2")</pre>

<p>我们还可以使用<kbd>tf.get_variable()</kbd>创建一个TensorFlow变量。它有三个重要的参数，分别是<kbd>name</kbd>、<kbd>shape</kbd>和<kbd>initializer</kbd>。</p>

<p>与<kbd>tf.Variable()</kbd>不同，我们不能将值直接传递给<kbd>tf.get_variable()</kbd>；相反，我们使用<kbd>initializer</kbd>。有几个初始化器可用于初始化值。例如，<kbd>tf.constant_initializer(value)</kbd>用一个常数值初始化变量，而<kbd>tf.random_normal_initializer(mean, stddev)</kbd>用指定的平均值和标准偏差从随机正态分布中抽取值来初始化变量。</p>

<pre>x = tf.Variable(1212)<br/>init = tf.global_variables_initializer()<br/><br/>with tf.Session() as sess:<br/>  sess.run(init) <br/>  print sess.run(x)</pre>

<p>使用<kbd>tf.Variable()</kbd>创建的变量是不能共享的，每次我们调用<kbd>tf.Variable()</kbd>的时候，它都会创建一个新的变量。但是<kbd>tf.get_variable()</kbd>用指定的参数检查现有变量的计算图。如果变量已经存在，那么它将被重用；否则，将创建一个新变量:</p>

<p>因此，前面的代码检查是否有任何已经存在的变量带有给定的参数。如果是，那么它将重用它；否则，它将创建一个新的变量。</p>

<p>由于我们使用<kbd>tf.get_variable()</kbd>重用变量，为了避免名称冲突，我们使用<kbd>tf.variable_scope</kbd>，如下面的代码所示。变量作用域基本上是一种名称作用域技术，它只是在作用域内给变量添加一个前缀，以避免命名冲突:</p>

<pre>W3 = tf.get_variable(name = 'weights', shape = [500, 111], initializer = random_normal_initializer()))</pre>

<p class="mce-root">如果你打印<kbd>a.name</kbd>和<kbd>b.name</kbd>，那么它会返回同一个名字，就是<kbd>scope/x:0</kbd>。如您所见，我们在名为<kbd>scope</kbd>的变量作用域中指定了<kbd>reuse=True</kbd>参数，这意味着变量可以共享。如果我们不设置<kbd>reuse = True</kbd>，那么它会给出一个错误，说变量已经存在。</p>

<p class="mce-root">建议使用<kbd>tf.get_variable()</kbd>而不是<kbd>tf.Variable()</kbd>，因为<kbd>tf.get_variable</kbd>，允许你共享变量，而且会让代码重构更容易。</p>

<p>常数</p>

<pre>with tf.variable_scope("scope"):<br/> a = tf.get_variable('x', [2])<br/><br/>with tf.variable_scope("scope", reuse = True):<br/> b = tf.get_variable('x', [2])</pre>

<p>与变量不同，常量的值不能改变。也就是说，常数是不可变的。一旦它们被赋予了值，它们就不能被改变。我们可以使用<kbd>tf.constant()</kbd>创建常量，如下面的代码所示:</p>

<p>占位符和提要词典</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Constants</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们可以把占位符看作变量，在这里我们只定义类型和维度，但不赋值。占位符的值将在运行时输入。我们使用占位符将数据输入计算图形。占位符被定义为没有值。</h1>

                

            

            

                

<p>可以使用<kbd>tf.placeholder()</kbd>定义占位符。它采用一个名为<kbd>shape</kbd>的可选参数，表示数据的维度。如果<kbd>shape</kbd>设置为<kbd>None</kbd>，那么我们可以在运行时输入任意大小的数据。占位符可以定义如下:</p>

<pre> x = tf.constant(13)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Placeholders and feed dictionaries</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">简单来说，我们使用<kbd>tf.Variable</kbd>存储数据，使用<kbd>tf.placeholder</kbd>提供外部数据。</h1>

                

            

            

                

<p>让我们考虑一个简单的例子来更好地理解占位符:</p>

<p>如果我们运行前面的代码，那么它将返回一个错误，因为我们试图计算<kbd>y</kbd>，其中<kbd>y= x+3</kbd>和<kbd>x</kbd>是一个占位符，其值没有被赋值。正如我们所了解的，占位符的值将在运行时分配。我们使用<kbd>feed_dict</kbd>参数分配占位符的值。<kbd>feed_dict</kbd>参数基本上是一个字典，其中键代表占位符的名称，值代表占位符的值。</p>

<pre> x = tf.placeholder("float", shape=None)</pre>

<p>正如您在下面的代码中看到的，我们设置了<kbd>feed_dict = {x:5}</kbd>，这意味着<kbd>x</kbd>占位符的值是<kbd>5</kbd>:</p>

<p>前面的代码返回<kbd>8.0</kbd>。</p>

<pre>x = tf.placeholder("float", None)<br/>y = x +3<br/><br/>with tf.Session() as sess:<br/>    result = sess.run(y)<br/>    print(result)</pre>

<p>如果我们想对<kbd>x</kbd>使用多个值呢？由于我们没有为占位符定义任何形状，因此它接受任意数量的值，如以下代码所示:</p>

<p>它将返回以下内容:</p>

<pre>with tf.Session() as sess:<br/>    result = sess.run(y, feed_dict={x: 5})<br/>    print(result)</pre>

<p>假设我们将<kbd>x</kbd>的形状定义为<kbd>[None,2]</kbd>，如以下代码所示:</p>

<p>What if we want to use multiple values for <kbd>x</kbd>? As we have not defined any shapes for the placeholders, it takes any number of values, as shown in the following code:</p>

<pre>with tf.Session() as sess:<br/>    result = sess.run(y, feed_dict={x: [3,6,9]})<br/>    print(result)</pre>

<p>It will return the following:</p>

<pre>[ 6.  9. 12.]</pre>

<p>Let's say we define the shape of <kbd>x</kbd> as <kbd>[None,2]</kbd>, as shown in the following code:</p>

<pre>x = tf.placeholder("float", [None, 2])</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root">这意味着<kbd>x</kbd>可以接受除了<kbd>2</kbd>列之外的任何行的矩阵，如下面的代码所示:</p>

<p class="mceNonEditable">上述代码返回以下内容:</p>

<p class="mceNonEditable">介绍TensorBoard</p>

<p>TensorBoard是TensorFlow的可视化工具，可以用来可视化一个计算图形。它还可以用来绘制各种定量指标和几个中间计算的结果。当我们在训练一个真正深度的神经网络时，当我们不得不调试网络时，它变得令人困惑。因此，如果我们可以在TensorBoard中可视化计算图形，我们就可以很容易地理解这样复杂的模型，调试它们，并优化它们。TensorBoard也支持分享。</p>

<pre class="mce-root">with tf.Session() as sess:<br/>    x_val = [[1, 2,], <br/>              [3,4],<br/>              [5,6],<br/>              [7,8],]<br/>    result = sess.run(y, feed_dict={x: x_val})<br/>    print(result)</pre>

<p>如下面的屏幕截图所示，TensorBoard面板由几个选项卡组成——标量、图像、音频、图形、分布、直方图和嵌入:</p>

<pre>[[ 4.  5.]

 [ 6.  7.]

 [ 8.  9.]

 [10. 11.]]</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Introducing TensorBoard</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title"><img class="alignnone size-full wp-image-1913 image-border" src="img/fdbe9a38-d2d4-464f-ba46-67bf44e48477.png" style="width:162.50em;height:6.42em;"/></h1>

                

            

            

                

<p>这些选项卡非常简单明了。标量选项卡显示了我们在程序中使用的标量变量的有用信息。例如，它显示了一个称为loss的标量变量的值如何在多次迭代中变化。</p>

<p>图形选项卡显示计算图形。分布和直方图选项卡显示变量的分布。例如，我们的模型的重量分布和直方图可以在这些选项卡下看到。<strong>嵌入</strong>选项卡用于可视化高维向量，如单词嵌入(我们将在<a href="d184e022-0b11-492a-8303-37a6021c4bf6.xhtml" target="_blank">第7章</a>、<em>学习文本表示</em>中详细了解)。</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1913 image-border" src="img/fdbe9a38-d2d4-464f-ba46-67bf44e48477.png" style="width:162.50em;height:6.42em;"/></p>

<p>让我们建立一个基本的计算图，并在TensorBoard中可视化。假设我们有四个变量，如下所示:</p>

<p>让我们将<kbd>x</kbd>和<kbd>y</kbd>以及<kbd>a</kbd>和<kbd>b</kbd>相乘，并保存为<kbd>prod1</kbd>和<kbd>prod2</kbd>，如下面的代码所示:</p>

<p class="mce-root">添加<kbd>prod1</kbd>和<kbd>prod2</kbd>并存储在<kbd>sum</kbd>中:</p>

<p>现在，我们可以在TensorBoard中可视化所有这些操作。为了在TensorBoard中可视化，我们首先需要保存我们的事件文件。可以使用<kbd>tf.summary.FileWriter()</kbd>来完成。它需要两个重要的参数，<kbd>logdir</kbd>和<kbd>graph</kbd>。</p>

<pre class="mce-root">x = tf.constant(1,name='x')<br/>y = tf.constant(1,name='y')<br/>a = tf.constant(3,name='a')<br/>b = tf.constant(3,name='b')</pre>

<p>顾名思义，<kbd>logdir</kbd>指定我们想要存储图形的目录，而<kbd>graph</kbd>指定我们想要存储哪个图形:</p>

<pre class="mce-root">prod1 = tf.multiply(x,y,name='prod1')<br/>prod2 = tf.multiply(a,b,name='prod2')</pre>

<p>在前面的代码中，<kbd>graphs</kbd>是我们存储事件文件的目录，<kbd>sess.graph</kbd>指定了TensorFlow会话中的当前图形。因此，我们将当前图形存储在目录<kbd>graphs</kbd>中的TensorFlow会话中。</p>

<pre class="mce-root">sum = tf.add(prod1,prod2,name='sum')</pre>

<p class="mce-root">要启动TensorBoard，请转到您的终端，找到工作目录，并键入以下内容:</p>

<p><kbd>logdir</kbd>参数表示存储事件文件的目录，<kbd>port</kbd>是端口号。运行前面的命令后，打开浏览器并键入<kbd>http://localhost:8000/</kbd>。</p>

<pre>with tf.Session() as sess:<br/>    writer = tf.summary.FileWriter(logdir='./graphs',graph=sess.graph)<br/>    print(sess.run(sum))</pre>

<p>在TensorBoard面板中，在“图形”选项卡下，您可以看到计算图形:</p>

<p><img class="alignnone size-full wp-image-1914 image-border" src="img/b63f2545-f298-46d0-a6fd-9cc136644f54.png" style="width:19.58em;height:9.75em;"/></p>

<pre><strong>tensorboard --logdir=graphs --port=8000</strong></pre>

<p>您可能已经注意到，我们定义的所有操作都清楚地显示在图表中。</p>

<p>创建名称范围</p>

<p class="CDPAlignCenter CDPAlign">作用域用于降低复杂性，并通过将相关节点组合在一起帮助我们更好地理解模型。拥有一个名称范围有助于我们在一个图中对相似的操作进行分组。当我们构建一个复杂的架构时，它会派上用场。可以使用<kbd>tf.name_scope()</kbd>创建范围。在前面的例子中，我们执行了两个操作，<kbd>Product</kbd>和<kbd>sum</kbd>。我们可以简单地将它们分成两个不同的名字范围，分别是<kbd>Product</kbd>和<kbd>sum</kbd>。</p>

<p>在上一节中，我们看到了<kbd>prod1</kbd>和<kbd>prod2</kbd>如何执行乘法并计算结果。我们将定义一个名为<kbd>Product</kbd>的名称范围，并将<kbd>prod1</kbd>和<kbd>prod2</kbd>操作分组，如以下代码所示:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Creating a name scope</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">现在，定义<kbd>sum</kbd>的名称范围:</h1>

                

            

            

                

<p>将文件存储在<kbd>graphs</kbd>目录中:</p>

<p>在TensorBoard中可视化图形:</p>

<pre>with tf.name_scope("Product"):<br/>    with tf.name_scope("prod1"):<br/>        prod1 = tf.multiply(x,y,name='prod1')<br/>        <br/>    with tf.name_scope("prod2"):<br/>        prod2 = tf.multiply(a,b,name='prod2')</pre>

<p class="mce-root">您可能注意到，现在我们只有两个节点，<strong> sum </strong>和<strong> Product </strong>:</p>

<pre>with tf.name_scope("sum"):<br/>    sum = tf.add(prod1,prod2,name='sum')</pre>

<p><img class="alignnone size-full wp-image-1915 image-border" src="img/fa3246f8-34bc-4496-a021-d8e191e5a085.png" style="width:12.92em;height:11.92em;"/></p>

<pre>with tf.Session() as sess:<br/>    writer = tf.summary.FileWriter('./graphs', sess.graph)<br/>    print(sess.run(sum))</pre>

<p>双击节点后，我们可以看到计算是如何进行的。如您所见，<strong> prod1 </strong>和<strong> prod2 </strong>节点被分组在<strong> Product </strong>范围下，它们的结果被发送到<strong> sum </strong>节点，在那里它们将被添加。您可以看到<strong> prod1 </strong>和<strong> prod2 </strong>节点如何计算它们的值:</p>

<pre><strong>tensorboard --logdir=graphs --port=8000</strong></pre>

<p><img class="alignnone size-full wp-image-1916 image-border" src="img/d11d7a2b-389d-497b-bc80-9bd9c9e9bdb4.png" style="width:42.25em;height:29.58em;"/></p>

<p class="CDPAlignCenter CDPAlign">前面的例子只是一个简单的例子。当我们处理一个有很多操作的复杂项目时，名字范围帮助我们将相似的操作组合在一起，并使我们能够更好地理解计算图。</p>

<p>基于张量流的手写数字分类</p>

<p class="CDPAlignCenter CDPAlign">把我们到目前为止学到的所有概念放在一起，我们将看到如何使用TensorFlow来建立一个识别手写数字的神经网络。如果你最近一直在玩深度学习，那么你一定遇到过MNIST数据集。它被称为深度学习的<em> hello world </em>。它由55，000个数据点的手写数字(0到9)组成。</p>

<p>在本节中，我们将看到如何使用我们的神经网络来识别这些手写数字，我们将掌握TensorFlow和TensorBoard的窍门。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Handwritten digit classification using TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">导入所需的库</h1>

                

            

            

                

<p>首先，让我们导入所有需要的库:</p>

<p>加载数据集</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Importing the required libraries</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用以下代码加载数据集:</h1>

                

            

            

                

<p>在前面的代码中，<kbd>data/mnist</kbd>表示我们存储MNIST数据集的位置，<kbd>one_hot=True</kbd>表示我们正在对标签(0到9)进行一次性编码。</p>

<pre class="mce-root">import warnings<br/>warnings.filterwarnings('ignore')<br/><br/>import tensorflow as tf<br/>from tensorflow.examples.tutorials.mnist import input_data<br/>tf.logging.set_verbosity(tf.logging.ERROR)<br/><br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Loading the dataset</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">通过执行以下代码，我们将看到我们的数据中有什么:</h1>

                

            

            

                

<p>我们在训练集中有<kbd>55000</kbd>图像，每个图像的大小是<kbd>784</kbd>，我们有<kbd>10</kbd>标签，实际上是0到9。类似地，我们在测试集中有<kbd>10000</kbd>个图像。</p>

<pre class="mce-root">mnist = input_data.read_data_sets("data/mnist", one_hot=True)</pre>

<p>现在，我们将绘制一个输入图像，看看它看起来像什么:</p>

<p>因此，我们的输入图像如下所示:</p>

<pre>print("No of images in training set {}".format(mnist.train.images.shape))<br/>print("No of labels in training set {}".format(mnist.train.labels.shape))<br/><br/>print("No of images in test set {}".format(mnist.test.images.shape))<br/>print("No of labels in test set {}".format(mnist.test.labels.shape))<br/><br/>No of images in training set (55000, 784)

No of labels in training set (55000, 10)

No of images in test set (10000, 784)

No of labels in test set (10000, 10)</pre>

<p><img class="alignnone size-full wp-image-1917 image-border" src="img/52193d70-1696-4ae6-8bfa-ed05915c159c.png" style="width:17.25em;height:17.00em;"/></p>

<p class="mce-root">定义每层中神经元的数量</p>

<pre>img1 = mnist.train.images[0].reshape(28,28)<br/>plt.imshow(img1, cmap='Greys')</pre>

<p>我们将建立一个具有三个隐藏层和一个输出层的四层神经网络。由于输入图像的大小是<kbd>784</kbd>，我们设置<kbd>num_input</kbd>到<kbd>784</kbd>，由于我们有10个手写数字(0到9)，我们在输出层设置<kbd>10</kbd>神经元。我们将每层中的神经元数量定义如下:</p>

<p class="CDPAlignCenter CDPAlign">定义占位符</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining the number of neurons in each layer</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">正如我们所了解的，我们首先需要为<kbd>input</kbd>和<kbd>output</kbd>定义占位符。占位符的值将在运行时通过<kbd>feed_dict</kbd>输入:</h1>

                

            

            

                

<p class="mce-root">由于我们有一个四层网络，我们有四个权重和四个偏差。我们通过从标准偏差为<kbd xmlns:epub="http://www.idpf.org/2007/ops">0.1</kbd>的截断正态分布中提取值来初始化我们的权重。记住，权重矩阵的维数应该是前一层中神经元的<em xmlns:epub="http://www.idpf.org/2007/ops">数量</em>和当前层中神经元的<em xmlns:epub="http://www.idpf.org/2007/ops">数量。例如，权重矩阵<kbd xmlns:epub="http://www.idpf.org/2007/ops">w3</kbd>的维数应该是隐含层2 </em>的神经元数<em xmlns:epub="http://www.idpf.org/2007/ops">x隐含层3 </em>的神经元数。</p>

<pre class="mce-root">#number of neurons in input layer<br/>num_input = 784<br/><br/>#num of neurons in hidden layer 1<br/>num_hidden1 = 512<br/><br/>#num of neurons in hidden layer 2<br/>num_hidden2 = 256<br/><br/>#num of neurons in hidden layer 3<br/>num_hidden_3 = 128<br/><br/>#num of neurons in output layer<br/>num_output = 10</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining placeholders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Defining placeholders</h1>

                

            

            

                

<p class="mce-root">我们通常在字典中定义所有的权重，如下所示:</p>

<pre>with tf.name_scope('input'):<br/>    X = tf.placeholder("float", [None, num_input])<br/><br/>with tf.name_scope('output'):<br/>    Y = tf.placeholder("float", [None, num_output])</pre>

<p class="mce-root">偏差的形状应该是当前层中神经元的数量。例如，<kbd>b2</kbd>偏差的维数是隐藏层2中神经元的数量。我们将偏置值设置为常数；<kbd>0.1</kbd>在所有层中:</p>

<p class="mce-root">正向传播</p>

<p>现在我们将定义正向传播操作。我们将在所有层中使用ReLU激活。在最后一层，我们将应用<kbd>sigmoid</kbd>激活，如下面的代码所示:</p>

<pre class="mce-root">with tf.name_scope('weights'):<br/> <br/> weights = {<br/> 'w1': tf.Variable(tf.truncated_normal([num_input, num_hidden1], stddev=0.1),name='weight_1'),<br/> 'w2': tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1),name='weight_2'),<br/> 'w3': tf.Variable(tf.truncated_normal([num_hidden2, num_hidden_3], stddev=0.1),name='weight_3'),<br/> 'out': tf.Variable(tf.truncated_normal([num_hidden_3, num_output], stddev=0.1),name='weight_4'),<br/> }</pre>

<p class="mce-root">计算损失和反向传播</p>

<pre>with tf.name_scope('biases'):<br/><br/>    biases = {<br/>        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),<br/>        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),<br/>        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3]),name='bias_3'),<br/>        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')<br/>    }</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Forward propagation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">接下来，我们将定义我们的损失函数。我们将使用softmax交叉熵作为我们的损失函数。TensorFlow提供了用于计算softmax交叉熵损失的<kbd>tf.nn.softmax_cross_entropy_with_logits()</kbd>函数。它将两个参数作为输入，<kbd>logits</kbd>和<kbd>labels</kbd>:</h1>

                

            

            

                

<p><kbd>logits</kbd>参数指定我们的网络预测的<kbd>logits</kbd>；例如，<kbd>y_hat</kbd></p>

<pre>with tf.name_scope('Model'):<br/>    <br/>    with tf.name_scope('layer1'):<br/>        layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['w1']), biases['b1']) ) <br/>    <br/>    with tf.name_scope('layer2'):<br/>        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']))<br/>        <br/>    with tf.name_scope('layer3'):<br/>        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['w3']), biases['b3']))<br/>        <br/>    with tf.name_scope('output_layer'):<br/>         y_hat = tf.nn.sigmoid(tf.matmul(layer_3, weights['out']) + biases['out'])</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing loss and backpropagation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title"><kbd>labels</kbd>参数指定实际标签；例如，真实标签，<kbd>Y</kbd></h1>

                

            

            

                

<p>我们使用<kbd>tf.reduce_mean()</kbd>取<kbd>loss</kbd>函数的平均值:</p>

<ul>

<li class="mce-root">现在，我们需要使用反向传播来最小化损失。放心吧！我们不必手动计算所有权重的导数。相反，我们可以使用TensorFlow的优化器。在本节中，我们将使用Adam优化器。它是我们在<a href="92f3c897-c0d4-40f8-8f63-bd11240f2189.xhtml" target="_blank">第一章</a>、<em>深度学习介绍</em>中了解到的梯度下降优化技术的变体。在<a href="28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml" target="_blank">第3章</a>、<em>梯度下降及其变体</em>中，我们将深入细节，看看Adam优化器和其他几个优化器到底是如何工作的。现在，假设我们使用Adam优化器作为反向传播算法:</li>

<li class="mce-root">The <kbd>labels</kbd> parameter specifies the actual labels; for example, true labels, <kbd>Y</kbd></li>

</ul>

<p class="mce-root">We take the mean of the <kbd>loss</kbd> function using <kbd>tf.reduce_mean()</kbd>:</p>

<pre>with tf.name_scope('Loss'):<br/>        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=Y))</pre>

<p>计算精度</p>

<pre>learning_rate = 1e-4<br/>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)</pre>

<p class="mce-root">我们计算模型的精确度如下:</p>

<p class="mce-root"><kbd>y_hat</kbd>参数表示我们的模型的每个类别的预测概率。因为我们有<kbd>10</kbd>类，我们将有<kbd>10</kbd>概率。如果在位置<kbd>7</kbd>的概率高，则意味着我们的网络以高概率将输入图像预测为数字<kbd>7</kbd>。<kbd>tf.argmax()</kbd>函数返回最大值的索引。因此，<kbd>tf.argmax(y_hat,1)</kbd>给出概率高的索引。因此，如果索引<kbd>7</kbd>处的概率很高，则返回<kbd>7</kbd>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Computing accuracy</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title"><kbd>Y</kbd>参数表示实际的标签，它们是独热编码值。也就是说，除了在实际图像的位置，它在任何地方都由零组成，在实际图像的位置，它由<kbd>1</kbd>组成。例如，如果输入图像是<kbd>7</kbd>，那么<kbd>Y</kbd>在除了索引<kbd>7</kbd>之外的所有索引处都是0，在索引<kbd>7</kbd>处它具有<kbd>1</kbd>。因此，<kbd>tf.argmax(Y,1)</kbd>返回<kbd>7</kbd>，因为在那里我们有一个高值<kbd>1</kbd>。</h1>

                

            

            

                

<p class="mce-root">因此，<kbd>tf.argmax(y_hat,1)</kbd>给出预测数字，而<kbd>tf.argmax(Y,1)</kbd>给出实际数字。</p>

<ul>

<li class="mce-root"><kbd>tf.equal(x, y)</kbd>函数将<kbd>x</kbd>和<kbd>y</kbd>作为输入，并逐元素返回<em> (x == y) </em>的真值。因此，<kbd>correct_pred = tf.equal(predicted_digit,actual_digit)</kbd>由实际和预测数字相同的<kbd>True</kbd>和实际和预测数字不同的<kbd>False</kbd>组成。我们使用TensorFlow的cast操作<kbd>tf.cast(correct_pred, tf.float32)</kbd>将<kbd>correct_pred</kbd>中的布尔值转换为浮点值。在将它们转换成浮点值后，我们使用<kbd>tf.reduce_mean()</kbd>取平均值。</li>

<li class="mce-root">因此，<kbd>tf.reduce_mean(tf.cast(correct_pred, tf.float32))</kbd>给了我们平均正确的预测:</li>

</ul>

<p class="mce-root">创建摘要</p>

<p class="mce-root">我们还可以在TensorBoard中可视化我们的模型在几次迭代中的损失和准确性如何变化。所以，我们使用<kbd>tf.summary()</kbd>来获得变量的汇总。由于损耗和精度是标量变量，我们使用<kbd>tf.summary.scalar()</kbd>，如以下代码所示:</p>

<p class="mce-root">接下来，我们使用<kbd>tf.summary.merge_all()</kbd>合并图表中使用的所有摘要。我们这样做是因为当我们有许多摘要时，运行和存储它们会变得低效，所以我们在会话中运行它们一次，而不是运行多次:</p>

<pre>with tf.name_scope('Accuracy'):<br/>    <br/>    predicted_digit = tf.argmax(y_hat, 1)<br/>    actual_digit = tf.argmax(Y, 1)<br/>    <br/>    correct_pred = tf.equal(predicted_digit,actual_digit)<br/>    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Creating summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练模型</h1>

                

            

            

                

<p>现在，是时候训练我们的模型了。正如我们所了解的，首先，我们需要初始化所有的变量:</p>

<pre>tf.summary.scalar("Accuracy", accuracy)<br/>tf.summary.scalar("Loss", loss)</pre>

<p>定义批次大小、迭代次数和学习率，如下所示:</p>

<pre>merge_summary = tf.summary.merge_all()</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">启动张量流会话:</h1>

                

            

            

                

<p class="mce-root">初始化所有变量:</p>

<pre class="mce-root">init = tf.global_variables_initializer()</pre>

<p class="mce-root">保存事件文件:</p>

<pre class="mce-root">learning_rate = 1e-4<br/>num_iterations = 1000<br/>batch_size = 128</pre>

<p>对模型进行多次迭代训练:</p>

<pre>with tf.Session() as sess:</pre>

<p>根据批量大小获取一批数据:</p>

<pre>    sess.run(init)</pre>

<p>训练网络:</p>

<pre>    summary_writer = tf.summary.FileWriter('./graphs', graph=sess.graph)</pre>

<p>每100次<sup xmlns:epub="http://www.idpf.org/2007/ops">迭代打印<kbd xmlns:epub="http://www.idpf.org/2007/ops">loss</kbd>和<kbd xmlns:epub="http://www.idpf.org/2007/ops">accuracy</kbd>:</sup></p>

<pre>    for i in range(num_iterations):</pre>

<p>您可能会从以下输出中注意到，在各种训练迭代中，损失会减少，精度会提高:</p>

<pre>        batch_x, batch_y = mnist.train.next_batch(batch_size)</pre>

<p>在TensorBoard中可视化图形</p>

<pre>        sess.run(optimizer, feed_dict={ X: batch_x, Y: batch_y})</pre>

<p>经过训练，我们可以在TensorBoard中可视化我们的计算图形，如下图所示。如您所见，我们的<strong>模型</strong>将输入、权重和偏差作为输入，并返回输出。我们根据模型的输出计算损耗和精度。我们通过计算梯度和更新权重来最小化损失。我们可以在下图中观察到这一切:</p>

<pre>        if i % 100 == 0:<br/><br/>            batch_loss, batch_accuracy,summary = sess.run(<br/>                [loss, accuracy, merge_summary], feed_dict={X: batch_x, Y: batch_y}<br/>                )<br/><br/>            #store all the summaries    <br/>            summary_writer.add_summary(summary, i)<br/><br/><br/>            print('Iteration: {}, Loss: {}, Accuracy: {}'.format(i,batch_loss,batch_accuracy))</pre>

<p><img class="alignnone size-full wp-image-1921 image-border" src="img/d073d2d4-a592-4e2a-871a-c3f4956612dd.png" style="width:44.00em;height:29.17em;"/></p>

<pre>Iteration: 0, Loss: 2.30789709091, Accuracy: 0.1171875

Iteration: 100, Loss: 1.76062202454, Accuracy: 0.859375

Iteration: 200, Loss: 1.60075569153, Accuracy: 0.9375

Iteration: 300, Loss: 1.60388696194, Accuracy: 0.890625

Iteration: 400, Loss: 1.59523034096, Accuracy: 0.921875

Iteration: 500, Loss: 1.58489584923, Accuracy: 0.859375

Iteration: 600, Loss: 1.51407408714, Accuracy: 0.953125

Iteration: 700, Loss: 1.53311181068, Accuracy: 0.9296875

Iteration: 800, Loss: 1.57677125931, Accuracy: 0.875

Iteration: 900, Loss: 1.52060437202, Accuracy: 0.9453125</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Visualizing graphs in TensorBoard</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">如果我们双击并展开<strong>模型</strong>，我们可以看到我们有三个隐藏层和一个输出层:</h1>

                

            

            

                

<p><img src="img/df7d8d3c-94b9-40f4-a951-3edb199393fc.png" style="width:51.25em;height:39.25em;"/></p>

<p class="CDPAlignCenter CDPAlign">同样，我们可以双击并查看每个节点。例如，如果我们打开<strong>权重</strong>，我们可以看到如何使用截尾正态分布初始化这四个权重，以及如何使用Adam优化器更新它:</p>

<p class="mce-root"><img src="img/662ac908-27ec-4570-88c8-0cad3354fb23.png"/></p>

<p class="CDPAlignCenter CDPAlign">正如我们所知，计算图有助于我们理解每个节点上发生的事情。双击<strong>精确度</strong>节点，我们可以看到精确度是如何计算的:</p>

<p><img src="img/d48fcb84-8a10-4716-a8a3-69006100ce30.png" style="width:67.50em;height:53.08em;"/></p>

<p class="CDPAlignCenter CDPAlign">请记住，我们还存储了我们的<kbd>loss</kbd>和<kbd>accuracy</kbd>变量的摘要。我们可以在TensorBoard的SCALARS选项卡下找到它们，如下图所示。我们可以看到这种损失是如何随着迭代减少的，如下面的屏幕截图所示:</p>

<p><img class="alignnone size-full wp-image-1922 image-border" src="img/9fa0158f-52d0-433f-9aa1-451efad738bc.png" style="width:26.25em;height:19.75em;"/></p>

<p class="CDPAlignCenter CDPAlign">下面的屏幕截图显示了精确度是如何随着迭代而增加的:</p>

<p><img class="alignnone size-full wp-image-1923 image-border" src="img/b81c6aa3-29d8-4988-a7ec-a916fe0727a7.png" style="width:26.08em;height:19.83em;"/></p>

<p class="CDPAlignCenter CDPAlign">引入热切执行</p>

<p>TensorFlow中的急切执行更加Pythonic化，并允许快速原型化。与每次执行任何操作都需要构建一个图的图形模式不同，急切执行遵循命令式编程范式，任何操作都可以立即执行，而不必创建图，就像我们在Python中一样。因此，有了热切的执行，我们就可以告别会话和占位符了。与图形模式不同，它还使调试过程变得更容易，并立即出现运行时错误。</p>

<p class="CDPAlignCenter CDPAlign">例如，在图形模式下，为了计算任何东西，我们运行会话。如下面的代码所示，为了计算<kbd>z</kbd>的值，我们必须运行TensorFlow会话:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Introducing eager execution</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">有了急切执行，我们不需要创建会话；我们可以简单地计算<kbd>z</kbd>，就像我们在Python中做的一样。为了启用急切执行，只需调用<kbd>tf.enable_eager_execution()</kbd>函数:</h1>

                

            

            

                

<p>它将返回以下内容:</p>

<p>为了获得输出值，我们可以打印以下内容:</p>

<pre class="mce-root">x = tf.constant(11)<br/>y = tf.constant(11)<br/>z = x*y<br/><br/>with tf.Session() as sess:<br/>    print sess.run(z)</pre>

<p class="mce-root">With eager execution, we don't need to create a session; we can simply compute <kbd>z</kbd>, just like we do in Python. In order to enable eager execution, just call the <kbd>tf.enable_eager_execution()</kbd> function:</p>

<pre class="mce-root">x = tf.constant(11)<br/>y = tf.constant(11)<br/>z = x*y<br/><br/>print z</pre>

<p>It will return the following:</p>

<pre>&lt;tf.Tensor: id=789, shape=(), dtype=int32, numpy=121&gt;</pre>

<p>张量流中的数学运算</p>

<pre>z.numpy()<br/><br/>121</pre>

<p class="mce-root">现在，我们将使用急切执行模式探索TensorFlow中的一些操作:</p>

<p class="mce-root">让我们从一些基本的算术运算开始。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Math operations in TensorFlow</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用<kbd>tf.add</kbd>添加两个数字:</h1>

                

            

            

                

<p><kbd>tf.subtract</kbd>函数用于找出两个数字之间的差异:</p>

<pre>x = tf.constant([1., 2., 3.])<br/>y = tf.constant([3., 2., 1.])</pre>

<p><kbd>tf.multiply</kbd>功能用于将两个数相乘:</p>

<p>使用<kbd>tf.divide</kbd>将两个数相除:</p>

<pre>sum = tf.add(x,y)<br/>sum.numpy()<br/><br/>array([4., 4., 4.], dtype=float32)</pre>

<p>点积的计算方法如下:</p>

<pre>difference = tf.subtract(x,y)<br/>difference.numpy()<br/><br/>array([-2.,  0.,  2.], dtype=float32)</pre>

<p>接下来，让我们找到最小和最大元素的索引:</p>

<pre>product = tf.multiply(x,y)<br/>product.numpy()<br/><br/>array([3., 4., 3.], dtype=float32)</pre>

<p>Divide two numbers using <kbd>tf.divide</kbd>:</p>

<pre>division = tf.divide(x,y)<br/>division.numpy()<br/><br/>array([0.33333334, 1.        , 3.        ], dtype=float32)</pre>

<p>使用<kbd>tf.argmin()</kbd>计算最小值的索引:</p>

<pre>dot_product = tf.reduce_sum(tf.multiply(x, y))<br/>dot_product.numpy()<br/><br/>10.0</pre>

<p class="mce-root">使用<kbd>tf.argmax()</kbd>计算最大值的指数:</p>

<pre>x = tf.constant([10, 0, 13, 9])</pre>

<p class="mce-root">运行以下代码，找出<kbd>x</kbd>和<kbd>y</kbd>之间的平方差:</p>

<p class="mce-root">让我们试试类型化；也就是说，从一种数据类型转换成另一种数据类型。</p>

<pre class="mce-root">tf.argmin(x).numpy()<br/><br/>1</pre>

<p>打印<kbd>x</kbd>的类型:</p>

<pre class="mce-root">tf.argmax(x).numpy()<br/><br/>2</pre>

<p>我们可以使用<kbd>tf.cast</kbd>将<kbd>x</kbd>的类型<kbd>tf.int32</kbd>转换为<kbd>tf.float32</kbd>，如下面的代码所示:</p>

<pre class="mce-root">x = tf.Variable([1,3,5,7,11])<br/>y = tf.Variable([1])<br/><br/>tf.math.squared_difference(x,y).numpy()<br/><br/>[  0,   4,  16,  36, 100]</pre>

<p class="mce-root">现在，检查<kbd>x</kbd>类型。它将是<kbd>tf.float32</kbd>，如下所示:</p>

<p>连接两个矩阵:</p>

<pre class="mce-root">print x.dtype<br/><br/>tf.int32</pre>

<p>We can convert the type of <kbd>x</kbd>, which is <kbd>tf.int32</kbd>, into <kbd>tf.float32</kbd> using <kbd>tf.cast</kbd>, as shown in the following code:</p>

<pre class="mce-root">x = tf.cast(x, dtype=tf.float32)</pre>

<p>Now, check the <kbd>x</kbd> type. It will be <kbd>tf.float32</kbd>, as follows:</p>

<pre class="mce-root">print x.dtype<br/><br/>tf.float32</pre>

<p class="mce-root">按行连接矩阵:</p>

<pre class="mce-root">x = [[3,6,9], [7,7,7]]<br/>y = [[4,5,6], [5,5,5]]</pre>

<p class="mce-root">使用以下代码按列连接矩阵:</p>

<p class="mce-root">使用<kbd>stack</kbd>函数堆叠<kbd>x</kbd>矩阵:</p>

<p class="mce-root">现在，让我们看看如何执行<kbd>reduce_mean</kbd>操作:</p>

<pre class="mce-root">tf.concat([x, y], 0).numpy()<br/><br/>array([[3, 6, 9],

       [7, 7, 7],

       [4, 5, 6],

       [5, 5, 5]], dtype=int32)</pre>

<p class="mce-root">计算<kbd>x</kbd>的平均值；即<em> (1.0 + 5.0 + 2.0 + 3.0) / 4 </em>:</p>

<pre class="mce-root">tf.concat([x, y], 1).numpy()<br/><br/>array([[3, 6, 9, 4, 5, 6],<br/>       [7, 7, 7, 5, 5, 5]], dtype=int32)</pre>

<p>计算整行的平均值；即<em> (1.0+5.0)/2，(2.0+3.0)/2 </em>:</p>

<pre class="mce-root">tf.stack(x, axis=1).numpy()<br/><br/>array([[3, 7],

       [6, 7],

       [9, 7]], dtype=int32)</pre>

<p class="mce-root">Now, let' see how to perform the <kbd>reduce_mean</kbd> operation:</p>

<pre>x = tf.Variable([[1.0, 5.0], [2.0, 3.0]])<br/><br/>x.numpy()<br/><br/>array([[1., 5.],

       [2., 3.]]</pre>

<p>计算整个列的平均值；即<em> (1.0+5.0)/2.0，(2.0+3.0)/2.0 </em>:</p>

<pre class="mce-root">tf.reduce_mean(input_tensor=x).numpy() <br/><br/>2.75</pre>

<p>从概率分布中抽取随机值:</p>

<pre class="mce-root">tf.reduce_mean(input_tensor=x, axis=0).numpy() <br/><br/>array([1.5, 4. ], dtype=float32)</pre>

<p class="mce-root">计算软最大概率:</p>

<p>现在，我们来看看如何计算梯度。</p>

<pre class="mce-root">tf.reduce_mean(input_tensor=x, axis=1, keepdims=True).numpy()<br/><br/>array([[3. ],

       [2.5]], dtype=float32)</pre>

<p class="mce-root">定义<kbd>square</kbd>功能:</p>

<pre class="mce-root">tf.random.normal(shape=(3,2), mean=10.0, stddev=2.0).numpy()<br/><br/>tf.random.uniform(shape = (3,2), minval=0, maxval=None, dtype=tf.float32,).numpy()</pre>

<p class="mce-root">可以使用<kbd>tf.GradientTape</kbd>为前面的<kbd>square</kbd>函数计算梯度，如下所示:</p>

<pre class="mce-root">x = tf.constant([7., 2., 5.])<br/><br/>tf.nn.softmax(x).numpy()<br/><br/>array([0.8756006 , 0.00589975, 0.11849965], dtype=float32)</pre>

<p class="mce-root">更多TensorFlow操作可在<a href="http://bit.ly/2YSYbYu">http://bit.ly/2YSYbYu</a>的GitHub上的笔记本中获得。<a href="https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python"/></p>

<p>TensorFlow远不止这些。随着本书的深入，我们将了解TensorFlow的各种重要功能。</p>

<pre>def square(x):<br/>  return tf.multiply(x, x)</pre>

<p>The gradients can be computed for the preceding <kbd>square</kbd> function using <kbd>tf.GradientTape</kbd>, as follows:</p>

<pre>with tf.GradientTape(persistent=True) as tape:<br/>     print square(6.).numpy()<br/><br/>36.0</pre>

<p>More TensorFlow operations are available in the Notebook on GitHub at <a href="http://bit.ly/2YSYbYu">http://bit.ly/2YSYbYu</a>.<a href="https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python"/></p>

<p>TensorFlow 2.0和Keras</p>

<p class="mce-root">TensorFlow 2.0有一些非常酷的功能。默认情况下，它设置急切执行模式。它提供了简化的工作流程，并使用Keras作为构建深度学习模型的主要API。它还向后兼容TensorFlow 1.x版本。</p>

<p class="mce-root">要安装TensorFlow 2.0，请打开终端并键入以下命令:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow 2.0 and Keras</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">由于TensorFlow 2.0使用Keras作为高级API，我们将在下一节中了解Keras的工作原理。</h1>

                

            

            

                

<p class="mce-root">你好，Keras</p>

<p>Keras是另一个广泛使用的深度学习库。它是由谷歌的Franç ois Chollet开发的。它以快速的原型制作而闻名，并且使模型构建变得简单。它是一个高级库，这意味着它本身不执行任何低级操作，如卷积。它使用后端引擎来完成这项工作，比如TensorFlow。Keras API在<kbd>tf.keras</kbd>中可用，TensorFlow 2.0将其作为主要API。</p>

<pre><strong>pip install tensorflow==2.0.0-alpha0</strong></pre>

<p>在Keras中构建模型包括四个重要步骤:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Bonjour Keras</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">定义模型</h1>

                

            

            

                

<p>编译模型</p>

<p>拟合模型</p>

<ol>

<li>评估模型</li>

<li>定义模型</li>

<li>第一步是定义模型。Keras提供了两种不同的API来定义模型:</li>

<li>顺序API</li>

</ol>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">功能API</h1>

                

            

            

                

<p>The first step is defining the model. Keras provides two different APIs to define the model:</p>

<ul>

<li>定义顺序模型</li>

<li>在顺序模型中，我们将每一层一层堆叠起来:</li>

</ul>

<p class="mce-root">首先，让我们将模型定义为一个<kbd>Sequential()</kbd>模型，如下所示:</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining a sequential model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">现在，定义第一层，如以下代码所示:</h1>

                

            

            

                

<p>在前面的代码中，<kbd>Dense</kbd>表示一个完全连接的层，<kbd>input_dim</kbd>表示我们输入的维度，<kbd>activation</kbd>指定我们使用的激活函数。我们可以想叠多少层就叠多少层，一层在另一层上面。</p>

<pre>from keras.models import Sequential<br/>from keras.layers import Dense</pre>

<p>通过激活<kbd>relu</kbd>定义下一层，如下所示:</p>

<pre>model = Sequential()</pre>

<p>用<kbd>sigmoid</kbd>激活定义输出层:</p>

<pre>model.add(Dense(13, input_dim=7, activation='relu'))</pre>

<p>顺序模型的最终代码块如下所示。如您所见，Keras代码比TensorFlow代码简单得多:</p>

<p>定义功能模型</p>

<pre>model.add(Dense(7, activation='relu'))</pre>

<p>功能模型比顺序模型提供了更多的灵活性。例如，在功能模型中，我们可以很容易地将任何一层连接到另一层，而在顺序模型中，每一层都是一层一层的堆叠。在创建复杂的模型时，例如有向无环图、具有多个输入值、多个输出值和共享图层的模型，函数模型非常方便。现在，我们将看到如何在Keras中定义一个功能模型。</p>

<pre>model.add(Dense(1, activation='sigmoid'))</pre>

<p>第一步是定义输入尺寸:</p>

<pre>model = Sequential()<br/>model.add(Dense(13, input_dim=7, activation='relu'))<br/>model.add(Dense(7, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Defining a functional model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Defining a functional model</h1>

                

            

            

                

<p>A functional model provides more flexibility than a sequential model. For instance, in a functional model, we can easily connect any layer to another layer, whereas, in a sequential model, each layer is in a stack of one above another. A functional model comes in handy when creating complex models, such as directed acyclic graphs, models with multiple input values, multiple output values, and shared layers. Now, we will see how to define a functional model in Keras.</p>

<p>现在，我们将使用<kbd>Dense</kbd>类定义第一个带有<kbd>10</kbd>神经元和<kbd>relu</kbd>激活的全连接层，如下所示:</p>

<pre>input = Input(shape=(2,))</pre>

<p class="mce-root">我们定义了<kbd>layer1</kbd>，但是<kbd>layer1</kbd>的输入来自哪里？我们需要在最后的括号符号中指定对<kbd>layer1</kbd>的输入，如下所示:</p>

<p class="mce-root">我们用<kbd>13</kbd>神经元和<kbd>relu</kbd>激活来定义下一层<kbd>layer2</kbd>。<kbd>layer2</kbd>的输入来自<kbd>layer1</kbd>，所以加在最后的括号里，如下面的代码所示:</p>

<p>现在，我们可以用<kbd>sigmoid</kbd>激活函数定义输出层。输出层的输入来自于<kbd>layer2</kbd>，所以在最后的括号中添加:</p>

<pre>layer1 = Dense(10, activation='relu')</pre>

<p>在定义了所有的层之后，我们使用一个<kbd>Model</kbd>类来定义模型，这里我们需要指定<kbd>inputs</kbd>和<kbd>outputs</kbd>，如下所示:</p>

<pre>layer1 = Dense(10, activation='relu')(input)</pre>

<p>功能模型的完整代码如下所示:</p>

<pre>layer2 = Dense(10, activation='relu')(layer1)</pre>

<p>编译模型</p>

<pre>output = Dense(1, activation='sigmoid')(layer2)</pre>

<p>既然我们已经定义了模型，下一步就是编译它。在这个阶段，我们设置模型应该如何学习。我们在编译模型时定义了三个参数:</p>

<pre>model = Model(inputs=input, outputs=output)</pre>

<p><kbd>optimizer</kbd>参数:这定义了我们想要使用的优化算法；比如梯度下降，在这种情况下。</p>

<pre>input = Input(shape=(2,))<br/>layer1 = Dense(10, activation='relu')(input)<br/>layer2 = Dense(10, activation='relu')(layer1)<br/>output = Dense(1, activation='sigmoid')(layer2)<br/>model = Model(inputs=input, outputs=output)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Compiling the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title"><kbd>loss</kbd>参数:这是我们试图最小化的目标函数；例如，均方误差或交叉熵损失。</h1>

                

            

            

                

<p><kbd>metrics</kbd>参数:这是我们想要用来评估模型性能的度量标准；比如<kbd>accuracy</kbd>。我们还可以指定多个度量。</p>

<ul>

<li>运行以下代码来编译模型:</li>

<li>训练模型</li>

<li>我们定义并编译了模型。现在，我们将训练模型。可以使用<kbd>fit</kbd>功能来训练模型。我们指定我们的特性，<kbd>x</kbd>；<kbd>y</kbd>标签；我们要训练的<kbd>epochs</kbd>的数量；和<kbd>batch_size</kbd>，如下:</li>

</ul>

<p>评估模型</p>

<pre>model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练模型后，我们将在测试集上评估模型:</h1>

                

            

            

                

<p>我们还可以在相同的训练集上评估模型，这将有助于我们了解训练的准确性:</p>

<pre>model.fit(x=data, y=labels, epochs=100, batch_size=10)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Evaluating the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用TensorFlow 2.0进行MNIST数字分类</h1>

                

            

            

                

<p>现在，我们将看看如何使用TensorFlow 2.0执行MNIST手写数字分类。相比TensorFlow 1.x只需要几行代码，据我们了解，TensorFlow 2.0使用Keras作为其高级API我们只需要将<kbd>tf.keras</kbd>添加到Keras代码中。</p>

<pre>model.evaluate(x=data_test,y=labels_test)</pre>

<p>让我们从加载数据集开始:</p>

<pre>model.evaluate(x=data,y=labels)</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>MNIST digit classification using TensorFlow 2.0</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">MNIST digit classification using TensorFlow 2.0</h1>

                

            

            

                

<p>Now, we will see how we can perform MNIST handwritten digit classification, using TensorFlow 2.0. It requires only a few lines of code compared to TensorFlow 1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just need to add <kbd>tf.keras</kbd> to the Keras code.</p>

<p>使用以下代码创建一个训练和测试集:</p>

<pre>mnist = tf.keras.datasets.mnist</pre>

<p class="mce-root">通过将<kbd>x</kbd>的值除以<kbd>x</kbd>的最大值来标准化训练和测试集；也就是<kbd>255.0</kbd>:</p>

<p class="mce-root">按如下方式定义顺序模型:</p>

<p>现在，让我们给模型添加层。我们使用三层网络，其中<kbd>relu</kbd>功能和<kbd>softmax</kbd>位于最后一层:</p>

<pre>(x_train,y_train), (x_test, y_test) = mnist.load_data()</pre>

<p>通过运行以下代码行来编译模型:</p>

<pre>x_train, x_test = tf.cast(x_train/255.0, tf.float32), tf.cast(x_test/255.0, tf.float32)<br/>y_train, y_test = tf.cast(y_train,tf.int64),tf.cast(y_test,tf.int64)</pre>

<p>训练模型:</p>

<pre>model = tf.keras.models.Sequential()</pre>

<p>评估模型:</p>

<pre>model.add(tf.keras.layers.Flatten())<br/>model.add(tf.keras.layers.Dense(256, activation="relu"))<br/>model.add(tf.keras.layers.Dense(128, activation="relu"))<br/>model.add(tf.keras.layers.Dense(10, activation="softmax"))</pre>

<p>就是这样！用Keras API编写代码就是这么简单。</p>

<pre>model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])</pre>

<p>应该用Keras还是TensorFlow？</p>

<pre>model.fit(x_train, y_train, batch_size=32, epochs=10)</pre>

<p>我们了解到TensorFlow 2.0使用Keras作为高级API。使用高级API可以实现快速原型制作。但是当我们想在低层建立一个模型，或者想建立一个高层API没有提供的东西时，我们就不能使用高层API。</p>

<pre>model.evaluate(x_test, y_test)</pre>

<p>除此之外，从零开始编写代码加强了我们对算法的了解，并且比直接潜入高级API更好地帮助我们理解和学习概念。这就是为什么在本书中，我们将使用TensorFlow从头开始编写大部分算法，而不使用Keras等高级API。我们将使用TensorFlow版本1.13.1。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Should we use Keras or TensorFlow?</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>我们从学习TensorFlow和它如何使用计算图开始这一章。我们了解到TensorFlow中的每个计算都由一个计算图来表示，计算图由几个节点和边组成，其中节点是数学运算，比如加法和乘法，边是张量。</p>

<p>我们了解到变量是用来存储值的容器，它们被用作计算图中其他几个操作的输入。后来，我们了解到占位符就像变量一样，我们只定义类型和维度，但不会赋值，占位符的值将在运行时输入。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">接下来，我们学习了TensorBoard，这是TensorFlow的可视化工具，可用于可视化计算图形。它还可以用来绘制各种定量指标和一些中间计算的结果。</h1>

                

            

            

                

<p>我们还学习了急切执行，它更Pythonic化，允许快速原型化。我们知道，与每次执行任何操作都需要构建图形的图形模式不同，急切执行遵循命令式编程范式，任何操作都可以立即执行，而无需创建图形，就像我们在Python中一样。</p>

<p>在下一章，我们将学习梯度下降和梯度下降算法的变体。</p>

<p>Going forward, we learned about TensorBoard, which is TensorFlow's visualization tool and can be used to visualize a computational graph. It can also be used to plot various quantitative metrics and the results of several intermediate calculations.</p>

<p>We also learned about eager execution, which is more Pythonic, and allows for rapid prototyping. We understood that, unlike the graph mode, where we need to construct a graph every time to perform any operations, eager execution follows the imperative programming paradigm, where any operations can be performed immediately, without having to create a graph, just like we do in Python.</p>

<p>问题</p>

<p class="mce-root">通过回答以下问题来评估您对TensorFlow的了解:</p>

<p class="mce-root">定义一个计算图。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Questions</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">什么是会话？</h1>

                

            

            

                

<p>我们如何在TensorFlow中创建会话？</p>

<ol>

<li>变量和占位符有什么区别？</li>

<li>为什么我们需要TensorBoard？</li>

<li>名称范围是什么，是如何创建的？</li>

<li>什么是急切执行？</li>

<li>进一步阅读</li>

<li>你可以在<a href="https://www.tensorflow.org/tutorials">https://www.tensorflow.org/tutorials</a>查看官方文档，了解更多关于TensorFlow的信息。</li>

<li>What is eager execution?</li>

</ol>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Further reading</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:398be676-029b-4423-ae83-eae14f2a5a79" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Further reading</h1>

                

            

            

                

<p>You can learn more about TensorFlow by checking out the official documentation at <a href="https://www.tensorflow.org/tutorials">https://www.tensorflow.org/tutorials</a>.</p>





            



            

        

    </body>



</html></body></html>