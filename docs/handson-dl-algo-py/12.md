

# 九、了解更多关于 GAN 的信息

我们在[第八章](b71eb1cb-af20-41ea-9e3d-26c7d0b956ba.xhtml)、*使用甘生成图像*中学习了什么是**生成对抗网络** ( **甘**)以及不同类型的甘如何用于生成图像。

在这一章中，我们将揭示各种有趣的不同类型的甘。我们已经知道 GANs 可以用来生成新的图像，但是我们无法控制它们生成的图像。例如，如果我们希望我们的 GAN 生成一个具有特定特征的人脸，我们如何将这一信息告诉 GAN？我们不能，因为我们无法控制生成器生成的图像。

为了解决这个问题，我们使用了一种新的 GAN，称为**条件 GAN** ( **CGAN** )，我们可以通过指定想要生成的内容来调节生成器和鉴别器。我们将从理解如何使用 cgan 生成我们感兴趣的图像开始这一章，然后我们学习如何使用 **TensorFlow** 实现 cgan。

然后我们了解了 **InfoGANs** ，它是 CGAN 的一个无监督版本。我们将了解什么是 InfoGANs，它们与 cgan 有何不同，以及我们如何使用 TensorFlow 来实现它们以生成新图像。

然后，我们将学习**周期 GAN**，这是一种非常有趣的 GAN。他们试图学习从一个域中的图像分布到另一个域中的图像分布的映射。例如，为了将灰度图像转换为彩色图像，我们训练 CycleGAN 学习灰度图像和彩色图像之间的映射，这意味着它们学习从一个域映射到另一个域，最棒的是，与其他架构不同，它们甚至不需要配对的数据集。我们将详细调查他们究竟是如何学习这些映射及其架构的。我们将探讨如何实现 CycleGAN 将真实图片转换为绘画。

在本章的最后，我们将探索， **StackGAN** ，它可以将文本描述转换为照片级的逼真图像。我们将通过深入了解 StackGANs 的架构细节来了解它是如何做到这一点的。

在本章中，我们将了解以下内容:

*   条件甘斯
*   使用 CGAN 生成特定数字
*   InfoGAN
*   InfoGAN 的架构
*   使用 TensorFlow 构建 InfoGAN
*   CycleGAN
*   使用 CycleGAN 将图片转换为绘画
*   斯塔克根



# 条件甘斯

我们知道生成器通过学习真实的数据分布来生成新的图像，而鉴别器则考察生成器生成的图像是来自真实的数据分布还是虚假的数据分布。

然而，通过学习真实的数据分布，生成器具有生成新的和有趣的图像的能力。我们无法控制或影响生成器生成的图像。例如，假设我们的生成器正在生成人脸；我们如何告诉生成器生成一个具有某些特征的人脸，比如大眼睛和尖鼻子？

我们不能！因为我们无法控制生成器生成的图像。

为了克服这一点，我们引入了一种称为 **CGAN** 的 GAN 的小型变体，它对发生器和鉴别器都施加了一个条件。这个条件告诉 GAN 我们希望我们的生成器生成什么样的图像。因此，我们的两个组件——鉴别器和发生器——在这种情况下起作用。

让我们考虑一个简单的例子。假设我们正在使用 CGAN 和 MNIST 数据集生成手写数字。让我们假设我们更专注于生成数字 7 而不是其他数字。现在，我们需要将这个条件应用于我们的生成器和鉴别器。我们如何做到这一点？

生成器将噪声![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)作为输入，并生成图像。但是除了![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)，我们还传递了一个额外的输入![](img/92bb425b-a16a-4c8a-a295-243ecfdb386b.png)。这个![](img/9f1aaca1-ad1d-4079-9a27-8f12401d51fe.png)是一个一键编码的类标签。由于我们对生成数字 7 感兴趣，所以我们将第七个索引设置为 1，并将所有其他索引设置为 0，即[0，0，0，0，0，0，1，0，0]。

我们将潜在向量![](img/35a24ed1-1817-4456-ae54-3ab000857bcc.png)和独热编码条件变量![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)连接起来，并将其作为输入传递给生成器。然后，生成器开始生成数字 7。

鉴别器呢？我们知道鉴别器将图像![](img/b867ddbc-f08c-4126-b54a-2141fa745659.png)作为输入，并告诉我们该图像是真图像还是假图像。在 CGAN 中，我们希望鉴别器根据条件进行鉴别，这意味着它必须识别生成的图像是真数字 7 还是假数字 7。因此，除了传递输入![](img/e7df7672-8219-4515-8446-49fa00b78885.png)，我们还通过串联![](img/cb3b4dc9-80cf-4374-8cdb-a5e075850686.png)和![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)将条件变量![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)传递给鉴别器。

如下图所示，我们将![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)和![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)传递给生成器:

![](img/d46bd418-6cb7-416f-acb2-42e463616a17.png)

发电机以 **![](img/cd509d39-205f-4d4d-ad26-bc085d848f0b.png)** 上的信息为条件。类似地，除了将真实和伪造的图像传递给鉴别器，我们还将![](img/4fcaa738-2b08-4797-864f-5b47e6446731.png)传递给鉴别器。因此，发生器产生数字 7，鉴别器学会区分真 7 和假 7。

我们刚刚学习了如何使用 CGAN 生成一个特定的数字，但是 CGAN 的应用并没有到此为止。假设我们需要生成一个具有特定宽度和高度的数字。我们也可以在![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)上施加这个条件，让 GAN 产生任何想要的图像。



# CGAN 的损失函数

您可能已经注意到，我们的普通 GAN 和 CGAN 之间没有太大的区别，只是在 CGAN 中，我们连接了额外的输入，即调理变量![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)与发生器和鉴频器的输入。因此，发生器和鉴频器的损耗函数与普通 GAN 相同，只是它以![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)为条件。

因此，鉴频器的损耗函数如下:

![](img/e6a23fb2-a1aa-4d68-a8de-abf2f794ee16.png)

发电机的损耗函数由下式给出:

![](img/3daefbdb-8cb3-4431-924b-2856bf8d53ad.png)

CGAN 通过使用梯度下降最小化损失函数来学习。



# 使用 CGAN 生成特定的手写数字

我们刚刚了解了 CGAN 的工作原理和架构。为了加强我们的理解，现在我们将学习如何在 TensorFlow 中实现 CGAN 来生成特定手写数字(比如数字 7)的图像。

首先，加载所需的库:

```
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
tf.logging.set_verbosity(tf.logging.ERROR)
tf.reset_default_graph()

import matplotlib.pyplot as plt
%matplotlib inline

from IPython import display
```

加载 MNIST 数据集:

```
data = input_data.read_data_sets("data/mnist",one_hot=True)
```



# 定义生成器

生成器*G*将噪声![](img/3fba73ea-bd4f-4352-8535-f23286d101b8.png)和条件变量![](img/2076de9c-04e1-47b2-93f1-8d9cd520e9a0.png)作为输入并返回图像。我们将发电机定义为一个简单的双层前馈网络:

```
def generator(z, c,reuse=False):
    with tf.variable_scope('generator', reuse=reuse):
```

初始化权重:

```
            w_init = tf.contrib.layers.xavier_initializer()
```

连接噪声![](img/c017b2d2-3c5d-4fe5-a6de-29cf18af21b0.png)和条件变量![](img/c5a5b62a-728c-4a2b-a938-24c2b58fbb0c.png):

```
            inputs = tf.concat([z, c], 1)
```

定义第一层:

```
            dense1 = tf.layers.dense(inputs, 128, kernel_initializer=w_init)
            relu1 = tf.nn.relu(dense1)
```

定义第二层并用`tanh`激活函数计算输出:

```
            logits = tf.layers.dense(relu1, 784, kernel_initializer=w_init)
            output = tf.nn.tanh(logits)

            return output
```



# 定义鉴别器

我们知道鉴别器，![](img/92dd0d61-61e9-4979-bef8-5fa25b08d846.png)，返回概率；也就是说，它会告诉我们给定图像是真实的概率。除了输入图像![](img/21e9ebf0-fbab-4938-9890-29594707d2ba.png)，它还将条件变量![](img/c9b15d4e-d63d-4776-bbce-9bf5d1544b81.png)作为输入。我们还将鉴别器定义为一个简单的双层前馈网络:

```
def discriminator(x, c, reuse=False):
    with tf.variable_scope('discriminator', reuse=reuse):
```

初始化权重:

```
            w_init = tf.contrib.layers.xavier_initializer()
```

连接输入，![](img/6b91c2b8-d0ad-4fdd-93c7-2027143d3a78.png)和条件变量，![](img/f16638c8-eebc-417a-8500-983bb5984248.png):

```
            inputs = tf.concat([x, c], 1)
```

定义第一层:

```
            dense1 = tf.layers.dense(inputs, 128, kernel_initializer=w_init)
            relu1 = tf.nn.relu(dense1)
```

定义第二层并用`sigmoid`激活函数计算输出:

```
             logits = tf.layers.dense(relu1, 1, kernel_initializer=w_init)
             output = tf.nn.sigmoid(logits)

             return output
```

定义输入的占位符![](img/6b8eef0e-1ec8-4195-8e3b-acb7878fc080.png)、条件变量![](img/03fe94eb-54c9-4a6c-ac36-6613626a2b9d.png)和噪声![](img/aa946952-416d-46e4-8f61-b4525a754412.png):

```
x = tf.placeholder(tf.float32, shape=(None, 784))
c = tf.placeholder(tf.float32, shape=(None, 10))
z = tf.placeholder(tf.float32, shape=(None, 100))
```



# 启动甘！

首先，我们将噪声![](img/7a9ce465-36be-4928-a62e-93fad76038e5.png)和条件变量![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png)提供给生成器，它将输出假图像，即![](img/718a88c0-6582-4fe1-bd16-212f9204db72.png):

```
fake_x = generator(z, c)
```

现在，我们将真实图像![](img/f8310e0e-b680-4cf7-862f-050b010071da.png)和条件变量![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png)一起提供给鉴别器![](img/ce7b8ed9-614f-4ddc-b414-da102cdc0e64.png)，并获得它们为真实的概率:

```
D_logits_real = discriminator(x,c)
```

类似地，我们将假图像`fake_x`和条件变量![](img/6adb17cf-a37e-4c68-9ecf-c9da309fa5fb.png)提供给鉴别器![](img/c4d9de79-ac6a-45f8-afa6-c46ffd2933ab.png)，并获得它们为真的概率:

```
D_logits_fake = discriminator(fake_x, c, reuse=True)
```



# 计算损失函数

现在我们来看看如何计算损失函数。除了我们添加了一个条件变量之外，它与传统的 GAN 基本相同。



# 鉴频器损耗

鉴频器损耗计算如下:

![](img/6c621f81-f100-4980-9d7a-7d9012be17e1.png)

首先，我们将实现第一个术语，即![](img/c5c53cc8-1af4-4ec4-b1df-779b67a80b69.png):

```
D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real,
               labels=tf.ones_like(D_logits_real)))
```

现在我们将实现第二个术语，![](img/4c06f526-38fe-4aeb-9332-e3ace5521540.png):

```
D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,  
               labels=tf.zeros_like(D_logits_fake)))
```

最终损失可以写成:

```
D_loss = D_loss_real + D_loss_fake
```



# 发电机损耗

发电机损耗如下所示:

![](img/b294fdb0-97f2-463a-82aa-cf250794a4bf.png)

发电机损耗可表示为:

```
G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake,
               labels=tf.ones_like(D_logits_fake)))
```



# 优化损失

我们需要优化我们的生成器和鉴别器。因此，我们将鉴频器和发生器的参数分别记为`theta_D`和`theta_G`:

```
training_vars = tf.trainable_variables()
theta_D = [var for var in training_vars if var.name.startswith('discriminator')]
theta_G = [var for var in training_vars if var.name.startswith('generator')]
```

使用 Adam 优化器优化损失:

```
learning_rate = 0.001

D_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(D_loss,         
                 var_list=theta_D)
G_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(G_loss, 
                       var_list=theta_G)
```



# 开始训练 CGAN

启动 TensorFlow 会话并初始化变量:

```
session = tf.InteractiveSession()
tf.global_variables_initializer().run()
```

定义`batch_size:`

```
batch_size = 128
```

定义时期数和类别数:

```
num_epochs = 500
num_classes = 10
```

定义图像和标签:

```
images = (data.train.images)
labels = data.train.labels
```



# 生成手写数字，7

我们设置数字(`label`)生成为`7`:

```
label_to_generate = 7
onehot = np.eye(10)
```

设置迭代次数:

```
for epoch in range(num_epochs):

    for i in range(len(images) // batch_size):
```

基于批量的样本图像:

```
        batch_image = images[i * batch_size:(i + 1) * batch_size]
```

采样条件，即我们要生成的数字:

```
        batch_c = labels[i * batch_size:(i + 1) * batch_size]
```

样本噪声:

```
        batch_noise = np.random.normal(0, 1, (batch_size, 100))
```

训练发电机并计算发电机损耗:

```
        generator_loss, _ = session.run([D_loss, D_optimizer], {x: batch_image, c: batch_c, z: batch_noise})    
```

训练鉴频器并计算鉴频器损耗:

```
        discriminator_loss, _ = session.run([G_loss, G_optimizer], {x: batch_image, c: batch_c, z: batch_noise})
```

随机采样噪声:

```
    noise = np.random.rand(1,100)
```

选择我们要生成的数字:

```
    gen_label = np.array([[label_to_generate]]).reshape(-1)
```

将所选数字转换为独热编码向量:

```
    one_hot_targets = np.eye(num_classes)[gen_label]
```

将噪声和一个热编码条件馈送到生成器，并生成伪图像:

```
    _fake_x = session.run(fake_x, {z: noise, c: one_hot_targets})
    _fake_x = _fake_x.reshape(28,28)
```

打印发生器和鉴别器的损耗，并绘制发生器图像:

```
    print("Epoch: {},Discriminator Loss:{}, Generator Loss: {}".format(epoch,discriminator_loss,generator_loss))

    #plot the generated image
    display.clear_output(wait=True)
    plt.imshow(_fake_x) 
    plt.show()
```

如下图所示，生成器现在已经学会生成数字 7，而不是随机生成其他数字:

![](img/4816e841-0f8b-4f5c-829a-c9c066a3bca1.png)



# 了解 InfoGAN

InfoGAN 是 CGAN 的无人监管版本。在 CGAN 中，我们学习了如何调节发生器和鉴别器，以生成我们想要的图像。但是，当数据集中没有标签时，我们该如何做呢？假设我们有一个没有标签的 MNIST 数据集；我们如何告诉生成器生成我们感兴趣的特定图像？由于数据集是未标记的，我们甚至不知道数据集中存在的类。

我们知道生成器使用噪声 *z* 作为输入并生成图像。生成器将关于图像的所有必要信息封装在 *z* 中，它被称为**纠缠表示**。它基本上是学习图像在 *z* 中的语义表示。如果我们能解开这个向量，那么我们就能发现我们图像的有趣特征。

所以，我们将把这个 *z* 一分为二:

*   普通噪音
*   代码 *c*

代码是什么？代码 *c* 基本上是可解释的不清楚的信息。假设我们有 MNIST 数据，那么，代码 *c1* 表示数字标签，代码 *c2* 表示宽度， *c3* 表示数字的笔画，等等。我们统称他们为 *c* 。

既然有了 *z* 和 *c* ，如何学习有意义的代码 *c* ？我们能用生成器生成的图像学习有意义的代码吗？假设一个生成器生成了 7 的图像。现在我们可以说代码 *c1 是 7* ，因为我们知道 c1 意味着数字标签。

但是既然代码可以表示任何东西，比如标签、手指的宽度、笔画、旋转角度等等——我们怎么才能知道我们想要什么呢？代码 *c* 将基于先验的选择而被学习。例如，如果我们为 *c* 选择一个多项式先验，那么我们的 InfoGAN 可能会为 *c* 分配一个数字标签。比方说，我们分配一个高斯先验，然后它可能分配一个旋转角度，等等。我们也可以有一个以上的先验。

先前 *c* 的分布可以是任何东西。InfoGAN 根据分布分配不同的属性。在 InfoGAN 中，代码 *c* 是根据生成器输出自动推断出来的，这与 CGAN 不同，在 CGAN 中，我们显式指定了 *c* 。

简而言之，我们是根据发电机输出![](img/90798245-af2f-48c9-97b5-4f0e5a14b3e3.png)来推断![](img/0c7149c6-3be8-4775-b309-45e3be1b8545.png)。但是我们到底是如何推断的呢？我们使用信息论中的一个概念，叫做**互信息**。



# 交互信息

两个随机变量之间的互信息告诉我们可以从一个随机变量通过另一个随机变量获得的信息量。两个随机变量 *x* 和 *y* 之间的互信息可以给出如下:

![](img/cd64a09b-fa5f-407f-b4e1-a0b07db0e49a.png)

基本上就是 *y* 的熵和给定 *x* 的 *y* 的条件熵之差。

代码![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)和发生器输出![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)之间的交互信息告诉我们通过![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)可以获得多少关于![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)的信息。如果互信息 *c* 和![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)为高，那么我们可以说知道发电机输出有助于我们推断 *c* 。但是如果互信息很低，那么我们不能从发电机输出中推断出 *c* 。我们的目标是最大化相互信息。

代码![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)和发生器输出![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)之间的相互信息如下所示:

![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)

让我们看看公式的元素:

*   ![](img/77b4d2e1-3669-4218-9056-fcdd47af1e82.png)是代码的熵
*   ![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png)是给定发生器输出的代码 c 的条件熵![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)

但问题是，我们如何计算![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png)？因为要计算这个值，我们需要知道后验概率，![](img/ea74a871-385f-4fd1-a422-29c2dbe4076b.png)，这个我们还不知道。所以，我们用辅助分布来估计后验概率，![](img/8197d00c-71c7-43b0-bb9f-bde673eeb1d2.png):

![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)

比方说![](img/7cf8508c-5680-44f2-9436-4809e8b458dd.png)，那么我们可以推导出如下互信息:

![](img/77307ccf-295e-499c-8c4f-3413deffc80f.png)

因此，我们可以说:

![](img/504b15ff-8dbe-4e7b-8f33-f85da19dc791.png)

最大化互信息，![](img/04461b06-9679-4dfd-8a03-3a8c0dbce1b7.png)基本上意味着在给定生成的输出的情况下，我们最大化关于 *c* 的知识，也就是说，通过一个变量了解另一个变量。



# 信息根的体系结构

好吧。这到底是怎么回事？我们为什么要这么做？简单来说，我们将发电机的输入分成两部分: *z* 和 *c* 。由于 *z* 和 *c* 都用于生成图像，它们捕捉图像的语义。代码 *c* 给了我们关于图像的可解释的清晰信息。因此，给定发电机输出，我们试图找到 *c* 。但是，由于我们不知道后验概率![](img/1a09fb91-ad1e-4640-9ee1-7858444eeb75.png)，所以我们不能轻易做到这一点，所以我们使用一个辅助分布![](img/59e766d6-4103-4d50-a928-6a24384de828.png)来学习 *c* 。

这个辅助分布基本上是另一个神经网络；我们姑且称这个网络为 *Q* 网络。 *Q* 网络的作用是在给定一个发生器图像 *x* 并由![](img/8bdc985a-0401-4abe-88ab-4a6af6ac2097.png)给出的情况下，预测 *c* 的可能性。

首先，我们从一个先验样本 *c* ， *p(c)* 。然后，我们将 *c* 和 *z* 连接起来，并将它们馈送给生成器。接下来，我们将![](img/36423d4e-7ade-4f6e-87b3-b71551f5b61c.png)给出的生成器结果馈送给鉴别器。我们知道鉴别器的作用是输出给定图像真实的概率。因此，它获取生成器生成的图像并返回概率。此外， *Q* 网络获取生成的图像，并返回给定生成的图像的 *c* 的估计值。

鉴别器 *D* 和 *Q* 网络都接收发生器图像并返回输出，因此它们共享一些层。由于它们共享一些层，我们将 *Q* 网络连接到鉴别器，如下图所示:

![](img/cbc47881-40df-4e43-a508-96ffd57dfa4f.png)

因此，鉴别器返回两个输出:

*   图像真实的概率
*   *c、*的估计值，即给定发生器图像时 *c* 的概率

我们在损失函数中加入了互信息项。

因此，鉴频器的损耗函数为:

![](img/2a5568d6-8ba3-495a-a04d-3adf366f9206.png)

发电机的损耗函数由下式给出:

![](img/0c69ce3b-afea-432e-a4b8-e52ac1ee4a26.png)

前面的两个等式意味着我们在最大化互信息的同时最小化 GAN 的损耗。还在困惑 InfoGANs？放心吧！我们将通过在 TensorFlow 中实现 InfoGANs 来逐步更好地了解它们。



# 在 TensorFlow 中构建 InfoGAN

通过在 TensorFlow 中一步步实现 InfoGANs，我们会更好地理解它们。我们将使用 MNIST 数据集，并了解 InfoGAN 如何基于生成器输出自动推断代码![](img/285c7e36-2c3b-451b-9d4c-ec3e14821693.png)。我们建立一个信息 DCGAN 也就是说，我们在生成器和鉴别器中使用卷积层，而不是普通的神经网络。

首先，我们将导入所有必需的库:

```
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
tf.logging.set_verbosity(tf.logging.ERROR)

import matplotlib.pyplot as plt
%matplotlib inline
```

加载 MNIST 数据集:

```
data = input_data.read_data_sets("data/mnist",one_hot=True)
```

定义泄漏 ReLU 激活函数:

```
def lrelu(X, leak=0.2):
    f1 = 0.5 * (1 + leak)
    f2 = 0.5 * (1 - leak)
    return f1 * X + f2 * tf.abs(X)
```



# 定义生成器

生成器![](img/be85bd9d-7482-49d4-a48e-ec8024aeea1c.png)，它将噪声![](img/315bac8e-4528-42d4-b2db-971178e21db7.png)和变量![](img/8562e754-dcfe-4432-b70f-d9825a801f78.png)作为输入，并返回图像。我们没有在生成器中使用完全连接的层，而是使用解卷积网络，就像我们研究 DCGANs 时一样:

```
def generator(c, z,reuse=None):
```

首先，连接噪声、 *z、*和变量、![](img/e6c9289c-bc98-4d2e-9085-3d0dad48a5b8.png):

```
    input_combined = tf.concat([c, z], axis=1)
```

定义第一层，这是一个具有批量标准化和 ReLU 激活的完全连接的层:

```
    fuly_connected1 = tf.layers.dense(input_combined, 1024)
    batch_norm1 = tf.layers.batch_normalization(fuly_connected1, training=is_train)
    relu1 = tf.nn.relu(batch_norm1)
```

定义第二层，它也完全与批量标准化和 ReLU 激活相关:

```
    fully_connected2 = tf.layers.dense(relu1, 7 * 7 * 128)
    batch_norm2 = tf.layers.batch_normalization(fully_connected2, training=is_train)
    relu2 = tf.nn.relu(batch_norm2)
```

展平第二层的结果:

```
    relu_flat = tf.reshape(relu2, [batch_size, 7, 7, 128])
```

第三层由**反褶积**组成；即转置卷积运算，随后是批量归一化和 ReLU 激活:

```
    deconv1 = tf.layers.conv2d_transpose(relu_flat, 
                                          filters=64,
                                          kernel_size=4,
                                          strides=2,
                                          padding='same',
                                          activation=None)
    batch_norm3 = tf.layers.batch_normalization(deconv1, training=is_train)
    relu3 = tf.nn.relu(batch_norm3)
```

第四层是另一个转置卷积运算:

```
    deconv2 = tf.layers.conv2d_transpose(relu3, 
                                          filters=1,
                                          kernel_size=4,
                                          strides=2,
                                          padding='same',
                                          activation=None)
```

将 sigmoid 函数应用于第四层的结果，并获得输出:

```
    output = tf.nn.sigmoid(deconv2) 

    return output
```



# 定义鉴别器

我们了解到鉴别器![](img/ef5d0704-9421-4960-99c5-7a189b57d943.png)和 *Q* 网络都接收发生器镜像并返回输出，因此它们共享一些层。由于它们共享一些层，我们将 *Q* 网络连接到鉴别器，正如我们在 InfoGAN 架构中所学的。正如我们在 DCGAN 的鉴频器中了解到的那样，我们使用卷积网络，而不是在鉴频器中使用全连接层:

```
def discriminator(x,reuse=None):
```

定义第一层，该层执行卷积运算，随后是泄漏 ReLU 激活:

```
    conv1 = tf.layers.conv2d(x, 
                             filters=64, 
                             kernel_size=4,
                             strides=2,
                             padding='same',
                             kernel_initializer=tf.contrib.layers.xavier_initializer(),
                             activation=None)
    lrelu1 = lrelu(conv1, 0.2)
```

我们还在第二层中执行卷积运算，随后是批量归一化和泄漏 ReLU 激活:

```
    conv2 = tf.layers.conv2d(lrelu1, 
                             filters=128,
                             kernel_size=4,
                             strides=2,
                             padding='same',
                             kernel_initializer=tf.contrib.layers.xavier_initializer(),
                             activation=None)
    batch_norm2 = tf.layers.batch_normalization(conv2, training=is_train)
    lrelu2 = lrelu(batch_norm2, 0.2)
```

展平第二层的结果:

```
   lrelu2_flat = tf.reshape(lrelu2, [batch_size, -1])
```

将展平的结果提供给完全连接的层，这是第三层，随后是批量标准化和泄漏 ReLU 激活:

```
    full_connected = tf.layers.dense(lrelu2_flat, 
                          units=1024, 
                          activation=None)
    batch_norm_3 = tf.layers.batch_normalization(full_connected, training=is_train)
    lrelu3 = lrelu(batch_norm_3, 0.2)
```

计算鉴频器输出:

```
    d_logits = tf.layers.dense(lrelu3, units=1, activation=None)
```

正如我们所知，我们将 *Q* 网络连接到鉴频器。定义将鉴频器的最后一层作为输入的 *Q* 网络的第一层:

```
    full_connected_2 = tf.layers.dense(lrelu3, 
                                     units=128, 
                                     activation=None)

    batch_norm_4 = tf.layers.batch_normalization(full_connected_2, training=is_train)
    lrelu4 = lrelu(batch_norm_4, 0.2)
```

定义第二层 *Q* 网络:

```
    q_net_latent = tf.layers.dense(lrelu4, 
                                    units=74, 
                                    activation=None)
```

估计值 *c* :

```
    q_latents_categoricals_raw = q_net_latent[:,0:10]

    c_estimates = tf.nn.softmax(q_latents_categoricals_raw, dim=1)
```

返回鉴别器`logits`和估算的 *c* 值作为输出:

```
    return d_logits, c_estimates
```



# 定义输入占位符

现在我们定义输入的占位符![](img/1198ce65-9257-470f-9ebe-fa1dd5af2b41.png)、噪声![](img/47af0b1c-6f7b-4b5e-8963-0f17cbdcca52.png)和代码![](img/7f206bce-181b-4732-a826-72a1001e9921.png):

```
batch_size = 64
input_shape = [batch_size, 28,28,1]

x = tf.placeholder(tf.float32, input_shape)
z = tf.placeholder(tf.float32, [batch_size, 64])
c = tf.placeholder(tf.float32, [batch_size, 10])

is_train = tf.placeholder(tf.bool)
```



# 启动 GAN

首先，我们将噪声![](img/541c0403-7faf-4e90-904f-3fabb2287695.png)和代码![](img/19dc097f-0ef5-4373-9c71-2c13948ea07a.png)馈送给生成器，它将根据等式![](img/d4bb1f15-c827-442a-b4a3-2e2706b728a9.png)输出假图像:

```
fake_x = generator(c, z)
```

现在，我们将真实图像![](img/c3d7298c-8e7f-4dec-a5ff-4c36a29078c8.png)提供给鉴别器![](img/19be0dee-d6ea-4c21-a901-3acd2320a789.png)，并获得图像真实的概率。与此同时，我们也获得了对真实图像的![](img/235ed536-2d0f-48d3-88b9-3013d10a46d1.png)的估计:

```
D_logits_real, c_posterior_real = discriminator(x)
```

类似地，我们将假图像提供给鉴别器，得到图像是真实图像的概率以及假图像的估计值![](img/5e52ecb7-3193-45c3-b47f-1c66192759cb.png):

```
D_logits_fake, c_posterior_fake = discriminator(fake_x,reuse=True)
```



# 计算损失函数

现在我们来看看如何计算损失函数。



# 鉴频器损耗

鉴频器损耗计算如下:

![](img/4835bff6-b8a1-4f22-b460-2011ba5ed03d.png)

由于 InfoGAN 的鉴频器损耗与 CGAN 相同，因此实现鉴频器损耗与我们在 CGAN 部分所学的相同:

```
#real loss
D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real, 
 labels=tf.ones(dtype=tf.float32, shape=[batch_size, 1])))

#fake loss
D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, 
 labels=tf.zeros(dtype=tf.float32, shape=[batch_size, 1])))

#final discriminator loss
D_loss = D_loss_real + D_loss_fake
```



# 发电机损耗

发电机的损耗函数如下所示:

![](img/82e62838-ebe0-4eab-9f5b-823ec4f35519.png)

发电机损耗实现为:

```
G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, 
 labels=tf.ones(dtype=tf.float32, shape=[batch_size, 1])))
```



# 交互信息

我们从鉴别器和发电机损耗中减去**互信息**。因此，鉴频器和发生器的最终损耗函数如下所示:

![](img/60d35e00-184e-445d-9314-985adea697e2.png)

![](img/6969ec81-922a-4d37-a3c4-d395a2bee81a.png)

交互信息可以计算如下:

![](img/288708fe-7ab4-4eb1-84b2-a7b46b2eb6eb.png)

首先，我们为![](img/3db44084-161b-4594-95d0-a12b9a741445.png)定义一个先验:

```
c_prior = 0.10 * tf.ones(dtype=tf.float32, shape=[batch_size, 10])
```

![](img/db8e2ca2-24ae-4c99-aeee-2fd273971ec8.png)的熵表示为![](img/2bbf0f0f-c47b-4f20-aaa0-c2dfcb91e0ce.png)。我们知道熵的计算方法是![](img/b02529c2-01a4-4ae3-ad88-d9ac0311d711.png):

```
entropy_of_c = tf.reduce_mean(-tf.reduce_sum(c * tf.log(tf.clip_by_value(c_prior, 1e-12, 1.0)),axis=-1))
```

给定![](img/84c1b4e1-f67d-4298-8f91-860d6ec8ac2f.png)时![](img/00a39cdd-ada4-44a6-a583-2dcf3de42643.png)的条件熵为![](img/68c0e69b-b302-47f4-bb8d-ba72a7fefaad.png)。条件熵的代码如下:

```
log_q_c_given_x = tf.reduce_mean(tf.reduce_sum(c * tf.log(tf.clip_by_value(c_posterior_fake, 1e-12, 1.0)), axis=-1))
```

互信息给定为![](img/fd79fef3-96a8-4c96-94f5-3d0b50ce42e7.png):

```
mutual_information = entropy_of_c + log_q_c_given_x
```

鉴频器和发生器的最终损耗由下式给出:

```
D_loss = D_loss - mutual_information
G_loss = G_loss - mutual_information
```



# 优化损失

现在我们需要优化我们的生成器和鉴别器。因此，我们将鉴频器和发生器的参数分别收集为![](img/ae3bf55a-b765-41d3-8a7f-2040f62b651d.png)和![](img/00357b65-480a-4f02-8551-88c1d95a4c81.png):

```
training_vars = tf.trainable_variables()

theta_D = [var for var in training_vars if 'discriminator' in var.name]
theta_G = [var for var in training_vars if 'generator' in var.name]
```

使用 Adam 优化器优化损失:

```
learning_rate = 0.001

D_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss,var_list = theta_D)
G_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list = theta_G)
```



# 开始训练

定义批次大小和历元数，并初始化所有 TensorFlow 变量:

```
num_epochs = 100
session = tf.InteractiveSession()
session.run(tf.global_variables_initializer())
```

定义可视化结果的辅助函数:

```
def plot(c, x):

    c_ = np.argmax(c, 1)

    sort_indices = np.argsort(c_, 0)

    x_reshape = np.reshape(x[sort_indices], [batch_size, 28, 28])

    x_reshape = np.reshape( np.expand_dims(x_reshape, axis=0), [4, (batch_size // 4), 28, 28])

    values = []

    for i in range(0,4):
        row = np.concatenate( [x_reshape[i,j,:,:] for j in range(0,(batch_size // 4))], axis=1)
        values.append(row)

    return np.concatenate(values, axis=0)
```



# 生成手写数字

开始训练并生成图像。对于每一次`100`迭代，我们打印由生成器生成的图像:

```
onehot = np.eye(10)

for epoch in range(num_epochs):

    for i in range(0, data.train.num_examples // batch_size):
```

对图像进行采样:

```
        x_batch, _ = data.train.next_batch(batch_size)
        x_batch = np.reshape(x_batch, (batch_size, 28, 28, 1))
```

采样 *c* 的值:

```
        c_ = np.random.randint(low=0, high=10, size=(batch_size,))
        c_one_hot = onehot[c_]
```

样本噪声 *z* :

```
        z_batch = np.random.uniform(low=-1.0, high=1.0, size=(batch_size,64))
```

优化发生器和鉴频器的损耗:

```
        feed_dict={x: x_batch, c: c_one_hot, z: z_batch, is_train: True}

        _ = session.run(D_optimizer, feed_dict=feed_dict)
        _ = session.run(G_optimizer, feed_dict=feed_dict)
```

每 100 次^(迭代打印一次发生器图像:)

```

        if i % 100 == 0:

            discriminator_loss = D_loss.eval(feed_dict)
            generator_loss = G_loss.eval(feed_dict)

            _fake_x = fake_x.eval(feed_dict)

            print("Epoch: {}, iteration: {}, Discriminator Loss:{}, Generator Loss: {}".format(epoch,i,discriminator_loss,generator_loss))
            plt.imshow(plot(c_one_hot, _fake_x))
            plt.show() 
```

我们可以看到生成器是如何在每次迭代中进化并生成更好的数字的:

![](img/302b0cac-d211-4e5c-b999-7fc25215d834.png)



# 使用 CycleGAN 翻译图像

我们已经学习了几种类型的 gan，它们的应用是无穷无尽的。我们已经看到生成器如何学习真实数据的分布并生成新的真实样本。我们现在将看到一种真正不同且非常创新的 GAN 类型，称为 **CycleGAN** 。

与其他 GAN 不同，CycleGAN 将数据从一个域映射到另一个域，这意味着我们在这里试图了解从一个域的图像分布到另一个域的图像分布的映射。简单来说，我们把图像从一个领域翻译到另一个领域。

这是什么意思？假设我们想要将灰度图像转换成彩色图像。灰度图像是一个域，彩色图像是另一个域。CycleGAN 学习这两个域之间的映射，并在它们之间进行翻译。这意味着给定灰度图像，CycleGAN 将图像转换成彩色图像。

CycleGANs 的应用有很多，例如将真实照片转换为艺术图片、季节转换、照片增强等等。如下图所示，您可以看到 CycleGAN 如何在不同的域之间转换图像:

![](img/24bfb704-2d29-46b4-ad20-2bfbb147825c.png)

但是 CycleGANs 有什么特别之处呢？这是他们在没有任何配对例子的情况下将图像从一个领域转换到另一个领域的能力。假设我们正在将照片(源)转换为绘画(目标)。在一个普通的图像到图像的翻译中，我们如何做到这一点？我们通过成对收集一些照片及其对应的绘画来准备训练数据，如下图所示:

![](img/f0f516ad-affa-420a-9e51-175f6100a9b8.png)

为每个用例收集这些配对的数据点是一项昂贵的任务，我们可能没有很多记录或配对。这就是 CycleGAN 的最大优势所在。它不要求数据成对对齐。从照片转换成画，我们只需要一堆照片和一堆画。它们不必相互映射或对齐。

如下图，我们一栏有些照片，另一栏有些画；如你所见，它们没有配对。它们是完全不同的图像:

![](img/caffee85-96ab-46a7-aef3-6779d717a2cf.png)

因此，要将图像从任何源域转换到目标域，我们只需要两个域中的一组图像，而不必配对。现在让我们看看它们是如何工作的，以及它们是如何学习源域和目标域之间的映射的。

与其他 GAN 不同，CycleGAN 由两个发生器和两个鉴别器组成。让我们用![](img/e6cbd39c-20ee-4b2e-96c5-b068afd8734d.png)表示源域中的图像，用![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)表示目标域中的图像。我们需要学习![](img/519f9f4e-65c0-4e32-a83c-9bf20af36f52.png)和![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)之间的映射。

假设我们正在学习将一幅真实的图片![](img/60960d1b-d4e6-4b9c-87e1-8ee02750cd75.png)转换成一幅画![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)，如下图所示:

![](img/57884722-f016-4486-adea-5228202949a5.png)



# 发电机的作用

我们有两个发电机，![](img/f2387305-924d-40a4-9d6a-3a575d4c4a91.png)和![](img/f26d3b48-e48a-47e2-9b52-3b893a2ca07e.png)。![](img/4ae0c2af-662a-4c7c-8d91-3049e76f238e.png)的作用是学习从![](img/c74866d7-29b8-4604-b4cc-282c077598f3.png)到![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)的映射。如上所述，![](img/884f31f8-643c-4b27-845f-0b4d6996a1b5.png)的作用是学习将照片翻译成画作，如下图所示:

![](img/591401f1-c8e0-4d69-ba80-a805d400bbd5.png)

它试图生成一个假的目标图像，这意味着它将源图像![](img/39408d7b-f87b-4bd0-b3ed-d9f192d9d5cd.png)作为输入，并生成一个假的目标图像![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png):

![](img/1a62a24b-575c-4bf3-8cb5-5aaecba7e7e3.png)

生成器![](img/1b2e1c2d-6249-4a04-b264-4ff7d01d97ff.png)的作用是学习从![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)到![](img/a4872b2a-894e-4f82-9517-fbf12be37e3a.png)的映射，学习从绘画翻译成真实的图片，如下图所示:

![](img/f2572f4c-4e50-405a-aff3-7f8b4eeb3799.png)

它试图生成一个假的源图像，这意味着它将目标图像![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)作为输入，并生成一个假的源图像![](img/a92caa9a-5f73-4ac8-b8cc-b1d5fa0df87c.png):

![](img/545b880a-c12b-48db-95a9-66280805e4ef.png)



# 歧视者的角色

类似于生成器，我们有两个鉴别器，![](img/80d3f113-745d-4b0f-b80f-be4a1819cd4f.png)和![](img/96eaeabb-208c-4166-b365-7573fc3e077f.png)。鉴别器![](img/ff3805fa-fe28-45ba-91b5-e4a966d8e749.png)的作用是鉴别真正的源图像![](img/155da74d-cd57-4190-b1f6-a24d9214df6a.png)和假的源图像![](img/24043c10-de75-4b46-bba4-dec621e86dc9.png)。我们知道假的源图像是由生成器![](img/d7a67cf6-df09-41fb-8099-9b079109065d.png)生成的。

给定一幅图像给鉴别器![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png)，它返回该图像是真实源图像的概率:

![](img/e2370055-f57b-4877-9ee3-222b56144bda.png)

下图显示了鉴别器![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png)，如您所见，它将生成器 F 生成的真实源图像 x 和伪源图像作为输入，并返回图像是真实源图像的概率:

![](img/61193748-e404-42e8-bc74-7a5fe90a4e30.png)

鉴别器![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)的作用是鉴别真实目标图像![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)和虚假目标图像![](img/e9cf6ebe-aea1-4a4f-a472-11f5fd14d49f.png)。我们知道假目标图像是由生成器生成的，![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)。给鉴别器![](img/88040175-da0f-4cfa-ba2e-3f20aaa8e923.png)一个图像，它返回图像是真实目标图像的概率:

![](img/927703eb-3676-40a3-afd4-5c1640434ff7.png)

下图显示了鉴别器![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)，如您所见，它将真实目标图像![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)和生成器生成的虚假目标图像![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)作为输入，并返回图像为真实目标图像的概率:

![](img/6c26c935-7df9-4449-9337-db63fb930635.png)



# 损失函数

在 CycleGANs 中，我们有两个生成器和两个鉴别器。生成器学习将图像从一个域翻译到另一个域，鉴别器尝试在翻译的图像之间进行鉴别。

因此，我们可以说鉴频器![](img/df0890f7-556a-4a6b-acde-ba0debf5b68d.png)的损耗函数可以表示如下:

![](img/447e0f80-e9ca-4e2a-b5df-31ee82a9250e.png)

类似地，鉴频器![](img/dbc8900b-9e03-420c-b703-d61ecdabd2cb.png)的损耗函数可以表示如下:

![](img/9d48690d-01f6-42fc-a996-5c8e1f47103e.png)

发电机![](img/171a85e6-72fc-44ca-813e-50e907dd78aa.png)的损失函数可表示如下:

![](img/e50dae88-b189-429d-9e31-ef711f415e4a.png)

发电机![](img/5666a423-e13a-481e-9f41-42277b0ee5b4.png)的损失函数可给出如下:

![](img/6b7edf58-4bda-435f-b4e3-87cdfd384ec2.png)

总的来说，最终损失可以写成如下形式:

![](img/cabb8433-6bad-4d41-95ce-c60c6ab50bff.png)



# 循环一致性损失

对抗损失本身并不能确保图像的正确映射。例如，生成器可以将来自源域的图像映射到可以匹配目标分布的目标域中的图像的随机排列。

因此，为了避免这种情况，我们引入一种称为**周期一致 lo**ss 的额外损耗。它强制两个发生器 *G* 和 *F* 保持周期一致。

让我们回忆一下发电机的功能:

*   **发生器*G*:将 *x* 转换为 *y***
*   **发生器*F*:将 *y* 转换为 *x***

我们知道生成器 *G* 获取源图像 *x* 并将其转换为假目标图像 *y* 。现在，如果我们将这个生成的假目标图像 *y* 提供给生成器 *F* ，它必须返回原始源图像 *x* 。很困惑，对吧？

看下图；我们有一个源图像， *x* 。首先，我们将这个图像提供给生成器 *G* ，它返回假的目标图像。现在我们获取这个假的目标图像， *y* ，并将其馈送给生成器 *F* ，它必须返回原始的源图像:

![](img/2acdf8f9-b887-4af7-a557-2ecf88bb42fe.png)

上述等式可以表示如下:

![](img/16f10554-ff0c-45c7-bb4e-726914dd8b4f.png)

这被称为**前向一致性丢失**，可以表示如下:

![](img/b3cde238-f5f1-4b51-ab2e-45b3b7e229ec.png)

同样，我们可以指定向后一致损失，如下图所示。假设我们有一个原始目标图像， *y* 。我们将这个 *y* 馈送给鉴别器 *F* ，它返回伪源图像 *x* 。现在我们将这个假的源图像 *x* 馈送给生成器 *G* ，它必须返回原始的目标图像 *y* :

![](img/28cc50f4-c98d-4edb-868b-32c46c7169d9.png)

前面的等式可以表示为:

![](img/e90a057d-6622-4304-b62f-6544beced8e6.png)

向后一致性损失可以表示如下:

![](img/9527c35d-b8c7-48cd-8cb1-b004a7228ca9.png)

因此，加上前向和后向一致性损失，我们可以将周期一致性损失写成:

![](img/921caae3-cc6c-4005-8cc1-87399e37c099.png)

![](img/3fc60b22-7713-48c1-a1a0-9ab1596b4486.png)

我们希望我们的发电机循环一致，因此，我们用循环一致损耗乘以它们的损耗。因此，最终损失函数可由下式给出:

![](img/77f55660-3c8a-42e3-a6d8-1eea3b607e54.png)



# 使用 CycleGAN 将照片转换为绘画

现在我们将学习如何在 TensorFlow 中实现一个 CycleGAN。我们将了解如何使用 CycleGAN 将图片转换为绘画:

![](img/c98cbbab-07be-4a1e-ada3-198683726f4a.png)

本节使用的数据集可以从[https://people . eecs . Berkeley . edu/~ tae sung _ park/cycle gan/datasets/Monet 2 photo . zip](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip)下载。下载完数据集后，解压存档文件；它将由四个文件夹组成，`trainA`、`trainB`、`testA`和`testB`，包含训练和测试图像。

`trainA`文件夹包含油画(莫奈)，而`trainB`文件夹包含照片。由于我们将照片( *x* )映射到绘画( *y* )，包含照片的`trainB`文件夹将是我们的源图像，![](img/1b539a3e-4e00-4e5a-9f26-6f8824318419.png)，包含绘画的`trainA`文件夹将是我们的目标图像，![](img/413d8fcb-4439-4bc7-952e-21ec209ba5b7.png)。

CycleGAN 的完整代码以及一步一步的解释可以在 Jupyter 笔记本上找到，网址是[https://github . com/packt publishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python)。

我们将只看如何在 TensorFlow 中实现 CycleGAN 并将源图像映射到目标域，而不是看整个代码。你也可以在[https://github . com/packt publishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python)查看完整代码。

定义`CycleGAN`类:

```
class CycleGAN:
        def __init__(self):
```

定义输入的占位符`X`和输出的占位符`Y`:

```
        self.X = tf.placeholder("float", shape=[batchsize, image_height, image_width, 3])
        self.Y = tf.placeholder("float", shape=[batchsize, image_height, image_width, 3])
```

定义将![](img/56fe4f4b-9eda-4272-8872-10529152edae.png)映射到![](img/9c32ba4d-98d2-44d7-aeb6-d768786c4630.png)的生成器![](img/182c4d15-8ba5-40f8-b249-97c18125d781.png):

```
        G = generator("G")
```

定义将![](img/5f3eaf60-fe74-4775-a7d9-3cc407a97c35.png)映射到![](img/d45bf48f-aada-4436-ac4f-b125d352e8b1.png)的生成器![](img/36bef03d-9e31-47c0-82c4-5fca2416aad6.png):

```
        F = generator("F")
```

定义鉴别器![](img/3a6cab69-81bb-4464-ab61-c35a59115f15.png)，用于鉴别真实源图像和伪造源图像:

```
         self.Dx = discriminator("Dx")       
```

定义鉴别器![](img/356ef7c9-c99f-4961-b689-ac89e4523e90.png)，用于鉴别真实目标图像和虚假目标图像:

```
        self.Dy = discriminator("Dy")
```

生成伪源图像:

```
        self.fake_X = F(self.Y)
```

生成假目标图像:

```
        self.fake_Y = G(self.X)        
```

得到`logits`:

```
        #real source image logits
        self.Dx_logits_real = self.Dx(self.X) 

        #fake source image logits
        self.Dx_logits_fake = self.Dx(self.fake_X, True)

        #real target image logits
        self.Dy_logits_fake = self.Dy(self.fake_Y, True)

        #fake target image logits
        self.Dy_logits_real = self.Dy(self.Y)
```

我们知道循环一致性损失给出如下:

![](img/1b2a7610-0aa3-445a-a9a5-47650edd03e9.png)

我们可以如下实现周期一致性丢失:

```
        self.cycle_loss = tf.reduce_mean(tf.abs(F(self.fake_Y, True) - self.X)) + \
                        tf.reduce_mean(tf.abs(G(self.fake_X, True) - self.Y))
```

定义我们两个鉴别器![](img/3e052ee5-2ec7-44d5-a708-309775749f01.png)和![](img/cc26f199-c253-4dde-8238-3cbe052233ed.png)的损失。

我们可以用 Wasserstein 距离重写鉴别器的损失函数:

![](img/dfe69197-13a3-4fd4-8cc2-6b43783a27f5.png)

![](img/112ce76a-1cec-4244-b827-b6779ad589d3.png)

因此，鉴频器的损耗实现如下:

```
        self.Dx_loss = -tf.reduce_mean(self.Dx_logits_real) + tf.reduce_mean(self.Dx_logits_fake) 

        self.Dy_loss = -tf.reduce_mean(self.Dy_logits_real) + tf.reduce_mean(self.Dy_logits_fake)
```

定义两个发电机的损耗，![](img/5a278307-2f9e-4d77-997b-5ca957c0ddeb.png)和![](img/89c696f8-ad04-4042-a889-c93eb7505a7d.png)。我们可以用 Wasserstein 距离将发电机的损失函数改写为:

![](img/433704ef-29de-4c76-b26c-de00e1561b24.png)

![](img/4a705c83-3217-4ded-8f50-660f415c4073.png)

因此，两个发电机的损耗乘以循环一致性损耗，`cycle_loss`实现为:

```
        self.G_loss = -tf.reduce_mean(self.Dy_logits_fake) + 10\. * self.cycle_loss

        self.F_loss = -tf.reduce_mean(self.Dx_logits_fake) + 10\. * self.cycle_loss
```

使用 Adam 优化器优化鉴别器和生成器:

```
       #optimize the discriminator
        self.Dx_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.Dx_loss, var_list=[self.Dx.var])

        self.Dy_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.Dy_loss, var_list=[self.Dy.var])

        #optimize the generator
        self.G_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.G_loss, var_list=[G.var])

        self.F_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0., beta2=0.9).minimize(self.F_loss, var_list=[F.var])
```

一旦我们开始训练模型，我们可以看到鉴别器和生成器的损失如何随着迭代而减少:

```
Epoch: 0, iteration: 0, Dx Loss: -0.6229429245, Dy Loss: -2.42867970467, G Loss: 1385.33557129, F Loss: 1383.81530762, Cycle Loss: 138.448059082

Epoch: 0, iteration: 50, Dx Loss: -6.46077537537, Dy Loss: -7.29514217377, G Loss: 629.768066406, F Loss: 615.080932617, Cycle Loss: 62.6807098389

Epoch: 1, iteration: 100, Dx Loss: -16.5891685486, Dy Loss: -16.0576553345, G Loss: 645.53137207, F Loss: 649.854919434, Cycle Loss: 63.9096908569
```



# 斯塔克根

现在我们将看到一种最有趣、最迷人的 GAN，它被称为**堆叠 GAN** 。如果我说 StackGANs 仅仅基于文本描述就能生成照片般逼真的图像，你能相信吗？嗯，是的。他们能做到。给定文本描述，它们可以生成逼真的图像。

我们先来了解一下艺术家是如何画出一个形象的。在第一阶段，艺术家绘制原始形状，并创建一个基本的轮廓，形成图像的初始版本。在下一阶段，他们通过使图像更加真实和吸引人来增强图像。

StackGANs 以类似的方式工作。他们将生成图像的过程分为两个阶段。就像艺术家绘画一样，在第一阶段，他们生成一个基本的轮廓，原始的形状，并创建一个低分辨率版本的图像，在第二阶段，他们通过使其更加逼真来增强第一阶段生成的图片，然后将它们转换为高分辨率的图像。

但是 StackGANs 是怎么做到的呢？

他们使用两个 GANs，每个阶段一个。第一级中的 GAN 生成基本图像并将其发送到下一级中的 GAN，下一级将基本低分辨率图像转换成适当的高分辨率图像。下图显示了 StackGANs 如何根据文本描述在每个阶段生成图像:

![](img/d14ffa08-2b92-42b6-9faf-41852ea42806.png)

资料来源:https://arxiv.org/pdf/1612.03242.pdf

正如你所看到的，在第一阶段，我们有一个低分辨率版本的图像，但在第二阶段，我们有良好的清晰度高分辨率图像。但是，StackGAN 是怎么做到的呢？还记得吗，当我们学习有条件的 GAN 时，我们可以通过调节它们来使 GAN 生成我们想要的图像。

我们只是在两个阶段都用到它们。在第一阶段，我们的网络是基于文本描述的。有了这个文本描述，他们就生成了一个图像的基本版本。在第二阶段，我们的网络基于第一阶段生成的图像以及文本描述进行调节。

但是为什么我们在第二阶段又要以文字描述为条件呢？因为在第一阶段，我们错过了文本描述中指定的一些细节来创建图像的基本版本。因此，在第二阶段，我们再次以文本描述为条件来修复缺失的信息，并使我们的图像更加真实。

凭借这种仅基于文本生成图片的能力，它被用于许多应用。它在娱乐业中被大量使用，例如，仅仅基于描述创建框架，它也可以用于生成漫画等等。



# 堆栈根的架构

现在我们已经对 StackGANs 的工作原理有了一个基本的了解，我们将更仔细地研究它们的架构，看看它们是如何从文本中生成图片的。

下图显示了堆栈根的完整体系结构:

![](img/75cf1c71-6b88-4b7f-a1e4-f37f5487a4c6.png)

资料来源:https://arxiv.org/pdf/1612.03242.pdf

我们将逐一查看每个组件。



# 条件增强

我们有一个文本描述作为 GAN 的输入。基于这些描述，它必须生成图像。但是他们是如何理解文字的含义来生成图片的呢？

首先，我们使用编码器将文本转换成嵌入文本。我们用![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)来表示这个文本嵌入。我们能创造出![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)的变体吗？通过创建文本嵌入的变体，![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)，我们可以有额外的训练对，并且我们还可以增加对小扰动的鲁棒性。

设![](img/3ecca366-0d12-43d0-8ba6-d70d5af45f18.png)为均值，![](img/00891861-b74f-4d4b-849d-aadc3615125b.png)为我们文本嵌入的对角协方差矩阵，![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)。现在我们从独立的高斯分布![](img/0cde2c39-1d06-4b38-9ee5-95e83080118f.png)中随机抽取一个额外的条件变量![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)。它帮助我们创造不同的文本描述和它们的含义。我们知道相同的文本可以用不同的方式书写，所以使用条件变量![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)，我们可以将不同版本的文本映射到图像。

因此，一旦我们有了文本描述，我们将使用编码器提取它们的嵌入，然后我们计算它们的均值和协方差。然后，我们从文本嵌入的高斯分布中采样![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)，![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)。



# 第一阶段

好了，现在我们有一个文本嵌入，![](img/b98b457b-5113-4138-87a1-5ecc312f9ee4.png)，还有一个条件变量，![](img/2d0a756c-3fa1-4604-8921-217706c768a7.png)。我们将看到它是如何被用来生成图像的基本版本的。



# 发电机

我们知道生成器的目标是通过学习真实的数据分布来生成一个假图像。首先，我们从高斯分布中采样噪声，并创建 *z* 。然后，我们将 *z* 与我们的条件变量![](img/2049be92-cbff-4589-9f92-654889e904da.png)连接起来，并将其作为输入提供给生成器，该生成器输出图像的基本版本。

发电机的损耗函数如下所示:

![](img/3563799a-e439-4332-81d9-9870c6fb465f.png)

让我们来看看这个公式:

*   ![](img/8fefbb15-57bb-408f-a6f4-18edae972eec.png)表示我们从伪数据分布中抽取 z 样本，即噪声先验。
*   ![](img/aa262917-373b-423e-b8ff-5295ea45fa41.png)暗示我们从真实数据分布中抽取文本描述![](img/045cf8e4-5c8b-4dcc-a914-8e2e7bcc15ef.png)。
*   ![](img/8a6a0eeb-b335-4e97-b511-4fe7c92f8d02.png)暗示发生器获取噪声，调节变量返回图像。我们将生成的图像提供给鉴别器。
*   ![](img/89856ddf-2b18-4581-a8e3-4d502f407bf8.png)暗示生成的图像是假的对数概率。

除了这种损失，我们还将正则项![](img/fa97f5c1-7daa-4454-b679-108d3f13d2c5.png)添加到我们的损失函数中，这意味着标准高斯分布和条件高斯分布之间的 KL 散度。这有助于我们避免过度拟合。

因此，发电机的最终损失函数变为:

![](img/f75101c3-d7b2-4b82-b0fb-027f1e3cf0f2.png)



# 鉴别器

现在，我们将生成的图像提供给鉴别器，鉴别器返回图像真实的概率。鉴频器损耗计算如下:

![](img/ff6aed90-dd55-40f7-9541-dfd28d64318d.png)

这里:

*   ![](img/cf8d3d28-dce2-4c91-9fb5-d57329b166e1.png)暗示真实的图像，![](img/79417429-af4b-4492-86f7-dbe46ae3762a.png)，以文字描述为条件，![](img/788b3eff-5b35-41c9-b2d4-03cecadd3ba2.png)
*   ![](img/59e7da3c-1310-432d-8855-869dc1caa9bd.png)暗示生成的假图像



# 第二阶段

我们已经了解了第一阶段如何生成图像的基本版本。现在，在第二阶段，我们修复了第一阶段生成的图像的缺陷，并生成了更真实的图像版本。我们用前一阶段生成的图像以及文本嵌入来调节我们的网络。



# 发电机

阶段 II 中的生成器不是将噪声作为输入，而是将前一阶段生成的图像作为输入，并且它以文本描述为条件。

这里，![](img/dfdca6f9-f1c9-4754-bb16-1dd14a2485c4.png)意味着我们从![](img/385a1be2-69ec-4ebd-bdd7-eade9473467c.png)中抽取![](img/4412e70a-76f9-42cf-a621-441ef7c58ff1.png)。这基本上意味着我们正在对第一阶段生成的图像进行采样。

![](img/532013c8-a184-4d77-b833-45398433ea46.png)暗示我们正在从给定的真实数据分布中抽取文本，![](img/5ebebb89-77fd-4d4b-b3e3-7dfcde29d765.png)。

那么发电机损耗可由下式给出:

![](img/5a59d6ec-a0dc-4b5b-a7cc-0b37e179e788.png)

随着正则化，我们的发电机的损失函数变成:

![](img/ecc0d204-c26a-429a-802d-bca587641079.png)



# 鉴别器

鉴别器的目标是告诉我们图像是来自真实分布还是生成器分布。因此，鉴频器的损耗函数如下所示:

![](img/f51864d9-cdd0-4a47-a689-6c2f3823d189.png)



# 摘要

本章一开始，我们学习了条件句，以及如何用它们来生成我们感兴趣的图像。

后来，我们学习了 InfoGANs，在 info gans 中，代码 *c* 是根据生成的输出自动推断出来的，不像 CGAN，在 CGAN 中我们显式地指定了 *c* 。为了推断出 *c* ，我们需要找到后验概率![](img/6681a392-47b2-4fde-b928-ea3e9dd3f0e7.png)，而我们没有这个后验概率。所以，我们使用辅助分布。我们使用互信息来最大化互信息![](img/30eec497-b316-4712-8e24-0cc194eb757e.png)，在给定发电机输出的情况下，最大化我们关于 *c* 的知识。

然后，我们学习了 CycleGANs，它将数据从一个域映射到另一个域。我们试图学习从照片领域的图像分布到绘画领域的图像分布的映射。最后，我们理解了 StackGANs 是如何从文本描述生成照片级真实感图像的。

在下一章，我们将学习自编码器**和它们的类型。**



# 问题

回答以下问题，衡量你从本章中学到了多少:

1.  条件甘和香草甘有什么不同？
2.  InfoGAN 中的代码叫什么？
3.  什么是互信息？
4.  为什么我们在 InfoGANs 中需要辅助分发？
5.  什么是周期一致性损失？
6.  解释发电机在循环发电中的作用。
7.  StackGANs 如何把文字描述转换成图片？



# 进一步阅读

有关更多信息，请参考以下链接:

*   *迈赫迪·米尔扎和西蒙·奥森德罗的条件生成对抗网络*，[https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)
*   *InfoGAN:通过信息最大化生成对抗网的可解释表示学习*陈曦等人，[https://arxiv.org/pdf/1606.03657.pdf](https://arxiv.org/pdf/1606.03657.pdf)
*   *使用循环一致对抗网络的不成对图像到图像翻译*朱俊彦等人，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)