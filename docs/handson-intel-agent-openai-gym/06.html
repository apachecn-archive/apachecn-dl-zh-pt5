<html><head/><body>

  
    <title>Implementing an Intelligent Agent for Optimal Control using Deep Q-Learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">使用深度Q学习实现最优控制的智能代理</h1>
                
            
            
                
<p class="calibre2">在前一章中，我们实现了一个智能代理，它使用Q-learning在双核笔记本电脑CPU上在大约七分钟内从零开始解决山地汽车问题。在这一章中，我们将实现Q-learning的一个高级版本，称为深度Q-learning，它可以用来解决几个比山地车问题复杂得多的离散控制问题。离散控制问题是(顺序)决策问题，其中动作空间被离散成有限数量的值。在前一章中，学习代理使用一个二维状态空间向量作为输入，它包含了关于手推车的位置和速度的信息，以采取最佳的控制行动。在这一章中，我们将看到如何实现一个学习代理，它将(屏幕上的)视觉图像作为输入，并学习采取最佳控制行动。这和我们解决问题的方式很接近，不是吗？我们人类不会通过计算一个物体的位置和速度来决定下一步该做什么。我们只是观察正在发生的事情，然后学习采取行动，随着时间的推移不断改进，最终完全解决问题。</p>
<p class="calibre2">本章将指导您如何逐步构建一个更好的代理，方法是使用最近发布的具有深度神经网络函数逼近的稳定Q学习方法，逐步改进我们的Q学习代理实现。到本章结束时，你将学会如何实现和训练一个深度Q学习代理，它观察屏幕上的像素，并使用雅达利健身房环境玩雅达利游戏，并获得相当好的分数！我们还将讨论如何在学习过程中直观显示和比较代理的表现。您将看到相同的代理算法如何在几个不同的Atari游戏上进行训练，并且代理仍然能够很好地学习玩游戏。如果你迫不及待地想看一些实际的东西，或者如果你想在深入研究之前看一看你将要开发的东西，你可以在本书的代码库中的<kbd class="calibre12">ch6</kbd>文件夹下查看本章的代码，并在几个Atari游戏上试用预先训练好的代理程序！关于如何运行预培训代理的说明可在<kbd class="calibre12">ch6/README.md</kbd>文件中找到。</p>
<p class="calibre2">这一章有许多技术细节，使您具备足够的背景和知识，理解改进基本Q学习算法和基于深度Q学习构建更有能力和智能的代理的逐步过程，以及以系统方式训练和测试代理所需的几个模块和工具。以下是本章将涉及的高级主题的概述:</p>
<ul class="calibre10">
<li class="calibre11">改进Q-learning agent的各种方法，包括以下几种:<ul class="calibre68">
<li class="calibre11">作用值函数的神经网络逼近</li>
<li class="calibre11">体验回放</li>
<li class="calibre11">勘探时间表</li>
</ul>
</li>
<li class="calibre11">用PyTorch实现深度卷积神经网络的动作值函数逼近</li>
<li class="calibre11">使用目标网络稳定深度Q网络</li>
<li class="calibre11">使用TensorBoard记录和监控PyTorch代理的学习性能</li>
<li class="calibre11">管理参数和配置</li>
<li class="calibre11">雅达利健身房环境</li>
<li class="calibre11">训练深度Q学习者玩雅达利游戏</li>
</ul>
<p class="calibre2">让我们从第一个主题开始，看看我们如何从上一章停止的地方开始，继续朝着更有能力和更智能的代理前进。</p>


            

            
        
    



  
    <title>Improving the Q-learning agent</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">改进Q学习代理</h1>
                
            
            
                
<p class="calibre2">在上一章中，我们回顾了Q学习算法并实现了<kbd class="calibre12">Q_Learner</kbd>类。对于山地汽车环境，我们使用一个形状为51x51x3的多维数组来表示动作值函数<img src="img/00130.jpeg" class="calibre69"/>。注意，我们已经将状态空间离散化为由<kbd class="calibre12">NUM_DISCRETE_BINS</kbd>配置参数给定的固定数量的箱(我们使用50)。我们本质上用低维的离散表示来量化或近似观察，以减少n维数组中可能元素的数量。通过观察/状态空间的这种离散化，我们将汽车的可能位置限制到50个位置的固定集合，并将汽车的可能速度限制到50个值的固定集合。任何其他位置或速度值都将近似于这些固定值集合中的一个。因此，当汽车实际位于不同位置时，代理可能会收到相同的位置值。对于某些环境，这可能是一个问题。代理人可能没有学到足够的知识来区分掉下悬崖和站在悬崖边缘向前跳跃。在下一节中，我们将研究如何使用更强大的函数逼近器来表示动作值函数，而不是有其局限性的简单n维数组/表。</p>


            

            
        
    



  
    <title>Using neural networks to approximate Q-functions</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">用神经网络逼近Q函数</h1>
                
            
            
                
<p class="calibre2">神经网络被证明是有效的通用函数逼近器。事实上，有一个普适的逼近定理，即单个隐层前馈神经网络可以逼近任何在<img class="fm-editor-equation72" src="img/00131.jpeg"/>中闭且有界的连续函数。它基本上意味着即使简单的(浅层的)神经网络也可以逼近几个函数。您可以使用一个具有固定数量的权重/参数的简单神经网络来近似几乎任何函数，这是不是感觉太好了？这实际上是真的，除了一点使它不能在任何地方使用。虽然单个隐藏层神经网络可以用有限的一组参数逼近任何函数，但是我们没有一个普遍保证的方法来<em class="calibre13">学习</em>那些可以最好地代表任何函数的参数。你会看到，研究人员已经能够使用神经网络来逼近几个复杂而有用的函数。今天，无处不在的智能手机中内置的大部分智能都是由(高度优化的)神经网络驱动的。几个性能最好的系统，根据照片中的人、地点和背景自动将照片组织到相册中，识别你的脸和声音的系统，或者自动为你撰写电子邮件回复的系统，都是由神经网络驱动的。就连你从谷歌助手(Google Assistant)等语音助手那里听到的生成类似人类的逼真声音的最先进技术，也是由神经网络驱动的。</p>
<p>谷歌助手目前使用Deepmind开发的WaveNet和WaveNet2进行<strong class="calibre27">文本到语音</strong> ( <strong class="calibre27"> TTS </strong>)合成，这比迄今为止开发的任何其他TTS系统都要真实得多。</p>
<p class="calibre2">我希望这足以激励你使用神经网络来逼近Q函数！在本节中，我们将从用浅(不深)单隐层神经网络逼近Q函数开始，并用它来解决著名的Cart极点问题。虽然神经网络是强大的函数逼近器，但我们将会看到，即使训练一个单层神经网络来逼近强化学习问题的Q函数也不是微不足道的。我们将研究一些用神经网络近似法来改进Q学习的方法，在本章的后面部分，我们将研究如何使用具有更强表示能力的深度神经网络来近似Q函数。</p>
<p class="calibre2">让我们从神经网络近似开始，首先回顾一下我们在前一章中实现的<kbd class="calibre12">Q_Learner</kbd>类的<kbd class="calibre12">__init__(...)</kbd>方法:</p>
<pre class="calibre17">class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        <strong class="calibre1">self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,</strong><br class="title-page-name"/><strong class="calibre1">                           self.action_shape))  # (51 x 51 x 3)</strong><br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0</pre>
<p class="calibre2">在前面的代码中，粗体字行是我们将Q函数初始化为多维NumPy数组的地方。在下面几节中，我们将看到如何用一个更强大的神经网络表示来代替NumPy数组表示。</p>


            

            
        
    



  
    <title>Implementing a shallow Q-network using PyTorch </title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">用PyTorch实现浅Q网络</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将开始使用PyTorch的神经网络模块实现一个简单的神经网络，然后看看我们如何使用它来取代多维数组的Q action-value表状函数。</p>
<p class="calibre2">先说神经网络实现。以下代码说明了如何使用PyTorch实现一个<strong class="calibre4">单层感知器</strong> ( <strong class="calibre4"> SLP </strong>):</p>
<pre class="calibre17">import torch<br class="title-page-name"/><br class="title-page-name"/>class SLP(torch.nn.Module):<br class="title-page-name"/>    """<br class="title-page-name"/>    A Single Layer Perceptron (SLP) class to approximate functions<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, input_shape, output_shape, device=torch.device("cpu")):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param input_shape: Shape/dimension of the input<br class="title-page-name"/>        :param output_shape: Shape/dimension of the output<br class="title-page-name"/>        :param device: The device (cpu or cuda) that the SLP should use to store the inputs for the forward pass<br class="title-page-name"/>        """<br class="title-page-name"/>        super(SLP, self).__init__()<br class="title-page-name"/>        self.device = device<br class="title-page-name"/>        self.input_shape = input_shape[0]<br class="title-page-name"/>        self.hidden_shape = 40<br class="title-page-name"/>        self.linear1 = torch.nn.Linear(self.input_shape, self.hidden_shape)<br class="title-page-name"/>        self.out = torch.nn.Linear(self.hidden_shape, output_shape)<br class="title-page-name"/><br class="title-page-name"/>    def forward(self, x):<br class="title-page-name"/>        x = torch.from_numpy(x).float().to(self.device)<br class="title-page-name"/>        x = torch.nn.functional.relu(self.linear1(x))<br class="title-page-name"/>        x = self.out(x)<br class="title-page-name"/>        return x</pre>
<p class="calibre2">SLP类使用<kbd class="calibre12">torch.nn.Linear</kbd>类实现了在输入和输出层之间具有40个隐藏单元的单层神经网络，并使用<strong class="calibre4">整流线性单元</strong> ( <strong class="calibre4"> ReLU </strong>或<strong class="calibre4"> relu </strong>)作为激活函数。该代码在本书的代码库中以<kbd class="calibre12">ch6/function_approximator/perceptron.py</kbd>的名称出现。数字40没什么特别的，所以可以随意改变神经网络中隐藏单元的数量。</p>


            

            
        
    



  
    <title>Implementing the Shallow_Q_Learner</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现Shallow_Q_Learner</h1>
                
            
            
                
<p class="calibre2">然后我们可以修改<kbd class="calibre12">Q_Learner</kbd>类来使用这个SLP来表示Q函数。请注意，我们还必须修改<kbd class="calibre12">Q_Learner</kbd>类<kbd class="calibre12">learn(...)</kbd>方法，以计算相对于SLP权重的损失梯度，并对其进行反向传播，从而更新和优化神经网络的权重，以改善其Q值表示，使其接近实际值。我们还将稍微修改一下<kbd class="calibre12">get_action(...)</kbd>方法，通过正向传递神经网络来获得Q值。下面用粗体显示了对<kbd class="calibre12">Q_Learner</kbd>类实现进行更改后的<kbd class="calibre12">Shallow_Q_Learner</kbd>类代码，以便于您一目了然地看到不同之处:</p>
<pre class="calibre17"><strong class="calibre1">import torch<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/><br class="title-page-name"/></strong>EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class <strong class="calibre1">Shallow_Q_Learner</strong>(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        <strong class="calibre1">self.Q = SLP(self.obs_shape, self.action_shape)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-5)</strong><br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            <strong class="calibre1">return np.argmax(self.Q(discretized_obs).data.to(torch.device('cpu')).numpy())</strong><br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        <strong class="calibre1">#discretized_obs = self.discretize(obs)</strong><br class="title-page-name"/>        <strong class="calibre1">#discretized_next_obs = self.discretize(next_obs)</strong><br class="title-page-name"/>       <strong class="calibre1"> td_target = reward + self.gamma * torch.max(self.Q(next_obs))</strong><br class="title-page-name"/>        <strong class="calibre1">td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)</strong><br class="title-page-name"/>        <strong class="calibre1">#self.Q[discretized_obs][action] += self.alpha * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/></strong></pre>
<p>这里讨论的<kbd class="calibre28">Shallow_Q_Learner</kbd>类实现只是为了让你更容易理解如何实现一个基于神经网络的Q函数近似来代替传统的表格Q学习实现。</p>


            

            
        
    



  
    <title>Solving the Cart Pole problem using a Shallow Q-Network</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">用浅Q网络求解Cart极点问题</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将使用我们在上一节中开发的浅层<kbd class="calibre12">Q_Learner</kbd>类实现一个完整的训练脚本来解决Cart Pole问题:</p>
<pre class="calibre17">#!/usr/bin/env python import gym import random import torch from torch.autograd import Variable import numpy as np from utils.decay_schedule import LinearDecaySchedule from function_approximator.perceptron import SLP<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>env = gym.make("CartPole-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 100000<br class="title-page-name"/>MAX_STEPS_PER_EPISODE = 300<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Shallow_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, learning_rate=0.005,<br class="title-page-name"/>                 gamma=0.98):<br class="title-page-name"/>        self.state_shape = state_shape<br class="title-page-name"/>        self.action_shape = action_shape<br class="title-page-name"/>        self.gamma = gamma # Agent's discount factor<br class="title-page-name"/>        self.learning_rate = learning_rate # Agent's Q-learning rate<br class="title-page-name"/>        # self.Q is the Action-Value function. This agent represents Q using a<br class="title-page-name"/>        # Neural Network.<br class="title-page-name"/>        self.Q = SLP(state_shape, action_shape)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-3)<br class="title-page-name"/>        # self.policy is the policy followed by the agent. This agents follows<br class="title-page-name"/>        # an epsilon-greedy policy w.r.t it's Q estimate.<br class="title-page-name"/>        self.policy = self.epsilon_greedy_Q<br class="title-page-name"/>        self.epsilon_max = 1.0<br class="title-page-name"/>        self.epsilon_min = 0.05<br class="title-page-name"/>        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,<br class="title-page-name"/>                                    final_value=self.epsilon_min,<br class="title-page-name"/>                                    max_steps= 0.5 * MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)<br class="title-page-name"/>        self.step_num = 0<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        return self.policy(observation)<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        # Decay Epsilion/exploratin as per schedule<br class="title-page-name"/>        if random.random() &lt; self.epsilon_decay(self.step_num):<br class="title-page-name"/>            action = random.choice([i for i in range(self.action_shape)])<br class="title-page-name"/>        else:<br class="title-page-name"/>            action = np.argmax(self.Q(observation).data.numpy())<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>        return action<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next):<br class="title-page-name"/>        td_target = r + self.gamma * torch.max(self.Q(s_next))<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss(self.Q(s)[a], td_target)<br class="title-page-name"/>        # Update Q estimate<br class="title-page-name"/>        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    observation_shape = env.observation_space.shape<br class="title-page-name"/>    action_shape = env.action_space.n<br class="title-page-name"/>    agent = Shallow_Q_Learner(observation_shape, action_shape)<br class="title-page-name"/>    first_episode = True<br class="title-page-name"/>    episode_rewards = list()<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        for step in range(MAX_STEPS_PER_EPISODE):<br class="title-page-name"/>            # env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                if first_episode: # Initialize max_reward at the end of first episode<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    first_episode = False<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; max_reward:<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))<br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()</pre>
<p class="calibre2">用前面的代码在<kbd class="calibre12">ch6</kbd>文件夹中创建一个名为<kbd class="calibre12">shallow_Q_Learner.py</kbd>的脚本，并像这样运行它:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch6$ python shallow_Q_Learner.py</strong></pre>
<p class="calibre2">您将看到代理人在健身房的<kbd class="calibre12">CartPole-v0</kbd>环境中学习平衡推车上的杆。您应该在控制台上看到剧集编号、在剧集结束前座席采取的步骤数、座席获得的剧集奖励、座席获得的平均剧集奖励以及座席迄今获得的最佳剧集奖励。如果您想直观地了解Cart Pole环境以及代理如何学习和平衡该环境，您可以取消对<kbd class="calibre12">env.render()</kbd>线的注释。</p>
<p><kbd class="calibre28">Shallow_Q_Learner</kbd>类实现和完整的训练脚本显示了如何使用简单的神经网络来逼近Q函数。解决像雅达利这样复杂的游戏不是一个好的方法。在接下来的部分中，我们将使用新技术系统地提高他们的性能。我们还将实现一个深度卷积Q网络，该网络可以将原始屏幕图像作为输入，并预测Q值，代理可以使用该Q值玩各种雅达利游戏。</p>
<p class="calibre2">您可能会注意到，代理需要很长时间才能改进并最终能够解决问题。在下一节中，我们将实现体验重放的概念来提高性能。</p>


            

            
        
    



  
    <title>Experience replay </title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">体验重放</h1>
                
            
            
                
<p class="calibre2">在大多数环境下，代理接收的信息不是<strong class="calibre4">独立且同分布的</strong> ( <strong class="calibre4"> i.i.d </strong>)。这意味着代理接收到的观察结果与其先前接收到的观察结果以及将接收到的下一个观察结果有很大关系。这是可以理解的，因为通常情况下，代理在典型的强化学习环境中解决的问题是连续的。如果样本是i.i.d，则神经网络收敛得更好。</p>
<p class="calibre2">经验重放还可以重用代理人过去的经验。神经网络更新，特别是在较低的学习速率下，需要几个反向传播和优化步骤才能收敛到较好的值。重复使用过去的经验数据，特别是以小批量的方式更新神经网络，有助于Q网络收敛到接近真实的动作值。</p>


            

            
        
    



  
    <title>Implementing the experience memory</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现经验记忆</h1>
                
            
            
                
<p class="calibre2">让我们实现一个体验记忆类来存储代理收集的体验。在此之前，让我们巩固一下我们对<em class="calibre13">体验</em>的理解。在使用<strong class="calibre4">马尔可夫决策过程</strong> ( <strong class="calibre4"> MDP </strong>)来表示问题的强化学习中，我们在<a href="part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">第2章</a>、<em class="calibre13">强化学习和深度强化学习</em>中讨论过，将一个体验表示为一个数据结构是有效的，该数据结构由在时间步长<em class="calibre13"> t </em>的观察、在该观察之后采取的行动、该行动获得的奖励以及环境由于代理的行动而转变到的下一个观察(或状态)组成。还包括“完成”布尔值是有用的，该布尔值表示该特定的下一个观察是否标志着该集的结束。让我们使用来自集合库的Python的<kbd class="calibre12">namedtuple</kbd>来表示这样的数据结构，如下面的代码片段所示:</p>
<pre class="calibre17">from collections import namedtuple<br class="title-page-name"/>Experience = namedtuple("Experience", ['obs', 'action', 'reward', 'next_obs',<br class="title-page-name"/>                                       'done'])</pre>
<p class="calibre2"><kbd class="calibre12">namedtuple</kbd>数据结构使得使用名称属性(如‘OBS’、‘action’等)而不是数字索引(如0、1等)来访问元素变得很方便。</p>
<p class="calibre2">我们现在可以继续使用我们刚刚创建的经验数据结构来实现经验内存类。为了弄清楚我们需要在体验记忆类中实现什么方法，让我们考虑一下以后我们将如何使用它。</p>
<p class="calibre2">首先，我们希望能够在代理收集的经验记忆中存储新的经验。然后，当我们想要重放以更新Q函数时，我们想要从经验记忆中成批地采样或检索经验。所以，本质上，我们需要一个可以存储新体验的方法和一个可以对单个或一批体验进行采样的方法。</p>
<p class="calibre2">让我们深入体验一下内存实现，从初始化方法开始，我们用期望的容量初始化内存，如下所示:</p>
<pre class="calibre17">class ExperienceMemory(object):<br class="title-page-name"/>    """<br class="title-page-name"/>    A cyclic/ring buffer based Experience Memory implementation<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, capacity=int(1e6)):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param capacity: Total capacity (Max number of Experiences)<br class="title-page-name"/>        :return:<br class="title-page-name"/>        """<br class="title-page-name"/>        self.capacity = capacity<br class="title-page-name"/>        self.mem_idx = 0 # Index of the current experience<br class="title-page-name"/>        self.memory = []</pre>
<p class="calibre2"><kbd class="calibre12">mem_idx</kbd>成员变量将被用来指向当前的写头或索引位置，当新的体验到达时，我们将在那里存储新的体验。</p>
<p>“循环缓冲区”还有一个您可能听说过的名称:“循环缓冲区”、“环形缓冲区”和“循环队列”。它们都表示相同的底层数据结构，该结构使用环状固定大小的数据表示。</p>
<p class="calibre2">接下来，我们将看看<kbd class="calibre12">store</kbd>方法的实现:</p>
<pre class="calibre17">def store(self, experience):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param experience: The Experience object to be stored into the memory<br class="title-page-name"/>        :return:<br class="title-page-name"/>        """<br class="title-page-name"/>        self.memory.insert(self.mem_idx % self.capacity, experience)<br class="title-page-name"/>        self.mem_idx += 1</pre>
<p class="calibre2">很简单，对吧？就像我们讨论过的，我们在<kbd class="calibre12">mem_idx</kbd>储存经验。</p>
<p class="calibre2">下一段代码是我们的<kbd class="calibre12">sample</kbd>方法实现:</p>
<pre class="calibre17">import random<br class="title-page-name"/>    def sample(self, batch_size):<br class="title-page-name"/>        """<br class="title-page-name"/><br class="title-page-name"/>        :param batch_size: Sample batch_size<br class="title-page-name"/>        :return: A list of batch_size number of Experiences sampled at random from mem<br class="title-page-name"/>        """<br class="title-page-name"/>        assert batch_size &lt;= len(self.memory), "Sample batch_size is more than available exp in mem"<br class="title-page-name"/>        return random.sample(self.memory, batch_size)<br class="title-page-name"/><br class="title-page-name"/>    </pre>
<p class="calibre2">在前面的代码中，我们利用Python的random库从经验记忆中随机地统一采样经验。我们还将实现一个简单的<kbd class="calibre12">get_size</kbd> helper方法，我们将使用它来找出有多少经验已经存储在经验存储器中:</p>
<pre class="calibre17">def get_size(self):<br class="title-page-name"/>        """<br class="title-page-name"/><br class="title-page-name"/>        :return: Number of Experiences stored in the memory<br class="title-page-name"/>        """<br class="title-page-name"/>        return len(self.memory)</pre>
<p class="calibre2">体验记忆类的完整实现可以在本书的代码库中的<kbd class="calibre12">ch6/utils/experience_memory.py</kbd>处找到。</p>
<p class="calibre2">接下来，我们将看看如何重放从经验记忆中取样的经验来更新代理的Q函数。</p>


            

            
        
    



  
    <title>Implementing the replay experience method for the Q-learner class</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">为Q-learner班级实施重放体验法</h1>
                
            
            
                
<p class="calibre2">因此，我们为代理实现了一个内存系统，使用一个简单的循环缓冲区来存储它过去的经验。在本节中，我们将了解如何使用体验记忆在Q-learner课堂中回放体验。</p>
<pre>replay_experience method that shows how we sample from the experience memory and call a soon-to-be-implemented method that lets the agent learn from the sampled batch of experiences:</pre>
<pre class="calibre17">def replay_experience(self, batch_size=REPLAY_BATCH_SIZE):<br class="title-page-name"/>        """<br class="title-page-name"/>        Replays a mini-batch of experience sampled from the Experience Memory<br class="title-page-name"/>        :param batch_size: mini-batch size to sample from the Experience Memory<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        experience_batch = self.memory.sample(batch_size)<br class="title-page-name"/>        self.learn_from_batch_experience(experience_batch)</pre>
<p class="calibre2">在像SARSA这样的在线学习方法的情况下，动作值估计在代理和环境之间的每一步交互之后被更新。这样，更新会传播代理刚刚经历的信息。如果代理不经常经历某些事情，这样的更新可能会让代理忘记那些经历，并且可能会在代理将来遇到类似情况时导致糟糕的性能。这是不希望的，尤其是对于具有许多参数(或权重)的神经网络，这些参数(或权重)需要被调整到正确的一组值。这是在更新Q行动值估计值期间使用经验记忆和重放过去经验的主要动机之一。我们现在将实现<kbd class="calibre12">learn_from_batch_experience</kbd>方法，该方法扩展了我们在前一章中实现的<kbd class="calibre12">learn</kbd>方法，以便从一批经验中学习，而不是从单个经验中学习。下面是该方法的实现:</p>
<pre class="calibre17">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br class="title-page-name"/>def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        """<br class="title-page-name"/>        Updated the DQN based on the learning from a mini-batch of experience.<br class="title-page-name"/>        :param experiences: A mini-batch of experience<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss(<br class="title-page-name"/>            self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>            td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        self.Q_optimizer.step()</pre>
<p class="calibre2">该方法接收一批(或小批)经验，并首先分别提取观察批、行动批、奖励批和下一批观察批，以便在后续步骤中单独使用它们。<kbd class="calibre12">done_batch</kbd>表示每一次经历的下一次观察是否是一集的结束。然后，我们用最大过动作计算<strong class="calibre4">时间差</strong> ( <strong class="calibre4"> TD </strong>)误差，这就是Q学习目标。请注意，我们将<kbd class="calibre12">td_target</kbd>计算中的第二项乘以<kbd class="calibre12">~done_batch</kbd>。</p>
<p class="calibre2">这负责为终端状态指定一个零值。因此，如果<kbd class="calibre12">next_obs_batch</kbd>中的某个<kbd class="calibre12">next_obs</kbd>是终端，第二项将变成0，结果只有<kbd class="calibre12">td_target = rewards_batch</kbd>。</p>
<p class="calibre2">然后，我们计算<kbd class="calibre12">td_target</kbd>(目标Q值)和Q网络预测的Q值之间的均方误差。我们使用该误差作为引导信号，并在进行优化步骤来更新参数/权重以最小化误差之前，将其反向传播到神经网络中的所有节点。</p>


            

            
        
    



  
    <title>Revisiting the epsilon-greedy action policy</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">重新审视ε-贪婪行动政策</h1>
                
            
            
                
<p class="calibre2">在前一章中，我们讨论了<img class="fm-editor-equation59" src="img/00132.jpeg"/>-贪婪行动选择策略，它以1- <img class="fm-editor-equation73" src="img/00133.jpeg"/>的概率根据代理的行动值估计采取最佳行动，并以ε，<img class="fm-editor-equation73" src="img/00134.jpeg"/>给定的概率采取随机行动。ε是一个超参数，可以根据实验调整到一个好的值。较高的<img class="fm-editor-equation59" src="img/00135.jpeg"/>值意味着代理的行动将是随机的，而较低的<img class="fm-editor-equation59" src="img/00136.jpeg"/>值意味着代理的行动将更有可能利用它已经知道的关于环境的信息，而不会试图探索。我应该通过采取从未尝试过/尝试较少的行动来探索更多吗？或者我应该利用我已经知道的，并采取最好的行动，我的知识可能是有限的？这就是强化学习代理所遭遇的探索-开发困境。</p>
<p class="calibre2">直观地说，在代理学习过程的初始阶段为<img class="fm-editor-equation59" src="img/00137.jpeg"/>设置一个非常高的值(最大值为1.0)是很有帮助的，这样代理就可以通过采取大部分随机动作来探索状态空间。一旦它获得了足够的经验并对环境有了更好的了解，降低<img class="fm-editor-equation59" src="img/00138.jpeg"/>值将让代理更经常地根据它认为最好的行动采取行动。拥有一个负责改变<img class="fm-editor-equation59" src="img/00139.jpeg"/>值的效用函数会很有用，对吗？让我们在下一节实现这样一个函数。</p>


            

            
        
    



  
    <title>Implementing an epsilon decay schedule</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实施ε衰变计划</h1>
                
            
            
                
<p class="calibre2">我们可以线性地(在下面的左侧图中)、指数地(在下面的右侧图中)或使用一些其他衰减时间表来衰减(或减少)<img class="fm-editor-equation59" src="img/00140.jpeg"/>值。线性和指数时间表是勘探参数<img class="fm-editor-equation59" src="img/00141.jpeg"/>最常用的衰减时间表:</p>
<div><img src="img/00142.jpeg" class="calibre70"/></div>
<p class="calibre2">在前面的图表中，您可以看到epsilon (exploration)值是如何随着不同的计划方案而变化的(左边的图表是线性的，右边的图表是指数的)。前面图表中显示的衰减时间表在线性情况下使用ε_ max(开始)值1，ε_ min(最终)值0.01，在指数情况下使用exp(-10000/2000)，两者在10，000集后保持ε_ min的恒定值。</p>
<p class="calibre2">下面的代码实现了<kbd class="calibre12">LinearDecaySchedule</kbd>，我们将在我们的<kbd class="calibre12">Deep_Q_Learning</kbd>代理实现中使用它来玩Atari游戏:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>class LinearDecaySchedule(object):<br class="title-page-name"/>    def __init__(self, initial_value, final_value, max_steps):<br class="title-page-name"/>        assert initial_value &gt; final_value, "initial_value should be &lt; final_value"<br class="title-page-name"/>        self.initial_value = initial_value<br class="title-page-name"/>        self.final_value = final_value<br class="title-page-name"/>        self.decay_factor = (initial_value - final_value) / max_steps<br class="title-page-name"/><br class="title-page-name"/>    def __call__(self, step_num):<br class="title-page-name"/>        current_value = self.initial_value - self.decay_factor * step_num<br class="title-page-name"/>        if current_value &lt; self.final_value:<br class="title-page-name"/>            current_value = self.final_value<br class="title-page-name"/>        return current_value<br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    import matplotlib.pyplot as plt<br class="title-page-name"/>    epsilon_initial = 1.0<br class="title-page-name"/>    epsilon_final = 0.05<br class="title-page-name"/>    MAX_NUM_EPISODES = 10000<br class="title-page-name"/>    MAX_STEPS_PER_EPISODE = 300<br class="title-page-name"/>    linear_sched = LinearDecaySchedule(initial_value = epsilon_initial,<br class="title-page-name"/>                                    final_value = epsilon_final,<br class="title-page-name"/>                                    max_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)<br class="title-page-name"/>    epsilon = [linear_sched(step) for step in range(MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)]<br class="title-page-name"/>    plt.plot(epsilon)<br class="title-page-name"/>    plt.show()</pre>
<p class="calibre2">前面的脚本可以在本书的代码库中的<kbd class="calibre12">ch6/utils/decay_schedule.py</kbd>找到。如果您运行该脚本，您将会看到<kbd class="calibre12">main </kbd>函数为epsilon创建了一个线性衰减时间表并绘制了该值。您可以使用不同的<kbd class="calibre12">MAX_NUM_EPISODES</kbd>、<kbd class="calibre12">MAX_STEPS_PER_EPISODE</kbd>、<kbd class="calibre12">epsilon_initial</kbd>和<kbd class="calibre12">epsilon_final</kbd>值进行实验，以直观地查看ε值如何随步数而变化。在下一节中，我们将实现<kbd class="calibre12">get_action(...)</kbd>方法，该方法实现了<img class="fm-editor-equation59" src="img/00143.jpeg"/>-贪婪动作选择策略。</p>


            

            
        
    



  
    <title>Implementing a deep Q-learning agent</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现深度Q学习代理</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将讨论如何将我们的浅层Q学习器扩展为一个更加复杂和强大的基于深层Q学习器的智能体，它可以学习根据原始视觉图像输入采取行动，我们将在本章结束时使用它来训练智能体玩好Atari游戏。请注意，您可以在任何具有离散动作空间的学习环境中训练这个深度Q学习代理。Atari游戏环境就是我们将在本书中使用的一类有趣的环境。</p>
<p class="calibre2">我们将从一个深度卷积Q网络实现开始，并将其合并到我们的Q学习器中。然后，我们将看到如何使用目标Q网络的技术来提高深度Q学习者的稳定性。然后，我们将结合我们到目前为止讨论的所有技术，将基于deep Q学习的代理的完整实现放在一起。</p>


            

            
        
    



  
    <title>Implementing a deep convolutional Q-network in PyTorch</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">在PyTorch中实现深度卷积Q网络</h1>
                
            
            
                
<p class="calibre2">让我们实现一个3层深度<strong class="calibre4">卷积神经网络</strong> ( <strong class="calibre4"> CNN </strong>)，它将Atari游戏屏幕像素作为输入，并输出该特定游戏的每个可能动作的动作值，这是在OpenAI Gym环境中定义的。以下代码适用于CNN类:</p>
<pre class="calibre17">import torch<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class CNN(torch.nn.Module):<br class="title-page-name"/>    """<br class="title-page-name"/>    A Convolution Neural Network (CNN) class to approximate functions with visual/image inputs<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, input_shape, output_shape, device="cpu"):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param input_shape: Shape/dimension of the input image. Assumed to be resized to C x 84 x 84<br class="title-page-name"/>        :param output_shape: Shape/dimension of the output.<br class="title-page-name"/>        :param device: The device (cpu or cuda) that the CNN should use to store the inputs for the forward pass<br class="title-page-name"/>        """<br class="title-page-name"/>        # input_shape: C x 84 x 84<br class="title-page-name"/>        super(CNN, self).__init__()<br class="title-page-name"/>        self.device = device<br class="title-page-name"/>        self.layer1 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2, padding=1),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.layer2 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=0),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.layer3 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.out = torch.nn.Linear(18 * 18 * 32, output_shape)<br class="title-page-name"/><br class="title-page-name"/>    def forward(self, x):<br class="title-page-name"/>        x = torch.from_numpy(x).float().to(self.device)<br class="title-page-name"/>        x = self.layer1(x)<br class="title-page-name"/>        x = self.layer2(x)<br class="title-page-name"/>        x = self.layer3(x)<br class="title-page-name"/>        x = x.view(x.shape[0], -1)<br class="title-page-name"/>        x = self.out(x)<br class="title-page-name"/>        return x</pre>
<p>如您所见，向神经网络添加更多层很容易。我们可以使用具有三层以上的更深层次的网络，但这将以需要更多计算能力和时间为代价。在深度强化学习中，尤其是在具有函数逼近的Q学习中，没有被证明的收敛保证。因此，在通过使用更深层次的神经网络来增加Q/值函数表示的容量之前，我们应该确保我们的代理的实现足够好，能够很好地学习和取得进展。</p>


            

            
        
    



  
    <title>Using the target Q-network to stabilize an agent's learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">使用目标Q网络来稳定智能体的学习</h1>
                
            
            
                
<p class="calibre2">一种简单的技术是将Q网络冻结固定数量的步骤，然后使用它来生成Q学习目标以更新深度Q网络的参数，这种技术被证明在减少振荡和稳定具有神经网络逼近的Q学习方面相当有效。这个技巧相对来说比较简单，但事实证明它对稳定学习非常有帮助。</p>
<p class="calibre2">实现将会简单明了。我们必须对现有的deep Q-learner类进行两项更改或更新:</p>
<ol class="calibre14">
<li value="1" class="calibre11">创建一个目标Q网络，并定期与原始Q网络同步/更新</li>
<li value="2" class="calibre11">使用目标Q网络来生成Q学习目标</li>
</ol>
<p class="calibre2">要比较代理在使用和不使用目标Q-network的情况下的性能，您可以使用我们在本章前面几节中开发的参数管理器以及日志记录和可视化工具来直观地验证启用目标Q-network后的性能提升。</p>
<pre>self.DQN in the <kbd class="calibre12">deep_Q_learner.py</kbd> script:</pre>
<pre class="calibre17">self.Q = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)<br class="title-page-name"/><strong class="calibre1">if self.params['use_target_network']:</strong><br class="title-page-name"/><strong class="calibre1">    self.Q_target = self.DQN(state_shape, action_shape, device).to(device)</strong></pre>
<p class="calibre2">然后，我们可以修改前面实现的<kbd class="calibre12">learn_from_batch_experience</kbd>方法，使用目标Q网络来创建Q学习目标。下面的代码片段以粗体显示了我们第一次实现中的变化:</p>
<pre class="calibre17">def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        <strong class="calibre1">if self.params['use_target_network']:</strong><br class="title-page-name"/><strong class="calibre1">            if self.step_num % self.params['target_network_update_freq'] == 0:</strong><br class="title-page-name"/><strong class="calibre1">                # The *update_freq is the Num steps after which target net is updated.</strong><br class="title-page-name"/><strong class="calibre1">                # A schedule can be used instead to vary the update freq.</strong><br class="title-page-name"/><strong class="calibre1">                self.Q_target.load_state_dict(self.Q.state_dict())</strong><br class="title-page-name"/><strong class="calibre1">            td_target = reward_batch + ~done_batch * \</strong><br class="title-page-name"/><strong class="calibre1">                np.tile(self.gamma, len(next_obs_batch)) * \</strong><br class="title-page-name"/><strong class="calibre1">                self.Q_target(next_obs_batch).max(1)[0].data</strong><br class="title-page-name"/>       <strong class="calibre1"> else:</strong><br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>                                                       td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        writer.add_scalar("DQL/td_error", td_error.mean(), self.step_num)<br class="title-page-name"/>        self.Q_optimizer.step()</pre>
<p class="calibre2">这就完成了我们的目标Q网络实现。</p>
<p class="calibre2">我们如何知道代理是否受益于目标Q网络和我们在前面章节中讨论的其他改进？在下一节中，我们将研究记录和可视化代理性能的方法，以便我们可以监视和判断我们讨论的改进是否真的会带来更好的性能。</p>


            

            
        
    



  
    <title>Logging and visualizing an agent's learning process</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">记录和可视化代理的学习过程</h1>
                
            
            
                
<p class="calibre2">我们现在有了一个学习代理，它使用神经网络来学习Q值，并自我更新以更好地完成任务。代理在开始明智地行动之前需要一段时间来学习。我们如何知道代理在给定时间发生了什么？我们如何知道代理是在进步还是只是在装傻？我们如何看到和测量代理随时间的进展？我们应该坐以待毙吗？不，应该有更好的办法，你不觉得吗？</p>
<p class="calibre2">是的，确实有！对于我们这些代理的开发者来说，能够观察代理是如何执行的，以便弄清楚实现是否有问题，或者某些超参数是否太差，以至于代理学不到任何东西，这实际上是很重要的。我们已经有了日志记录的初步版本，并且看到了代理的学习是如何通过使用打印语句生成的控制台输出来进行的。这给了我们第一手资料，包括剧集数量、剧集奖励、最高奖励等等，但它更像是特定时间的一张快照。我们希望能够看到进展的历史，看看代理的学习是否收敛，学习误差是否减少，等等。这将使我们能够朝着正确的方向思考，以更新我们的实现或调整参数来提高代理的学习性能。</p>
<p class="calibre2">TensorFlow深度学习库提供了一个叫做TensorBoard的工具。它是一个强大的工具，可以可视化神经网络图，绘制量化指标，如学习错误、奖励，等等。它甚至可以用来可视化图像和一些其他有用的数据类型。这使得理解、识别和调试我们的深度学习算法实现变得更加容易。在下一节中，我们将看到如何使用TensorBoard来记录和可视化代理的进度。</p>


            

            
        
    



  
    <title>Using TensorBoard for logging and visualizing a PyTorch RL agent's progress</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">使用TensorBoard记录和可视化PyTorch RL代理的进度</h1>
                
            
            
                
<p class="calibre2">虽然TensorBoard是为TensorFlow深度学习库发布的工具，但它本身是一个灵活的工具，可以与PyTorch等其他深度学习库一起使用。基本上，TensorBoard工具从日志文件中读取TensorFlow事件摘要数据，并定期更新可视化和绘图。幸运的是，我们有一个名为<kbd class="calibre12">tensorboardX</kbd>的库，它提供了一个方便的接口来创建TensorBoard可以处理的事件。这样，我们可以很容易地从代理训练代码中生成适当的事件，来记录和可视化代理的学习过程是如何进行的。这个库的使用非常简单明了。我们导入<kbd class="calibre12">tensorboardX</kbd>并创建一个具有所需日志文件名的<kbd class="calibre12">SummaryWriter</kbd>对象。然后，我们可以使用<kbd class="calibre12">SummaryWriter</kbd>对象添加新的标量(以及其他支持的数据),将新的数据点添加到图表中，图表将定期更新。下面的屏幕截图是TensorBoard输出的一个示例，我们将在代理培训脚本中记录这类信息，以可视化其进度:</p>
<p class="cdpaligncenter4"><img src="img/00144.jpeg" class="calibre71"/></p>
<p class="calibre2">在前面的截图中，最右下角的标题为<strong class="calibre4"> main/mean_ep_reward </strong>的图显示了代理如何学习随着时间的推移逐步获得越来越高的奖励。在前面截图的所有图中，<em class="calibre13">x</em>-轴显示训练步数，而<em class="calibre13">y</em>-轴显示记录的数据值，如每个图的标题所示。</p>
<p class="calibre2">现在，我们知道了如何记录和可视化代理在训练时的表现。但是仍然有一个问题，那就是我们如何比较代理是否有我们在本章前面讨论的一个或多个改进。我们讨论了几个改进，每个都增加了新的超参数。为了管理各种超参数，并方便地打开和关闭改进和配置，在下一节中，我们将讨论通过构建一个简单的参数管理类来实现这一点的方法。</p>


            

            
        
    



  
    <title>Managing hyperparameters and configuration parameters</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">管理超参数和配置参数</h1>
                
            
            
                
<p class="calibre2">您可能已经注意到，我们的代理有几个超参数，如学习率、gamma、epsilon开始/最小值等等。还有几个针对代理和环境的配置参数，我们希望能够轻松地修改和运行这些参数，而不是在代码中搜索参数的定义位置。当我们想要自动执行训练过程或运行参数扫描或其他方法来调整和找到为代理工作的最佳参数集时，有一个简单且好的方法来管理这些参数也很有帮助。</p>
<p class="calibre2">在下面的两个小节中，我们将了解如何使用JSON文件以一种易于使用的方式指定参数和超参数，并实现一个参数管理器类来处理这些外部可配置的参数，以更新代理和环境配置。</p>


            

            
        
    



  
    <title>Using a JSON file to easily configure parameters</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">使用JSON文件轻松配置参数</h1>
                
            
            
                
<pre>parameters.json file that we will use to configure the parameters of the agent and the environment. The <strong class="calibre4">JavaScript Object Notation</strong> (<strong class="calibre4">JSON</strong>) file is a convenient and human-readable format for such data representation. We will discuss what each of these parameters mean in the later sections of this chapter. For now, we will concentrate on how we can use such a file to specify or change the parameters used by the agent and the environment:</pre>
<pre class="calibre17">{<br class="title-page-name"/>  "agent": {<br class="title-page-name"/>    "max_num_episodes": 70000,<br class="title-page-name"/>    "max_steps_per_episode": 300,<br class="title-page-name"/>    "replay_batch_size": 2000,<br class="title-page-name"/>    "use_target_network": true,<br class="title-page-name"/>    "target_network_update_freq": 2000,<br class="title-page-name"/>    "lr": 5e-3,<br class="title-page-name"/>    "gamma": 0.98,<br class="title-page-name"/>    "epsilon_max": 1.0,<br class="title-page-name"/>    "epsilon_min": 0.05,<br class="title-page-name"/>    "seed": 555,<br class="title-page-name"/>    "use_cuda": true,<br class="title-page-name"/>    "summary_filename_prefix": "logs/DQL_"<br class="title-page-name"/>  },<br class="title-page-name"/>  "env": {<br class="title-page-name"/>    "type": "Atari",<br class="title-page-name"/>    "episodic_life": "True",<br class="title-page-name"/>    "clip_reward": "True",<br class="title-page-name"/>    "useful_region": {<br class="title-page-name"/>        "Default":{<br class="title-page-name"/>                "crop1": 34,<br class="title-page-name"/>                "crop2": 34,<br class="title-page-name"/>                "dimension2": 80<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>}</pre>


            

            
        
    



  
    <title>The parameters manager</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">参数管理器</h1>
                
            
            
                
<p class="calibre2">您喜欢刚才看到的参数配置文件示例吗？我希望你做到了。在本节中，我们将实现一个参数管理器，它将帮助我们根据需要加载、获取和设置这些参数。</p>
<p class="calibre2">我们将首先创建一个名为<kbd class="calibre12">ParamsManger</kbd>的Python类，它使用JSON Python库从<kbd class="calibre12">params_file</kbd>中读取的参数字典初始化<kbd class="calibre12">params</kbd>成员变量，如下所示:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import JSON<br class="title-page-name"/><br class="title-page-name"/>class ParamsManager(object):<br class="title-page-name"/>    def __init__(self, params_file):<br class="title-page-name"/>        """<br class="title-page-name"/>        A class to manage the Parameters. Parameters include configuration parameters and Hyper-parameters<br class="title-page-name"/>        :param params_file: Path to the parameters JSON file<br class="title-page-name"/>        """<br class="title-page-name"/>        self.params = JSON.load(open(params_file, 'r'))</pre>
<p class="calibre2">然后我们将实现一些对我们来说方便的方法。我们将从返回我们从JSON文件中读取的整个参数字典的<kbd class="calibre12">get_params</kbd>方法开始:</p>
<pre class="calibre17">    def get_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns all the parameters<br class="title-page-name"/>        :return: The whole parameter dictionary<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params</pre>
<p class="calibre2">有时，我们可能只想获得与代理或环境相对应的参数，这些参数可以在初始化代理或环境时传入。由于我们已经在上一节看到的<kbd class="calibre12">parameters.json</kbd>文件中巧妙地分离了代理和环境参数，所以实现很简单，如下所示:</p>
<pre class="calibre17"> def get_env_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns the environment configuration parameters<br class="title-page-name"/>        :return: A dictionary of configuration parameters used for the environment<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params['env']<br class="title-page-name"/>    def get_agent_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns the hyper-parameters and configuration parameters used by the agent<br class="title-page-name"/>        :return: A dictionary of parameters used by the agent<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params['agent']</pre>
<p class="calibre2">我们还将实现另一个简单的方法来更新代理参数，以便我们在启动培训脚本时也可以从命令行提供/读取代理参数:</p>
<pre class="calibre17">   <br class="title-page-name"/>    def update_agent_params(self, **kwargs):<br class="title-page-name"/>        """<br class="title-page-name"/>        Update the hyper-parameters (and configuration parameters) used by the agent<br class="title-page-name"/>        :param kwargs: Comma-separated, hyper-parameter-key=value pairs. Eg.: lr=0.005, gamma=0.98<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        for key, value in kwargs.items():<br class="title-page-name"/>            if key in self.params['agent'].keys():<br class="title-page-name"/>                self.params['agent'][key] = value</pre>
<p class="calibre2">前面的参数管理器实现，以及一个简单的测试过程，可以在本书的代码库中的<kbd class="calibre12">ch6/utils/params_manager.py</kbd>获得。在下一节中，我们将整合我们迄今为止讨论和实现的所有技术，以构建一个完整的基于深度Q学习的代理。</p>


            

            
        
    



  
    <title>A complete deep Q-learner to solve complex problems with raw pixel input</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">一个完整的深度Q学习器，解决原始像素输入的复杂问题</h1>
                
            
            
                
<p class="calibre2">从本章开始，我们已经实现了一些额外的技术和实用工具来改进代理。在这一节中，我们将把我们到目前为止讨论过的所有改进和实用工具整合到一个统一的<kbd class="calibre12">deep_Q_Learner.py</kbd>脚本中。在下一节中，我们将使用这个统一的代理脚本在Atari健身房环境中进行训练，并观察代理随着时间的推移改进其性能并获得越来越多的分数。</p>
<p class="calibre2">以下代码是统一版本，利用了我们在本章前面几节中开发的以下功能:</p>
<ul class="calibre10">
<li class="calibre11">体验记忆</li>
<li class="calibre11">体验回放，从(小)批体验中学习</li>
<li class="calibre11">线性ε衰变时间表</li>
<li class="calibre11">稳定学习的目标网络</li>
</ul>
<ul class="calibre10">
<li class="calibre11">使用JSON文件进行参数管理</li>
<li class="calibre11">使用TensorBoard进行性能可视化和记录；</li>
</ul>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>import gym<br class="title-page-name"/>import torch<br class="title-page-name"/>import random<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>import environment.atari as Atari<br class="title-page-name"/>from utils.params_manager import ParamsManager<br class="title-page-name"/>from utils.decay_schedule import LinearDecaySchedule<br class="title-page-name"/>from utils.experience_memory import Experience, ExperienceMemory<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/>from function_approximator.cnn import CNN<br class="title-page-name"/>from tensorboardX import SummaryWriter<br class="title-page-name"/>from datetime import datetime<br class="title-page-name"/>from argparse import ArgumentParser<br class="title-page-name"/><br class="title-page-name"/>args = ArgumentParser("deep_Q_learner")<br class="title-page-name"/>args.add_argument("--params-file",<br class="title-page-name"/>                  help="Path to the parameters JSON file. Default is parameters.JSON",<br class="title-page-name"/>                  default="parameters.JSON",<br class="title-page-name"/>                  type=str,<br class="title-page-name"/>                  metavar="PFILE")<br class="title-page-name"/>args.add_argument("--env-name",<br class="title-page-name"/>                  help="ID of the Atari environment available in OpenAI Gym. Default is Pong-v0",<br class="title-page-name"/>                  default="Pong-v0",<br class="title-page-name"/>                  type=str,<br class="title-page-name"/>                  metavar="ENV")<br class="title-page-name"/>args = args.parse_args()<br class="title-page-name"/><br class="title-page-name"/>params_manager= ParamsManager(args.params_file)<br class="title-page-name"/>seed = params_manager.get_agent_params()['seed']<br class="title-page-name"/>summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']<br class="title-page-name"/>summary_file_name = summary_file_path_prefix + args.env_name + "_" + datetime.now().strftime("%y-%m-%d-%H-%M")<br class="title-page-name"/>writer = SummaryWriter(summary_file_name)<br class="title-page-name"/>global_step_num = 0<br class="title-page-name"/>use_cuda = params_manager.get_agent_params()['use_cuda']<br class="title-page-name"/># new in PyTorch 0.4<br class="title-page-name"/>device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")<br class="title-page-name"/>torch.manual_seed(seed)<br class="title-page-name"/>np.random.seed(seed)<br class="title-page-name"/>if torch.cuda.is_available() and use_cuda:<br class="title-page-name"/>    torch.cuda.manual_seed_all(seed)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Deep_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, params):<br class="title-page-name"/>        """<br class="title-page-name"/>        self.Q is the Action-Value function. This agent represents Q using a Neural Network<br class="title-page-name"/>        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional<br class="title-page-name"/>        image, use a Convolutional-Neural-Network<br class="title-page-name"/><br class="title-page-name"/>        :param state_shape: Shape (tuple) of the observation/state<br class="title-page-name"/>        :param action_shape: Shape (number) of the discrete action space<br class="title-page-name"/>        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters<br class="title-page-name"/>        """<br class="title-page-name"/>        self.state_shape = state_shape<br class="title-page-name"/>        self.action_shape = action_shape<br class="title-page-name"/>        self.params = params<br class="title-page-name"/>        self.gamma = self.params['gamma'] # Agent's discount factor<br class="title-page-name"/>        self.learning_rate = self.params['lr'] # Agent's Q-learning rate<br class="title-page-name"/><br class="title-page-name"/>        if len(self.state_shape) == 1: # Single dimensional observation/state space<br class="title-page-name"/>            self.DQN = SLP<br class="title-page-name"/>        elif len(self.state_shape) == 3: # 3D/image observation/state<br class="title-page-name"/>            self.DQN = CNN<br class="title-page-name"/><br class="title-page-name"/>        self.Q = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)<br class="title-page-name"/>        if self.params['use_target_network']:<br class="title-page-name"/>            self.Q_target = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>        # self.policy is the policy followed by the agent. This agents follows<br class="title-page-name"/>        # an epsilon-greedy policy w.r.t it's Q estimate.<br class="title-page-name"/>        self.policy = self.epsilon_greedy_Q<br class="title-page-name"/>        self.epsilon_max = 1.0<br class="title-page-name"/>        self.epsilon_min = 0.05<br class="title-page-name"/>        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,<br class="title-page-name"/>                                    final_value=self.epsilon_min,<br class="title-page-name"/>                                    max_steps= self.params['epsilon_decay_final_step'])<br class="title-page-name"/>        self.step_num = 0<br class="title-page-name"/>                <br class="title-page-name"/>        self.memory = ExperienceMemory(capacity=int(self.params['experience_memory_capacity'])) # Initialize an Experience memory with 1M capacity<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        if len(observation.shape) == 3: # Single image (not a batch)<br class="title-page-name"/>            if observation.shape[2] &lt; observation.shape[0]: # Probably observation is in W x H x C format<br class="title-page-name"/>                # Reshape to C x H x W format as per PyTorch's convention<br class="title-page-name"/>                observation = observation.reshape(observation.shape[2], observation.shape[1], observation.shape[0])<br class="title-page-name"/>            observation = np.expand_dims(observation, 0) # Create a batch dimension<br class="title-page-name"/>        return self.policy(observation)<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        # Decay Epsilon/exploration as per schedule<br class="title-page-name"/>        writer.add_scalar("DQL/epsilon", self.epsilon_decay(self.step_num), self.step_num)<br class="title-page-name"/>        self.step_num +=1<br class="title-page-name"/>        if random.random() &lt; self.epsilon_decay(self.step_num):<br class="title-page-name"/>            action = random.choice([i for i in range(self.action_shape)])<br class="title-page-name"/>        else:<br class="title-page-name"/>            action = np.argmax(self.Q(observation).data.to(torch.device('cpu')).numpy())<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>        return action<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next, done):<br class="title-page-name"/>        # TD(0) Q-learning<br class="title-page-name"/>        if done: # End of episode<br class="title-page-name"/>            td_target = reward + 0.0 # Set the value of terminal state to zero<br class="title-page-name"/>        else:<br class="title-page-name"/>            td_target = r + self.gamma * torch.max(self.Q(s_next))<br class="title-page-name"/>        td_error = td_target - self.Q(s)[a]<br class="title-page-name"/>        # Update Q estimate<br class="title-page-name"/>        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/>    def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        if self.params['use_target_network']:<br class="title-page-name"/>            if self.step_num % self.params['target_network_update_freq'] == 0:<br class="title-page-name"/>                # The *update_freq is the Num steps after which target net is updated.<br class="title-page-name"/>                # A schedule can be used instead to vary the update freq.<br class="title-page-name"/>                self.Q_target.load_state_dict(self.Q.state_dict())<br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q_target(next_obs_batch).max(1)[0].data<br class="title-page-name"/>        else:<br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>                                                       td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        writer.add_scalar("DQL/td_error", td_error.mean(), self.step_num)<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/>    def replay_experience(self, batch_size = None):<br class="title-page-name"/>        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']<br class="title-page-name"/>        experience_batch = self.memory.sample(batch_size)<br class="title-page-name"/>        self.learn_from_batch_experience(experience_batch)<br class="title-page-name"/><br class="title-page-name"/>    def save(self, env_name):<br class="title-page-name"/>        file_name = self.params['save_dir'] + "DQL_" + env_name + ".ptm"<br class="title-page-name"/>        torch.save(self.Q.state_dict(), file_name)<br class="title-page-name"/>        print("Agent's Q model state saved to ", file_name)<br class="title-page-name"/><br class="title-page-name"/>    def load(self, env_name):<br class="title-page-name"/>        file_name = self.params['load_dir'] + "DQL_" + env_name + ".ptm"<br class="title-page-name"/>        self.Q.load_state_dict(torch.load(file_name))<br class="title-page-name"/>        print("Loaded Q model state from", file_name)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env_conf = params_manager.get_env_params()<br class="title-page-name"/>    env_conf["env_name"] = args.env_name<br class="title-page-name"/>    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default<br class="title-page-name"/>    custom_region_available = False<br class="title-page-name"/>    for key, value in env_conf['useful_region'].items():<br class="title-page-name"/>        if key in args.env_name:<br class="title-page-name"/>            env_conf['useful_region'] = value<br class="title-page-name"/>            custom_region_available = True<br class="title-page-name"/>            break<br class="title-page-name"/>    if custom_region_available is not True:<br class="title-page-name"/>        env_conf['useful_region'] = env_conf['useful_region']['Default']<br class="title-page-name"/>    print("Using env_conf:", env_conf)<br class="title-page-name"/>    env = Atari.make_env(args.env_name, env_conf)<br class="title-page-name"/>    observation_shape = env.observation_space.shape<br class="title-page-name"/>    action_shape = env.action_space.n<br class="title-page-name"/>    agent_params = params_manager.get_agent_params()<br class="title-page-name"/>    agent = Deep_Q_Learner(observation_shape, action_shape, agent_params)<br class="title-page-name"/>    if agent_params['load_trained_model']:<br class="title-page-name"/>        try:<br class="title-page-name"/>            agent.load(env_conf["env_name"])<br class="title-page-name"/>        except FileNotFoundError:<br class="title-page-name"/>            print("WARNING: No trained model found for this environment. Training from scratch.")<br class="title-page-name"/>    first_episode = True<br class="title-page-name"/>    episode_rewards = list()<br class="title-page-name"/>    for episode in range(agent_params['max_num_episodes']):<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        done = False<br class="title-page-name"/>        step = 0<br class="title-page-name"/>        #for step in range(agent_params['max_steps_per_episode']):<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            if env_conf['render']:<br class="title-page-name"/>                env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            #agent.learn(obs, action, reward, next_obs, done)<br class="title-page-name"/>            agent.memory.store(Experience(obs, action, reward, next_obs, done))<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/>            step += 1<br class="title-page-name"/>            global_step_num +=1<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                if first_episode: # Initialize max_reward at the end of first episode<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    first_episode = False<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; max_reward:<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    agent.save(env_conf['env_name'])<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))<br class="title-page-name"/>                writer.add_scalar("main/ep_reward", cum_reward, global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/mean_ep_reward", np.mean(episode_rewards), global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/max_ep_rew", max_reward, global_step_num)<br class="title-page-name"/>                if agent.memory.get_size() &gt;= 2 * agent_params['replay_batch_size']:<br class="title-page-name"/>                    agent.replay_experience()<br class="title-page-name"/><br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()<br class="title-page-name"/>    writer.close()</pre>
<p class="calibre2">前面的代码，以及我们将在下一节讨论的使用Atari包装器所需的一些额外的更改，可以在本书的代码库中的<kbd class="calibre12">ch6/deep_Q_Learner.py</kbd>处获得。在我们完成下一部分<em class="calibre13">雅达利健身房环境</em>后，我们将使用<kbd class="calibre12">deep_Q_Learner.py</kbd>中的代理实现来培训雅达利游戏上的代理，并最终观察他们的表现。</p>
<p>这本书的代码库将有最新的代码实现，以及改进和错误修复，这些将在这本书出版后提交。因此，在GitHub上观看这本书的代码库来获得关于这些变化和改进的自动更新是一个好主意。</p>


            

            
        
    



  
    <title>The Atari Gym environment</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">雅达利健身房环境</h1>
                
            
            
                
<p class="calibre2">在<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">第4章</a>、<em class="calibre13">探索健身房及其功能</em>中，我们查看了健身房中可用的各种环境列表，包括Atari游戏类别，并使用一个脚本列出了您计算机上可用的所有健身房环境。我们还查看了环境名称的命名法，尤其是Atari游戏。在这一节中，我们将使用Atari环境，看看如何用Gym环境包装器定制环境。以下是来自9个不同雅达利环境的9个截图的拼贴:</p>
<div><img src="img/00145.jpeg" class="calibre72"/></div>


            

            
        
    



  
    <title>Customizing the Atari Gym environment</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">定制雅达利健身房环境</h1>
                
            
            
                
<p class="calibre2">有时，我们可能希望改变环境发送回观察结果的方式，或者改变奖励的比例，以便我们的代理可以学习得更好，或者在代理接收到它们之前过滤掉一些信息，或者改变环境在屏幕上的呈现方式。到目前为止，我们一直在开发和定制我们的代理，使其在环境中表现良好。围绕环境如何以及向代理发送什么样的信息有一些灵活性不是很好吗，这样我们就可以定制代理学习如何行动了。幸运的是，借助于Gym环境包装器，Gym库可以很容易地扩展或定制环境发送的信息。包装器接口允许我们子类化和添加例程，作为先前例程之上的层。我们可以将自定义处理语句添加到Gym环境类的以下一个或多个方法中:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">__init__(self, env)__</kbd></li>
<li class="calibre11"><kbd class="calibre12">_seed</kbd></li>
<li class="calibre11"><kbd class="calibre12">_reset</kbd></li>
<li class="calibre11"><kbd class="calibre12">_step</kbd></li>
<li class="calibre11"><kbd class="calibre12">_render</kbd></li>
<li class="calibre11"><kbd class="calibre12">_close</kbd></li>
</ul>
<p class="calibre2">根据我们想要对环境进行的定制，我们可以决定我们想要扩展哪些方法。例如，如果我们想要改变观察的形状/大小，我们可以扩展<kbd class="calibre12">_step</kbd>和<kbd class="calibre12">_reset</kbd>方法。在下一小节中，我们将看到如何利用包装器接口来定制Atari健身房环境。</p>


            

            
        
    



  
    <title>Implementing custom Gym environment wrappers</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现定制的体育馆环境包装器</h1>
                
            
            
                
<p class="calibre2">在这一节中，我们将研究一些对体育馆Atari环境特别有用的体育馆环境包装器。我们将在本节中实现的大多数包装器也可以用于其他环境，以提高代理的学习性能。</p>
<p class="calibre2">下表列出了将在下一节中实现的包装器，并对每个包装器进行了简要描述，以提供一个概述:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">包装器</strong></td>
<td class="calibre48"><strong class="calibre1">目的简述</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">ClipRewardEnv</kbd></p>
</td>
<td class="calibre48">要实现奖励削波</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">AtariRescale</kbd></td>
<td class="calibre48">要将屏幕像素重新调整为84x84x1灰度图像</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">NormalizedEnv</kbd></td>
<td class="calibre48">基于在环境中观察到的平均值和方差来归一化图像</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">NoopResetEnv</kbd></td>
<td class="calibre48">复位时执行随机数量的<kbd class="calibre12">noop</kbd>(空)动作，以采样不同的初始状态</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">FireResetEnv</kbd></td>
<td class="calibre48">重置时执行触发操作</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">EpisodicLifeEnv</kbd></td>
<td class="calibre48">将生命结束标记为剧集结束，并在游戏结束时重置</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">MaxAndSkipEnv</kbd></td>
<td class="calibre48">重复固定次数的动作(使用<kbd class="calibre12">skip</kbd>参数指定；默认值为4)步</td>
</tr>
</tbody>
</table>


            

            
        
    



  
    <title>Reward clipping</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">奖励剪报</h1>
                
            
            
                
<p class="calibre2">不同的问题或环境提供不同范围的奖励值。例如，我们在前一章中看到，在<kbd class="calibre12">Mountain Car v0</kbd>环境中，代理人在剧集终止前的每一个时间步都会收到-1的奖励，无论代理人以哪种方式移动汽车。在<kbd class="calibre12">Cart Pole v0</kbd>环境中，直到剧集结束，代理人每一个时间步都会收到+1的奖励。在像MS Pac-Man这样的雅达利游戏环境中，如果代理吃掉一个幽灵，它将获得高达+1600的奖励。我们可以开始看到奖励的大小以及奖励的时机在不同的环境和学习问题中有很大的不同。如果我们的深度Q学习者代理算法必须解决各种各样的问题，而不需要我们尝试微调超参数来独立地为每个环境工作，我们必须对不同的奖励等级做些什么。这正是奖励削减背后的直觉，其中我们根据从环境中收到的实际奖励的符号，将奖励削减为-1、0或+1。通过这种方式，我们限制了奖励的幅度，而奖励的幅度在不同的环境中会有很大的不同。我们可以通过从<kbd class="calibre12">gym.RewardWrapper</kbd>类继承并修改<kbd class="calibre12">reward(...) </kbd>函数来实现这个简单的奖励裁剪技术，并将其应用到我们的环境中，如下面的代码片段所示:</p>
<pre class="calibre17">class ClipRewardEnv(gym.RewardWrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        gym.RewardWrapper.__init__(self, env)<br class="title-page-name"/><br class="title-page-name"/>    def reward(self, reward):<br class="title-page-name"/>        """ Clip rewards to be either -1, 0 or +1 based on the sign"""<br class="title-page-name"/>        return np.sign(reward)</pre>
<p>将奖励削减到(-1，0，1)的技术对Atari游戏很有效。但是，很高兴知道，这可能不是最好的技术，普遍处理不同的奖励幅度和频率的环境。削减奖励值修改了代理的学习目标，并且有时可能导致代理所学习的策略在质量上不同于所期望的。</p>


            

            
        
    



  
    <title>Preprocessing Atari screen image frames</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">预处理Atari屏幕图像帧</h1>
                
            
            
                
<p class="calibre2">Atari Gym环境生成的观测值通常具有210x160x3的形状，这表示宽度为210像素、高度为160像素的RGB(彩色)图像。虽然原始分辨率为210x160x3的彩色图像有更多的像素，因此也有更多的信息，但事实证明，降低分辨率通常可以获得更好的性能。更低的分辨率意味着代理在每一步处理的数据更少，这意味着更快的训练时间，尤其是在你我拥有的消费级计算硬件上。</p>
<p class="calibre2">让我们创建一个预处理管道，它将获取原始观察图像(Atari屏幕的)并执行以下操作:</p>
<div><img src="img/00146.jpeg" class="calibre73"/></div>
<p class="calibre2">我们可以裁剪掉屏幕上没有任何关于代理环境的有用信息的区域。</p>
<p class="calibre2">最后，我们将图像的尺寸调整为84x84。我们可以选择不同的数字，而不是84，只要它包含合理数量的像素。然而，使用正方形矩阵(如84x84或80x80)是有效的，因为卷积运算(例如CUDA)针对此类正方形输入进行了优化:</p>
<pre class="calibre17">def process_frame_84(frame, conf):<br class="title-page-name"/>    frame = frame[conf["crop1"]:conf["crop2"] + 160, :160]<br class="title-page-name"/>    frame = frame.mean(2)<br class="title-page-name"/>    frame = frame.astype(np.float32)<br class="title-page-name"/>    frame *= (1.0 / 255.0)<br class="title-page-name"/>    frame = cv2.resize(frame, (84, conf["dimension2"]))<br class="title-page-name"/>    frame = cv2.resize(frame, (84, 84))<br class="title-page-name"/>    frame = np.reshape(frame, [1, 84, 84])<br class="title-page-name"/>    return frame<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class AtariRescale(gym.ObservationWrapper):<br class="title-page-name"/>    def __init__(self, env, env_conf):<br class="title-page-name"/>        gym.ObservationWrapper.__init__(self, env)<br class="title-page-name"/>        self.observation_space = Box(0.0, 1.0, [1, 84, 84])<br class="title-page-name"/>        self.conf = env_conf<br class="title-page-name"/><br class="title-page-name"/>    def observation(self, observation):<br class="title-page-name"/>        return process_frame_84(observation, self.conf)</pre>
<p>注意，对于一个数据类型为<kbd class="calibre28">numpy.float32</kbd>的观察帧，分辨率为84×84像素，占用4个字节，我们需要大约4×84×84 = 28，224个字节。您可能还记得<em class="calibre46">体验内存</em>部分，一个体验对象包含两个帧(一个用于观察，另一个用于下一次观察)，这意味着我们需要2x 28，224 = 56，448字节(+ 2字节用于<em class="calibre46">动作</em> + 4字节用于<em class="calibre46">奖励)。</em>56，448字节(或0.056448 MB)可能看起来不多，但如果您考虑到通常使用1e6(百万)数量级的体验内存容量这一事实，您可能会意识到我们大约需要1e6 x 0.056448 MB = 56，448 MB或56.448 GB！这意味着我们将需要56.448 GB的RAM，仅用于容量为100万次体验的体验内存！</p>
<p>您可以进行一些内存优化，以减少训练代理所需的RAM。在某些游戏中，使用较小的经验内存是减少内存占用的直接方法。在某些环境中，拥有更大的经验记忆将有助于代理更快地学习。减少内存占用的一种方法是在存储时不缩放帧(除以255)，这需要浮点表示(<kbd class="calibre28">numpy.float32</kbd>)，而是将帧存储为numpy.uint8，这样我们每像素只需要1个字节，而不是4个字节，这将有助于将内存需求减少到原来的4倍。然后，当我们想要使用我们正向传递到深度Q网络的网络中存储的经验来获得Q值预测时，我们可以将图像缩放到0.0到1.0的范围内。</p>


            

            
        
    



  
    <title>Normalizing observations</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">标准化观察</h1>
                
            
            
                
<p class="calibre2">在某些情况下，规范化观测值有助于加快收敛速度。最常用的标准化过程包括两个步骤:</p>
<ol class="calibre14">
<li value="1" class="calibre11">使用均值减法进行零居中</li>
<li value="2" class="calibre11">使用标准偏差进行缩放</li>
</ol>
<p class="calibre2">本质上，以下是规范化过程:</p>
<pre class="calibre17">(x - numpy.mean(x)) / numpy.std(x)</pre>
<p class="calibre2">在前面的过程中，x是观测值。注意，根据期望的标准化值的范围，也使用其他标准化过程。例如，如果我们希望归一化后的值介于0和1之间，我们可以使用以下公式:</p>
<pre class="calibre17">(x - numpy.min(x)) / (numpy.max(x) - numpy.min(x))</pre>
<p class="calibre2">在前面的过程中，我们不是减去平均值，而是减去最小值，然后除以最大值和最小值之差。这样，观察值/x中的最小值被规范化为0，最大值被规范化为1。</p>
<p class="calibre2">或者，如果我们希望归一化后的值介于-1和+1之间，则可以使用以下公式:</p>
<pre class="calibre17">2 * (x - numpy.min(x)) / (numpy.max(x) - numpy.min(x)) - 1</pre>
<p class="calibre2">在我们的环境规范化包装器实现中，我们将使用第一种方法，其中我们使用均值减法对观察数据进行零中心化，并使用观察数据的标准偏差进行缩放。事实上，我们将更进一步，计算到目前为止我们收到的所有观察值的移动平均值和标准偏差，以根据代理到目前为止观察到的观察数据的分布来标准化观察值。这更合适，因为来自同一环境的不同观测值之间可能存在较大差异。下面是我们讨论的规范化包装器的实现代码:</p>
<pre class="calibre17">class NormalizedEnv(gym.ObservationWrapper):<br class="title-page-name"/>    def __init__(self, env=None):<br class="title-page-name"/>        gym.ObservationWrapper.__init__(self, env)<br class="title-page-name"/>        self.state_mean = 0<br class="title-page-name"/>        self.state_std = 0<br class="title-page-name"/>        self.alpha = 0.9999<br class="title-page-name"/>        self.num_steps = 0<br class="title-page-name"/><br class="title-page-name"/>    def observation(self, observation):<br class="title-page-name"/>        self.num_steps += 1<br class="title-page-name"/>        self.state_mean = self.state_mean * self.alpha + \<br class="title-page-name"/>            observation.mean() * (1 - self.alpha)<br class="title-page-name"/>        self.state_std = self.state_std * self.alpha + \<br class="title-page-name"/>            observation.std() * (1 - self.alpha)<br class="title-page-name"/><br class="title-page-name"/>        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))<br class="title-page-name"/>        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))<br class="title-page-name"/><br class="title-page-name"/>        return (observation - unbiased_mean) / (unbiased_std + 1e-8)</pre>
<p>我们从环境中观察到的图像帧(即使在我们的预处理包装器之后)已经是相同的比例(0-255或0.0到1.0)。在这种情况下，归一化过程中的缩放步骤可能不是很有帮助。一般来说，这种包装器可以用于其他环境类型，并且对于像Atari这样的健身房环境中已经缩放的图像观察来说，也没有观察到有损于性能。</p>


            

            
        
    



  
    <title>Random no-ops on reset</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">复位时随机无操作</h1>
                
            
            
                
<p class="calibre2">当重置环境时，代理通常从相同的初始状态开始，因此在重置时会收到相同的观察结果。代理可能会记住或习惯于一个游戏级别的开始状态，以至于他们可能会在稍微不同的位置或游戏级别开始时表现不佳。有时，发现随机化初始状态是有帮助的，例如从代理开始情节的不同初始状态取样。为了实现这一点，我们可以添加一个健身房包装器，在重置后发送第一个观察结果之前，执行随机数量的“无操作”。健身房图书馆用于Atari环境的Atari 2600的街机学习环境支持“NOOP”或无操作动作，这在健身房图书馆中被编码为值为0的动作。因此，在将观察结果返回给代理之前，我们将用随机数<em class="calibre13">动作</em> =0 <em class="calibre13"> </em>来步进环境，如下面的代码片段所示:</p>
<pre class="calibre17">class NoopResetEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env, noop_max=30):<br class="title-page-name"/>        """Sample initial states by taking random number of no-ops on reset.<br class="title-page-name"/>        No-op is assumed to be action 0.<br class="title-page-name"/>        """<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        self.noop_max = noop_max<br class="title-page-name"/>        self.noop_action = 0<br class="title-page-name"/>        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """ Do no-op action for a number of steps in [1, noop_max]."""<br class="title-page-name"/>        self.env.reset()<br class="title-page-name"/>        noops = random.randrange(1, self.noop_max + 1) # pylint: disable=E1101<br class="title-page-name"/>        assert noops &gt; 0<br class="title-page-name"/>        obs = None<br class="title-page-name"/>        for _ in range(noops):<br class="title-page-name"/>            obs, _, done, _ = self.env.step(self.noop_action)<br class="title-page-name"/>        return obs<br class="title-page-name"/><br class="title-page-name"/>    def step(self, ac):<br class="title-page-name"/>        return self.env.step(ac)</pre>


            

            
        
    



  
    <title>Fire on reset</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">重置时触发</h1>
                
            
            
                
<p class="calibre2">一些Atari游戏要求玩家按下开火按钮来开始游戏。有些游戏要求在每条生命丧失后按下开火键。通常情况下，这是发射按钮的唯一用途！虽然我们意识到这一点可能看起来微不足道，但有时强化学习代理可能很难自己发现这一点。这并不是说他们没有能力学习。事实上，他们有能力找出游戏中许多隐藏的故障或模式，而这些人类从来没有发现过！例如，在Qbert的游戏中，一个使用进化策略(这是一种受遗传算法启发的黑盒型学习策略)训练的代理人想出了一种奇特的方法，它可以不断获得分数，永远不会让游戏结束！你知道经纪人能拿到多少吗？~1,000,000!他们只能得到这么多，因为由于时间限制，游戏被人为重置。你能试着在Qbert的游戏中得分那么多吗？你可以在这里看到代理得分:<a href="https://youtu.be/meE5aaRJ0Zs" class="calibre9">https://www.youtube.com/watch?v=meE5aaRJ0Zs</a>。</p>
<p class="calibre2">关键不在于代理人有多聪明，能把这些事情都搞清楚。他们当然可以，但是大多数时候，这不利于代理在合理的时间内取得进展。当我们希望一个代理处理几个不同种类的游戏(一次一个)时，尤其如此。我们最好从更简单的假设开始，在我们能够使用更简单的假设训练代理玩得更好之后，让它们变得更复杂。</p>
<p class="calibre2">因此，我们将实现一个<kbd class="calibre12">FireResetEnv</kbd> Gym包装器，它将在每次重置时按下Fire按钮，并为代理启动环境。该代码的实现如下:</p>
<pre class="calibre17">class FireResetEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        """Take action on reset for environments that are fixed until firing."""<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'<br class="title-page-name"/>        assert len(env.unwrapped.get_action_meanings()) &gt;= 3<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        self.env.reset()<br class="title-page-name"/>        obs, _, done, _ = self.env.step(1)<br class="title-page-name"/>        if done:<br class="title-page-name"/>            self.env.reset()<br class="title-page-name"/>        obs, _, done, _ = self.env.step(2)<br class="title-page-name"/>        if done:<br class="title-page-name"/>            self.env.reset()<br class="title-page-name"/>        return obs<br class="title-page-name"/><br class="title-page-name"/>    def step(self, ac):<br class="title-page-name"/>        return self.env.step(ac)</pre>


            

            
        
    



  
    <title>Episodic life</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">短暂的生活</h1>
                
            
            
                
<p class="calibre2">在许多游戏中，包括Atari游戏，玩家可以玩不止一个生命。</p>
<p class="calibre2">据Deepmind观察、使用和报告，当一个生命丧失时终止一集有助于代理更好地学习。必须注意的是，意图是向代理人表明失去生命是一件坏事。在这种情况下，当这一集结束时，我们不会重置环境，而是继续，直到游戏实际结束，之后我们重置环境。如果我们在每一次生命损失后重置游戏，我们将会限制代理人对仅用一次生命就可以收集的观察和经验的暴露，这通常对代理人的学习表现不利。</p>
<p class="calibre2">为了实现我们刚刚讨论的内容，我们将使用<kbd class="calibre12">EpisodicLifeEnv</kbd>类，该类在生命丧失时标记一集的结束，并在游戏结束时重置环境，如下面的代码片段所示:</p>
<pre class="calibre17">class EpisodicLifeEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        """Make end-of-life == end-of-episode, but only reset on true game over.<br class="title-page-name"/>        Done by DeepMind for the DQN and co. since it helps value estimation.<br class="title-page-name"/>        """<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        self.lives = 0<br class="title-page-name"/>        self.was_real_done = True<br class="title-page-name"/><br class="title-page-name"/>    def step(self, action):<br class="title-page-name"/>        obs, reward, done, info = self.env.step(action)<br class="title-page-name"/>        self.was_real_done = True<br class="title-page-name"/>        # check current lives, make loss of life terminal,<br class="title-page-name"/>        # then update lives to handle bonus lives<br class="title-page-name"/>        lives = info['ale.lives']<br class="title-page-name"/>        if lives &lt; self.lives and lives &gt; 0:<br class="title-page-name"/>            # for Qbert sometimes we stay in lives == 0 condition for a few frames<br class="title-page-name"/>            # so its important to keep lives &gt; 0, so that we only reset once<br class="title-page-name"/>            # the environment advertises done.<br class="title-page-name"/>            done = True<br class="title-page-name"/>            self.was_real_done = False<br class="title-page-name"/>        self.lives = lives<br class="title-page-name"/>        return obs, reward, done, info<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """Reset only when lives are exhausted.<br class="title-page-name"/>        This way all states are still reachable even though lives are episodic,<br class="title-page-name"/>        and the learner need not know about any of this behind-the-scenes.<br class="title-page-name"/>        """<br class="title-page-name"/>        if self.was_real_done:<br class="title-page-name"/>            obs = self.env.reset()<br class="title-page-name"/>            self.lives = 0<br class="title-page-name"/>        else:<br class="title-page-name"/>            # no-op step to advance from terminal/lost life state<br class="title-page-name"/>            obs, _, _, info = self.env.step(0)<br class="title-page-name"/>            self.lives = info['ale.lives']<br class="title-page-name"/>        return obs</pre>


            

            
        
    



  
    <title>Max and skip-frame</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">最大值和跳帧</h1>
                
            
            
                
<p class="calibre2"><kbd class="calibre12">Gym</kbd>库提供了ID中有<kbd class="calibre12">NoFrameskip</kbd>的环境，我们在<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">第4章</a>、<em class="calibre13">探索体育馆及其特性</em>中讨论过，在那里我们讨论了体育馆环境的命名。您可能还记得我们在<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">第四章</a>、<em class="calibre13">探索健身房及其特性</em>中的讨论，默认情况下，如果环境名称中没有<kbd class="calibre12">Deterministic</kbd>或<kbd class="calibre12">NoFrameskip</kbd>，那么发送到环境的动作会重复执行<em class="calibre13"> n </em>帧，其中<em class="calibre13"> n </em>是从(2，3，4)中统一采样的。如果我们想以特定的速率逐步通过环境，我们可以使用ID中带有<kbd class="calibre12">NoFrameskip</kbd>的Gym Atari环境，它将逐步通过底层环境，而不会改变步骤的持续时间。在这种情况下，步进率为<img class="fm-editor-equation74" src="img/00147.jpeg"/>秒，即每秒60帧。然后，我们可以自定义环境来跳过我们选择的跳过速率(<em class="calibre13"> k </em>)来以特定的速率步进。这种自定义步进/跳跃速率的实现如下:</p>
<pre class="calibre17">class MaxAndSkipEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env=None, skip=4):<br class="title-page-name"/>        """Return only every `skip`-th frame"""<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        # most recent raw observations (for max pooling across time steps)<br class="title-page-name"/>        self._obs_buffer = deque(maxlen=2)<br class="title-page-name"/>        self._skip = skip<br class="title-page-name"/><br class="title-page-name"/>    def step(self, action):<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        done = None<br class="title-page-name"/>        for _ in range(self._skip):<br class="title-page-name"/>            obs, reward, done, info = self.env.step(action)<br class="title-page-name"/>            self._obs_buffer.append(obs)<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>            if done:<br class="title-page-name"/>                break<br class="title-page-name"/><br class="title-page-name"/>        max_frame = np.max(np.stack(self._obs_buffer), axis=0)<br class="title-page-name"/>        return max_frame, total_reward, done, info<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """Clear past frame buffer and init. to first obs. from inner env."""<br class="title-page-name"/>        self._obs_buffer.clear()<br class="title-page-name"/>        obs = self.env.reset()<br class="title-page-name"/>        self._obs_buffer.append(obs)<br class="title-page-name"/>        return obs</pre>
<p class="calibre2">请注意，我们还获取跳过的帧的最大像素值，并将其作为观察值发送，而不是完全忽略跳过的所有中间图像帧。</p>


            

            
        
    



  
    <title>Wrapping the Gym environment</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">包装健身房环境</h1>
                
            
            
                
<p class="calibre2">最后，我们将应用我们基于使用<kbd class="calibre12">parameters.JSON</kbd>文件指定的环境配置开发的前述包装器:</p>
<pre class="calibre17">def make_env(env_id, env_conf):<br class="title-page-name"/>    env = gym.make(env_id)<br class="title-page-name"/>    if 'NoFrameskip' in env_id:<br class="title-page-name"/>        assert 'NoFrameskip' in env.spec.id<br class="title-page-name"/>        env = NoopResetEnv(env, noop_max=30)<br class="title-page-name"/>        env = MaxAndSkipEnv(env, skip=env_conf['skip_rate'])<br class="title-page-name"/><br class="title-page-name"/>    if env_conf['episodic_life']:<br class="title-page-name"/>        env = EpisodicLifeEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    if 'FIRE' in env.unwrapped.get_action_meanings():<br class="title-page-name"/>        env = FireResetEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    env = AtariRescale(env, env_conf['useful_region'])<br class="title-page-name"/>    env = NormalizedEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    if env_conf['clip_reward']:<br class="title-page-name"/>        env = ClipRewardEnv(env)<br class="title-page-name"/>    return env<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">我们之前讨论的所有环境包装器都在本书的代码库中的<kbd class="calibre12">ch6/environment/atari.py</kbd>中实现并可用。</p>


            

            
        
    



  
    <title>Training the deep Q-learner to play Atari games</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">训练深度Q学习者玩雅达利游戏</h1>
                
            
            
                
<p class="calibre2">在本章中，我们已经学习了几种新技术。你走了这么远，值得表扬！现在开始有趣的部分，您可以让您的代理自己训练玩几个雅达利游戏，看看他们进展如何。我们的deep Q-learner的伟大之处在于，我们可以使用同一个代理来训练和玩任何Atari游戏！</p>
<p class="calibre2">在本节结束时，您应该能够使用我们的deep Q learning agent来观察屏幕上的像素，并通过向Atari Gym环境发送操纵杆命令来采取行动，就像下面的屏幕截图所示:</p>
<p class="calibre2"/>
<div><img src="img/00148.jpeg" class="calibre74"/></div>


            

            
        
    



  
    <title>Putting together a comprehensive deep Q-learner</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">构建一个全面的深度Q学习者</h1>
                
            
            
                
<p class="calibre2">现在是时候将我们讨论的所有技术结合到一个全面的实现中，利用所有这些技术来获得最大的性能。我们将使用在上一节中创建的<kbd class="calibre12">environment.atari</kbd>模块和几个有用的健身房环境包装器。让我们看一下代码大纲来理解代码的结构:</p>
<p>您会注意到，为了简洁起见，代码的某些部分被删除，并替换为<kbd class="calibre28">...</kbd>，表示该部分中的代码已经被折叠/隐藏。你可以在本书的代码库中<kbd class="calibre28">ch6/deep_Q_Learner.py</kbd>找到最新版本的完整代码。</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>import gym<br class="title-page-name"/>import torch<br class="title-page-name"/>import random<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>import environment.atari as Atari<br class="title-page-name"/>import environment.utils as env_utils<br class="title-page-name"/>from utils.params_manager import ParamsManager<br class="title-page-name"/>from utils.decay_schedule import LinearDecaySchedule<br class="title-page-name"/>from utils.experience_memory import Experience, ExperienceMemory<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/>from function_approximator.cnn import CNN<br class="title-page-name"/>from tensorboardX import SummaryWriter<br class="title-page-name"/>from datetime import datetime<br class="title-page-name"/>from argparse import ArgumentParser<br class="title-page-name"/><br class="title-page-name"/>args = ArgumentParser("deep_Q_learner")<br class="title-page-name"/>args.add_argument("--params-file", help="Path to the parameters json file. Default is parameters.json",<br class="title-page-name"/>                 default="parameters.json", metavar="PFILE")<br class="title-page-name"/>args.add_argument("--env-name", help="ID of the Atari environment available in OpenAI Gym. Default is Seaquest-v0",<br class="title-page-name"/>                  default="Seaquest-v0", metavar="ENV")<br class="title-page-name"/>args.add_argument("--gpu-id", help="GPU device ID to use. Default=0", default=0, type=int, metavar="GPU_ID")<br class="title-page-name"/>args.add_argument("--render", help="Render environment to Screen. Off by default", action="store_true", default=False)<br class="title-page-name"/>args.add_argument("--test", help="Test mode. Used for playing without learning. Off by default", action="store_true",<br class="title-page-name"/>                  default=False)<br class="title-page-name"/>args = args.parse_args()<br class="title-page-name"/><br class="title-page-name"/>params_manager= ParamsManager(args.params_file)<br class="title-page-name"/>seed = params_manager.get_agent_params()['seed']<br class="title-page-name"/>summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']<br class="title-page-name"/>summary_file_path= summary_file_path_prefix + args.env_name + "_" + datetime.now().strftime("%y-%m-%d-%H-%M")<br class="title-page-name"/>writer = SummaryWriter(summary_file_path)<br class="title-page-name"/># Export the parameters as json files to the log directory to keep track of the parameters used in each experiment<br class="title-page-name"/>params_manager.export_env_params(summary_file_path + "/" + "env_params.json")<br class="title-page-name"/>params_manager.export_agent_params(summary_file_path + "/" + "agent_params.json")<br class="title-page-name"/>global_step_num = 0<br class="title-page-name"/>use_cuda = params_manager.get_agent_params()['use_cuda']<br class="title-page-name"/># new in PyTorch 0.4<br class="title-page-name"/>device = torch.device("cuda:" + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else "cpu")<br class="title-page-name"/>torch.manual_seed(seed)<br class="title-page-name"/>np.random.seed(seed)<br class="title-page-name"/>if torch.cuda.is_available() and use_cuda:<br class="title-page-name"/>    torch.cuda.manual_seed_all(seed)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Deep_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, params):<br class="title-page-name"/>        ...<br class="title-page-name"/>        <br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next, done):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def replay_experience(self, batch_size = None):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def load(self, env_name):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env_conf = params_manager.get_env_params()<br class="title-page-name"/>    env_conf["env_name"] = args.env_name<br class="title-page-name"/>    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default<br class="title-page-name"/>    ...<br class="title-page-name"/>    # If a saved (pre-trained) agent's brain model is available load it as per the configuration<br class="title-page-name"/>    if agent_params['load_trained_model']:<br class="title-page-name"/>    ...<br class="title-page-name"/><br class="title-page-name"/>    # Start the training process<br class="title-page-name"/>    episode = 0<br class="title-page-name"/>    while global_step_num &lt;= agent_params['max_training_steps']:<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        done = False<br class="title-page-name"/>        step = 0<br class="title-page-name"/>        #for step in range(agent_params['max_steps_per_episode']):<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            if env_conf['render'] or args.render:<br class="title-page-name"/>                env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            #agent.learn(obs, action, reward, next_obs, done)<br class="title-page-name"/>            agent.memory.store(Experience(obs, action, reward, next_obs, done))<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/>            step += 1<br class="title-page-name"/>            global_step_num +=1<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                episode += 1<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; agent.best_reward:<br class="title-page-name"/>                    agent.best_reward = cum_reward<br class="title-page-name"/>                if np.mean(episode_rewards) &gt; prev_checkpoint_mean_ep_rew:<br class="title-page-name"/>                    num_improved_episodes_before_checkpoint += 1<br class="title-page-name"/>                if num_improved_episodes_before_checkpoint &gt;= agent_params["save_freq_when_perf_improves"]:<br class="title-page-name"/>                    prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)<br class="title-page-name"/>                    agent.best_mean_reward = np.mean(episode_rewards)<br class="title-page-name"/>                    agent.save(env_conf['env_name'])<br class="title-page-name"/>                    num_improved_episodes_before_checkpoint = 0<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), agent.best_reward))<br class="title-page-name"/>                writer.add_scalar("main/ep_reward", cum_reward, global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/mean_ep_reward", np.mean(episode_rewards), global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/max_ep_rew", agent.best_reward, global_step_num)<br class="title-page-name"/>                # Learn from batches of experience once a certain amount of xp is available unless in test only mode<br class="title-page-name"/>                if agent.memory.get_size() &gt;= 2 * agent_params['replay_start_size'] and not args.test:<br class="title-page-name"/>                    agent.replay_experience()<br class="title-page-name"/><br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()<br class="title-page-name"/>    writer.close()</pre>
<p class="calibre2"> </p>


            

            
        
    



  
    <title>Hyperparameters</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">超参数</h1>
                
            
            
                
<p class="calibre2">以下是我们的深度Q学习者使用的超参数列表，简要描述了它们是什么以及它们取值的类型:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">超参数</strong></td>
<td class="calibre48"><strong class="calibre1">简要说明</strong></td>
<td class="calibre48"><strong class="calibre1">值类型</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">max_num_episodes</kbd></td>
<td class="calibre48">运行代理的最大集数。</td>
<td class="calibre48">整数(例如，100，000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">replay_memory_capacity</kbd></td>
<td class="calibre48">经验记忆容量的总容量。</td>
<td class="calibre48">整数或指数表示法(例如，1e6)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">replay_batch_size</kbd></td>
<td class="calibre48">体验回放期间，在每次更新迭代中，在(小)批次中用于更新Q函数的转换数量。</td>
<td class="calibre48">整数(例如，2000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">use_target_network</kbd></td>
<td class="calibre48">是否要使用目标Q网络。</td>
<td class="calibre48">布尔型(真/假)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">target_network_update_freq</kbd></td>
<td class="calibre48">使用主Q网络更新目标Q网络之前的步骤数。</td>
<td class="calibre48">整数(例如，1，000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">lr</kbd></td>
<td class="calibre48">深度Q网络的学习速率。</td>
<td class="calibre48">浮动(例如，1e-4)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">gamma</kbd></td>
<td class="calibre48">MDP的折扣系数。</td>
<td class="calibre48">
<p class="calibre2">浮点型(例如，0.98)</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">epsilon_max</kbd></td>
<td class="calibre48">衰减开始时ε的最大值。</td>
<td class="calibre48">浮点型(例如，1.0)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">epsilon_min</kbd></td>
<td class="calibre48">衰减最终达到的ε的最小值。</td>
<td class="calibre48">浮点型(例如:0.05)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">seed</kbd></td>
<td class="calibre48">种子用来播种numpy和torch(以及<kbd class="calibre12">torch.cuda</kbd>)以便能够重现(在某种程度上)那些库引入的随机性。</td>
<td class="calibre48">整数(例如:555)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">use_cuda</kbd></td>
<td class="calibre48">如果GPU可用，是否使用基于CUDA的GPU。</td>
<td class="calibre48">布尔值(例如:true)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">load_trained_model</kbd></td>
<td class="calibre48">如果存在针对此环境/问题的已训练模型，是否加载该模型。如果此参数设置为true，但没有可用的训练模型，则将从头开始训练模型。</td>
<td class="calibre48">布尔值(例如:true)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">load_dir</kbd></td>
<td class="calibre48">目录的路径(包括正斜杠),从中应加载已定型模型以恢复定型。</td>
<td class="calibre48">字符串(例如，“trained_models/”)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">save_dir</kbd></td>
<td class="calibre48">保存模型的目录路径。每次代理达到新的最佳分数/奖励时，都会保存新的模型。</td>
<td class="calibre48">字符串(例如:trained_models/")</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">请参考本书代码库中的<kbd class="calibre12">ch6/parameters.JSON</kbd>文件，获取代理使用的参数的更新列表。</p>


            

            
        
    



  
    <title>Launching the training process</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">启动培训流程</h1>
                
            
            
                
<p class="calibre2">我们现在已经把深度Q学习者的所有部分放在一起，并准备好培训代理！请务必从本书的代码库中查看/下载最新的代码。</p>
<p class="calibre2">您可以从Atari环境列表中选择任何环境，并使用以下命令训练我们开发的代理:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "ENV_ID"</strong></pre>
<p class="calibre2">在前面的命令中，<kbd class="calibre12">ENV_ID</kbd>是Atari Gym环境的名称/ID。例如，如果您想在没有跳帧的情况下在<kbd class="calibre12">pong</kbd>环境中训练代理，您可以运行以下命令:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "PongNoFrameskip-v4"</strong></pre>
<p class="calibre2">默认情况下，训练日志将被保存到<kbd class="calibre12">./logs/DQL_{ENV}_{T}</kbd>，其中<kbd class="calibre12">{ENV}</kbd>是环境的名称，<kbd class="calibre12">{T}</kbd>是运行代理时获得的时间戳。如果使用以下命令启动TensorBoard实例:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ tensorboard --logdir=logs/</strong></pre>
<p class="calibre2">默认情况下，我们的<kbd class="calibre12">deep_Q_learner.py</kbd>脚本将使用与读取可配置参数值的脚本位于同一目录的<kbd class="calibre12">parameters.JSON</kbd>文件。您可以使用命令行参数<kbd class="calibre12">--params-file</kbd>用不同的参数配置文件覆盖。</p>
<p class="calibre2">如果在<kbd class="calibre12">parameters.JSON</kbd>文件中<kbd class="calibre12">load_trained_model</kbd>参数被设置为<kbd class="calibre12">true</kbd>，并且如果为所选环境保存的模型可用，我们的脚本将尝试用它之前学习的模型初始化代理，以便它可以从它停止的地方恢复，而不是从头开始训练。</p>


            

            
        
    



  
    <title>Testing performance of your deep Q-learner in Atari games</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">测试你的深度Q学习者在雅达利游戏中的表现</h1>
                
            
            
                
<p class="calibre2">感觉很棒，不是吗？您现在已经开发了一个代理，它可以学习玩任何雅达利游戏，并且自己玩得更好！一旦你让你的代理在任何Atari游戏上接受了训练，你就可以使用脚本的测试模式，根据代理到目前为止的学习情况来测试它的表现。您可以通过使用<kbd class="calibre12">deep_q_learner.py</kbd>脚本中的<kbd class="calibre12">--test</kbd>参数来启用测试模式。启用环境渲染也很有用，这样您可以直观地看到(除了控制台上打印的奖励)代理的表现。例如，您可以使用以下命令测试<kbd class="calibre12">Seaquest</kbd> Atari游戏中的代理:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "Seaquest-v0" --test --render</strong></pre>
<p class="calibre2">你会看到Seaquest游戏窗口出现，代理显示其技能！</p>
<p class="calibre2">关于<kbd class="calibre12">test</kbd>模式，需要注意以下几点:</p>
<ul class="calibre10">
<li class="calibre11">测试模式将关闭代理的学习例程。因此，代理在测试模式下不会学习或更新自身。此模式仅用于测试训练有素的代理的表现。如果您想查看代理在学习过程中的表现，您可以只使用<kbd class="calibre12">--render</kbd>选项而不使用<kbd class="calibre12">--test</kbd>选项。</li>
<li class="calibre11">测试模式假设您选择的环境的训练模型存在于<kbd class="calibre12">trained_models</kbd>文件夹中。否则，一个新生的代理人，在没有任何先验知识的情况下，会从零开始玩这个游戏。此外，由于学习被禁用，您将看不到代理改进！</li>
</ul>
<p class="calibre2">现在，轮到你出去，实验，回顾，比较我们在不同的Atari健身房环境中实现的代理的性能，看看代理可以得分多少！如果你训练一个代理在游戏中玩得很好，你可以通过从你的fork打开这本书的代码库的pull请求，向其他读者展示和分享它。你会出现在页面上！</p>
<p class="calibre2">一旦你习惯了使用我们开发的代码库，你就可以用它做一些实验。例如，您可以关闭目标Q网络或增加/减少经验记忆/重放批量大小，只需更改<kbd class="calibre12">parameters.JSON</kbd>文件并使用非常方便的TensorBoard仪表板比较性能。</p>


            

            
        
    



  
    <title>Summary</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">摘要</h1>
                
            
            
                
<p class="calibre2">我们开始这一章的宏伟目标是开发能够在Atari游戏中取得巨大分数的智能学习代理。我们通过实施几项技术来改进我们在前一章开发的Q-learner，在这方面取得了渐进的进展。我们首先开始学习如何使用神经网络来逼近Q动作值函数，并通过实际实现一个浅层神经网络来解决著名的Cart极点问题，从而使我们的学习具体化。然后，我们实现了经验记忆和经验重放，使代理能够从(小)随机抽样的批量经验中学习，通过打破代理交互之间的相关性，提高代理先前经验的批量重放的样本效率，从而帮助提高性能。然后，我们重新研究了ε-贪婪行为选择策略，并实现了一个衰减时间表，以减少基于时间表的探索，让代理更加依赖于它的学习。</p>
<p class="calibre2">然后，我们看了如何将TensorBoard的日志记录和可视化功能与我们基于PyTorch的学习代理一起使用，以便我们能够以简单直观的方式观察代理的训练进度。我们还实现了一个简洁的参数管理器类，使我们能够使用外部易读的JSON文件配置代理的超参数和其他配置参数。</p>
<p class="calibre2">在我们有了一个好的基线和有用的实用工具之后，我们开始了深度Q学习器的实现。我们通过在PyTorch中实现一个深度卷积神经网络来开始这一部分，然后我们用它来表示我们代理的Q(动作值)函数。然后我们看到使用目标Q网络来稳定代理的Q学习过程是多么容易。然后，我们组装了基于deep Q学习的代理，它可以根据健身房环境中的原始像素观察来学习行动。</p>
<p class="calibre2">然后，我们将目光和手放在Atari健身房环境上，查看了使用健身房环境包装器定制健身房环境的几种方法。我们还讨论了Atari环境的几个有用的包装器，并具体实现了包装器来裁剪奖励、预处理观察图像帧、在所有整个采样观察分布上归一化观察、在复位时发送随机noop动作以采样不同的开始状态、在复位时按下Fire按钮，以及通过跳帧以自定义速率步进。我们终于看到如何将所有这些整合到一个全面的代理培训代码库，并在任何Atari游戏上培训代理，并在TensorBoard上查看进度摘要。我们还研究了如何保存状态，并从以前保存的状态恢复对代理的训练，而不是从头开始重新运行训练。最后，我们看到了我们实现和训练的代理的改进性能。</p>
<p class="calibre2">我们希望你在这一整章中都玩得很开心。我们将在下一章中研究和实现一种不同的算法，它可以用来采取更复杂的行动，而不是一组离散的按钮按压，以及我们如何使用它来训练一个代理在模拟中自动控制一辆汽车！</p>


            

            
        
    
</body></html>