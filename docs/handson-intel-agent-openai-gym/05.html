<html><head/><body>

  
    <title>Implementing your First Learning Agent - Solving the Mountain Car problem</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现你的第一个学习代理——解决山地汽车问题</h1>
                
            
            
                
<p class="calibre2">干得好，走到了这一步！在前几章中，我们很好地介绍了OpenAI Gym，它的特性，以及如何在你自己的程序中安装、配置和使用它。我们还讨论了强化学习的基础知识以及什么是深度强化学习，我们建立了PyTorch深度学习库来开发深度强化学习应用。在本章中，您将开始开发您的第一个学习代理！您将开发一个智能代理，它将学习如何解决山地汽车问题。在接下来的章节中，我们将逐步解决越来越具有挑战性的问题，因为你越来越习惯于开发强化学习算法来解决OpenAI Gym中的问题。我们将从理解山地汽车问题开始这一章，山地汽车问题一直是强化学习和最优控制领域的热门问题。我们将从头开始开发我们的学习代理，然后训练它使用健身房中的山地车环境来解决山地车问题。我们将最终看到代理是如何进步的，并简要地看一下我们可以改进代理的方法，以便用它来解决更复杂的问题。我们将在本章中讨论的主题如下:</p>
<ul class="calibre10">
<li class="calibre11">理解山地汽车问题</li>
<li class="calibre11">实现一个基于强化学习的智能体来解决山地车问题</li>
<li class="calibre11">在健身房培训强化学习代理</li>
<li class="calibre11">测试代理的性能</li>
</ul>


            

            
        
    



  
    <title>Understanding the Mountain Car problem</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">理解山地汽车问题</h1>
                
            
            
                
<p class="calibre2">对于任何强化学习问题，不管我们使用什么样的学习算法，关于这个问题的两个基本定义都是很重要的。它们是状态空间和动作空间的定义。我们在本书前面提到过，状态和动作空间可以是离散的，也可以是连续的。通常，在大多数问题中，状态空间由连续值组成，并表示为向量、矩阵或张量(多维矩阵)。与连续值问题和环境相比，具有离散行动空间的问题和环境相对容易。在本书中，我们将为一些问题和环境开发学习算法，这些问题和环境混合了状态空间和动作空间的组合，这样当你开始自己为应用程序开发智能代理和算法时，你就可以轻松地处理任何这样的变化。</p>
<p class="calibre2">让我们先从高层次的描述来理解山地汽车问题，然后再看山地汽车环境的状态和动作空间。</p>


            

            
        
    



  
    <title>The Mountain Car problem and environment</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">山地汽车问题与环境</h1>
                
            
            
                
<p class="calibre2">在山地汽车健身房环境中，汽车位于两座山之间的一维轨道上。目标是把车开上右边的山；然而，这辆车的发动机不够强劲，即使以最大速度行驶也无法上山。所以，成功的唯一方法就是来回开车造势。简而言之，山地车问题就是让一辆动力不足的车到达山顶。</p>
<p class="calibre2">在你实现你的代理算法之前，理解环境、问题、状态和动作空间会有很大的帮助。我们如何在健身房里找出山地车环境的状态和动作空间？嗯，我们已经从第4章、<em class="calibre13">探索健身房及其特点</em>中知道了如何去做。我们编写了一个名为<kbd class="calibre12">get_observation_action_space.py</kbd>的脚本，它将打印出环境的状态、观察和动作空间，其名称作为第一个参数传递给脚本。让我们用下面的命令让它打印出<kbd class="calibre12">MountainCar-v0</kbd>环境的空格:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch4$ python get_observation_action_space.py 'MountainCar-v0'</strong></pre>
<p>注意，命令提示符有<kbd class="calibre28">rl_gym_book</kbd>前缀，这表示我们已经激活了<kbd class="calibre28">rl_gym_book</kbd> conda Python虚拟环境。此外，当前目录<kbd class="calibre28">~/rl_gym_book/ch4</kbd>表示脚本是从与本书代码库中的<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre45">第4章</a>、<em class="calibre46">探索健身房及其特性</em>的代码对应的<kbd class="calibre28">ch4</kbd>目录运行的。</p>
<p>前面的命令将产生如下输出:</p>
<pre class="calibre17">Observation Space:<br class="title-page-name"/>Box(2,)<br class="title-page-name"/><br class="title-page-name"/> space.low: [-1.20000005 -0.07 ]<br class="title-page-name"/><br class="title-page-name"/> space.high: [ 0.60000002 0.07 ]<br class="title-page-name"/>Action Space:<br class="title-page-name"/>Discrete(3)</pre>
<p class="calibre2">从这个输出中，我们可以看到状态和观察空间是一个二维的盒子，而动作空间是三维的和离散的。</p>
<p>如果你想重温一下<strong class="calibre27">框</strong>和<strong class="calibre27">离散的</strong>空间的含义，你可以快速翻到<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre45">第4章</a>、<em class="calibre46">探索健身房及其特点</em>，我们在健身房 <strong class="calibre27"> </strong>部分的<em class="calibre46">空间下讨论了这些空间及其含义。理解它们很重要。</em></p>
<p class="calibre2">下表总结了状态和动作空间类型、描述以及允许值的范围，供您参考:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2"><strong class="calibre4"> MountainCar-v0环境</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">类型</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">描述</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">范围</strong></p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">状态矢量空间</p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">Box(2,)</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2">(位置、速度)</p>
</td>
<td class="calibre48">
<p class="calibre2">位置:-1.2到0.6</p>
<p class="calibre2">速度:-0.07到0.07</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">行为空间</p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">Discrete(3)</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2">0:向左</p>
<p class="calibre2">1:滑行/无所事事</p>
<p class="calibre2">2:向右走</p>
</td>
<td class="calibre48">
<p class="calibre2">0, 1, 2</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">所以比如汽车以零速度从<em class="calibre13"> -0.6 </em>和<em class="calibre13"> -0.4 </em>之间的任意位置出发，目标是到达右侧的山顶，也就是位置<em class="calibre13"> 0.5 </em>。(该车技术上可以超越<em class="calibre13"> 0.5、</em>达到<em class="calibre13"> 0.6 </em>，也是考虑的。)环境会在每一个时间步发送<em class="calibre13"> -1 </em>作为奖励，直到到达目标位置(<em class="calibre13"> 0.5 </em>)。环境会终止这一集。如果轿厢到达<em class="calibre13"> 0.5 </em>位置或步数达到200，则<kbd class="calibre12">done</kbd>变量将等于<kbd class="calibre12">True</kbd>。</p>


            

            
        
    



  
    <title>Implementing a Q-learning agent from scratch</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">从头开始实现Q-learning agent</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将开始逐步实现我们的智能代理。我们将使用OpenAI Gym库的<kbd class="calibre12">NumPy</kbd>库和<kbd class="calibre12">MountainCar-V0</kbd>环境实现著名的Q-learning算法。</p>
<p class="calibre2">让我们重温一下我们在<a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">第4章</a>、<em class="calibre13">探索健身房及其功能</em>中使用的强化学习健身房锅炉板代码，如下所示:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>env = gym.make("Qbert-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 10<br class="title-page-name"/>MAX_STEPS_PER_EPISODE = 500<br class="title-page-name"/>for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    for step in range(MAX_STEPS_PER_EPISODE):<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms<br class="title-page-name"/>        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not<br class="title-page-name"/>        obs = next_state<br class="title-page-name"/><br class="title-page-name"/>        if done is True:<br class="title-page-name"/>            print("\n Episode #{} ended in {} steps.".format(episode, step+1))<br class="title-page-name"/>            break</pre>
<p class="calibre2">这段代码是一个很好的起点(又名样板！)来开发我们的强化学习代理。我们首先将环境名称从<kbd class="calibre12">Qbert-v0</kbd>改为<kbd class="calibre12">MountainCar-v0</kbd>。请注意，在前面的脚本中，我们正在设置<kbd class="calibre12">MAX_STEPS_PER_EPISODE</kbd>。这是代理在剧集结束前可以采取的步骤或操作的数量。这在持续的、永久的或循环的环境中可能是有用的，在这些环境中，环境本身不会结束情节。这里，我们为代理设置了一个限制，以避免无限循环。然而，OpenAI Gym中定义的大多数环境都有一个剧集终止条件，一旦满足其中任何一个条件，由<kbd class="calibre12">env.step(...)</kbd>函数返回的<kbd class="calibre12">done</kbd>变量将被设置为<em class="calibre13"> True </em>。我们在上一节看到，对于我们感兴趣的山地车问题，如果车到达目标位置(<em class="calibre13"> 0.5 </em>)或者走的步数达到<em class="calibre13"> 200 </em>，环境会终止该集。因此，对于山地汽车环境，我们可以进一步简化样板代码，如下所示:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>env = gym.make("MountainCar-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 5000<br class="title-page-name"/><br class="title-page-name"/>for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0 # To keep track of the total reward obtained in each episode<br class="title-page-name"/>    step = 0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms<br class="title-page-name"/>        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>        step += 1<br class="title-page-name"/>        obs = next_state<br class="title-page-name"/><br class="title-page-name"/>    print("\n Episode #{} ended in {} steps. total_reward={}".format(episode, step+1, total_reward))<br class="title-page-name"/>env.close()</pre>
<p class="calibre2">如果您运行前面的脚本，您将看到山地汽车环境出现在一个新窗口中，汽车随机左右移动1000集。您还会看到每集末尾打印的集号、采取的步骤和获得的总奖励，如下面的屏幕截图所示:</p>
<p class="cdpaligncenter4"><img src="img/00119.jpeg" class="calibre62"/></p>
<p class="calibre2">示例输出应该类似于下面的屏幕截图:</p>
<div><img src="img/00120.jpeg" class="calibre63"/><br class="title-page-name"/></div>
<p class="calibre2">你应该还记得我们上一节的内容，代理人每走一步会得到<em class="calibre13"> -1 </em>的奖励，并且<kbd class="calibre12">MountainCar-v0</kbd>环境会在<em class="calibre13"> 200 </em>步后终止剧集；这就是为什么你这个代理有时候可能会获得<em class="calibre13"> -200的总奖励！</em>毕竟，代理正在采取随机行动，而没有思考或从其先前的行动中学习。理想情况下，我们希望代理能够计算出如何以最少的步数到达山顶(靠近旗帜、接近、位于或超过位置<em class="calibre13"> 0.5 </em>)。不要担心——我们将在本章结束时构建这样一个智能代理！</p>
<p>记住，在运行脚本之前，一定要激活<kbd class="calibre28">rl_gym_book</kbd> conda环境！否则，您可能会遇到不必要的模块未找到错误。您可以通过查看shell前缀来直观地确认您是否已经激活了环境，它将显示如下内容:<kbd class="calibre28">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch5$</kbd>。</p>
<p class="calibre2">让我们继续看一下Q-learning部分。</p>


            

            
        
    



  
    <title>Revisiting Q-learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">重新审视Q学习</h1>
                
            
            
                
<p class="calibre2">在第2章、<em class="calibre13">强化学习和深度强化学习</em>中，我们讨论了SARSA和Q-learning算法。这两种算法都提供了一种系统的方法来更新由<img class="fm-editor-equation70" src="img/00121.jpeg"/>表示的动作值函数的估计值。具体来说，我们看到Q-learning是一种偏离策略的学习算法，它更新当前状态的动作值估计，并在随后的状态<img class="fm-editor-equation58" src="img/00122.jpeg"/>中朝着可获得的最大动作值更新动作，代理将根据其策略结束该状态。我们还看到，Q学习更新由以下公式给出:</p>
<div><img class="fm-editor-equation71" src="img/00123.jpeg"/></div>
<p class="calibre2">在下一节中，我们将用Python实现一个<kbd class="calibre12">Q_Learner</kbd> <strong class="calibre4"> </strong>类，它实现了这个学习更新规则以及其他必要的函数和方法。</p>


            

            
        
    



  
    <title>Implementing a Q-learning agent using Python and NumPy</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">使用Python和NumPy实现Q-learning agent</h1>
                
            
            
                
<p class="calibre2">让我们通过实现<kbd class="calibre12">Q_Learner</kbd>类开始实现我们的Q-learning代理。该类的主要方法如下:</p>
<ul class="calibre10">
<li class="calibre11">__init__(self，env)</li>
<li class="calibre11">离散化(自我，观察)</li>
<li class="calibre11">get_action(self，obs)</li>
<li class="calibre11">学习(自我，观察，行动，奖励，下一个观察)</li>
</ul>
<p class="calibre2">你稍后会发现这里的方法很常见，并且存在于我们在本书中将要实现的几乎所有代理中。这让你很容易掌握它们，因为这些方法会被一次又一次地重复(有一些修改)。</p>
<p class="calibre2">一般来说,<kbd class="calibre12">discretize()</kbd>函数对于代理实现来说不是必需的，但是当状态空间很大并且连续时，最好将空间离散成可数个区间或值的范围，以简化表示。这也减少了Q-learning算法需要学习的值的数量，因为它现在只需要学习有限的一组值，这些值可以用表格格式或者通过使用<em class="calibre13"> n </em>维数组而不是复杂函数来简明地表示。此外，用于最优控制的Q学习算法保证收敛于Q值的表格表示。</p>


            

            
        
    



  
    <title>Defining the hyperparameters</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">定义超参数</h1>
                
            
            
                
<p class="calibre2">在我们的<kbd class="calibre12">Q_Learner</kbd>类声明之前，我们将初始化一些有用的超参数。下面是我们将在<kbd class="calibre12">Q_Learner</kbd>实现中使用的超参数:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">EPSILON_MIN</kbd>:这是我们希望代理在遵循epsilon-greedy策略时使用的epsilon值的最小值。</li>
<li class="calibre11"><kbd class="calibre12">MAX_NUM_EPISODES</kbd> : <strong class="calibre1"> </strong>我们希望代理与环境交互的最大剧集数。</li>
<li class="calibre11"><kbd class="calibre12">STEPS_PER_EPISODE</kbd>:这是每集的步数。这可能是环境允许每集执行的最大步骤数，也可能是我们希望根据时间预算限制的自定义值。每集允许更多的步骤意味着每集可能需要更长的时间来完成，并且在非终止环境中，即使代理停留在同一点，环境也不会重置，直到达到此限制。</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">ALPHA</kbd>:这是我们希望代理使用的学习率。这是上一节列出的Q学习更新等式中的alpha。一些算法随着训练的进行改变学习速率。</li>
<li class="calibre11"><kbd class="calibre12">GAMMA</kbd>:这是代理将用于计算未来奖励的折扣系数。该值对应于上一节中Q学习更新等式中的gamma。</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">NUM_DISCRETE_BINS</kbd>:这是状态空间将被离散化成的值的箱的数量。对于山地汽车环境，我们将把状态空间离散化为<em class="calibre25"> 30个</em>箱。您可以尝试更高/更低的值。</li>
</ul>
<p>请注意，<kbd class="calibre28">MAX_NUM_EPISODES</kbd> <strong class="calibre27"> </strong>和<kbd class="calibre28">STEPS_PER_EPISODE</kbd>已经在我们在本章前面的一节中所学的样板代码中定义了。</p>
<p>这些超参数在Python代码中是这样定义的，带有一些初始值:</p>
<pre class="calibre17">EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim</pre>


            

            
        
    



  
    <title>Implementing the Q_Learner class's __init__ method</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现Q_Learner类的__init__方法</h1>
                
            
            
                
<p class="calibre2">接下来，让我们看看<kbd class="calibre12">Q_Learner</kbd>类的成员函数定义。<kbd class="calibre12">__init__(self, env)</kbd>函数将环境实例<kbd class="calibre12">env</kbd>作为输入参数，初始化观察空间和动作空间的尺寸/形状，并根据我们设置的<kbd class="calibre12">NUM_DISCRETE_BINS</kbd>确定离散化观察空间的参数。<kbd class="calibre12">__init__(self, env)</kbd>函数还根据离散观察空间的形状和动作空间的维度，将Q函数初始化为NumPy数组。<kbd class="calibre12">__init__(self, env)</kbd>的实现很简单，因为我们只是为代理初始化必要的值。下面是我们的实现:</p>
<pre class="calibre17">class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                          self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0</pre>
<p class="calibre2"/>


            

            
        
    



  
    <title>Implementing the Q_Learner class's discretize method</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现Q_Learner类的离散化方法</h1>
                
            
            
                
<p class="calibre2">让我们花一点时间来理解我们是如何离散化观察空间的。离散化观察空间(以及一般的度量空间)的最简单且有效的方法是将值范围的跨度分成一组有限的值，称为箱。值的跨度/范围由空间的每个维度中的最大可能值和最小可能值之间的差给出。一旦我们计算出跨度，我们就可以用它除以我们决定的<kbd class="calibre12">NUM_DISCRETE_BINS</kbd>来得到箱子的宽度。我们在<kbd class="calibre12">__init__</kbd>函数中计算了面元宽度，因为它不会随着每次新的观察而改变。<kbd class="calibre12">discretize(self, obs)</kbd>函数接收每一个新函数，并应用离散化步骤在离散化空间中找到观察值所属的箱。就像这样简单:</p>
<pre class="calibre17">(obs - self.obs_low) / self.bin_width)</pre>
<p class="calibre2">我们希望它属于任何一个T14桶(而不是介于两者之间的某个地方)；因此，我们将前面的代码转换成一个<kbd class="calibre12">integer</kbd>:</p>
<pre class="calibre17">((obs - self.obs_low) / self.bin_width).astype(int)</pre>
<p class="calibre2">最后，我们将这个离散化的观察结果作为一个元组返回。所有这些操作都可以用一行Python代码编写，如下所示:</p>
<pre class="calibre17">def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))</pre>


            

            
        
    



  
    <title>Implementing the Q_Learner's get_action method</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现Q_Learner的get_action方法</h1>
                
            
            
                
<p class="calibre2">我们希望代理根据观察结果采取行动。<kbd class="calibre12">get_action(self, obs)</kbd> <strong class="calibre4"> </strong>是我们定义的函数，在<kbd class="calibre12">obs</kbd>中给定一个观察，生成一个动作。<strong class="calibre4"> </strong>使用最广泛的行动选择策略是ε-贪婪策略，它以<em class="calibre13"> 1- </em> <img src="img/00124.jpeg" class="calibre64"/>的(高)概率根据智能体的估计采取最佳行动，并以ε<img class="fm-editor-equation59" src="img/00125.jpeg"/>给出的(小)概率采取随机行动。我们使用NumPy的随机模块中的<kbd class="calibre12">random()</kbd>方法实现ε贪婪策略，如下所示:</p>
<pre class="calibre17"> def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])</pre>


            

            
        
    



  
    <title>Implementing the Q_learner class's learn method</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">实现Q_learner类的learn方法</h1>
                
            
            
                
<p class="calibre2">正如您可能已经猜到的，这是<kbd class="calibre12">Q_Learner</kbd>类最重要的方法，它具有学习Q值的魔力，从而使代理能够随着时间的推移采取智能行动！最棒的是，实现起来没有那么复杂！这仅仅是我们前面看到的Q学习更新等式的实现。我说实现简单你不相信？！好了，下面是学习功能的实现:</p>
<pre class="calibre17"> def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error</pre>
<p class="calibre2">现在你同意了吗？:)</p>
<p class="calibre2">我们可以用一行代码编写Q学习更新规则，如下所示:</p>
<pre class="calibre17">self.Q[discretized_obs][action] += self.alpha * (reward + self.gamma * np.max(self.Q[discretized_next_obs] - self.Q[discretized_obs][action]</pre>
<p class="calibre2">但是，在单独的一行上计算每一项会更容易阅读和理解。</p>


            

            
        
    



  
    <title>Full Q_Learner class implementation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">完整的Q_Learner类实现</h1>
                
            
            
                
<p class="calibre2">如果我们将所有的方法实现放在一起，我们将得到如下代码片段:</p>
<pre class="calibre17">EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                           self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error</pre>
<p class="calibre2">所以，我们已经准备好了代理。你可能会问，下一步我们该怎么办。嗯，我们应该在健身房环境中训练代理人！在下一节中，我们将了解培训程序。</p>


            

            
        
    



  
    <title>Training the reinforcement learning agent at the Gym</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">在健身房培训强化学习代理</h1>
                
            
            
                
<p class="calibre2">训练Q-learning agent的过程对您来说可能已经很熟悉了，因为它有许多与我们以前使用的样板代码相同的代码行，也有类似的结构。我们现在使用<kbd class="calibre12">agent.get_action(obs)</kbd>方法从代理那里获得动作，而不是从环境的动作空间中选择随机动作。我们还在将代理的动作发送到环境并收到反馈后调用<kbd class="calibre12">agent.learn(obs, action, reward, next_obs)</kbd>方法。此处列出了培训功能:</p>
<pre class="calibre17">def train(agent, env):<br class="title-page-name"/>    best_reward = -float('inf')<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        done = False<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>        if total_reward &gt; best_reward:<br class="title-page-name"/>            best_reward = total_reward<br class="title-page-name"/>        print("Episode#:{} reward:{} best_reward:{} eps:{}".format(episode,<br class="title-page-name"/>                                     total_reward, best_reward, agent.epsilon))<br class="title-page-name"/>    # Return the trained policy<br class="title-page-name"/>    return np.argmax(agent.Q, axis=2)</pre>


            

            
        
    



  
    <title>Testing and recording the performance of the agent</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">测试和记录代理的性能</h1>
                
            
            
                
<p class="calibre2">一旦我们让代理在健身房训练，我们希望能够衡量它学得有多好。为了做到这一点，我们让代理人通过一个测试。就像在学校一样！<kbd class="calibre12">test(agent, env, policy)</kbd>采用代理对象、环境实例和代理策略来测试环境中代理的性能，并返回一个完整剧集的总报酬。它类似于我们前面看到的<kbd class="calibre12">train(agent, env)</kbd>函数，但是它不允许代理学习或更新它的Q值估计:</p>
<pre class="calibre17">def test(agent, env, policy):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        action = policy[agent.discretize(obs)]<br class="title-page-name"/>        next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>        obs = next_obs<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>    return total_reward
1,000 episodes and save the recorded agent's action in the environment as video files in the <kbd class="calibre12">gym_monitor_path</kbd> directory:</pre>
<pre class="calibre17">if __name__ == "__main__":<br class="title-page-name"/>    env = gym.make('MountainCar-v0')<br class="title-page-name"/>    agent = Q_Learner(env)<br class="title-page-name"/>    learned_policy = train(agent, env)<br class="title-page-name"/>    # Use the Gym Monitor wrapper to evalaute the agent and record video<br class="title-page-name"/>    gym_monitor_path = "./gym_monitor_output"<br class="title-page-name"/>    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)<br class="title-page-name"/>    for _ in range(1000):<br class="title-page-name"/>        test(agent, env, learned_policy)<br class="title-page-name"/>    env.close()</pre>


            

            
        
    



  
    <title>A simple and complete Q-Learner implementation for solving the Mountain Car problem</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">一个简单完整的Q学习器实现来解决山地车问题</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将把整个代码放在一个Python脚本中，以初始化环境，启动代理的训练过程，获得训练好的策略，测试代理的性能，并记录它在环境中的行为！</p>
<pre class="calibre17">#!/usr/bin/env/ python<br class="title-page-name"/>import gym<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>MAX_NUM_EPISODES = 50000<br class="title-page-name"/>STEPS_PER_EPISODE = 200 #  This is specific to MountainCar. May change with env<br class="title-page-name"/>EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                           self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error<br class="title-page-name"/><br class="title-page-name"/>def train(agent, env):<br class="title-page-name"/>    best_reward = -float('inf')<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        done = False<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>        if total_reward &gt; best_reward:<br class="title-page-name"/>            best_reward = total_reward<br class="title-page-name"/>        print("Episode#:{} reward:{} best_reward:{} eps:{}".format(episode,<br class="title-page-name"/>                                     total_reward, best_reward, agent.epsilon))<br class="title-page-name"/>    # Return the trained policy<br class="title-page-name"/>    return np.argmax(agent.Q, axis=2)<br class="title-page-name"/><br class="title-page-name"/>def test(agent, env, policy):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        action = policy[agent.discretize(obs)]<br class="title-page-name"/>        next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>        obs = next_obs<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>    return total_reward<br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env = gym.make('MountainCar-v0')<br class="title-page-name"/>    agent = Q_Learner(env)<br class="title-page-name"/>    learned_policy = train(agent, env)<br class="title-page-name"/>    # Use the Gym Monitor wrapper to evalaute the agent and record video<br class="title-page-name"/>    gym_monitor_path = "./gym_monitor_output"<br class="title-page-name"/>    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)<br class="title-page-name"/>    for _ in range(1000):<br class="title-page-name"/>        test(agent, env, learned_policy)<br class="title-page-name"/>    env.close()</pre>
<p class="calibre2">这个脚本可以在名为<kbd class="calibre12">Q_learner_MountainCar.py</kbd>的<kbd class="calibre12">ch5</kbd>文件夹下的代码库中找到。</p>
<p class="calibre2">激活<kbd class="calibre12">rl_gym_book</kbd> conda环境并启动脚本来查看它的运行情况！当您启动该脚本时，您将看到类似于该屏幕截图所示的初始输出:</p>
<div><img src="img/00126.jpeg" class="calibre65"/></div>
<p class="calibre2">在最初的培训集中，当代理刚刚开始学习时，你会看到它总是以<em class="calibre13"> -200 </em>的奖励结束。从您对健身房的山地车环境如何工作的理解中，您可以看到代理没有在<em class="calibre13"> 200 </em>时间步长内到达山顶，因此环境会自动重置环境；这样，代理只能得到<em class="calibre13"> -200 </em>。还可以观察<strong class="calibre4"> <img class="fm-editor-equation22" src="img/00127.jpeg"/> </strong> ( <strong class="calibre4"> eps </strong>)探索值慢慢衰减。</p>
<p class="calibre2">如果你让代理人学习足够长的时间，你会看到代理人在进步，学习用越来越少的步骤到达山顶。以下是在典型的笔记本电脑硬件上进行<em class="calibre13"> 5 </em>分钟训练后的进度示例:</p>
<div><img src="img/00128.jpeg" class="calibre66"/></div>
<p class="calibre2">一旦脚本运行完成，您将在<kbd class="calibre12">gym_monitor_output</kbd>文件夹中看到代理人表演的录制视频(以及一些<kbd class="calibre12">.stats.json</kbd>和<kbd class="calibre12">.meta.json</kbd>文件)。你可以观看视频，看看你的经纪人表现如何！</p>
<p class="calibre2">下面是一个截图，显示了代理成功地将汽车驶向山顶:</p>
<div><img src="img/00129.jpeg" class="calibre67"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">万岁！</p>


            

            
        
    



  
    <title>Summary</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">摘要</h1>
                
            
            
                
<p class="calibre2">我们在这一章中学到了很多。更重要的是，我们实现了一个代理，它学会了在7分钟左右智能地解决山地车问题！</p>
<p class="calibre2">我们从了解著名的山地车问题开始，看看在健身房的<kbd class="calibre12">MountainCar-v0</kbd>环境中，环境、观察空间、状态空间和奖励是如何设计的。我们重温了在上一章中使用的强化学习健身房样板代码，并对其进行了一些改进，这些也可以在本书的代码库中找到。</p>
<p class="calibre2">然后，我们为Q学习代理定义了超参数，并开始从头开始实现Q学习算法。我们首先实现了代理的初始化函数，使用一个NumPy <em class="calibre13"> n </em>维数组来初始化代理的内部状态变量，包括Q值表示。然后，我们实现了<kbd class="calibre12">discretize</kbd>方法来离散化<kbd class="calibre12">state space</kbd>；基于ε-贪婪策略选择动作的<kbd class="calibre12">get_action(...)</kbd>方法；最后是<kbd class="calibre12">learn(...)</kbd>函数，它实现Q-learning更新规则并构成代理的核心。我们看到实现它们是多么简单！我们还实现了训练、测试和评估代理性能的功能。</p>
<p class="calibre2">我希望您在实现代理和观看它在健身房解决山地汽车问题的过程中获得了很多乐趣！我们将在下一章探讨解决各种更具挑战性问题的高级方法。</p>


            

            
        
    
</body></html>