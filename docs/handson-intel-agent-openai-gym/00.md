

# 零、前言

这本书将指导你实现你自己的智能体的过程，以解决离散值和连续值的顺序决策问题，所有的基本构件开发，调试，训练，可视化，定制，并在各种学习环境中测试你的智能体实现，从山地汽车和手推车杆问题到 Atari 游戏和 CARLA 一个先进的自动驾驶模拟器。



# 这本书是给谁的

如果你是一名学生，一名游戏/机器学习开发人员，或者一名人工智能爱好者，希望开始构建智能体和算法，使用带有 OpenAI Gym 界面的学习环境来解决各种问题，这本书就是为你准备的。如果你想学习如何构建基于深度强化学习的人工智能体来解决你感兴趣领域的问题，你也会发现这本书很有用。虽然这本书涵盖了你需要知道的所有基本概念，但是一些 Python 的工作知识将帮助你最大限度地利用它。



# 这本书涵盖的内容

第 1 章，*介绍智能体和学习环境*，它使得几个人工智能系统的开发成为可能。它揭示了工具包的重要特性，它为您提供了无限的机会来创建自治的智能体来解决一些算法任务、游戏和控制任务。到本章结束时，你将知道如何使用 Python 创建一个体育馆环境的实例。

第 2 章，*强化学习和深度强化学习*，提供了对强化学习中基本术语和概念的简明
解释。章节
会让你很好的理解
开发 AI 智能体的基本强化学习框架。这一章还将介绍深度强化学习，并且
向你展示算法能够让你
解决的高级问题的类型。

第 3 章，*开始使用 OpenAI Gym 和深度强化学习*，直接进入并让您的开发机器/计算机准备好使用学习环境和 PyTorch 开发深度学习算法所需的所有必要安装和配置。

第 4 章*探索Gym及其功能*，带您浏览Gym图书馆可用的学习环境清单，首先概述环境如何分类和命名，这将帮助您从 700 多个可用的学习环境中选择正确的环境版本和类型。然后你将学习探索体育馆，测试你想测试的任何环境，理解各种环境的界面和描述。

第五章，*实现你的第一个学习智能体——解决山地汽车问题*，解释了如何使用强化学习实现一个 AI 智能体来解决山地
汽车问题。您将实现智能体，训练它，并看到它自己的改进。
的实现细节将使你能够应用这些概念来开发和训练一个智能体
来解决各种其他任务和/或游戏。

第 6 章*使用深度 Q 学习*实现优化控制的智能体，涵盖了改善 Q 学习的各种方法，包括使用深度神经网络的动作值函数逼近、经验重放、目标网络，以及通常用于训练和测试深度强化学习智能体的必要实用程序和构建模块。您将实现一个基于 DQN 的智能体来采取最佳的离散控制行动，并训练它玩几个 Atari 游戏，观察智能体的表现。

第 7 章，*创建定制的 OpenAI Gym环境——卡拉驾驶模拟器*，将教你如何将现实世界的问题转化为与 OpenAI Gym接口兼容的学习环境。您将学习Gym环境的剖析，并基于 Carla 模拟器创建您的定制学习环境，该模拟器可在Gym注册，并用于我们开发的培训智能体。

第 8 章，*使用深度演员评论家算法*实现智能&自动汽车驾驶智能体，向您传授基于策略梯度的强化学习算法的基础知识，并帮助您直观地理解深度 n 步优势演员评论家算法。然后，您将学习实现一个超级智能体，它可以在 Carla 模拟器中使用深度 n 步优势演员评论家算法的同步和异步实现来自动驾驶汽车。

第 9 章，*探索学习环境——Roboschool、Gym-Retro、StarCraft-II、DeepMindLab* ,带您了解体育馆之外的其他完善的学习环境，您可以用它们来训练您的智能体。您将了解并学习使用各种机器人学校环境、GymRetro环境、非常受欢迎的星际争霸 2 环境和 DeepMind 实验室环境。

Chapter 9, *Exploring the Learning Environment Landscape – Roboschool, Gym-Retro, StarCraft-II, DeepMindLab*, takes you beyond the Gym and shows you around other well developed suite of learning environments that you can use to train your intelligent agents. You will understand and learn to use the various Roboschool environments, the Gym Retro environments, the very popular Star Craft II environment and the DeepMind Lab environments.

第 10 章，*探索学习算法的前景——DDPG(演员兼评论家)，PPO(策略梯度)，Rainbow(基于价值)*，提供了对最新深度强化学习算法的见解，并根据您在本书前几章中所学的知识，解开了这些算法的基础。您将快速了解三种不同类别的深度强化学习算法中最佳算法背后的核心概念，即:基于演员评论家的深度确定性策略梯度(DDPG)算法、基于策略梯度的近似策略优化(PPO)和基于值的彩虹算法。

从这本书中获得最大收益



# 将需要以下内容:

一些 Python 编程的工作知识，以便理解语法、模块导入和库安装

*   有一些使用 Linux 或 macOS X 命令行执行基本任务的经验，比如导航文件系统和运行 Python 脚本
*   下载示例代码文件



# 你可以从你在[www.packtpub.com](http://www.packtpub.com)的账户下载本书的示例代码文件。如果你在别处购买了这本书，你可以访问 www.packtpub.com/support 并注册，让文件直接通过电子邮件发送给你。

您可以按照以下步骤下载代码文件:

在[www.packtpub.com](http://www.packtpub.com/support)登录或注册。

1.  选择支持选项卡。
2.  点击代码下载和勘误表。
3.  在搜索框中输入图书名称，然后按照屏幕指示进行操作。
4.  下载文件后，请确保使用最新版本的解压缩或解压文件夹:

WinRAR/7-Zip for Windows

*   适用于 Mac 的 Zipeg/iZip/UnRarX
*   用于 Linux 的 7-Zip/PeaZip
*   7-Zip/PeaZip for Linux

该书的代码包也托管在 GitHub 上，网址为[https://GitHub . com/packt publishing/Hands-On-Intelligent-Agents-with-open ai-Gym](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym)。如果代码有更新，它将在现有的 GitHub 库中更新。

我们在 https://github.com/PacktPublishing/也有丰富的书籍和视频目录中的其他代码包。看看他们！

下载彩色图像



# 我们还提供了一个 PDF 文件，其中有本书中使用的截图/图表的彩色图像。可以在这里下载:[http://www . packtpub . com/sites/default/files/downloads/handsonintelligentgentswiopenaigym _ color images . pdf](http://www.packtpub.com/sites/default/files/downloads/HandsOnIntelligentAgentswithOpenAIGym_ColorImages.pdf)。

使用的惯例



# 本书通篇使用了许多文本约定。

`CodeInText`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。下面是一个例子:“将下载的`WebStorm-10*.dmg`磁盘镜像文件挂载为系统中的另一个磁盘。”

代码块设置如下:

当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:

```
#!/usr/bin/env python
import gym
env = gym.make("Qbert-v0")
MAX_NUM_EPISODES = 10
MAX_STEPS_PER_EPISODE = 500
```

任何命令行输入或输出都按如下方式编写:

```
for episode in range(MAX_NUM_EPISODES):
    obs = env.reset()
    for step in range(MAX_STEPS_PER_EPISODE):
        env.render()
```

Any command-line input or output is written as follows:

```
$ python get_observation_action_space.py 'MountainCar-v0'
```

**粗体**:表示新术语、重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词出现在文本中，如下所示。下面是一个例子:“从管理面板中选择系统信息。”

警告或重要提示如下所示。

提示和技巧是这样出现的。

取得联系



# 我们随时欢迎读者的反馈。

**总体反馈**:给`feedback@packtpub.com`发邮件，在邮件主题中提及书名。如果您对本书的任何方面有任何疑问，请通过`questions@packtpub.com`发邮件给我们。

**勘误表**:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问[www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的图书，点击勘误表提交表格链接，并输入详细信息。

**盗版**:如果您在互联网上遇到我们作品的任何形式的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过`copyright@packtpub.com`联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有一个你擅长的主题，并且你有兴趣写一本书或者为一本书投稿，请访问[authors.packtpub.com](http://authors.packtpub.com/)。

**If you are interested in becoming an author**: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit [authors.packtpub.com](http://authors.packtpub.com/).

复习



# 请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！

更多关于 Packt 的信息，请访问[packtpub.com](https://www.packtpub.com/)。

For more information about Packt, please visit [packtpub.com](https://www.packtpub.com/).